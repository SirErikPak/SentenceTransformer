{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22bc6fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import textwrap\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Local Paths\n",
    "# MODEL_PATH =\"/Users/sir/Downloads/HuggingFace/sentence_transformer/intfloat_e5-large-v2\"\n",
    "LLM_PATH = \"/Users/sir/Downloads/HuggingFace/LLM/meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# use mps if available, else cuda, else cpu\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14feb3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Generator: /Users/sir/Downloads/HuggingFace/LLM/meta-Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db558c8f9f584b1187e76a1c5ceacbc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- MODEL 1: THE \"GENERATOR\" (Llama 3.1 for summarizing) ---\n",
    "generator_model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "print(f\"Loading Generator: {LLM_PATH}\")\n",
    "\n",
    "# This line will now work correctly\n",
    "generator_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_PATH, \n",
    "    device_map=DEVICE, # Automatically map to your M3 GPU\n",
    "    dtype=torch.bfloat16, # Use bfloat16 for M3\n",
    "    trust_remote_code=True\n",
    ")\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(LLM_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f74d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODEL 1: THE \"GENERATOR\" (Llama 3.1 for summarizing) ---\n",
    "print(f\"Loading Local LLM: {LLM_PATH}\")\n",
    "\n",
    "# Load the model that can READ and WRITE\n",
    "# Using bfloat16 for better performance on M-series chips\n",
    "generator_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_PATH, \n",
    "    device_map=DEVICE, # Automatically map to your M3 GPU\n",
    "    dtype=torch.bfloat16, # Use bfloat16 for M3\n",
    "    trust_remote_code=True\n",
    ")\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(LLM_PATH)\n",
    "\n",
    "# --- MODEL 2: THE \"RETRIEVER\" (E5 for searching) ---\n",
    "print(f\"Loading Retriever: {LLM_PATH}\")\n",
    "\n",
    "# Load the model that can SEARCH\n",
    "retriever_model = SentenceTransformer(LLM_PATH, device=DEVICE)\n",
    "print(\"\\n--- Models Loaded Successfully ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8227890b",
   "metadata": {},
   "source": [
    "### Summarization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d2fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"We survey 146 papers analyzing “bias” in\n",
    "NLP systems, finding that their motivations\n",
    "are often vague, inconsistent, and lacking\n",
    "in normative reasoning, despite the fact that\n",
    "analyzing “bias” is an inherently normative\n",
    "process. We further find that these papers’\n",
    "proposed quantitative techniques for measuring\n",
    "or mitigating “bias” are poorly matched to\n",
    "their motivations and do not engage with the\n",
    "relevant literature outside of NLP. Based on\n",
    "these findings, we describe the beginnings of a\n",
    "path forward by proposing three recommendations\n",
    "that should guide work analyzing “bias”\n",
    "in NLP systems. These recommendations rest\n",
    "on a greater recognition of the relationships\n",
    "between language and social hierarchies,\n",
    "encouraging researchers and practitioners\n",
    "to articulate their conceptualizations of\n",
    "“bias”—i.e., what kinds of system behaviors\n",
    "are harmful, in what ways, to whom, and why,\n",
    "as well as the normative reasoning underlying\n",
    "these statements—and to center work around\n",
    "the lived experiences of members of communities\n",
    "affected by NLP systems, while interrogating\n",
    "and reimagining the power relations\n",
    "between technologists and such communities.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2845a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question\n",
    "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Clearly summarize the following text in one concise paragraph:\n",
    "\n",
    "{text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "input = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **input,\n",
    "        max_new_tokens=500,         # Number of tokens to generate\n",
    "        do_sample=True,             # Enable sampling for more natural output\n",
    "        temperature=0.01,            # Controls randomness\n",
    "        top_p=0.9,                  # Nucleus sampling\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2457e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# answer_only = response_text.split(\"### Response:\")[1].split(\"###\")[0].strip()\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84733ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284c4b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question\n",
    "prompt = \"### Instruction:\\nWhat is the capital of South Africa?\\n\\n### Response:\"\n",
    "# prompt = f\"{question}\\nAnswer:\"\n",
    "\n",
    "input = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **input,\n",
    "        max_new_tokens=500,         # Number of tokens to generate\n",
    "        do_sample=True,             # Enable sampling for more natural output\n",
    "        temperature=0.01,            # Controls randomness\n",
    "        top_p=0.9,                  # Nucleus sampling\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a922fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "answer_only = response_text.split(\"### Response:\")[1].split(\"###\")[0].strip()\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3314a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a4d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question\n",
    "prompt = \"### Instruction:\\nWhy did UPS plane with 3 crew members crashed near the Louisville airport?\\n\\n### Response:\"\n",
    "# prompt = f\"{question}\\nAnswer:\"\n",
    "\n",
    "input = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **input,\n",
    "        max_new_tokens=500,         # Number of tokens to generate\n",
    "        do_sample=True,             # Enable sampling for more natural output\n",
    "        temperature=0.01,            # Controls randomness\n",
    "        top_p=0.9,                  # Nucleus sampling\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b469660",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "answer_only = response_text.split(\"### Response:\")[1].split(\"###\")[0].strip()\n",
    "print(answer_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc11616",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060cf2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"A UPS MD-11 plane crashed shortly after take-off near the Louisville, Kentucky, airport, according to the Federal Aviation Administration. UPS Flight 2976 crashed \n",
    "just after 5 p.m. local time and was headed to Daniel K. Inouye International Airport in Honolulu, according to a statement from the FAA, which is investigating the crash \n",
    "along with the National Transportation Safety Board. The NTSB will lead the investigation, the FAA said Tuesday.\n",
    "Three crewmembers were on the plane, according to a statement from UPS that said in part, “At this time, we have not confirmed any injuries/casualties.”\n",
    "Louisville Metro Police Department and other agencies are responding to the crash, LMPD said in an X post. Injuries have been reported, police said.\n",
    "A massive plume of black smoke is rising not far from the tarmac at Louisville Muhammad Ali International Airport, videos from CNN affiliate WAVE show.\n",
    "Louisville Muhammad Ali International Airport is the worldwide air hub for UPS. The company’s Worldport is more than 5 million square feet where more \n",
    "than 12,000 UPS employees process more than two million packages a day, according to the company.\n",
    "A shelter-in-place has been issued for all locations within 5 miles of the airport, police added.\n",
    "“LMPD and multiple other agencies are responding to reports of a plan crash near Fern Valley and Grade Lane,” the post said. “Grade lane will be \n",
    "closed indefinitely between Stooges and Crittenden.” The McDonnell Douglas MD-11F is a freight transport aircraft manufactured originally by McDonnell \n",
    "Douglas and later by Boeing. The aircraft is primarily flown by FedEx Express, Lufthansa Cargo and UPS Airlines for cargo.\n",
    "The plane also served as a popular wide-bodied passenger airplane after it was first flown in 1990. The aircraft involved in Tuesday’s crash was built in 1991.\n",
    "As fuel costs increased for the three engine jets many of them were converted to freighters. The plane can take off weighing in at a maximum 633,000 pounds and \n",
    "carrying more than 38,000 gallons of fuel, according to Boeing, which bought McDonnell Douglass.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd073ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0684fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question\n",
    "prompt = f\"### Instruction:\\nClearly summarize the following text in one concise paragraph within 500 words:\\n{text}\\n### Response:\"\n",
    "input = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **input,\n",
    "        max_new_tokens=750,         # Number of tokens to generate\n",
    "        do_sample=True,             # Enable sampling for more natural output\n",
    "        temperature=0.06,            # Controls randomness\n",
    "        top_p=0.9,                  # Nucleus sampling\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f4985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "# 1. Decode the raw text\n",
    "raw_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 2. Wrap the text to 80 characters per line\n",
    "answer_only = raw_text.split(\"### Response:\")[1].split(\"###\")[0].strip()\n",
    "formatted_text = textwrap.fill(answer_only, width=80)\n",
    "\n",
    "# 3. Print the perfectly formatted output\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2777938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Decode the raw text\n",
    "raw_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 2. Wrap the text to 80 characters per line\n",
    "formatted_text = textwrap.fill(raw_text, width=80)\n",
    "\n",
    "# 3. Print the perfectly formatted output\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530ff86b",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6579ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fd1675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Your Original Text (This is what E5 would find) ---\n",
    "text = \"\"\"\n",
    "A UPS MD-11 plane crashed shortly after take-off near the Louisville, Kentucky, airport, according to the Federal Aviation Administration. UPS Flight 2976 crashed \n",
    "just after 5 p.m. local time and was headed to Daniel K. Inouye International Airport in Honolulu, according to a statement from the FAA, which is investigating the crash \n",
    "along with the National Transportation Safety Board. The NTSB will lead the investigation, the FAA said Tuesday.\n",
    "Three crewmembers were on the plane, according to a statement from UPS that said in part, “At this time, we have not confirmed any injuries/casualties.”\n",
    "Louisville Metro Police Department and other agencies are responding to the crash, LMPD said in an X post. Injuries have been reported, police said.\n",
    "A massive plume of black smoke is rising not far from the tarmac at Louisville Muhammad Ali International Airport, videos from CNN affiliate WAVE show.\n",
    "Louisville Muhammad Ali International Airport is the worldwide air hub for UPS. The company’s Worldport is more than 5 million square feet where more \n",
    "than 12,000 UPS employees process more than two million packages a day, according to the company.\n",
    "A shelter-in-place has been issued for all locations within 5 miles of the airport, police added.\n",
    "“LMPD and multiple other agencies are responding to reports of a plan crash near Fern Valley and Grade Lane,” the post said. “Grade lane will be \n",
    "closed indefinitely between Stooges and Crittenden.” The McDonnell Douglas MD-11F is a freight transport aircraft manufactured originally by McDonnell \n",
    "Douglas and later by Boeing. The aircraft is primarily flown by FedEx Express, Lufthansa Cargo and UPS Airlines for cargo.\n",
    "The plane also served as a popular wide-bodied passenger airplane after it was first flown in 1990. The aircraft involved in Tuesday’s crash was built in 1991.\n",
    "As fuel costs increased for the three engine jets many of them were converted to freighters. The plane can take off weighing in at a maximum 633,000 pounds and \n",
    "carrying more than 38,000 gallons of fuel, according to Boeing, which bought McDonnell Douglass.\n",
    "\"\"\"\n",
    "\n",
    "# --- STEP 1: USE THE \"RETRIEVER\" (E5 Model) ---\n",
    "# (Simulated) In a real RAG app, E5 would search a database and *find* \n",
    "# this text chunk.\n",
    "# E5's job is to make a vector:\n",
    "vector = retriever_model.encode([text])\n",
    "print(f\"\\nE5 Model (Retriever) created a vector of shape: {vector.shape}\")\n",
    "\n",
    "\n",
    "# --- STEP 2: USE THE \"GENERATOR\" (Llama-3.1-Instruct Model) ---\n",
    "# Now, we build a prompt and ask the GENERATOR to summarize the text\n",
    "# that the RETRIEVER found.\n",
    "\n",
    "# CRITICAL: Use the Llama 3.1 prompt format\n",
    "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Clearly summarize the following text in one concise paragraph:\n",
    "\n",
    "{text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the prompt for the GENERATOR\n",
    "# We must set pad_token_id to eos_token_id for Llama 3\n",
    "if generator_tokenizer.pad_token_id is None:\n",
    "    generator_tokenizer.pad_token_id = generator_tokenizer.eos_token_id\n",
    "\n",
    "input = generator_tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "# Generate text using the GENERATOR model\n",
    "print(\"\\nGenerating summary with Llama-3.1-Instruct...\")\n",
    "with torch.no_grad():\n",
    "    # We must also pass the eos_token_id to stop generation\n",
    "    outputs = generator_model.generate(\n",
    "        **input,\n",
    "        max_new_tokens=750,        \n",
    "        do_sample=True,\n",
    "        temperature=0.7,         # A good temperature for creative summary\n",
    "        top_p=0.9,\n",
    "        # Llama 3.1 uses <|eot_id|> as its end token\n",
    "        eos_token_id=generator_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# 1. Decode the raw text\n",
    "# We only want the generated part, not the input prompt\n",
    "output_token_ids = outputs[0][len(input['input_ids'][0]):]\n",
    "raw_output = generator_tokenizer.decode(output_token_ids, skip_special_tokens=True)\n",
    "\n",
    "# 2. Clean the output (it might have extra spaces)\n",
    "response_only = raw_output.strip()\n",
    "\n",
    "# 3. Wrap and print the final, correct summary\n",
    "formatted_text = textwrap.fill(response_only, width=80)\n",
    "print(\"\\n--- GENERATED SUMMARY ---\")\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71faa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The question you want to ask\n",
    "question = \"What is the capital of United States?\"\n",
    "\n",
    "# Build the prompt using the Llama 3.1 template\n",
    "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Tokenize the prompt for the GENERATOR\n",
    "# We must set pad_token_id to eos_token_id for Llama 3\n",
    "if generator_tokenizer.pad_token_id is None:\n",
    "    generator_tokenizer.pad_token_id = generator_tokenizer.eos_token_id\n",
    "\n",
    "input = generator_tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "# Generate text using the GENERATOR model\n",
    "print(\"\\nGenerating summary with Llama-3.1-Instruct...\")\n",
    "with torch.no_grad():\n",
    "    # We must also pass the eos_token_id to stop generation\n",
    "    outputs = generator_model.generate(\n",
    "        **input,\n",
    "        max_new_tokens=750,        \n",
    "        do_sample=True,\n",
    "        temperature=0.7,         # A good temperature for creative summary\n",
    "        top_p=0.9,\n",
    "        # Llama 3.1 uses <|eot_id|> as its end token\n",
    "        eos_token_id=generator_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# 1. Decode the raw text\n",
    "# We only want the generated part, not the input prompt\n",
    "output_token_ids = outputs[0][len(input['input_ids'][0]):]\n",
    "raw_output = generator_tokenizer.decode(output_token_ids, skip_special_tokens=True)\n",
    "\n",
    "# 2. Clean the output (it might have extra spaces)\n",
    "response_only = raw_output.strip()\n",
    "\n",
    "# 3. Wrap and print the final, correct summary\n",
    "formatted_text = textwrap.fill(response_only, width=80)\n",
    "print(\"\\n--- GENERATED SUMMARY ---\")\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81e59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c46324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b258a93",
   "metadata": {},
   "source": [
    "### Function to extract text from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337cf0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = extract_text_from_pdfs(PDF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772df732",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (pdf[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27074cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b687850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdfs(pdf_folder: str) -> list[dict]:\n",
    "    texts = []\n",
    "    for pdf_file in Path(pdf_folder).glob(\"*.pdf\"):\n",
    "        doc = fitz.open(pdf_file)\n",
    "        text = \"\\n\".join(page.get_text() for page in doc)\n",
    "        texts.append({\"source\": str(pdf_file), \"text\": text})\n",
    "    return texts\n",
    "\n",
    "\n",
    "def chunk_extracted_text(extracted_data):\n",
    "    \"\"\"Splits large texts into smaller, overlapping chunks.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    all_chunks = []\n",
    "    for item in extracted_data:\n",
    "        source_text = item[\"text\"]\n",
    "        chunks = text_splitter.create_documents([source_text])\n",
    "        \n",
    "        for chunk in chunks:\n",
    "             all_chunks.append({\n",
    "                 \"source\": item[\"source\"],\n",
    "                 \"text\": chunk.page_content\n",
    "             })\n",
    "             \n",
    "    print(f\"-> Successfully created {len(all_chunks)} text chunks.\")\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_positive_pairs(chunks: List[str]) -> List[InputExample]:\n",
    "    return [\n",
    "        InputExample(texts=[chunks[i], chunks[i+1]], label=1.0)\n",
    "        for i in range(len(chunks) - 1)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d8ecb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d56f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = extract_text_from_pdfs(PDF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1df98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_text = chunk_extracted_text(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f2acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunked_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e2d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_text[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37363a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = create_positive_pairs(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f1881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the output\n",
    "for pair in pairs:\n",
    "    print(f\"Texts: {pair.texts}, Label: {pair.label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a459b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22bc6fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF for PDF extraction\n",
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "# Imports for LLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# Imports for Text Chunking\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Imports for Contrastive Learning / Fine-Tuning\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "# typing imports\n",
    "from typing import List\n",
    "\n",
    "# Local Paths\n",
    "MODEL_PATH =\"/Users/sir/Downloads/HuggingFace/sentence_transformer/intfloat_e5-large-v2\"\n",
    "LLM_PATH = \"/Users/sir/Downloads/HuggingFace/LLM/meta-Llama-3.1-8B-Instruct\"\n",
    "MODEL_OUTPUT = \"/Users/sir/Desktop/Project/SentenceTransformer/FineTune/intfloat_e5-large-v2-FineTuned\"\n",
    "PDF_PATH = \"/Users/sir/Downloads/Data/PDF/test/\"\n",
    "TRAINING_METHOD = 'CONTIGUOUS_CHUNKS'\n",
    "\n",
    "# Fine-Tuning Hyperparameters\n",
    "BATCH_SIZE = 16 \n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_SEQ_LENGTH = 512\n",
    "\n",
    "# Chunking Parameters\n",
    "CHUNK_SIZE = 512\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7740f87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039eb3c92d4d4eb08e2d161e2e197475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Local tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(LLM_PATH).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49306d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer max length: 131072\n"
     ]
    }
   ],
   "source": [
    "# Check tokenizer's max length\n",
    "print(\"Tokenizer max length:\", tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2593a2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70412074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"What is the capital of France?\"\n",
    "prompt = \"### Instruction:\\nWhat is the capital of France?\\n\\n### Response:\"\n",
    "# prompt = f\"{question}\\nAnswer:\"\n",
    "\n",
    "input = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **input,\n",
    "        max_new_tokens=500,         # Number of tokens to generate\n",
    "        do_sample=True,             # Enable sampling for more natural output\n",
    "        temperature=0.01,            # Controls randomness\n",
    "        top_p=0.9,                  # Nucleus sampling\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade6be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "What is the capital of France?\n",
      "\n",
      "### Response: \n",
      "The capital of France is Paris. \n",
      "\n",
      "### Explanation:\n",
      "This is a simple question that requires a basic knowledge of geography. The correct answer is Paris, which is widely known as the capital of France. The question does not require any complex reasoning or analysis, but rather a recall of a basic fact. \n",
      "\n",
      "### Analysis:\n",
      "This question is a good example of a question that requires recall of a basic fact. It does not require any critical thinking or analysis, but rather a simple recall of a widely known piece of information. This type of question is often used in multiple-choice tests or quizzes to assess a person's knowledge of basic facts. \n",
      "\n",
      "### Implications:\n",
      "This question has implications for how we assess knowledge and understanding. It suggests that recall of basic facts is an important aspect of knowledge, and that it can be assessed through simple questions. It also highlights the importance of having a broad base of knowledge, including basic facts about geography, history, and other subjects. \n",
      "\n",
      "### Variations:\n",
      "This question can be varied in several ways, such as:\n",
      "\n",
      "* Asking about the capital of a different country\n",
      "* Asking about a different type of capital (e.g. cultural capital, economic capital)\n",
      "* Asking about a different type of fact (e.g. historical fact, scientific fact)\n",
      "* Asking the question in a different format (e.g. multiple-choice, true/false, short answer) \n",
      "\n",
      "### Example Use Cases:\n",
      "This question can be used in a variety of contexts, such as:\n",
      "\n",
      "* Multiple-choice tests or quizzes\n",
      "* Educational assessments or evaluations\n",
      "* Job interviews or aptitude tests\n",
      "* Language learning or cultural immersion programs \n",
      "\n",
      "### Code:\n",
      "This question can be represented in code as a simple string or variable, such as:\n",
      "```python\n",
      "question = \"What is the capital of France?\"\n",
      "answer = \"Paris\"\n",
      "``` \n",
      "This code can be used to generate the question and store the answer, and can be modified to include additional features such as multiple-choice options or scoring. \n",
      "\n",
      "### Discussion:\n",
      "This question is a good example of a question that requires recall of a basic fact. It does not require any critical thinking or analysis, but rather a simple recall of a widely known piece of information. This type of question is often used in multiple-choice tests or quizzes to assess a person's knowledge of basic facts. \n",
      "\n",
      "### Conclusion:\n",
      "In conclusion, this question is a good example of a question that requires recall of a basic fact. It highlights the importance of having a broad base of knowledge,\n"
     ]
    }
   ],
   "source": [
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f543d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "answer_only = response_text.split(\"### Response:\")[1].split(\"###\")[0].strip()\n",
    "print(answer_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a07db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use prompt to ask question\n",
    "prompt = \"### Instruction:\\nWhat is the capital of France?\\n\\n### Response:\"\n",
    "\n",
    "input = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **input,\n",
    "        max_new_tokens=500,         # Number of tokens to generate\n",
    "        do_sample=True,             # Enable sampling for more natural output\n",
    "        temperature=0.01,            # Controls randomness\n",
    "        top_p=0.9,                  # Nucleus sampling\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d2fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"We survey 146 papers analyzing “bias” in\n",
    "NLP systems, finding that their motivations\n",
    "are often vague, inconsistent, and lacking\n",
    "in normative reasoning, despite the fact that\n",
    "analyzing “bias” is an inherently normative\n",
    "process. We further find that these papers’\n",
    "proposed quantitative techniques for measuring\n",
    "or mitigating “bias” are poorly matched to\n",
    "their motivations and do not engage with the\n",
    "relevant literature outside of NLP. Based on\n",
    "these findings, we describe the beginnings of a\n",
    "path forward by proposing three recommendations\n",
    "that should guide work analyzing “bias”\n",
    "in NLP systems. These recommendations rest\n",
    "on a greater recognition of the relationships\n",
    "between language and social hierarchies,\n",
    "encouraging researchers and practitioners\n",
    "to articulate their conceptualizations of\n",
    "“bias”—i.e., what kinds of system behaviors\n",
    "are harmful, in what ways, to whom, and why,\n",
    "as well as the normative reasoning underlying\n",
    "these statements—and to center work around\n",
    "the lived experiences of members of communities\n",
    "affected by NLP systems, while interrogating\n",
    "and reimagining the power relations\n",
    "between technologists and such communities.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2845a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"What is the capital of France?\"\n",
    "prompt = \"### Instruction:\\nSummarize the following text:\\n\\n### Response:\"\n",
    "# prompt = f\"{question}\\nAnswer:\"\n",
    "\n",
    "input = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **input,\n",
    "        max_new_tokens=500,         # Number of tokens to generate\n",
    "        do_sample=True,             # Enable sampling for more natural output\n",
    "        temperature=0.01,            # Controls randomness\n",
    "        top_p=0.9,                  # Nucleus sampling\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b258a93",
   "metadata": {},
   "source": [
    "### Function to extract text from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "337cf0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = extract_text_from_pdfs(PDF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "772df732",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (pdf[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27074cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language (Technology) is Power: A Critical Survey of “Bias” in NLP\n",
      "Su Lin Blodgett\n",
      "College of Information and Computer Sciences\n",
      "University of Massachusetts Amherst\n",
      "blodgett@cs.umass.edu\n",
      "Solon Barocas\n",
      "Microsoft Research\n",
      "Cornell University\n",
      "solon@microsoft.com\n",
      "Hal Daumé III\n",
      "Microsoft Research\n",
      "University of Maryland\n",
      "me@hal3.name\n",
      "Hanna Wallach\n",
      "Microsoft Research\n",
      "wallach@microsoft.com\n",
      "Abstract\n",
      "We survey 146 papers analyzing “bias” in\n",
      "NLP systems, ﬁnding that their motivations\n",
      "are often vague, inconsistent, and lacking\n",
      "in normative reasoning, despite the fact that\n",
      "analyzing “bias” is an inherently normative\n",
      "process.\n",
      "We further ﬁnd that these papers’\n",
      "proposed quantitative techniques for measur-\n",
      "ing or mitigating “bias” are poorly matched to\n",
      "their motivations and do not engage with the\n",
      "relevant literature outside of NLP. Based on\n",
      "these ﬁndings, we describe the beginnings of a\n",
      "path forward by proposing three recommenda-\n",
      "tions that should guide work analyzing “bias”\n",
      "in NLP systems. These recommendations rest\n",
      "on a greater recognition of the relationships\n",
      "between\n",
      "language\n",
      "and\n",
      "social\n",
      "hierarchies,\n",
      "encouraging\n",
      "researchers\n",
      "and\n",
      "practitioners\n",
      "to\n",
      "articulate\n",
      "their\n",
      "conceptualizations\n",
      "of\n",
      "“bias”—i.e., what kinds of system behaviors\n",
      "are harmful, in what ways, to whom, and why,\n",
      "as well as the normative reasoning underlying\n",
      "these statements—and to center work around\n",
      "the lived experiences of members of commu-\n",
      "nities affected by NLP systems, while inter-\n",
      "rogating and reimagining the power relations\n",
      "between technologists and such communities.\n",
      "1\n",
      "Introduction\n",
      "A large body of work analyzing “bias” in natural\n",
      "language processing (NLP) systems has emerged\n",
      "in recent years, including work on “bias” in embed-\n",
      "ding spaces (e.g., Bolukbasi et al., 2016a; Caliskan\n",
      "et al., 2017; Gonen and Goldberg, 2019; May\n",
      "et al., 2019) as well as work on “bias” in systems\n",
      "developed for a breadth of tasks including language\n",
      "modeling (Lu et al., 2018; Bordia and Bowman,\n",
      "2019), coreference resolution (Rudinger et al.,\n",
      "2018; Zhao et al., 2018a), machine translation (Van-\n",
      "massenhove et al., 2018; Stanovsky et al., 2019),\n",
      "sentiment analysis (Kiritchenko and Mohammad,\n",
      "2018), and hate speech/toxicity detection (e.g.,\n",
      "Park et al., 2018; Dixon et al., 2018), among others.\n",
      "Although these papers have laid vital ground-\n",
      "work by illustrating some of the ways that NLP\n",
      "systems can be harmful, the majority of them fail\n",
      "to engage critically with what constitutes “bias”\n",
      "in the ﬁrst place. Despite the fact that analyzing\n",
      "“bias” is an inherently normative process—in\n",
      "which some system behaviors are deemed good\n",
      "and others harmful—papers on “bias” in NLP\n",
      "systems are rife with unstated assumptions about\n",
      "what kinds of system behaviors are harmful, in\n",
      "what ways, to whom, and why. Indeed, the term\n",
      "“bias” (or “gender bias” or “racial bias”) is used\n",
      "to describe a wide range of system behaviors, even\n",
      "though they may be harmful in different ways, to\n",
      "different groups, or for different reasons. Even\n",
      "papers analyzing “bias” in NLP systems developed\n",
      "for the same task often conceptualize it differently.\n",
      "For example, the following system behaviors\n",
      "are all understood to be self-evident statements of\n",
      "“racial bias”: (a) embedding spaces in which embed-\n",
      "dings for names associated with African Americans\n",
      "are closer (compared to names associated with\n",
      "European Americans) to unpleasant words than\n",
      "pleasant words (Caliskan et al., 2017); (b) senti-\n",
      "ment analysis systems yielding different intensity\n",
      "scores for sentences containing names associated\n",
      "with African Americans and sentences containing\n",
      "names associated with European Americans (Kir-\n",
      "itchenko and Mohammad, 2018); and (c) toxicity\n",
      "arXiv:2005.14050v2  [cs.CL]  29 May 2020\n",
      "\n",
      "detection systems scoring tweets containing fea-\n",
      "tures associated with African-American English as\n",
      "more offensive than tweets without these features\n",
      "(Davidson et al., 2019; Sap et al., 2019). Moreover,\n",
      "some of these papers focus on “racial bias”\n",
      "expressed in written text, while others focus on\n",
      "“racial bias” against authors. This use of imprecise\n",
      "terminology obscures these important differences.\n",
      "We survey 146 papers analyzing “bias” in NLP\n",
      "systems, ﬁnding that their motivations are often\n",
      "vague and inconsistent. Many lack any normative\n",
      "reasoning for why the system behaviors that are\n",
      "described as “bias” are harmful, in what ways, and\n",
      "to whom. Moreover, the vast majority of these\n",
      "papers do not engage with the relevant literature\n",
      "outside of NLP to ground normative concerns when\n",
      "proposing quantitative techniques for measuring\n",
      "or mitigating “bias.” As a result, we ﬁnd that many\n",
      "of these techniques are poorly matched to their\n",
      "motivations, and are not comparable to one another.\n",
      "We then describe the beginnings of a path\n",
      "forward by proposing three recommendations\n",
      "that should guide work analyzing “bias” in NLP\n",
      "systems. We argue that such work should examine\n",
      "the relationships between language and social hi-\n",
      "erarchies; we call on researchers and practitioners\n",
      "conducting such work to articulate their conceptu-\n",
      "alizations of “bias” in order to enable conversations\n",
      "about what kinds of system behaviors are harmful,\n",
      "in what ways, to whom, and why; and we recom-\n",
      "mend deeper engagements between technologists\n",
      "and communities affected by NLP systems. We\n",
      "also provide several concrete research questions\n",
      "that are implied by each of our recommendations.\n",
      "2\n",
      "Method\n",
      "Our survey includes all papers known to us\n",
      "analyzing “bias” in NLP systems—146 papers in\n",
      "total. We omitted papers about speech, restricting\n",
      "our survey to papers about written text only. To\n",
      "identify the 146 papers, we ﬁrst searched the ACL\n",
      "Anthology1 for all papers with the keywords “bias”\n",
      "or “fairness” that were made available prior to May\n",
      "2020. We retained all papers about social “bias,”\n",
      "and discarded all papers about other deﬁnitions of\n",
      "the keywords (e.g., hypothesis-only bias, inductive\n",
      "bias, media bias). We also discarded all papers us-\n",
      "ing “bias” in NLP systems to measure social “bias”\n",
      "in text or the real world (e.g., Garg et al., 2018).\n",
      "To ensure that we did not exclude any relevant\n",
      "1https://www.aclweb.org/anthology/\n",
      "NLP task\n",
      "Papers\n",
      "Embeddings (type-level or contextualized)\n",
      "54\n",
      "Coreference resolution\n",
      "20\n",
      "Language modeling or dialogue generation\n",
      "17\n",
      "Hate-speech detection\n",
      "17\n",
      "Sentiment analysis\n",
      "15\n",
      "Machine translation\n",
      "8\n",
      "Tagging or parsing\n",
      "5\n",
      "Surveys, frameworks, and meta-analyses\n",
      "20\n",
      "Other\n",
      "22\n",
      "Table 1: The NLP tasks covered by the 146 papers.\n",
      "papers without the keywords “bias” or “fairness,”\n",
      "we also traversed the citation graph of our initial\n",
      "set of papers, retaining any papers analyzing “bias”\n",
      "in NLP systems that are cited by or cite the papers\n",
      "in our initial set. Finally, we manually inspected\n",
      "any papers analyzing “bias” in NLP systems from\n",
      "leading machine learning, human–computer inter-\n",
      "action, and web conferences and workshops, such\n",
      "as ICML, NeurIPS, AIES, FAccT, CHI, and WWW,\n",
      "along with any relevant papers that were made\n",
      "available in the “Computation and Language” and\n",
      "“Computers and Society” categories on arXiv prior\n",
      "to May 2020, but found that they had already been\n",
      "identiﬁed via our traversal of the citation graph. We\n",
      "provide a list of all 146 papers in the appendix. In\n",
      "Table 1, we provide a breakdown of the NLP tasks\n",
      "covered by the papers. We note that counts do not\n",
      "sum to 146, because some papers cover multiple\n",
      "tasks. For example, a paper might test the efﬁcacy\n",
      "of a technique for mitigating “bias” in embed-\n",
      "ding spaces in the context of sentiment analysis.\n",
      "Once identiﬁed, we then read each of the 146 pa-\n",
      "pers with the goal of categorizing their motivations\n",
      "and their proposed quantitative techniques for mea-\n",
      "suring or mitigating “bias.” We used a previously\n",
      "developed taxonomy of harms for this categoriza-\n",
      "tion, which differentiates between so-called alloca-\n",
      "tional and representational harms (Barocas et al.,\n",
      "2017; Crawford, 2017). Allocational harms arise\n",
      "when an automated system allocates resources (e.g.,\n",
      "credit) or opportunities (e.g., jobs) unfairly to dif-\n",
      "ferent social groups; representational harms arise\n",
      "when a system (e.g., a search engine) represents\n",
      "some social groups in a less favorable light than\n",
      "others, demeans them, or fails to recognize their\n",
      "existence altogether. Adapting and extending this\n",
      "taxonomy, we categorized the 146 papers’ motiva-\n",
      "tions and techniques into the following categories:\n",
      "▷Allocational harms.\n",
      "\n",
      "Papers\n",
      "Category\n",
      "Motivation\n",
      "Technique\n",
      "Allocational harms\n",
      "30\n",
      "4\n",
      "Stereotyping\n",
      "50\n",
      "58\n",
      "Other representational harms\n",
      "52\n",
      "43\n",
      "Questionable correlations\n",
      "47\n",
      "42\n",
      "Vague/unstated\n",
      "23\n",
      "0\n",
      "Surveys, frameworks, and\n",
      "meta-analyses\n",
      "20\n",
      "20\n",
      "Table 2: The categories into which the 146 papers fall.\n",
      "▷Representational harms:2\n",
      "▷Stereotyping that propagates negative gen-\n",
      "eralizations about particular social groups.\n",
      "▷Differences in system performance for dif-\n",
      "ferent social groups, language that misrep-\n",
      "resents the distribution of different social\n",
      "groups in the population, or language that\n",
      "is denigrating to particular social groups.\n",
      "▷Questionable correlations between system be-\n",
      "havior and features of language that are typi-\n",
      "cally associated with particular social groups.\n",
      "▷Vague descriptions of “bias” (or “gender\n",
      "bias” or “racial bias”) or no description at all.\n",
      "▷Surveys, frameworks, and meta-analyses.\n",
      "In Table 2 we provide counts for each of the\n",
      "six categories listed above. (We also provide a\n",
      "list of the papers that fall into each category in the\n",
      "appendix.) Again, we note that the counts do not\n",
      "sum to 146, because some papers state multiple\n",
      "motivations, propose multiple techniques, or pro-\n",
      "pose a single technique for measuring or mitigating\n",
      "multiple harms. Table 3, which is in the appendix,\n",
      "contains examples of the papers’ motivations and\n",
      "techniques across a range of different NLP tasks.\n",
      "3\n",
      "Findings\n",
      "Categorizing the 146 papers’ motivations and pro-\n",
      "posed quantitative techniques for measuring or miti-\n",
      "gating “bias” into the six categories listed above en-\n",
      "abled us to identify several commonalities, which\n",
      "we present below, along with illustrative quotes.\n",
      "2We grouped several types of representational harms into\n",
      "two categories to reﬂect that the main point of differentiation\n",
      "between the 146 papers’ motivations and proposed quantitative\n",
      "techniques for measuring or mitigating “bias” is whether or not\n",
      "they focus on stereotyping. Among the papers that do not fo-\n",
      "cus on stereotyping, we found that most lack sufﬁciently clear\n",
      "motivations and techniques to reliably categorize them further.\n",
      "3.1\n",
      "Motivations\n",
      "Papers state a wide range of motivations,\n",
      "multiple motivations, vague motivations, and\n",
      "sometimes no motivations at all.\n",
      "We found that\n",
      "the papers’ motivations span all six categories, with\n",
      "several papers falling into each one. Appropriately,\n",
      "papers that provide surveys or frameworks for an-\n",
      "alyzing “bias” in NLP systems often state multiple\n",
      "motivations (e.g., Hovy and Spruit, 2016; Bender,\n",
      "2019; Sun et al., 2019; Rozado, 2020; Shah et al.,\n",
      "2020). However, as the examples in Table 3 (in the\n",
      "appendix) illustrate, many other papers (33%) do\n",
      "so as well. Some papers (16%) state only vague\n",
      "motivations or no motivations at all. For example,\n",
      "“[N]o human should be discriminated on the basis\n",
      "of demographic attributes by an NLP system.”\n",
      "—Kaneko and Bollegala (2019)\n",
      "“[P]rominent word embeddings [...] encode\n",
      "systematic biases against women and black people\n",
      "[...] implicating many NLP systems in scaling up\n",
      "social injustice.”\n",
      "—May et al. (2019)\n",
      "These examples leave unstated what it might mean\n",
      "for an NLP system to “discriminate,” what con-\n",
      "stitutes “systematic biases,” or how NLP systems\n",
      "contribute to “social injustice” (itself undeﬁned).\n",
      "Papers’ motivations sometimes include no nor-\n",
      "mative reasoning.\n",
      "We found that some papers\n",
      "(32%) are not motivated by any apparent normative\n",
      "concerns, often focusing instead on concerns about\n",
      "system performance. For example, the ﬁrst quote\n",
      "below includes normative reasoning—namely that\n",
      "models should not use demographic information\n",
      "to make predictions—while the other focuses on\n",
      "learned correlations impairing system performance.\n",
      "“In [text classiﬁcation], models are expected to\n",
      "make predictions with the semantic information\n",
      "rather than with the demographic group identity\n",
      "information (e.g., ‘gay’, ‘black’) contained in the\n",
      "sentences.”\n",
      "—Zhang et al. (2020a)\n",
      "“An over-prevalence of some gendered forms in the\n",
      "training data leads to translations with identiﬁable\n",
      "errors. Translations are better for sentences\n",
      "involving men and for sentences containing\n",
      "stereotypical gender roles.”\n",
      "—Saunders and Byrne (2020)\n",
      "Even when papers do state clear motivations,\n",
      "they are often unclear about why the system be-\n",
      "haviors that are described as “bias” are harm-\n",
      "ful, in what ways, and to whom.\n",
      "We found that\n",
      "even papers with clear motivations often fail to ex-\n",
      "plain what kinds of system behaviors are harmful,\n",
      "in what ways, to whom, and why. For example,\n",
      "\n",
      "“Deploying these word embedding algorithms in\n",
      "practice, for example in automated translation\n",
      "systems or as hiring aids, runs the serious risk of\n",
      "perpetuating problematic biases in important\n",
      "societal contexts.”\n",
      "—Brunet et al. (2019)\n",
      "“[I]f the systems show discriminatory behaviors in\n",
      "the interactions, the user experience will be\n",
      "adversely affected.”\n",
      "—Liu et al. (2019)\n",
      "These examples leave unstated what “problematic\n",
      "biases” or non-ideal user experiences might look\n",
      "like, how the system behaviors might result in\n",
      "these things, and who the relevant stakeholders\n",
      "or users might be. In contrast, we ﬁnd that papers\n",
      "that provide surveys or frameworks for analyzing\n",
      "“bias” in NLP systems often name who is harmed,\n",
      "acknowledging that different social groups may\n",
      "experience these systems differently due to their\n",
      "different relationships with NLP systems or\n",
      "different social positions. For example, Ruane\n",
      "et al. (2019) argue for a “deep understanding of\n",
      "the user groups [sic] characteristics, contexts, and\n",
      "interests” when designing conversational agents.\n",
      "Papers about NLP systems developed for the\n",
      "same task often conceptualize “bias” differ-\n",
      "ently.\n",
      "Even papers that cover the same NLP task\n",
      "often conceptualize “bias” in ways that differ sub-\n",
      "stantially and are sometimes inconsistent. Rows 3\n",
      "and 4 of Table 3 (in the appendix) contain machine\n",
      "translation papers with different conceptualizations\n",
      "of “bias,” leading to different proposed techniques,\n",
      "while rows 5 and 6 contain papers on “bias” in em-\n",
      "bedding spaces that state different motivations, but\n",
      "propose techniques for quantifying stereotyping.\n",
      "Papers’ motivations conﬂate allocational and\n",
      "representational harms.\n",
      "We found that the pa-\n",
      "pers’ motivations sometimes (16%) name imme-\n",
      "diate representational harms, such as stereotyping,\n",
      "alongside more distant allocational harms, which,\n",
      "in the case of stereotyping, are usually imagined as\n",
      "downstream effects of stereotypes on résumé ﬁlter-\n",
      "ing. Many of these papers use the imagined down-\n",
      "stream effects to justify focusing on particular sys-\n",
      "tem behaviors, even when the downstream effects\n",
      "are not measured. Papers on “bias” in embedding\n",
      "spaces are especially likely to do this because em-\n",
      "beddings are often used as input to other systems:\n",
      "“However, none of these papers [on embeddings]\n",
      "have recognized how blatantly sexist the\n",
      "embeddings are and hence risk introducing biases\n",
      "of various types into real-world systems.”\n",
      "—Bolukbasi et al. (2016a)\n",
      "“It is essential to quantify and mitigate gender bias\n",
      "in these embeddings to avoid them from affecting\n",
      "downstream applications.”\n",
      "—Zhou et al. (2019)\n",
      "In contrast, papers that provide surveys or frame-\n",
      "works for analyzing “bias” in NLP systems treat\n",
      "representational harms as harmful in their own\n",
      "right. For example, Mayﬁeld et al. (2019) and\n",
      "Ruane et al. (2019) cite the harmful reproduction\n",
      "of dominant linguistic norms by NLP systems (a\n",
      "point to which we return in section 4), while Bender\n",
      "(2019) outlines a range of harms, including seeing\n",
      "stereotypes in search results and being made invis-\n",
      "ible to search engines due to language practices.\n",
      "3.2\n",
      "Techniques\n",
      "Papers’ techniques are not well grounded in the\n",
      "relevant literature outside of NLP.\n",
      "Perhaps un-\n",
      "surprisingly given that the papers’ motivations are\n",
      "often vague, inconsistent, and lacking in normative\n",
      "reasoning, we also found that the papers’ proposed\n",
      "quantitative techniques for measuring or mitigating\n",
      "“bias” do not effectively engage with the relevant\n",
      "literature outside of NLP. Papers on stereotyping\n",
      "are a notable exception: the Word Embedding\n",
      "Association Test (Caliskan et al., 2017) draws on\n",
      "the Implicit Association Test (Greenwald et al.,\n",
      "1998) from the social psychology literature, while\n",
      "several techniques operationalize the well-studied\n",
      "“Angry Black Woman” stereotype (Kiritchenko\n",
      "and Mohammad, 2018; May et al., 2019; Tan\n",
      "and Celis, 2019) and the “double bind” faced by\n",
      "women (May et al., 2019; Tan and Celis, 2019), in\n",
      "which women who succeed at stereotypically male\n",
      "tasks are perceived to be less likable than similarly\n",
      "successful men (Heilman et al., 2004). Tan and\n",
      "Celis (2019) also examine the compounding effects\n",
      "of race and gender, drawing on Black feminist\n",
      "scholarship on intersectionality (Crenshaw, 1989).\n",
      "Papers’ techniques are poorly matched to their\n",
      "motivations.\n",
      "We found that although 21% of the\n",
      "papers include allocational harms in their motiva-\n",
      "tions, only four papers actually propose techniques\n",
      "for measuring or mitigating allocational harms.\n",
      "Papers focus on a narrow range of potential\n",
      "sources of “bias.”\n",
      "We found that nearly all of the\n",
      "papers focus on system predictions as the potential\n",
      "sources of “bias,” with many additionally focusing\n",
      "on “bias” in datasets (e.g., differences in the\n",
      "number of gendered pronouns in the training data\n",
      "(Zhao et al., 2019)). Most papers do not interrogate\n",
      "\n",
      "the normative implications of other decisions made\n",
      "during the development and deployment lifecycle—\n",
      "perhaps unsurprising given that their motivations\n",
      "sometimes include no normative reasoning.\n",
      "A\n",
      "few papers are exceptions, illustrating the impacts\n",
      "of task deﬁnitions, annotation guidelines, and\n",
      "evaluation metrics: Cao and Daumé (2019) study\n",
      "how folk conceptions of gender (Keyes, 2018) are\n",
      "reproduced in coreference resolution systems that\n",
      "assume a strict gender dichotomy, thereby main-\n",
      "taining cisnormativity; Sap et al. (2019) focus on\n",
      "the effect of priming annotators with information\n",
      "about possible dialectal differences when asking\n",
      "them to apply toxicity labels to sample tweets, ﬁnd-\n",
      "ing that annotators who are primed are signiﬁcantly\n",
      "less likely to label tweets containing features asso-\n",
      "ciated with African-American English as offensive.\n",
      "4\n",
      "A path forward\n",
      "We now describe how researchers and practitioners\n",
      "conducting work analyzing “bias” in NLP systems\n",
      "might avoid the pitfalls presented in the previous\n",
      "section—the beginnings of a path forward. We\n",
      "propose three recommendations that should guide\n",
      "such work, and, for each, provide several concrete\n",
      "research questions. We emphasize that these ques-\n",
      "tions are not comprehensive, and are intended to\n",
      "generate further questions and lines of engagement.\n",
      "Our three recommendations are as follows:\n",
      "(R1) Ground work analyzing “bias” in NLP sys-\n",
      "tems in the relevant literature outside of NLP\n",
      "that explores the relationships between lan-\n",
      "guage and social hierarchies. Treat represen-\n",
      "tational harms as harmful in their own right.\n",
      "(R2) Provide explicit statements of why the\n",
      "system behaviors that are described as “bias”\n",
      "are harmful, in what ways, and to whom.\n",
      "Be forthright about the normative reasoning\n",
      "(Green, 2019) underlying these statements.\n",
      "(R3) Examine language use in practice by engag-\n",
      "ing with the lived experiences of members of\n",
      "communities affected by NLP systems. Inter-\n",
      "rogate and reimagine the power relations be-\n",
      "tween technologists and such communities.\n",
      "4.1\n",
      "Language and social hierarchies\n",
      "Turning ﬁrst to (R1), we argue that work analyzing\n",
      "“bias” in NLP systems will paint a much fuller pic-\n",
      "ture if it engages with the relevant literature outside\n",
      "of NLP that explores the relationships between\n",
      "language and social hierarchies. Many disciplines,\n",
      "including sociolinguistics, linguistic anthropology,\n",
      "sociology, and social psychology, study how\n",
      "language takes on social meaning and the role that\n",
      "language plays in maintaining social hierarchies.\n",
      "For example, language is the means through which\n",
      "social groups are labeled and one way that beliefs\n",
      "about social groups are transmitted (e.g., Maass,\n",
      "1999; Beukeboom and Burgers, 2019).\n",
      "Group\n",
      "labels can serve as the basis of stereotypes and thus\n",
      "reinforce social inequalities: “[T]he label content\n",
      "functions to identify a given category of people,\n",
      "and thereby conveys category boundaries and a\n",
      "position in a hierarchical taxonomy” (Beukeboom\n",
      "and Burgers, 2019).\n",
      "Similarly, “controlling\n",
      "images,” such as stereotypes of Black women,\n",
      "which are linguistically and visually transmitted\n",
      "through literature, news media, television, and so\n",
      "forth, provide “ideological justiﬁcation” for their\n",
      "continued oppression (Collins, 2000, Chapter 4).\n",
      "As a result, many groups have sought to bring\n",
      "about social changes through changes in language,\n",
      "disrupting patterns of oppression and marginal-\n",
      "ization via so-called “gender-fair” language\n",
      "(Sczesny et al., 2016; Menegatti and Rubini, 2017),\n",
      "language that is more inclusive to people with\n",
      "disabilities (ADA, 2018), and language that is less\n",
      "dehumanizing (e.g., abandoning the use of the term\n",
      "“illegal” in everyday discourse on immigration in\n",
      "the U.S. (Rosa, 2019)). The fact that group labels\n",
      "are so contested is evidence of how deeply inter-\n",
      "twined language and social hierarchies are. Taking\n",
      "“gender-fair” language as an example, the hope\n",
      "is that reducing asymmetries in language about\n",
      "women and men will reduce asymmetries in their\n",
      "social standing. Meanwhile, struggles over lan-\n",
      "guage use often arise from dominant social groups’\n",
      "desire to “control both material and symbolic\n",
      "resources”—i.e., “the right to decide what words\n",
      "will mean and to control those meanings”—as was\n",
      "the case in some white speakers’ insistence on\n",
      "using offensive place names against the objections\n",
      "of Indigenous speakers (Hill, 2008, Chapter 3).\n",
      "Sociolinguists and linguistic anthropologists\n",
      "have also examined language attitudes and lan-\n",
      "guage ideologies, or people’s metalinguistic beliefs\n",
      "about language: Which language varieties or prac-\n",
      "tices are taken as standard, ordinary, or unmarked?\n",
      "Which are considered correct, prestigious, or ap-\n",
      "propriate for public use, and which are considered\n",
      "incorrect, uneducated, or offensive (e.g., Campbell-\n",
      "\n",
      "Kibler, 2009; Preston, 2009; Loudermilk, 2015;\n",
      "Lanehart and Malik, 2018)? Which are rendered in-\n",
      "visible (Roche, 2019)?3 Language ideologies play\n",
      "a vital role in reinforcing and justifying social hi-\n",
      "erarchies because beliefs about language varieties\n",
      "or practices often translate into beliefs about their\n",
      "speakers (e.g. Alim et al., 2016; Rosa and Flores,\n",
      "2017; Craft et al., 2020). For example, in the U.S.,\n",
      "the portrayal of non-white speakers’ language\n",
      "varieties and practices as linguistically deﬁcient\n",
      "helped to justify violent European colonialism, and\n",
      "today continues to justify enduring racial hierar-\n",
      "chies by maintaining views of non-white speakers\n",
      "as lacking the language “required for complex\n",
      "thinking processes and successful engagement\n",
      "in the global economy” (Rosa and Flores, 2017).\n",
      "Recognizing the role that language plays in\n",
      "maintaining social hierarchies is critical to the\n",
      "future of work analyzing “bias” in NLP systems.\n",
      "First, it helps to explain why representational\n",
      "harms are harmful in their own right. Second, the\n",
      "complexity of the relationships between language\n",
      "and social hierarchies illustrates why studying\n",
      "“bias” in NLP systems is so challenging, suggesting\n",
      "that researchers and practitioners will need to move\n",
      "beyond existing algorithmic fairness techniques.\n",
      "We argue that work must be grounded in the\n",
      "relevant literature outside of NLP that examines\n",
      "the relationships between language and social\n",
      "hierarchies; without this grounding, researchers\n",
      "and practitioners risk measuring or mitigating\n",
      "only what is convenient to measure or mitigate,\n",
      "rather than what is most normatively concerning.\n",
      "More speciﬁcally, we recommend that work\n",
      "analyzing “bias” in NLP systems be reoriented\n",
      "around the following question: How are social\n",
      "hierarchies, language ideologies, and NLP systems\n",
      "coproduced? This question mirrors Benjamin’s\n",
      "(2020) call to examine how “race and technology\n",
      "are coproduced”—i.e., how racial hierarchies, and\n",
      "the ideologies and discourses that maintain them,\n",
      "create and are re-created by technology. We recom-\n",
      "mend that researchers and practitioners similarly\n",
      "ask how existing social hierarchies and language\n",
      "ideologies drive the development and deployment\n",
      "of NLP systems, and how these systems therefore\n",
      "reproduce these hierarchies and ideologies. As\n",
      "a starting point for reorienting work analyzing\n",
      "“bias” in NLP systems around this question, we\n",
      "3Language ideologies encompass much more than this; see,\n",
      "e.g., Lippi-Green (2012), Alim et al. (2016), Rosa and Flores\n",
      "(2017), Rosa and Burdick (2017), and Charity Hudley (2017).\n",
      "provide the following concrete research questions:\n",
      "▷How do social hierarchies and language\n",
      "ideologies inﬂuence the decisions made during\n",
      "the development and deployment lifecycle?\n",
      "What kinds of NLP systems do these decisions\n",
      "result in, and what kinds do they foreclose?\n",
      "⋄General assumptions: To which linguistic\n",
      "norms do NLP systems adhere (Bender,\n",
      "2019; Ruane et al., 2019)? Which language\n",
      "practices are implicitly assumed to be\n",
      "standard, ordinary, correct, or appropriate?\n",
      "⋄Task deﬁnition:\n",
      "For which speakers\n",
      "are NLP systems (and NLP resources)\n",
      "developed? (See Joshi et al. (2020) for\n",
      "a discussion.)\n",
      "How do task deﬁnitions\n",
      "discretize the world? For example, how\n",
      "are social groups delineated when deﬁning\n",
      "demographic attribute prediction tasks\n",
      "(e.g., Koppel et al., 2002; Rosenthal and\n",
      "McKeown, 2011; Nguyen et al., 2013)?\n",
      "What about languages in native language\n",
      "prediction tasks (Tetreault et al., 2013)?\n",
      "⋄Data: How are datasets collected, prepro-\n",
      "cessed, and labeled or annotated? What are\n",
      "the impacts of annotation guidelines, anno-\n",
      "tator assumptions and perceptions (Olteanu\n",
      "et al., 2019; Sap et al., 2019; Geiger et al.,\n",
      "2020), and annotation aggregation pro-\n",
      "cesses (Pavlick and Kwiatkowski, 2019)?\n",
      "⋄Evaluation: How are NLP systems evalu-\n",
      "ated? What are the impacts of evaluation\n",
      "metrics (Olteanu et al., 2017)? Are any\n",
      "non-quantitative evaluations performed?\n",
      "▷How do NLP systems reproduce or transform\n",
      "language ideologies? Which language varieties\n",
      "or practices come to be deemed good or bad?\n",
      "Might “good” language simply mean language\n",
      "that is easily handled by existing NLP sys-\n",
      "tems? For example, linguistic phenomena aris-\n",
      "ing from many language practices (Eisenstein,\n",
      "2013) are described as “noisy text” and often\n",
      "viewed as a target for “normalization.” How\n",
      "do the language ideologies that are reproduced\n",
      "by NLP systems maintain social hierarchies?\n",
      "▷Which\n",
      "representational\n",
      "harms\n",
      "are\n",
      "being\n",
      "measured or mitigated? Are these the most\n",
      "normatively concerning harms, or merely\n",
      "those that are well handled by existing algo-\n",
      "rithmic fairness techniques? Are there other\n",
      "representational harms that might be analyzed?\n",
      "\n",
      "4.2\n",
      "Conceptualizations of “bias”\n",
      "Turning now to (R2), we argue that work analyzing\n",
      "“bias” in NLP systems should provide explicit\n",
      "statements of why the system behaviors that are\n",
      "described as “bias” are harmful, in what ways,\n",
      "and to whom, as well as the normative reasoning\n",
      "underlying these statements.\n",
      "In other words,\n",
      "researchers and practitioners should articulate their\n",
      "conceptualizations of “bias.”\n",
      "As we described\n",
      "above, papers often contain descriptions of system\n",
      "behaviors that are understood to be self-evident\n",
      "statements of “bias.”\n",
      "This use of imprecise\n",
      "terminology has led to papers all claiming to\n",
      "analyze “bias” in NLP systems, sometimes even\n",
      "in systems developed for the same task, but with\n",
      "different or even inconsistent conceptualizations of\n",
      "“bias,” and no explanations for these differences.\n",
      "Yet analyzing “bias” is an inherently normative\n",
      "process—in which some system behaviors are\n",
      "deemed good and others harmful—even if assump-\n",
      "tions about what kinds of system behaviors are\n",
      "harmful, in what ways, for whom, and why are\n",
      "not stated. We therefore echo calls by Bardzell and\n",
      "Bardzell (2011), Keyes et al. (2019), and Green\n",
      "(2019) for researchers and practitioners to make\n",
      "their normative reasoning explicit by articulating\n",
      "the social values that underpin their decisions to\n",
      "deem some system behaviors as harmful, no matter\n",
      "how obvious such values appear to be. We further\n",
      "argue that this reasoning should take into account\n",
      "the relationships between language and social\n",
      "hierarchies that we described above. First, these\n",
      "relationships provide a foundation from which to\n",
      "approach the normative reasoning that we recom-\n",
      "mend making explicit. For example, some system\n",
      "behaviors might be harmful precisely because\n",
      "they maintain social hierarchies. Second, if work\n",
      "analyzing “bias” in NLP systems is reoriented\n",
      "to understand how social hierarchies, language\n",
      "ideologies, and NLP systems are coproduced, then\n",
      "this work will be incomplete if we fail to account\n",
      "for the ways that social hierarchies and language\n",
      "ideologies determine what we mean by “bias” in\n",
      "the ﬁrst place. As a starting point, we therefore\n",
      "provide the following concrete research questions:\n",
      "▷What kinds of system behaviors are described\n",
      "as “bias”? What are their potential sources (e.g.,\n",
      "general assumptions, task deﬁnition, data)?\n",
      "▷In what ways are these system behaviors harm-\n",
      "ful, to whom are they harmful, and why?\n",
      "▷What are the social values (obvious or not) that\n",
      "underpin this conceptualization of “bias?”\n",
      "4.3\n",
      "Language use in practice\n",
      "Finally, we turn to (R3). Our perspective, which\n",
      "rests on a greater recognition of the relationships\n",
      "between language and social hierarchies, suggests\n",
      "several directions for examining language use in\n",
      "practice. Here, we focus on two. First, because lan-\n",
      "guage is necessarily situated, and because different\n",
      "social groups have different lived experiences due\n",
      "to their different social positions (Hanna et al.,\n",
      "2020)—particularly groups at the intersections\n",
      "of multiple axes of oppression—we recommend\n",
      "that researchers and practitioners center work\n",
      "analyzing “bias” in NLP systems around the lived\n",
      "experiences of members of communities affected\n",
      "by these systems. Second, we recommend that\n",
      "the power relations between technologists and\n",
      "such communities be interrogated and reimagined.\n",
      "Researchers have pointed out that algorithmic\n",
      "fairness techniques, by proposing incremental\n",
      "technical mitigations—e.g., collecting new datasets\n",
      "or training better models—maintain these power\n",
      "relations by (a) assuming that automated systems\n",
      "should continue to exist, rather than asking\n",
      "whether they should be built at all, and (b) keeping\n",
      "development and deployment decisions in the\n",
      "hands of technologists (Bennett and Keyes, 2019;\n",
      "Cifor et al., 2019; Green, 2019; Katell et al., 2020).\n",
      "There are many disciplines for researchers and\n",
      "practitioners to draw on when pursuing these\n",
      "directions.\n",
      "For example, in human–computer\n",
      "interaction, Hamidi et al. (2018) study transgender\n",
      "people’s experiences with automated gender\n",
      "recognition systems in order to uncover how\n",
      "these systems reproduce structures of transgender\n",
      "exclusion by redeﬁning what it means to perform\n",
      "gender “normally.” Value-sensitive design provides\n",
      "a framework for accounting for the values of differ-\n",
      "ent stakeholders in the design of technology (e.g.,\n",
      "Friedman et al., 2006; Friedman and Hendry, 2019;\n",
      "Le Dantec et al., 2009; Yoo et al., 2019), while\n",
      "participatory design seeks to involve stakeholders\n",
      "in the design process itself (Sanders, 2002; Muller,\n",
      "2007; Simonsen and Robertson, 2013; DiSalvo\n",
      "et al., 2013). Participatory action research in educa-\n",
      "tion (Kemmis, 2006) and in language documenta-\n",
      "tion and reclamation (Junker, 2018) is also relevant.\n",
      "In particular, work on language reclamation to\n",
      "support decolonization and tribal sovereignty\n",
      "(Leonard, 2012) and work in sociolinguistics focus-\n",
      "\n",
      "ing on developing co-equal research relationships\n",
      "with community members and supporting linguis-\n",
      "tic justice efforts (e.g., Bucholtz et al., 2014, 2016,\n",
      "2019) provide examples of more emancipatory rela-\n",
      "tionships with communities. Finally, several work-\n",
      "shops and events have begun to explore how to em-\n",
      "power stakeholders in the development and deploy-\n",
      "ment of technology (Vaccaro et al., 2019; Givens\n",
      "and Morris, 2020; Sassaman et al., 2020)4 and how\n",
      "to help researchers and practitioners consider when\n",
      "not to build systems at all (Barocas et al., 2020).\n",
      "As a starting point for engaging with commu-\n",
      "nities affected by NLP systems, we therefore\n",
      "provide the following concrete research questions:\n",
      "▷How do communities become aware of NLP\n",
      "systems? Do they resist them, and if so, how?\n",
      "▷What additional costs are borne by communi-\n",
      "ties for whom NLP systems do not work well?\n",
      "▷Do NLP systems shift power toward oppressive\n",
      "institutions (e.g., by enabling predictions that\n",
      "communities do not want made, linguistically\n",
      "based unfair allocation of resources or oppor-\n",
      "tunities (Rosa and Flores, 2017), surveillance,\n",
      "or censorship), or away from such institutions?\n",
      "▷Who is involved in the development and\n",
      "deployment of NLP systems?\n",
      "How do\n",
      "decision-making processes maintain power re-\n",
      "lations between technologists and communities\n",
      "affected by NLP systems?\n",
      "Can these pro-\n",
      "cesses be changed to reimagine these relations?\n",
      "5\n",
      "Case study\n",
      "To illustrate our recommendations, we present a\n",
      "case study covering work on African-American\n",
      "English (AAE).5 Work analyzing “bias” in the con-\n",
      "text of AAE has shown that part-of-speech taggers,\n",
      "language identiﬁcation systems, and dependency\n",
      "parsers all work less well on text containing\n",
      "features associated with AAE than on text without\n",
      "these features (Jørgensen et al., 2015, 2016; Blod-\n",
      "gett et al., 2016, 2018), and that toxicity detection\n",
      "systems score tweets containing features associated\n",
      "with AAE as more offensive than tweets with-\n",
      "out them (Davidson et al., 2019; Sap et al., 2019).\n",
      "These papers have been critical for highlighting\n",
      "AAE as a language variety for which existing NLP\n",
      "4Also https://participatoryml.github.io/\n",
      "5This language variety has had many different names\n",
      "over the years,\n",
      "but is now generally called African-\n",
      "American English (AAE), African-American Vernacular En-\n",
      "glish (AAVE), or African-American Language (AAL) (Green,\n",
      "2002; Wolfram and Schilling, 2015; Rickford and King, 2016).\n",
      "systems may not work, illustrating their limitations.\n",
      "However, they do not conceptualize “racial bias” in\n",
      "the same way. The ﬁrst four of these papers simply\n",
      "focus on system performance differences between\n",
      "text containing features associated with AAE and\n",
      "text without these features. In contrast, the last\n",
      "two papers also focus on such system performance\n",
      "differences, but motivate this focus with the fol-\n",
      "lowing additional reasoning: If tweets containing\n",
      "features associated with AAE are scored as more\n",
      "offensive than tweets without these features, then\n",
      "this might (a) yield negative perceptions of AAE;\n",
      "(b) result in disproportionate removal of tweets\n",
      "containing these features, impeding participation\n",
      "in online platforms and reducing the space avail-\n",
      "able online in which speakers can use AAE freely;\n",
      "and (c) cause AAE speakers to incur additional\n",
      "costs if they have to change their language practices\n",
      "to avoid negative perceptions or tweet removal.\n",
      "More importantly, none of these papers engage\n",
      "with the literature on AAE, racial hierarchies in the\n",
      "U.S., and raciolinguistic ideologies. By failing to\n",
      "engage with this literature—thereby treating AAE\n",
      "simply as one of many non-Penn Treebank vari-\n",
      "eties of English or perhaps as another challenging\n",
      "domain—work analyzing “bias” in NLP systems\n",
      "in the context of AAE fails to situate these systems\n",
      "in the world. Who are the speakers of AAE? How\n",
      "are they viewed? We argue that AAE as a language\n",
      "variety cannot be separated from its speakers—\n",
      "primarily Black people in the U.S., who experience\n",
      "systemic anti-Black racism—and the language ide-\n",
      "ologies that reinforce and justify racial hierarchies.\n",
      "Even after decades of sociolinguistic efforts to\n",
      "legitimize AAE, it continues to be viewed as “bad”\n",
      "English and its speakers continue to be viewed as\n",
      "linguistically inadequate—a view called the deﬁcit\n",
      "perspective (Alim et al., 2016; Rosa and Flores,\n",
      "2017). This perspective persists despite demon-\n",
      "strations that AAE is rule-bound and grammatical\n",
      "(Mufwene et al., 1998; Green, 2002), in addition\n",
      "to ample evidence of its speakers’ linguistic adroit-\n",
      "ness (e.g., Alim, 2004; Rickford and King, 2016).\n",
      "This perspective belongs to a broader set of raciolin-\n",
      "guistic ideologies (Rosa and Flores, 2017), which\n",
      "also produce allocational harms; speakers of AAE\n",
      "are frequently penalized for not adhering to domi-\n",
      "nant language practices, including in the education\n",
      "system (Alim, 2004; Terry et al., 2010), when\n",
      "seeking housing (Baugh, 2018), and in the judicial\n",
      "system, where their testimony is misunderstood or,\n",
      "\n",
      "worse yet, disbelieved (Rickford and King, 2016;\n",
      "Jones et al., 2019). These raciolinguistic ideologies\n",
      "position\n",
      "racialized\n",
      "communities\n",
      "as\n",
      "needing\n",
      "linguistic intervention, such as language education\n",
      "programs, in which these and other harms can be\n",
      "reduced if communities accommodate to domi-\n",
      "nant language practices (Rosa and Flores, 2017).\n",
      "In the technology industry, speakers of AAE are\n",
      "often not considered consumers who matter. For\n",
      "example, Benjamin (2019) recounts an Apple em-\n",
      "ployee who worked on speech recognition for Siri:\n",
      "“As they worked on different English dialects —\n",
      "Australian, Singaporean, and Indian English — [the\n",
      "employee] asked his boss: ‘What about African\n",
      "American English?’ To this his boss responded:\n",
      "‘Well, Apple products are for the premium market.”’\n",
      "The reality, of course, is that speakers of AAE tend\n",
      "not to represent the “premium market” precisely be-\n",
      "cause of institutions and policies that help to main-\n",
      "tain racial hierarchies by systematically denying\n",
      "them the opportunities to develop wealth that are\n",
      "available to white Americans (Rothstein, 2017)—\n",
      "an exclusion that is reproduced in technology by\n",
      "countless decisions like the one described above.\n",
      "Engaging with the literature outlined above\n",
      "situates the system behaviors that are described\n",
      "as “bias,” providing a foundation for normative\n",
      "reasoning. Researchers and practitioners should\n",
      "be concerned about “racial bias” in toxicity\n",
      "detection systems not only because performance\n",
      "differences impair system performance,\n",
      "but\n",
      "because they reproduce longstanding injustices of\n",
      "stigmatization and disenfranchisement for speakers\n",
      "of AAE. In re-stigmatizing AAE, they reproduce\n",
      "language ideologies in which AAE is viewed as\n",
      "ungrammatical, uneducated, and offensive. These\n",
      "ideologies, in turn, enable linguistic discrimination\n",
      "and justify enduring racial hierarchies (Rosa and\n",
      "Flores, 2017). Our perspective, which understands\n",
      "racial hierarchies and raciolinguistic ideologies as\n",
      "structural conditions that govern the development\n",
      "and deployment of technology,\n",
      "implies that\n",
      "techniques for measuring or mitigating “bias”\n",
      "in NLP systems will necessarily be incomplete\n",
      "unless they interrogate and dismantle these\n",
      "structural conditions, including the power relations\n",
      "between technologists and racialized communities.\n",
      "We emphasize that engaging with the literature\n",
      "on AAE, racial hierarchies in the U.S., and\n",
      "raciolinguistic ideologies can generate new lines of\n",
      "engagement. These lines include work on the ways\n",
      "that the decisions made during the development\n",
      "and deployment of NLP systems produce stigmati-\n",
      "zation and disenfranchisement, and work on AAE\n",
      "use in practice, such as the ways that speakers\n",
      "of AAE interact with NLP systems that were not\n",
      "designed for them. This literature can also help re-\n",
      "searchers and practitioners address the allocational\n",
      "harms that may be produced by NLP systems, and\n",
      "ensure that even well-intentioned NLP systems\n",
      "do not position racialized communities as needing\n",
      "linguistic intervention or accommodation to\n",
      "dominant language practices. Finally, researchers\n",
      "and practitioners wishing to design better systems\n",
      "can also draw on a growing body of work on\n",
      "anti-racist language pedagogy that challenges the\n",
      "deﬁcit perspective of AAE and other racialized\n",
      "language practices (e.g. Flores and Chaparro, 2018;\n",
      "Baker-Bell, 2019; Martínez and Mejía, 2019), as\n",
      "well as the work that we described in section 4.3\n",
      "on reimagining the power relations between tech-\n",
      "nologists and communities affected by technology.\n",
      "6\n",
      "Conclusion\n",
      "By surveying 146 papers analyzing “bias” in NLP\n",
      "systems, we found that (a) their motivations are\n",
      "often vague, inconsistent, and lacking in norma-\n",
      "tive reasoning; and (b) their proposed quantitative\n",
      "techniques for measuring or mitigating “bias” are\n",
      "poorly matched to their motivations and do not en-\n",
      "gage with the relevant literature outside of NLP.\n",
      "To help researchers and practitioners avoid these\n",
      "pitfalls, we proposed three recommendations that\n",
      "should guide work analyzing “bias” in NLP sys-\n",
      "tems, and, for each, provided several concrete re-\n",
      "search questions. These recommendations rest on\n",
      "a greater recognition of the relationships between\n",
      "language and social hierarchies—a step that we\n",
      "see as paramount to establishing a path forward.\n",
      "Acknowledgments\n",
      "This paper is based upon work supported by the\n",
      "National Science Foundation Graduate Research\n",
      "Fellowship under Grant No. 1451512. Any opin-\n",
      "ion, ﬁndings, and conclusions or recommendations\n",
      "expressed in this material are those of the authors\n",
      "and do not necessarily reﬂect the views of the Na-\n",
      "tional Science Foundation. We thank the reviewers\n",
      "for their useful feedback, especially the sugges-\n",
      "tion to include additional details about our method.\n",
      "\n",
      "References\n",
      "Artem Abzaliev. 2019.\n",
      "On GAP coreference resolu-\n",
      "tion shared task: insights from the 3rd place solution.\n",
      "In Proceedings of the Workshop on Gender Bias in\n",
      "Natural Language Processing, pages 107–112, Flo-\n",
      "rence, Italy.\n",
      "ADA. 2018.\n",
      "Guidelines for Writing About Peo-\n",
      "ple With Disabilities.\n",
      "ADA National Network.\n",
      "https://bit.ly/2KREbkB.\n",
      "Oshin Agarwal, Funda Durupinar, Norman I. Badler,\n",
      "and Ani Nenkova. 2019. Word embeddings (also)\n",
      "encode human personality stereotypes. In Proceed-\n",
      "ings of the Joint Conference on Lexical and Com-\n",
      "putational Semantics, pages 205–211, Minneapolis,\n",
      "MN.\n",
      "H. Samy Alim. 2004. You Know My Steez: An Ethno-\n",
      "graphic and Sociolinguistic Study of Styleshifting in\n",
      "a Black American Speech Community. American Di-\n",
      "alect Society.\n",
      "H. Samy Alim, John R. Rickford, and Arnetha F. Ball,\n",
      "editors. 2016.\n",
      "Raciolinguistics:\n",
      "How Language\n",
      "Shapes Our Ideas About Race. Oxford University\n",
      "Press.\n",
      "Sandeep Attree. 2019. Gendered ambiguous pronouns\n",
      "shared task: Boosting model conﬁdence by evidence\n",
      "pooling. In Proceedings of the Workshop on Gen-\n",
      "der Bias in Natural Language Processing, Florence,\n",
      "Italy.\n",
      "Pinkesh Badjatiya,\n",
      "Manish Gupta,\n",
      "and Vasudeva\n",
      "Varma. 2019.\n",
      "Stereotypical bias removal for hate\n",
      "speech detection task using knowledge-based gen-\n",
      "eralizations.\n",
      "In Proceedings of the International\n",
      "World Wide Web Conference, pages 49–59, San Fran-\n",
      "cisco, CA.\n",
      "Eugene Bagdasaryan, Omid Poursaeed, and Vitaly\n",
      "Shmatikov. 2019.\n",
      "Differential Privacy Has Dis-\n",
      "parate Impact on Model Accuracy. In Proceedings\n",
      "of the Conference on Neural Information Processing\n",
      "Systems, Vancouver, Canada.\n",
      "April Baker-Bell. 2019.\n",
      "Dismantling anti-black lin-\n",
      "guistic racism in English language arts classrooms:\n",
      "Toward an anti-racist black language pedagogy. The-\n",
      "ory Into Practice.\n",
      "David Bamman, Sejal Popat, and Sheng Shen. 2019.\n",
      "An annotated dataset of literary entities. In Proceed-\n",
      "ings of the North American Association for Com-\n",
      "putational Linguistics (NAACL), pages 2138–2144,\n",
      "Minneapolis, MN.\n",
      "Xingce Bao and Qianqian Qiao. 2019. Transfer Learn-\n",
      "ing from Pre-trained BERT for Pronoun Resolution.\n",
      "In Proceedings of the Workshop on Gender Bias\n",
      "in Natural Language Processing, pages 82–88, Flo-\n",
      "rence, Italy.\n",
      "Shaowen Bardzell and Jeffrey Bardzell. 2011. Towards\n",
      "a Feminist HCI Methodology: Social Science, Femi-\n",
      "nism, and HCI. In Proceedings of the Conference on\n",
      "Human Factors in Computing Systems (CHI), pages\n",
      "675–684, Vancouver, Canada.\n",
      "Solon Barocas, Asia J. Biega, Benjamin Fish, J˛edrzej\n",
      "Niklas, and Luke Stark. 2020.\n",
      "When Not to De-\n",
      "sign, Build, or Deploy. In Proceedings of the Confer-\n",
      "ence on Fairness, Accountability, and Transparency,\n",
      "Barcelona, Spain.\n",
      "Solon Barocas, Kate Crawford, Aaron Shapiro, and\n",
      "Hanna Wallach. 2017. The Problem With Bias: Al-\n",
      "locative Versus Representational Harms in Machine\n",
      "Learning. In Proceedings of SIGCIS, Philadelphia,\n",
      "PA.\n",
      "Christine Basta, Marta R. Costa-jussà, and Noe Casas.\n",
      "2019. Evaluating the underlying gender bias in con-\n",
      "textualized word embeddings.\n",
      "In Proceedings of\n",
      "the Workshop on Gender Bias for Natural Language\n",
      "Processing, pages 33–39, Florence, Italy.\n",
      "John Baugh. 2018.\n",
      "Linguistics in Pursuit of Justice.\n",
      "Cambridge University Press.\n",
      "Emily M. Bender. 2019. A typology of ethical risks\n",
      "in language technology with an eye towards where\n",
      "transparent documentation can help.\n",
      "Presented at\n",
      "The Future of Artiﬁcial Intelligence:\n",
      "Language,\n",
      "Ethics, Technology Workshop. https://bit.ly/\n",
      "2P9t9M6.\n",
      "Ruha Benjamin. 2019. Race After Technology: Aboli-\n",
      "tionist Tools for the New Jim Code. John Wiley &\n",
      "Sons.\n",
      "Ruha Benjamin. 2020. 2020 Vision: Reimagining the\n",
      "Default Settings of Technology & Society. Keynote\n",
      "at ICLR.\n",
      "Cynthia L. Bennett and Os Keyes. 2019. What is the\n",
      "Point of Fairness?\n",
      "Disability, AI, and The Com-\n",
      "plexity of Justice.\n",
      "In Proceedings of the ASSETS\n",
      "Workshop on AI Fairness for People with Disabili-\n",
      "ties, Pittsburgh, PA.\n",
      "Camiel J. Beukeboom and Christian Burgers. 2019.\n",
      "How Stereotypes Are Shared Through Language: A\n",
      "Review and Introduction of the Social Categories\n",
      "and Stereotypes Communication (SCSC) Frame-\n",
      "work. Review of Communication Research, 7:1–37.\n",
      "Shruti Bhargava and David Forsyth. 2019.\n",
      "Expos-\n",
      "ing and Correcting the Gender Bias in Image\n",
      "Captioning Datasets and Models.\n",
      "arXiv preprint\n",
      "arXiv:1912.00578.\n",
      "Jayadev Bhaskaran and Isha Bhallamudi. 2019. Good\n",
      "Secretaries, Bad Truck Drivers? Occupational Gen-\n",
      "der Stereotypes in Sentiment Analysis. In Proceed-\n",
      "ings of the Workshop on Gender Bias in Natural Lan-\n",
      "guage Processing, pages 62–68, Florence, Italy.\n",
      "\n",
      "Su Lin Blodgett, Lisa Green, and Brendan O’Connor.\n",
      "2016.\n",
      "Demographic Dialectal Variation in Social\n",
      "Media: A Case Study of African-American English.\n",
      "In Proceedings of Empirical Methods in Natural\n",
      "Language Processing (EMNLP), pages 1119–1130,\n",
      "Austin, TX.\n",
      "Su Lin Blodgett and Brendan O’Connor. 2017. Racial\n",
      "Disparity in Natural Language Processing: A Case\n",
      "Study of Social Media African-American English.\n",
      "In Proceedings of the Workshop on Fairness, Ac-\n",
      "countability, and Transparency in Machine Learning\n",
      "(FAT/ML), Halifax, Canada.\n",
      "Su Lin Blodgett, Johnny Wei, and Brendan O’Connor.\n",
      "2018.\n",
      "Twitter Universal Dependency Parsing for\n",
      "African-American and Mainstream American En-\n",
      "glish. In Proceedings of the Association for Compu-\n",
      "tational Linguistics (ACL), pages 1415–1425, Mel-\n",
      "bourne, Australia.\n",
      "Tolga\n",
      "Bolukbasi,\n",
      "Kai-Wei\n",
      "Chang,\n",
      "James\n",
      "Zou,\n",
      "Venkatesh Saligrama,\n",
      "and Adam Kalai. 2016a.\n",
      "Man is to Computer Programmer as Woman is to\n",
      "Homemaker? Debiasing Word Embeddings. In Pro-\n",
      "ceedings of the Conference on Neural Information\n",
      "Processing Systems, pages 4349–4357, Barcelona,\n",
      "Spain.\n",
      "Tolga\n",
      "Bolukbasi,\n",
      "Kai-Wei\n",
      "Chang,\n",
      "James\n",
      "Zou,\n",
      "Venkatesh Saligrama, and Adam Kalai. 2016b.\n",
      "Quantifying and reducing stereotypes in word\n",
      "embeddings. In Proceedings of the ICML Workshop\n",
      "on #Data4Good: Machine Learning in Social Good\n",
      "Applications, pages 41–45, New York, NY.\n",
      "Shikha Bordia and Samuel R. Bowman. 2019. Identify-\n",
      "ing and reducing gender bias in word-level language\n",
      "models. In Proceedings of the NAACL Student Re-\n",
      "search Workshop, pages 7–15, Minneapolis, MN.\n",
      "Marc-Etienne Brunet, Colleen Alkalay-Houlihan, Ash-\n",
      "ton Anderson, and Richard Zemel. 2019.\n",
      "Under-\n",
      "standing the Origins of Bias in Word Embeddings.\n",
      "In Proceedings of the International Conference on\n",
      "Machine Learning, pages 803–811, Long Beach,\n",
      "CA.\n",
      "Mary Bucholtz, Dolores Inés Casillas, and Jin Sook\n",
      "Lee. 2016.\n",
      "Beyond Empowerment: Accompani-\n",
      "ment and Sociolinguistic Justice in a Youth Research\n",
      "Program. In Robert Lawson and Dave Sayers, edi-\n",
      "tors, Sociolinguistic Research: Application and Im-\n",
      "pact, pages 25–44. Routledge.\n",
      "Mary Bucholtz, Dolores Inés Casillas, and Jin Sook\n",
      "Lee. 2019.\n",
      "California Latinx Youth as Agents of\n",
      "Sociolinguistic Justice. In Netta Avineri, Laura R.\n",
      "Graham, Eric J. Johnson, Robin Conley Riner, and\n",
      "Jonathan Rosa, editors, Language and Social Justice\n",
      "in Practice, pages 166–175. Routledge.\n",
      "Mary Bucholtz, Audrey Lopez, Allina Mojarro, Elena\n",
      "Skapoulli, Chris VanderStouwe, and Shawn Warner-\n",
      "Garcia. 2014. Sociolinguistic Justice in the Schools:\n",
      "Student Researchers as Linguistic Experts.\n",
      "Lan-\n",
      "guage and Linguistics Compass, 8:144–157.\n",
      "Kaylee Burns, Lisa Anne Hendricks, Kate Saenko,\n",
      "Trevor Darrell, and Anna Rohrbach. 2018. Women\n",
      "also Snowboard: Overcoming Bias in Captioning\n",
      "Models. In Procedings of the European Conference\n",
      "on Computer Vision (ECCV), pages 793–811, Mu-\n",
      "nich, Germany.\n",
      "Aylin\n",
      "Caliskan,\n",
      "Joanna\n",
      "J.\n",
      "Bryson,\n",
      "and\n",
      "Arvind\n",
      "Narayanan. 2017. Semantics derived automatically\n",
      "from language corpora contain human-like biases.\n",
      "Science, 356(6334).\n",
      "Kathryn Campbell-Kibler. 2009.\n",
      "The nature of so-\n",
      "ciolinguistic perception.\n",
      "Language Variation and\n",
      "Change, 21(1):135–156.\n",
      "Yang Trista Cao and Hal Daumé, III. 2019.\n",
      "To-\n",
      "ward gender-inclusive coreference resolution. arXiv\n",
      "preprint arXiv:1910.13913.\n",
      "Rakesh Chada. 2019. Gendered pronoun resolution us-\n",
      "ing bert and an extractive question answering formu-\n",
      "lation. In Proceedings of the Workshop on Gender\n",
      "Bias in Natural Language Processing, pages 126–\n",
      "133, Florence, Italy.\n",
      "Kaytlin Chaloner and Alfredo Maldonado. 2019. Mea-\n",
      "suring Gender Bias in Word Embedding across Do-\n",
      "mains and Discovering New Gender Bias Word Cat-\n",
      "egories. In Proceedings of the Workshop on Gender\n",
      "Bias in Natural Language Processing, pages 25–32,\n",
      "Florence, Italy.\n",
      "Anne H. Charity Hudley. 2017. Language and Racial-\n",
      "ization. In Ofelia García, Nelson Flores, and Mas-\n",
      "similiano Spotti, editors, The Oxford Handbook of\n",
      "Language and Society. Oxford University Press.\n",
      "Won Ik Cho, Ji Won Kim, Seok Min Kim, and\n",
      "Nam Soo Kim. 2019. On measuring gender bias in\n",
      "translation of gender-neutral pronouns. In Proceed-\n",
      "ings of the Workshop on Gender Bias in Natural Lan-\n",
      "guage Processing, pages 173–181, Florence, Italy.\n",
      "Shivang Chopra, Ramit Sawhney, Puneet Mathur, and\n",
      "Rajiv Ratn Shah. 2020. Hindi-English Hate Speech\n",
      "Detection: Author Proﬁling, Debiasing, and Practi-\n",
      "cal Perspectives. In Proceedings of the AAAI Con-\n",
      "ference on Artiﬁcial Intelligence (AAAI), New York,\n",
      "NY.\n",
      "Marika Cifor, Patricia Garcia, T.L. Cowan, Jasmine\n",
      "Rault, Tonia Sutherland, Anita Say Chan, Jennifer\n",
      "Rode, Anna Lauren Hoffmann, Niloufar Salehi, and\n",
      "Lisa Nakamura. 2019.\n",
      "Feminist Data Manifest-\n",
      "No. Retrieved from https://www.manifestno.\n",
      "com/.\n",
      "Patricia Hill Collins. 2000.\n",
      "Black Feminist Thought:\n",
      "Knowledge, Consciousness, and the Politics of Em-\n",
      "powerment. Routledge.\n",
      "\n",
      "Justin T. Craft, Kelly E. Wright, Rachel Elizabeth\n",
      "Weissler, and Robin M. Queen. 2020.\n",
      "Language\n",
      "and Discrimination: Generating Meaning, Perceiv-\n",
      "ing Identities, and Discriminating Outcomes.\n",
      "An-\n",
      "nual Review of Linguistics, 6(1).\n",
      "Kate Crawford. 2017. The Trouble with Bias. Keynote\n",
      "at NeurIPS.\n",
      "Kimberle Crenshaw. 1989. Demarginalizing the Inter-\n",
      "section of Race and Sex: A Black Feminist Critique\n",
      "of Antidiscrmination Doctrine, Feminist Theory and\n",
      "Antiracist Politics. University of Chicago Legal Fo-\n",
      "rum.\n",
      "Amanda Cercas Curry and Verena Rieser. 2018.\n",
      "#MeToo: How Conversational Systems Respond to\n",
      "Sexual Harassment. In Proceedings of the Workshop\n",
      "on Ethics in Natural Language Processing, pages 7–\n",
      "14, New Orleans, LA.\n",
      "Karan Dabas, Nishtha Madaan, Gautam Singh, Vi-\n",
      "jay Arya, Sameep Mehta, and Tanmoy Chakraborty.\n",
      "2020. Fair Transfer of Multiple Style Attributes in\n",
      "Text. arXiv preprint arXiv:2001.06693.\n",
      "Thomas Davidson, Debasmita Bhattacharya, and Ing-\n",
      "mar Weber. 2019. Racial bias in hate speech and\n",
      "abusive language detection datasets. In Proceedings\n",
      "of the Workshop on Abusive Language Online, pages\n",
      "25–35, Florence, Italy.\n",
      "Maria De-Arteaga, Alexey Romanov, Hanna Wal-\n",
      "lach, Jennifer Chayes, Christian Borgs, Alexandra\n",
      "Chouldechova, Sahin Geyik, Krishnaram Kentha-\n",
      "padi, and Adam Tauman Kalai. 2019. Bias in bios:\n",
      "A case study of semantic representation bias in a\n",
      "high-stakes setting. In Proceedings of the Confer-\n",
      "ence on Fairness, Accountability, and Transparency,\n",
      "pages 120–128, Atlanta, GA.\n",
      "Sunipa Dev, Tao Li, Jeff Phillips, and Vivek Sriku-\n",
      "mar. 2019.\n",
      "On Measuring and Mitigating Biased\n",
      "Inferences of Word Embeddings.\n",
      "arXiv preprint\n",
      "arXiv:1908.09369.\n",
      "Sunipa Dev and Jeff Phillips. 2019. Attenuating Bias in\n",
      "Word Vectors. In Proceedings of the International\n",
      "Conference on Artiﬁcial Intelligence and Statistics,\n",
      "pages 879–887, Naha, Japan.\n",
      "Mark Díaz, Isaac Johnson, Amanda Lazar, Anne Marie\n",
      "Piper, and Darren Gergle. 2018.\n",
      "Addressing age-\n",
      "related bias in sentiment analysis. In Proceedings\n",
      "of the Conference on Human Factors in Computing\n",
      "Systems (CHI), Montréal, Canada.\n",
      "Emily Dinan, Angela Fan, Adina Williams, Jack Ur-\n",
      "banek, Douwe Kiela, and Jason Weston. 2019.\n",
      "Queens are Powerful too:\n",
      "Mitigating Gender\n",
      "Bias in Dialogue Generation.\n",
      "arXiv preprint\n",
      "arXiv:1911.03842.\n",
      "Carl DiSalvo, Andrew Clement, and Volkmar Pipek.\n",
      "2013. Communities: Participatory Design for, with\n",
      "and by communities. In Jesper Simonsen and Toni\n",
      "Robertson, editors, Routledge International Hand-\n",
      "book of Participatory Design, pages 182–209. Rout-\n",
      "ledge.\n",
      "Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain,\n",
      "and Lucy Vasserman. 2018. Measuring and mitigat-\n",
      "ing unintended bias in text classiﬁcation.\n",
      "In Pro-\n",
      "ceedings of the Conference on Artiﬁcial Intelligence,\n",
      "Ethics, and Society (AIES), New Orleans, LA.\n",
      "Jacob Eisenstein. 2013.\n",
      "What to do about bad lan-\n",
      "guage on the Internet. In Proceedings of the North\n",
      "American Association for Computational Linguistics\n",
      "(NAACL), pages 359–369.\n",
      "Kawin Ethayarajh. 2020. Is Your Classiﬁer Actually\n",
      "Biased? Measuring Fairness under Uncertainty with\n",
      "Bernstein Bounds. In Proceedings of the Associa-\n",
      "tion for Computational Linguistics (ACL).\n",
      "Kawin Ethayarajh, David Duvenaud, and Graeme Hirst.\n",
      "2019. Understanding Undesirable Word Embedding\n",
      "Assocations. In Proceedings of the Association for\n",
      "Computational Linguistics (ACL), pages 1696–1705,\n",
      "Florence, Italy.\n",
      "Joseph Fisher. 2019.\n",
      "Measuring social bias in\n",
      "knowledge graph embeddings.\n",
      "arXiv preprint\n",
      "arXiv:1912.02761.\n",
      "Nelson Flores and Soﬁa Chaparro. 2018. What counts\n",
      "as language education policy? Developing a materi-\n",
      "alist Anti-racist approach to language activism. Lan-\n",
      "guage Policy, 17(3):365–384.\n",
      "Omar U. Florez. 2019. On the Unintended Social Bias\n",
      "of Training Language Generation Models with Data\n",
      "from Local Media. In Proceedings of the NeurIPS\n",
      "Workshop on Human-Centric Machine Learning,\n",
      "Vancouver, Canada.\n",
      "Joel Escudé Font and Marta R. Costa-jussà. 2019.\n",
      "Equalizing gender biases in neural machine trans-\n",
      "lation with word embeddings techniques.\n",
      "In Pro-\n",
      "ceedings of the Workshop on Gender Bias for Natu-\n",
      "ral Language Processing, pages 147–154, Florence,\n",
      "Italy.\n",
      "Batya Friedman and David G. Hendry. 2019.\n",
      "Value\n",
      "Sensitive Design: Shaping Technology with Moral\n",
      "Imagination. MIT Press.\n",
      "Batya Friedman, Peter H. Kahn Jr., and Alan Borning.\n",
      "2006. Value Sensitive Design and Information Sys-\n",
      "tems. In Dennis Galletta and Ping Zhang, editors,\n",
      "Human-Computer Interaction in Management Infor-\n",
      "mation Systems: Foundations, pages 348–372. M.E.\n",
      "Sharpe.\n",
      "Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and\n",
      "James Zou. 2018. Word Embeddings Quantify 100\n",
      "Years of Gender and Ethnic Stereotypes. Proceed-\n",
      "ings of the National Academy of Sciences, 115(16).\n",
      "\n",
      "Sahaj Garg, Vincent Perot, Nicole Limtiaco, Ankur\n",
      "Taly, Ed H. Chi, and Alex Beutel. 2019. Counter-\n",
      "factual fairness in text classiﬁcation through robust-\n",
      "ness. In Proceedings of the Conference on Artiﬁcial\n",
      "Intelligence, Ethics, and Society (AIES), Honolulu,\n",
      "HI.\n",
      "Aparna Garimella, Carmen Banea, Dirk Hovy, and\n",
      "Rada Mihalcea. 2019. Women’s syntactic resilience\n",
      "and men’s grammatical luck: Gender bias in part-of-\n",
      "speech tagging and dependency parsing data. In Pro-\n",
      "ceedings of the Association for Computational Lin-\n",
      "guistics (ACL), pages 3493–3498, Florence, Italy.\n",
      "Andrew Gaut, Tony Sun, Shirlyn Tang, Yuxin Huang,\n",
      "Jing Qian,\n",
      "Mai ElSherief,\n",
      "Jieyu Zhao,\n",
      "Diba\n",
      "Mirza, Elizabeth Belding, Kai-Wei Chang, and\n",
      "William Yang Wang. 2020.\n",
      "Towards Understand-\n",
      "ing Gender Bias in Relation Extraction. In Proceed-\n",
      "ings of the Association for Computational Linguis-\n",
      "tics (ACL).\n",
      "R. Stuart Geiger, Kevin Yu, Yanlai Yang, Mindy Dai,\n",
      "Jie Qiu, Rebekah Tang, and Jenny Huang. 2020.\n",
      "Garbage In, Garbage Out?\n",
      "Do Machine Learn-\n",
      "ing Application Papers in Social Computing Report\n",
      "Where Human-Labeled Training Data Comes From?\n",
      "In Proceedings of the Conference on Fairness, Ac-\n",
      "countability, and Transparency, pages 325–336.\n",
      "Oguzhan Gencoglu. 2020.\n",
      "Cyberbullying Detec-\n",
      "tion with Fairness Constraints.\n",
      "arXiv preprint\n",
      "arXiv:2005.06625.\n",
      "Alexandra Reeve Givens and Meredith Ringel Morris.\n",
      "2020. Centering Disability Perspecives in Algorith-\n",
      "mic Fairness, Accountability, and Transparency. In\n",
      "Proceedings of the Conference on Fairness, Account-\n",
      "ability, and Transparency, Barcelona, Spain.\n",
      "Hila Gonen and Yoav Goldberg. 2019. Lipstick on a\n",
      "Pig: Debiasing Methods Cover up Systematic Gen-\n",
      "der Biases in Word Embeddings But do not Remove\n",
      "Them. In Proceedings of the North American As-\n",
      "sociation for Computational Linguistics (NAACL),\n",
      "pages 609–614, Minneapolis, MN.\n",
      "Hila Gonen and Kellie Webster. 2020.\n",
      "Auto-\n",
      "matically Identifying Gender Issues in Machine\n",
      "Translation using Perturbations.\n",
      "arXiv preprint\n",
      "arXiv:2004.14065.\n",
      "Ben Green. 2019. “Good” isn’t good enough. In Pro-\n",
      "ceedings of the AI for Social Good Workshop, Van-\n",
      "couver, Canada.\n",
      "Lisa J. Green. 2002. African American English: A Lin-\n",
      "guistic Introduction. Cambridge University Press.\n",
      "Anthony G. Greenwald, Debbie E. McGhee, and Jor-\n",
      "dan L.K. Schwartz. 1998. Measuring individual dif-\n",
      "ferences in implicit cognition: The implicit associa-\n",
      "tion test. Journal of Personality and Social Psychol-\n",
      "ogy, 74(6):1464–1480.\n",
      "Enoch Opanin Gyamﬁ, Yunbo Rao, Miao Gou, and\n",
      "Yanhua Shao. 2020. deb2viz: Debiasing gender in\n",
      "word embedding data using subspace visualization.\n",
      "In Proceedings of the International Conference on\n",
      "Graphics and Image Processing.\n",
      "Foad Hamidi, Morgan Klaus Scheuerman, and Stacy M.\n",
      "Branham. 2018. Gender Recognition or Gender Re-\n",
      "ductionism? The Social Implications of Automatic\n",
      "Gender Recognition Systems. In Proceedings of the\n",
      "Conference on Human Factors in Computing Sys-\n",
      "tems (CHI), Montréal, Canada.\n",
      "Alex Hanna, Emily Denton, Andrew Smart, and Jamila\n",
      "Smith-Loud. 2020. Towards a Critical Race Method-\n",
      "ology in Algorithmic Fairness. In Proceedings of the\n",
      "Conference on Fairness, Accountability, and Trans-\n",
      "parency, pages 501–512, Barcelona, Spain.\n",
      "Madeline E. Heilman, Aaaron S. Wallen, Daniella\n",
      "Fuchs, and Melinda M. Tamkins. 2004. Penalties\n",
      "for Success: Reactions to Women Who Succeed at\n",
      "Male Gender-Typed Tasks. Journal of Applied Psy-\n",
      "chology, 89(3):416–427.\n",
      "Jane H. Hill. 2008. The Everyday Language of White\n",
      "Racism. Wiley-Blackwell.\n",
      "Dirk Hovy, Federico Bianchi, and Tommaso Fornaciari.\n",
      "2020. Can You Translate that into Man? Commer-\n",
      "cial Machine Translation Systems Include Stylistic\n",
      "Biases. In Proceedings of the Association for Com-\n",
      "putational Linguistics (ACL).\n",
      "Dirk Hovy and Anders Søgaard. 2015. Tagging Per-\n",
      "formance Correlates with Author Age. In Proceed-\n",
      "ings of the Association for Computational Linguis-\n",
      "tics and the International Joint Conference on Nat-\n",
      "ural Language Processing, pages 483–488, Beijing,\n",
      "China.\n",
      "Dirk Hovy and Shannon L. Spruit. 2016. The social\n",
      "impact of natural language processing. In Proceed-\n",
      "ings of the Association for Computational Linguis-\n",
      "tics (ACL), pages 591–598, Berlin, Germany.\n",
      "Po-Sen Huang, Huan Zhang, Ray Jiang, Robert\n",
      "Stanforth, Johannes Welbl, Jack W. Rae, Vishal\n",
      "Maini, Dani Yogatama, and Pushmeet Kohli. 2019.\n",
      "Reducing Sentiment Bias in Language Models\n",
      "via Counterfactual Evaluation.\n",
      "arXiv preprint\n",
      "arXiv:1911.03064.\n",
      "Xiaolei Huang, Linzi Xing, Franck Dernoncourt, and\n",
      "Michael J. Paul. 2020.\n",
      "Multilingual Twitter Cor-\n",
      "pus and Baselines for Evaluating Demographic Bias\n",
      "in Hate Speech Recognition.\n",
      "In Proceedings of\n",
      "the Language Resources and Evaluation Conference\n",
      "(LREC), Marseille, France.\n",
      "Christoph Hube, Maximilian Idahl, and Besnik Fetahu.\n",
      "2020. Debiasing Word Embeddings from Sentiment\n",
      "Associations in Names. In Proceedings of the Inter-\n",
      "national Conference on Web Search and Data Min-\n",
      "ing, pages 259–267, Houston, TX.\n",
      "\n",
      "Ben Hutchinson, Vinodkumar Prabhakaran, Emily\n",
      "Denton, Kellie Webster, Yu Zhong, and Stephen De-\n",
      "nuyl. 2020. Social Biases in NLP Models as Barriers\n",
      "for Persons with Disabilities. In Proceedings of the\n",
      "Association for Computational Linguistics (ACL).\n",
      "Matei Ionita, Yury Kashnitsky, Ken Krige, Vladimir\n",
      "Larin, Dennis Logvinenko, and Atanas Atanasov.\n",
      "2019.\n",
      "Resolving gendered ambiguous pronouns\n",
      "with BERT.\n",
      "In Proceedings of the Workshop on\n",
      "Gender Bias in Natural Language Processing, pages\n",
      "113–119, Florence, Italy.\n",
      "Hailey James-Sorenson and David Alvarez-Melis.\n",
      "2019. Probabilistic Bias Mitigation in Word Embed-\n",
      "dings. In Proceedings of the Workshop on Human-\n",
      "Centric Machine Learning, Vancouver, Canada.\n",
      "Shengyu Jia, Tao Meng, Jieyu Zhao, and Kai-Wei\n",
      "Chang. 2020. Mitigating Gender Bias Ampliﬁcation\n",
      "in Distribution by Posterior Regularization. In Pro-\n",
      "ceedings of the Association for Computational Lin-\n",
      "guistics (ACL).\n",
      "Taylor Jones, Jessica Rose Kalbfeld, Ryan Hancock,\n",
      "and Robin Clark. 2019. Testifying while black: An\n",
      "experimental study of court reporter accuracy in tran-\n",
      "scription of African American English. Language,\n",
      "95(2).\n",
      "Anna Jørgensen, Dirk Hovy, and Anders Søgaard. 2015.\n",
      "Challenges of studying and processing dialects in\n",
      "social media. In Proceedings of the Workshop on\n",
      "Noisy User-Generated Text, pages 9–18, Beijing,\n",
      "China.\n",
      "Anna Jørgensen, Dirk Hovy, and Anders Søgaard. 2016.\n",
      "Learning a POS tagger for AAVE-like language. In\n",
      "Proceedings of the North American Association for\n",
      "Computational Linguistics (NAACL), pages 1115–\n",
      "1120, San Diego, CA.\n",
      "Pratik Joshi, Sebastian Santy, Amar Budhiraja, Kalika\n",
      "Bali, and Monojit Choudhury. 2020. The State and\n",
      "Fate of Linguistic Diversity and Inclusion in the\n",
      "NLP World. In Proceedings of the Association for\n",
      "Computational Linguistics (ACL).\n",
      "Jaap Jumelet, Willem Zuidema, and Dieuwke Hupkes.\n",
      "2019. Analysing Neural Language Models: Contex-\n",
      "tual Decomposition Reveals Default Reasoning in\n",
      "Number and Gender Assignment. In Proceedings\n",
      "of the Conference on Natural Language Learning,\n",
      "Hong Kong, China.\n",
      "Marie-Odile Junker. 2018.\n",
      "Participatory action re-\n",
      "search for Indigenous linguistics in the digital age.\n",
      "In Shannon T. Bischoff and Carmen Jany, editors,\n",
      "Insights from Practices in Community-Based Re-\n",
      "search, pages 164–175. De Gruyter Mouton.\n",
      "David Jurgens, Yulia Tsvetkov, and Dan Jurafsky. 2017.\n",
      "Incorporating Dialectal Variability for Socially Equi-\n",
      "table Language Identiﬁcation. In Proceedings of the\n",
      "Association for Computational Linguistics (ACL),\n",
      "pages 51–57, Vancouver, Canada.\n",
      "Masahiro Kaneko and Danushka Bollegala. 2019.\n",
      "Gender-preserving debiasing for pre-trained word\n",
      "embeddings. In Proceedings of the Association for\n",
      "Computational Linguistics (ACL), pages 1641–1650,\n",
      "Florence, Italy.\n",
      "Saket Karve, Lyle Ungar, and João Sedoc. 2019. Con-\n",
      "ceptor debiasing of word representations evaluated\n",
      "on WEAT. In Proceedings of the Workshop on Gen-\n",
      "der Bias in Natural Language Processing, pages 40–\n",
      "48, Florence, Italy.\n",
      "Michael Katell, Meg Young, Dharma Dailey, Bernease\n",
      "Herman, Vivian Guetler, Aaron Tam, Corinne Bintz,\n",
      "Danielle Raz, and P.M. Krafft. 2020.\n",
      "Toward sit-\n",
      "uated interventions for algorithmic equity: lessons\n",
      "from the ﬁeld. In Proceedings of the Conference on\n",
      "Fairness, Accountability, and Transparency, pages\n",
      "45–55, Barcelona, Spain.\n",
      "Stephen Kemmis. 2006. Participatory action research\n",
      "and the public sphere. Educational Action Research,\n",
      "14(4):459–476.\n",
      "Os Keyes. 2018.\n",
      "The Misgendering Machines:\n",
      "Trans/HCI\n",
      "Implications\n",
      "of\n",
      "Automatic\n",
      "Gender\n",
      "Recognition. Proceedings of the ACM on Human-\n",
      "Computer Interaction, 2(CSCW).\n",
      "Os Keyes, Josephine Hoy, and Margaret Drouhard.\n",
      "2019. Human-Computer Insurrection: Notes on an\n",
      "Anarchist HCI. In Proceedings of the Conference on\n",
      "Human Factors in Computing Systems (CHI), Glas-\n",
      "gow, Scotland, UK.\n",
      "Jae Yeon Kim, Carlos Ortiz, Sarah Nam, Sarah Santi-\n",
      "ago, and Vivek Datta. 2020. Intersectional Bias in\n",
      "Hate Speech and Abusive Language Datasets.\n",
      "In\n",
      "Proceedings of the Association for Computational\n",
      "Linguistics (ACL).\n",
      "Svetlana Kiritchenko and Saif M. Mohammad. 2018.\n",
      "Examining Gender and Race Bias in Two Hundred\n",
      "Sentiment Analysis Systems. In Proceedings of the\n",
      "Joint Conference on Lexical and Computational Se-\n",
      "mantics, pages 43–53, New Orleans, LA.\n",
      "Moshe Koppel, Shlomo Argamon, and Anat Rachel\n",
      "Shimoni. 2002.\n",
      "Automatically Categorizing Writ-\n",
      "ten Texts by Author Gender. Literary and Linguistic\n",
      "Computing, 17(4):401–412.\n",
      "Keita Kurita, Nidhi Vyas, Ayush Pareek, Alan W.\n",
      "Black, and Yulia Tsvetkov. 2019. Measuring bias\n",
      "in contextualized word representations. In Proceed-\n",
      "ings of the Workshop on Gender Bias for Natu-\n",
      "ral Language Processing, pages 166–172, Florence,\n",
      "Italy.\n",
      "Sonja L. Lanehart and Ayesha M. Malik. 2018. Black\n",
      "Is, Black Isn’t: Perceptions of Language and Black-\n",
      "ness. In Jeffrey Reaser, Eric Wilbanks, Karissa Woj-\n",
      "cik, and Walt Wolfram, editors, Language Variety in\n",
      "the New South. University of North Carolina Press.\n",
      "\n",
      "Brian N. Larson. 2017. Gender as a variable in natural-\n",
      "language processing: Ethical considerations. In Pro-\n",
      "ceedings of the Workshop on Ethics in Natural Lan-\n",
      "guage Processing, pages 30–40, Valencia, Spain.\n",
      "Anne Lauscher and Goran Glavaš. 2019. Are We Con-\n",
      "sistently Biased? Multidimensional Analysis of Bi-\n",
      "ases in Distributional Word Vectors. In Proceedings\n",
      "of the Joint Conference on Lexical and Computa-\n",
      "tional Semantics, pages 85–91, Minneapolis, MN.\n",
      "Anne Lauscher, Goran Glavaš, Simone Paolo Ponzetto,\n",
      "and Ivan Vuli´c. 2019. A General Framework for Im-\n",
      "plicit and Explicit Debiasing of Distributional Word\n",
      "Vector Spaces. arXiv preprint arXiv:1909.06092.\n",
      "Christopher A. Le Dantec, Erika Shehan Poole, and Su-\n",
      "san P. Wyche. 2009. Values as Lived Experience:\n",
      "Evolving Value Sensitive Design in Support of Value\n",
      "Discovery. In Proceedings of the Conference on Hu-\n",
      "man Factors in Computing Systems (CHI), Boston,\n",
      "MA.\n",
      "Nayeon Lee, Andrea Madotto, and Pascale Fung. 2019.\n",
      "Exploring Social Bias in Chatbots using Stereotype\n",
      "Knowledge.\n",
      "In Proceedings of the Workshop on\n",
      "Widening NLP, pages 177–180, Florence, Italy.\n",
      "Wesley Y. Leonard. 2012. Reframing language recla-\n",
      "mation programmes for everybody’s empowerment.\n",
      "Gender and Language, 6(2):339–367.\n",
      "Paul Pu Liang, Irene Li, Emily Zheng, Yao Chong Lim,\n",
      "Ruslan Salakhutdinov, and Louis-Philippe Morency.\n",
      "2019. Towards Debiasing Sentence Representations.\n",
      "In Proceedings of the NeurIPS Workshop on Human-\n",
      "Centric Machine Learning, Vancouver, Canada.\n",
      "Rosina Lippi-Green. 2012.\n",
      "English with an Ac-\n",
      "cent: Language, Ideology, and Discrimination in the\n",
      "United States. Routledge.\n",
      "Bo Liu. 2019. Anonymized BERT: An Augmentation\n",
      "Approach to the Gendered Pronoun Resolution Chal-\n",
      "lenge. In Proceedings of the Workshop on Gender\n",
      "Bias in Natural Language Processing, pages 120–\n",
      "125, Florence, Italy.\n",
      "Haochen Liu, Jamell Dacon, Wenqi Fan, Hui Liu, Zi-\n",
      "tao Liu, and Jiliang Tang. 2019. Does Gender Mat-\n",
      "ter? Towards Fairness in Dialogue Systems. arXiv\n",
      "preprint arXiv:1910.10486.\n",
      "Felipe Alfaro Lois, José A.R. Fonollosa, and Costa-jà.\n",
      "2019. BERT Masked Language Modeling for Co-\n",
      "reference Resolution. In Proceedings of the Work-\n",
      "shop on Gender Bias in Natural Language Process-\n",
      "ing, pages 76–81, Florence, Italy.\n",
      "Brandon C. Loudermilk. 2015. Implicit attitudes and\n",
      "the perception of sociolinguistic variation. In Alexei\n",
      "Prikhodkine and Dennis R. Preston, editors, Re-\n",
      "sponses to Language Varieties:\n",
      "Variability, pro-\n",
      "cesses and outcomes, pages 137–156.\n",
      "Anastassia Loukina, Nitin Madnani, and Klaus Zech-\n",
      "ner. 2019. The many dimensions of algorithmic fair-\n",
      "ness in educational applications. In Proceedings of\n",
      "the Workshop on Innovative Use of NLP for Build-\n",
      "ing Educational Applications, pages 1–10, Florence,\n",
      "Italy.\n",
      "Kaiji Lu, Peter Mardziel, Fangjing Wu, Preetam Aman-\n",
      "charla, and Anupam Datta. 2018.\n",
      "Gender bias in\n",
      "neural natural language processing. arXiv preprint\n",
      "arXiv:1807.11714.\n",
      "Anne Maass. 1999. Linguistic intergroup bias: Stereo-\n",
      "type perpetuation through language.\n",
      "Advances in\n",
      "Experimental Social Psychology, 31:79–121.\n",
      "Nitin Madnani, Anastassia Loukina, Alina von Davier,\n",
      "Jill Burstein, and Aoife Cahill. 2017. Building Bet-\n",
      "ter Open-Source Tools to Support Fairness in Auto-\n",
      "mated Scoring. In Proceedings of the Workshop on\n",
      "Ethics in Natural Language Processing, pages 41–\n",
      "52, Valencia, Spain.\n",
      "Thomas Manzini, Yao Chong Lim, Yulia Tsvetkov, and\n",
      "Alan W. Black. 2019. Black is to Criminal as Cau-\n",
      "casian is to Police: Detecting and Removing Multi-\n",
      "class Bias in Word Embeddings. In Proceedings of\n",
      "the North American Association for Computational\n",
      "Linguistics (NAACL), pages 801–809, Minneapolis,\n",
      "MN.\n",
      "Ramón Antonio Martínez and Alexander Feliciano\n",
      "Mejía. 2019.\n",
      "Looking closely and listening care-\n",
      "fully: A sociocultural approach to understanding\n",
      "the complexity of Latina/o/x students’ everyday lan-\n",
      "guage. Theory Into Practice.\n",
      "Rowan Hall Maudslay, Hila Gonen, Ryan Cotterell,\n",
      "and Simone Teufel. 2019. It’s All in the Name: Mit-\n",
      "igating Gender Bias with Name-Based Counterfac-\n",
      "tual Data Substitution. In Proceedings of Empirical\n",
      "Methods in Natural Language Processing (EMNLP),\n",
      "pages 5270–5278, Hong Kong, China.\n",
      "Chandler May, Alex Wang, Shikha Bordia, Samuel R.\n",
      "Bowman, and Rachel Rudinger. 2019. On Measur-\n",
      "ing Social Biases in Sentence Encoders. In Proceed-\n",
      "ings of the North American Association for Compu-\n",
      "tational Linguistics (NAACL), pages 629–634, Min-\n",
      "neapolis, MN.\n",
      "Elijah Mayﬁeld,\n",
      "Michael Madaio,\n",
      "Shrimai Prab-\n",
      "humoye, David Gerritsen, Brittany McLaughlin,\n",
      "Ezekiel Dixon-Roman, and Alan W. Black. 2019.\n",
      "Equity Beyond Bias in Language Technologies for\n",
      "Education. In Proceedings of the Workshop on Inno-\n",
      "vative Use of NLP for Building Educational Appli-\n",
      "cations, Florence, Italy.\n",
      "Katherine McCurdy and O˘guz Serbetçi. 2017. Gram-\n",
      "matical gender associations outweigh topical gender\n",
      "bias in crosslinguistic word embeddings.\n",
      "In Pro-\n",
      "ceedings of the Workshop for Women & Underrepre-\n",
      "sented Minorities in Natural Language Processing,\n",
      "Vancouver, Canada.\n",
      "\n",
      "Ninareh Mehrabi, Thamme Gowda, Fred Morstatter,\n",
      "Nanyun Peng, and Aram Galstyan. 2019. Man is to\n",
      "Person as Woman is to Location: Measuring Gender\n",
      "Bias in Named Entity Recognition. arXiv preprint\n",
      "arXiv:1910.10872.\n",
      "Michela Menegatti and Monica Rubini. 2017. Gender\n",
      "bias and sexism in language. In Oxford Research\n",
      "Encyclopedia of Communication. Oxford University\n",
      "Press.\n",
      "Inom Mirzaev, Anthony Schulte, Michael Conover, and\n",
      "Sam Shah. 2019. Considerations for the interpreta-\n",
      "tion of bias measures of word embeddings. arXiv\n",
      "preprint arXiv:1906.08379.\n",
      "Salikoko S. Mufwene, Guy Bailey, and John R. Rick-\n",
      "ford, editors. 1998.\n",
      "African-American English:\n",
      "Structure, History, and Use. Routledge.\n",
      "Michael J. Muller. 2007.\n",
      "Participatory Design: The\n",
      "Third Space in HCI. In The Human-Computer Inter-\n",
      "action Handbook, pages 1087–1108. CRC Press.\n",
      "Moin\n",
      "Nadeem,\n",
      "Anna\n",
      "Bethke,\n",
      "and\n",
      "Siva\n",
      "Reddy.\n",
      "2020.\n",
      "StereoSet:\n",
      "Measuring stereotypical bias\n",
      "in pretrained language models.\n",
      "arXiv preprint\n",
      "arXiv:2004.09456.\n",
      "Dong Nguyen, Rilana Gravel, Dolf Trieschnigg, and\n",
      "Theo Meder. 2013.\n",
      "“How Old Do You Think I\n",
      "Am?”: A Study of Language and Age in Twitter. In\n",
      "Proceedings of the Conference on Web and Social\n",
      "Media (ICWSM), pages 439–448, Boston, MA.\n",
      "Malvina Nissim, Rik van Noord, and Rob van der Goot.\n",
      "2020. Fair is better than sensational: Man is to doc-\n",
      "tor as woman is to doctor. Computational Linguis-\n",
      "tics.\n",
      "Debora Nozza, Claudia Volpetti, and Elisabetta Fersini.\n",
      "2019. Unintended Bias in Misogyny Detection. In\n",
      "Proceedings of the Conference on Web Intelligence,\n",
      "pages 149–155.\n",
      "Alexandra Olteanu, Carlos Castillo, Fernando Diaz,\n",
      "and Emre Kıcıman. 2019.\n",
      "Social Data: Biases,\n",
      "Methodological Pitfalls, and Ethical Boundaries.\n",
      "Frontiers in Big Data, 2.\n",
      "Alexandra Olteanu, Kartik Talamadupula, and Kush R.\n",
      "Varshney. 2017. The Limits of Abstract Evaluation\n",
      "Metrics: The Case of Hate Speech Detection.\n",
      "In\n",
      "Proceedings of the ACM Web Science Conference,\n",
      "Troy, NY.\n",
      "Orestis Papakyriakopoulos, Simon Hegelich, Juan Car-\n",
      "los Medina Serrano, and Fabienne Marco. 2020.\n",
      "Bias in word embeddings.\n",
      "In Proceedings of the\n",
      "Conference on Fairness, Accountability, and Trans-\n",
      "parency, pages 446–457, Barcelona, Spain.\n",
      "Ji Ho Park, Jamin Shin, and Pascale Fung. 2018. Re-\n",
      "ducing Gender Bias in Abusive Language Detection.\n",
      "In Proceedings of Empirical Methods in Natural\n",
      "Language Processing (EMNLP), pages 2799–2804,\n",
      "Brussels, Belgium.\n",
      "Ellie Pavlick and Tom Kwiatkowski. 2019. Inherent\n",
      "Disagreements in Human Textual Inferences. Trans-\n",
      "actions of the Association for Computational Lin-\n",
      "guistics, 7:677–694.\n",
      "Xiangyu Peng, Siyan Li, Spencer Frazier, and Mark\n",
      "Riedl. 2020. Fine-Tuning a Transformer-Based Lan-\n",
      "guage Model to Avoid Generating Non-Normative\n",
      "Text. arXiv preprint arXiv:2001.08764.\n",
      "Radomir Popovi´c, Florian Lemmerich, and Markus\n",
      "Strohmaier. 2020.\n",
      "Joint Multiclass Debiasing of\n",
      "Word Embeddings. In Proceedings of the Interna-\n",
      "tional Symposium on Intelligent Systems, Graz, Aus-\n",
      "tria.\n",
      "Vinodkumar Prabhakaran, Ben Hutchinson, and Mar-\n",
      "garet Mitchell. 2019. Perturbation Sensitivity Anal-\n",
      "ysis to Detect Unintended Model Biases.\n",
      "In Pro-\n",
      "ceedings of Empirical Methods in Natural Language\n",
      "Processing (EMNLP), pages 5744–5749,\n",
      "Hong\n",
      "Kong, China.\n",
      "Shrimai Prabhumoye, Elijah Mayﬁeld, and Alan W.\n",
      "Black. 2019. Principled Frameworks for Evaluating\n",
      "Ethics in NLP Systems. In Proceedings of the Work-\n",
      "shop on Innovative Use of NLP for Building Educa-\n",
      "tional Applications, Florence, Italy.\n",
      "Marcelo Prates, Pedro Avelar, and Luis C. Lamb. 2019.\n",
      "Assessing gender bias in machine translation: A\n",
      "case study with google translate. Neural Computing\n",
      "and Applications.\n",
      "Rasmus Précenth. 2019. Word embeddings and gender\n",
      "stereotypes in Swedish and English. Master’s thesis,\n",
      "Uppsala University.\n",
      "Dennis R. Preston. 2009.\n",
      "Are you really smart (or\n",
      "stupid, or cute, or ugly, or cool)? Or do you just talk\n",
      "that way? Language attitudes, standardization and\n",
      "language change. Oslo: Novus forlag, pages 105–\n",
      "129.\n",
      "Flavien Prost, Nithum Thain, and Tolga Bolukbasi.\n",
      "2019. Debiasing Embeddings for Reduced Gender\n",
      "Bias in Text Classiﬁcation. In Proceedings of the\n",
      "Workshop on Gender Bias in Natural Language Pro-\n",
      "cessing, pages 69–75, Florence, Italy.\n",
      "Reid Pryzant, Richard Diehl Martinez, Nathan Dass,\n",
      "Sadao Kurohashi, Dan Jurafsky, and Diyi Yang.\n",
      "2020. Automatically Neutralizing Subjective Bias\n",
      "in Text. In Proceedings of the AAAI Conference on\n",
      "Artiﬁcial Intelligence (AAAI), New York, NY.\n",
      "Arun K. Pujari, Ansh Mittal, Anshuman Padhi, An-\n",
      "shul Jain, Mukesh Jadon, and Vikas Kumar. 2019.\n",
      "Debiasing Gender biased Hindi Words with Word-\n",
      "embedding.\n",
      "In Proceedings of the International\n",
      "Conference on Algorithms, Computing and Artiﬁcial\n",
      "Intelligence, pages 450–456.\n",
      "Yusu Qian, Urwa Muaz, Ben Zhang, and Jae Won\n",
      "Hyun. 2019.\n",
      "Reducing gender bias in word-level\n",
      "\n",
      "language models with a gender-equalizing loss func-\n",
      "tion. In Proceedings of the ACL Student Research\n",
      "Workshop, pages 223–228, Florence, Italy.\n",
      "Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael\n",
      "Twiton, and Yoav Goldberg. 2020.\n",
      "Null It Out:\n",
      "Guarding Protected Attributes by Iterative Nullspace\n",
      "Projection.\n",
      "In Proceedings of the Association for\n",
      "Computational Linguistics (ACL).\n",
      "John R. Rickford and Sharese King. 2016. Language\n",
      "and linguistics on trial: Hearing Rachel Jeantel (and\n",
      "other vernacular speakers) in the courtroom and be-\n",
      "yond. Language, 92(4):948–988.\n",
      "Anthony Rios. 2020. FuzzE: Fuzzy Fairness Evalua-\n",
      "tion of Offensive Language Classiﬁers on African-\n",
      "American English. In Proceedings of the AAAI Con-\n",
      "ference on Artiﬁcial Intelligence (AAAI), New York,\n",
      "NY.\n",
      "Gerald Roche. 2019.\n",
      "Articulating language oppres-\n",
      "sion: colonialism, coloniality and the erasure of Ti-\n",
      "betâ ˘A´Zs minority languages. Patterns of Prejudice.\n",
      "Alexey Romanov, Maria De-Arteaga, Hanna Wal-\n",
      "lach, Jennifer Chayes, Christian Borgs, Alexandra\n",
      "Chouldechova, Sahin Geyik, Krishnaram Kentha-\n",
      "padi, Anna Rumshisky, and Adam Tauman Kalai.\n",
      "2019. What’s in a Name? Reducing Bias in Bios\n",
      "without Access to Protected Attributes. In Proceed-\n",
      "ings of the North American Association for Com-\n",
      "putational Linguistics (NAACL), pages 4187–4195,\n",
      "Minneapolis, MN.\n",
      "Jonathan Rosa. 2019. Contesting Representations of\n",
      "Migrant “Illegality” through the Drop the I-Word\n",
      "Campaign: Rethinking Language Change and So-\n",
      "cial Change.\n",
      "In Netta Avineri, Laura R. Graham,\n",
      "Eric J. Johnson, Robin Conley Riner, and Jonathan\n",
      "Rosa, editors, Language and Social Justice in Prac-\n",
      "tice. Routledge.\n",
      "Jonathan Rosa and Christa Burdick. 2017. Language\n",
      "Ideologies.\n",
      "In Ofelia García, Nelson Flores, and\n",
      "Massimiliano Spotti, editors, The Oxford Handbook\n",
      "of Language and Society. Oxford University Press.\n",
      "Jonathan Rosa and Nelson Flores. 2017.\n",
      "Unsettling\n",
      "race and language: Toward a raciolinguistic perspec-\n",
      "tive. Language in Society, 46:621–647.\n",
      "Sara Rosenthal and Kathleen McKeown. 2011.\n",
      "Age\n",
      "Prediction in Blogs: A Study of Style, Content, and\n",
      "Online Behavior in Pre- and Post-Social Media Gen-\n",
      "erations. In Proceedings of the North American As-\n",
      "sociation for Computational Linguistics (NAACL),\n",
      "pages 763–772, Portland, OR.\n",
      "Candace\n",
      "Ross,\n",
      "Boris\n",
      "Katz,\n",
      "and\n",
      "Andrei\n",
      "Barbu.\n",
      "2020.\n",
      "Measuring Social Biases in Grounded Vi-\n",
      "sion and Language Embeddings.\n",
      "arXiv preprint\n",
      "arXiv:2002.08911.\n",
      "Richard Rothstein. 2017. The Color of Law: A For-\n",
      "gotten History of How Our Government Segregated\n",
      "America. Liveright Publishing.\n",
      "David Rozado. 2020. Wide range screening of algo-\n",
      "rithmic bias in word embedding models using large\n",
      "sentiment lexicons reveals underreported bias types.\n",
      "PLOS One.\n",
      "Elayne Ruane, Abeba Birhane, and Anthony Ven-\n",
      "tresque. 2019. Conversational AI: Social and Ethi-\n",
      "cal Considerations. In Proceedings of the Irish Con-\n",
      "ference on Artiﬁcial Intelligence and Cognitive Sci-\n",
      "ence, Galway, Ireland.\n",
      "Rachel Rudinger,\n",
      "Chandler May,\n",
      "and Benjamin\n",
      "Van Durme. 2017. Social bias in elicited natural lan-\n",
      "guage inferences. In Proceedings of the Workshop\n",
      "on Ethics in Natural Language Processing, pages\n",
      "74–79, Valencia, Spain.\n",
      "Rachel Rudinger, Jason Naradowsky, Brian Leonard,\n",
      "and Benjamin Van Durme. 2018.\n",
      "Gender Bias\n",
      "in Coreference Resolution.\n",
      "In Proceedings of the\n",
      "North American Association for Computational Lin-\n",
      "guistics (NAACL), pages 8–14, New Orleans, LA.\n",
      "Elizabeth B.N. Sanders. 2002. From user-centered to\n",
      "participatory design approaches. In Jorge Frascara,\n",
      "editor, Design and the Social Sciences: Making Con-\n",
      "nections, pages 18–25. CRC Press.\n",
      "Brenda Salenave Santana, Vinicius Woloszyn, and Le-\n",
      "andro Krug Wives. 2018. Is there gender bias and\n",
      "stereotype in Portuguese word embeddings?\n",
      "In\n",
      "Proceedings of the International Conference on the\n",
      "Computational Processing of Portuguese Student Re-\n",
      "search Workshop, Canela, Brazil.\n",
      "Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,\n",
      "and Noah A. Smith. 2019. The risk of racial bias in\n",
      "hate speech detection. In Proceedings of the Asso-\n",
      "ciation for Computational Linguistics (ACL), pages\n",
      "1668–1678, Florence, Italy.\n",
      "Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Juraf-\n",
      "sky, Noah A. Smith, and Yejin Choi. 2020. Social\n",
      "Bias Frames: Reasoning about Social and Power Im-\n",
      "plications of Language. In Proceedings of the Asso-\n",
      "ciation for Computational Linguistics (ACL).\n",
      "Hanna Sassaman, Jennifer Lee, Jenessa Irvine, and\n",
      "Shankar Narayan. 2020.\n",
      "Creating Community-\n",
      "Based Tech Policy: Case Studies, Lessons Learned,\n",
      "and What Technologists and Communities Can Do\n",
      "Together. In Proceedings of the Conference on Fair-\n",
      "ness, Accountability, and Transparency, Barcelona,\n",
      "Spain.\n",
      "Danielle Saunders and Bill Byrne. 2020.\n",
      "Reducing\n",
      "Gender Bias in Neural Machine Translation as a Do-\n",
      "main Adaptation Problem. In Proceedings of the As-\n",
      "sociation for Computational Linguistics (ACL).\n",
      "Tyler Schnoebelen. 2017.\n",
      "Goal-Oriented Design for\n",
      "Ethical Machine Learning and NLP. In Proceedings\n",
      "of the Workshop on Ethics in Natural Language Pro-\n",
      "cessing, pages 88–93, Valencia, Spain.\n",
      "\n",
      "Sabine Sczesny, Magda Formanowicz, and Franziska\n",
      "Moser. 2016. Can gender-fair language reduce gen-\n",
      "der stereotyping and discrimination?\n",
      "Frontiers in\n",
      "Psychology, 7.\n",
      "João Sedoc and Lyle Ungar. 2019. The Role of Pro-\n",
      "tected Class Word Lists in Bias Identiﬁcation of Con-\n",
      "textualized Word Representations. In Proceedings\n",
      "of the Workshop on Gender Bias in Natural Lan-\n",
      "guage Processing, pages 55–61, Florence, Italy.\n",
      "Procheta Sen and Debasis Ganguly. 2020. Towards So-\n",
      "cially Responsible AI: Cognitive Bias-Aware Multi-\n",
      "Objective Learning.\n",
      "In Proceedings of the AAAI\n",
      "Conference on Artiﬁcial Intelligence (AAAI), New\n",
      "York, NY.\n",
      "Deven Shah, H. Andrew Schwartz, and Dirk Hovy.\n",
      "2020. Predictive Biases in Natural Language Pro-\n",
      "cessing Models:\n",
      "A Conceptual Framework and\n",
      "Overview.\n",
      "In Proceedings of the Association for\n",
      "Computational Linguistics (ACL).\n",
      "Judy Hanwen Shen, Lauren Fratamico, Iyad Rahwan,\n",
      "and Alexander M. Rush. 2018.\n",
      "Darling or Baby-\n",
      "girl? Investigating Stylistic Bias in Sentiment Anal-\n",
      "ysis. In Proceedings of the Workshop on Fairness,\n",
      "Accountability, and Transparency (FAT/ML), Stock-\n",
      "holm, Sweden.\n",
      "Emily Sheng, Kai-Wei Chang, Premkumar Natarajan,\n",
      "and Nanyun Peng. 2019. The Woman Worked as\n",
      "a Babysitter: On Biases in Language Generation.\n",
      "In Proceedings of Empirical Methods in Natural\n",
      "Language Processing (EMNLP), pages 3398–3403,\n",
      "Hong Kong, China.\n",
      "Emily Sheng, Kai-Wei Chang, Premkumar Natarajan,\n",
      "and Nanyun Peng. 2020.\n",
      "Towards Controllable\n",
      "Biases in Language Generation.\n",
      "arXiv preprint\n",
      "arXiv:2005.00268.\n",
      "Seungjae Shin, Kyungwoo Song, JoonHo Jang, Hyemi\n",
      "Kim, Weonyoung Joo, and Il-Chul Moon. 2020.\n",
      "Neutralizing Gender Bias in Word Embedding with\n",
      "Latent Disentanglement and Counterfactual Genera-\n",
      "tion. arXiv preprint arXiv:2004.03133.\n",
      "Jesper Simonsen and Toni Robertson, editors. 2013.\n",
      "Routledge International Handbook of Participatory\n",
      "Design. Routledge.\n",
      "Gabriel Stanovsky, Noah A. Smith, and Luke Zettle-\n",
      "moyer. 2019.\n",
      "Evaluating gender bias in machine\n",
      "translation. In Proceedings of the Association for\n",
      "Computational Linguistics (ACL), pages 1679–1684,\n",
      "Florence, Italy.\n",
      "Yolande Strengers, Lizhe Qu, Qiongkai Xu, and Jarrod\n",
      "Knibbe. 2020.\n",
      "Adhering, Steering, and Queering:\n",
      "Treatment of Gender in Natural Language Genera-\n",
      "tion. In Proceedings of the Conference on Human\n",
      "Factors in Computing Systems (CHI), Honolulu, HI.\n",
      "Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang,\n",
      "Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth\n",
      "Belding, Kai-Wei Chang, and William Yang Wang.\n",
      "2019.\n",
      "Mitigating Gender Bias in Natural Lan-\n",
      "guage Processing: Literature Review. In Proceed-\n",
      "ings of the Association for Computational Linguis-\n",
      "tics (ACL), pages 1630–1640, Florence, Italy.\n",
      "Adam Sutton, Thomas Lansdall-Welfare, and Nello\n",
      "Cristianini. 2018.\n",
      "Biased embeddings from wild\n",
      "data:\n",
      "Measuring, understanding and removing.\n",
      "In Proceedings of the International Symposium\n",
      "on Intelligent Data Analysis, pages 328–339, ’s-\n",
      "Hertogenbosch, Netherlands.\n",
      "Chris Sweeney and Maryam Najaﬁan. 2019. A Trans-\n",
      "parent Framework for Evaluating Unintended De-\n",
      "mographic Bias in Word Embeddings. In Proceed-\n",
      "ings of the Association for Computational Linguis-\n",
      "tics (ACL), pages 1662–1667, Florence, Italy.\n",
      "Chris Sweeney and Maryam Najaﬁan. 2020. Reduc-\n",
      "ing sentiment polarity for demographic attributes\n",
      "in word embeddings using adversarial learning. In\n",
      "Proceedings of the Conference on Fairness, Ac-\n",
      "countability, and Transparency, pages 359–368,\n",
      "Barcelona, Spain.\n",
      "Nathaniel Swinger, Maria De-Arteaga, Neil Thomas\n",
      "Heffernan, Mark D.M. Leiserson, and Adam Tau-\n",
      "man Kalai. 2019. What are the biases in my word\n",
      "embedding?\n",
      "In Proceedings of the Conference on\n",
      "Artiﬁcial Intelligence, Ethics, and Society (AIES),\n",
      "Honolulu, HI.\n",
      "Samson Tan, Shaﬁq Joty, Min-Yen Kan, and Richard\n",
      "Socher. 2020.\n",
      "It’s Morphin’ Time!\n",
      "Combating\n",
      "Linguistic Discrimination with Inﬂectional Perturba-\n",
      "tions. In Proceedings of the Association for Compu-\n",
      "tational Linguistics (ACL).\n",
      "Yi Chern Tan and L. Elisa Celis. 2019.\n",
      "Assessing\n",
      "Social and Intersectional Biases in Contextualized\n",
      "Word Representations. In Proceedings of the Con-\n",
      "ference on Neural Information Processing Systems,\n",
      "Vancouver, Canada.\n",
      "J. Michael Terry, Randall Hendrick, Evangelos Evan-\n",
      "gelou, and Richard L. Smith. 2010.\n",
      "Variable\n",
      "dialect switching among African American chil-\n",
      "dren: Inferences about working memory.\n",
      "Lingua,\n",
      "120(10):2463–2475.\n",
      "Joel Tetreault, Daniel Blanchard, and Aoife Cahill.\n",
      "2013. A Report on the First Native Language Iden-\n",
      "tiﬁcation Shared Task. In Proceedings of the Work-\n",
      "shop on Innovative Use of NLP for Building Educa-\n",
      "tional Applications, pages 48–57, Atlanta, GA.\n",
      "Mike Thelwall. 2018. Gender Bias in Sentiment Anal-\n",
      "ysis. Online Information Review, 42(1):45–57.\n",
      "Kristen Vaccaro, Karrie Karahalios, Deirdre K. Mul-\n",
      "ligan, Daniel Kluttz, and Tad Hirsch. 2019.\n",
      "Con-\n",
      "testability in Algorithmic Systems. In Conference\n",
      "Companion Publication of the 2019 on Computer\n",
      "\n",
      "Supported Cooperative Work and Social Computing,\n",
      "pages 523–527, Austin, TX.\n",
      "Ameya Vaidya, Feng Mai, and Yue Ning. 2019. Em-\n",
      "pirical Analysis of Multi-Task Learning for Reduc-\n",
      "ing Model Bias in Toxic Comment Detection. arXiv\n",
      "preprint arXiv:1909.09758v2.\n",
      "Eva Vanmassenhove, Christian Hardmeier, and Andy\n",
      "Way. 2018.\n",
      "Getting Gender Right in Neural Ma-\n",
      "chine Translation.\n",
      "In Proceedings of Empirical\n",
      "Methods in Natural Language Processing (EMNLP),\n",
      "pages 3003–3008, Brussels, Belgium.\n",
      "Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\n",
      "Sharon Qian, Daniel Nevo, Yaron Singer, and Stu-\n",
      "art Shieber. 2020.\n",
      "Causal Mediation Analysis for\n",
      "Interpreting Neural NLP: The Case of Gender Bias.\n",
      "arXiv preprint arXiv:2004.12265.\n",
      "Tianlu Wang, Xi Victoria Lin, Nazneen Fatema Ra-\n",
      "jani, Bryan McCann, Vicente Ordonez, and Caim-\n",
      "ing Xiong. 2020.\n",
      "Double-Hard Debias: Tailoring\n",
      "Word Embeddings for Gender Bias Mitigation. In\n",
      "Proceedings of the Association for Computational\n",
      "Linguistics (ACL).\n",
      "Zili Wang. 2019. MSnet: A BERT-based Network for\n",
      "Gendered Pronoun Resolution.\n",
      "In Proceedings of\n",
      "the Workshop on Gender Bias in Natural Language\n",
      "Processing, pages 89–95, Florence, Italy.\n",
      "Kellie Webster, Marta R. Costa-jussà, Christian Hard-\n",
      "meier, and Will Radford. 2019. Gendered Ambigu-\n",
      "ous Pronoun (GAP) Shared Task at the Gender Bias\n",
      "in NLP Workshop 2019. In Proceedings of the Work-\n",
      "shop on Gender Bias in Natural Language Process-\n",
      "ing, pages 1–7, Florence, Italy.\n",
      "Kellie Webster, Marta Recasens, Vera Axelrod, and Ja-\n",
      "son Baldridge. 2018. Mind the GAP: A balanced\n",
      "corpus of gendered ambiguous pronouns. Transac-\n",
      "tions of the Association for Computational Linguis-\n",
      "tics, 6:605–618.\n",
      "Walt Wolfram and Natalie Schilling. 2015. American\n",
      "English: Dialects and Variation, 3 edition. Wiley\n",
      "Blackwell.\n",
      "Austin P. Wright, Omar Shaikh, Haekyu Park, Will Ep-\n",
      "person, Muhammed Ahmed, Stephane Pinel, Diyi\n",
      "Yang, and Duen Horng (Polo) Chau. 2020.\n",
      "RE-\n",
      "CAST: Interactive Auditing of Automatic Toxicity\n",
      "Detection Models.\n",
      "In Proceedings of the Con-\n",
      "ference on Human Factors in Computing Systems\n",
      "(CHI), Honolulu, HI.\n",
      "Yinchuan Xu and Junlin Yang. 2019. Look again at\n",
      "the syntax: Relational graph convolutional network\n",
      "for gendered ambiguous pronoun resolution. In Pro-\n",
      "ceedings of the Workshop on Gender Bias in Natu-\n",
      "ral Language Processing, pages 96–101, Florence,\n",
      "Italy.\n",
      "Kai-Chou Yang, Timothy Niven, Tzu-Hsuan Chou, and\n",
      "Hung-Yu Kao. 2019.\n",
      "Fill the GAP: Exploiting\n",
      "BERT for Pronoun Resolution. In Proceedings of\n",
      "the Workshop on Gender Bias in Natural Language\n",
      "Processing, pages 102–106, Florence, Italy.\n",
      "Zekun Yang and Juan Feng. 2020. A Causal Inference\n",
      "Method for Reducing Gender Bias in Word Embed-\n",
      "ding Relations.\n",
      "In Proceedings of the AAAI Con-\n",
      "ference on Artiﬁcial Intelligence (AAAI), New York,\n",
      "NY.\n",
      "Daisy Yoo, Anya Ernest, Soﬁa Serholt, Eva Eriksson,\n",
      "and Peter Dalsgaard. 2019. Service Design in HCI\n",
      "Research: The Extended Value Co-creation Model.\n",
      "In Proceedings of the Halfway to the Future Sympo-\n",
      "sium, Nottingham, United Kingdom.\n",
      "Brian Hu Zhang,\n",
      "Blake Lemoine,\n",
      "and Margaret\n",
      "Mitchell. 2018. Mitigating unwanted biases with ad-\n",
      "versarial learning. In Proceedings of the Conference\n",
      "on Artiﬁcial Intelligence, Ethics, and Society (AIES),\n",
      "New Orleans, LA.\n",
      "Guanhua Zhang, Bing Bai, Junqi Zhang, Kun Bai, Con-\n",
      "ghui Zhu, and Tiejun Zhao. 2020a. Demographics\n",
      "Should Not Be the Reason of Toxicity: Mitigating\n",
      "Discrimination in Text Classiﬁcations with Instance\n",
      "Weighting.\n",
      "In Proceedings of the Association for\n",
      "Computational Linguistics (ACL).\n",
      "Haoran Zhang,\n",
      "Amy X. Lu,\n",
      "Mohamed Abdalla,\n",
      "Matthew\n",
      "McDermott,\n",
      "and\n",
      "Marzyeh\n",
      "Ghassemi.\n",
      "2020b. Hurtful Words: Quantifying Biases in Clin-\n",
      "ical Contextual Word Embeddings. In Proceedings\n",
      "of the ACM Conference on Health, Inference, and\n",
      "Learning.\n",
      "Jieyu Zhao, Subhabrata Mukherjee, Saghar Hosseini,\n",
      "Kai-Wei Chang, and Ahmed Hassan Awadallah.\n",
      "2020. Gender Bias in Multilingual Embeddings and\n",
      "Cross-Lingual Transfer. In Proceedings of the Asso-\n",
      "ciation for Computational Linguistics (ACL).\n",
      "Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cot-\n",
      "terell, Vicente Ordonez, and Kai-Wei Chang. 2019.\n",
      "Gender Bias in Contextualized Word Embeddings.\n",
      "In Proceedings of the North American Association\n",
      "for Computational Linguistics (NAACL), pages 629–\n",
      "634, Minneapolis, MN.\n",
      "Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\n",
      "donez, and Kai-Wei Chang. 2017.\n",
      "Men also like\n",
      "shopping: Reducing gender bias ampliﬁcation us-\n",
      "ing corpus-level constraints.\n",
      "In Proceedings of\n",
      "Empirical Methods in Natural Language Process-\n",
      "ing (EMNLP), pages 2979–2989, Copenhagen, Den-\n",
      "mark.\n",
      "Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\n",
      "donez, and Kai-Wei Chang. 2018a.\n",
      "Gender Bias\n",
      "in Coreference Resolution: Evaluation and Debias-\n",
      "ing Methods. In Proceedings of the North American\n",
      "Association for Computational Linguistics (NAACL),\n",
      "pages 15–20, New Orleans, LA.\n",
      "\n",
      "Jieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai-\n",
      "Wei Chang. 2018b. Learning Gender-Neutral Word\n",
      "Embeddings. In Proceedings of Empirical Methods\n",
      "in Natural Language Processing (EMNLP), pages\n",
      "4847–4853, Brussels, Belgium.\n",
      "Alina Zhiltsova, Simon Caton, and Catherine Mulwa.\n",
      "2019. Mitigation of Unintended Biases against Non-\n",
      "Native English Texts in Sentiment Analysis. In Pro-\n",
      "ceedings of the Irish Conference on Artiﬁcial Intelli-\n",
      "gence and Cognitive Science, Galway, Ireland.\n",
      "Pei Zhou, Weijia Shi, Jieyu Zhao, Kuan-Hao Huang,\n",
      "Muhao Chen, and Kai-Wei Chang. 2019. Examin-\n",
      "ing gender bias in languages with grammatical gen-\n",
      "ders. In Proceedings of Empirical Methods in Nat-\n",
      "ural Language Processing (EMNLP), pages 5279–\n",
      "5287, Hong Kong, China.\n",
      "Ran Zmigrod, S. J. Mielke, Hanna Wallach, and Ryan\n",
      "Cotterell. 2019. Counterfactual data augmentation\n",
      "for mitigating gender stereotypes in languages with\n",
      "rich morphology. In Proceedings of the Association\n",
      "for Computational Linguistics (ACL), pages 1651–\n",
      "1661, Florence, Italy.\n",
      "A\n",
      "Appendix\n",
      "In Table 3, we provide examples of the papers’ mo-\n",
      "tivations and techniques across several NLP tasks.\n",
      "A.1\n",
      "Categorization details\n",
      "In this section, we provide some additional details\n",
      "about our method—speciﬁcally, our categorization.\n",
      "What counts as being covered by an NLP task?\n",
      "We considered a paper to cover a given NLP task if\n",
      "it analyzed “bias” with respect to that task, but not\n",
      "if it only evaluated overall performance on that task.\n",
      "For example, a paper examining the impact of miti-\n",
      "gating “bias” in word embeddings on “bias” in sen-\n",
      "timent analysis would be counted as covering both\n",
      "NLP tasks. In contrast, a paper assessing whether\n",
      "performance on sentiment analysis degraded after\n",
      "mitigating “bias” in word embeddings would be\n",
      "counted only as focusing on embeddings.\n",
      "What counts as a motivation?\n",
      "We considered a\n",
      "motivation to include any description of the prob-\n",
      "lem that motivated the paper or proposed quantita-\n",
      "tive technique, including any normative reasoning.\n",
      "We excluded from the “Vague/unstated” cate-\n",
      "gory of motivations the papers that participated in\n",
      "the Gendered Ambiguous Pronoun (GAP) Shared\n",
      "Task at the First ACL Workshop on Gender Bias in\n",
      "NLP. In an ideal world, shared task papers would\n",
      "engage with “bias” more critically, but given the\n",
      "nature of shared tasks it is understandable that they\n",
      "do not. As a result, we excluded them from our\n",
      "counts for techniques as well. We cite the papers\n",
      "here; most propose techniques we would have cate-\n",
      "gorized as “Questionable correlations,” with a few\n",
      "as “Other representational harms” (Abzaliev, 2019;\n",
      "Attree, 2019; Bao and Qiao, 2019; Chada, 2019;\n",
      "Ionita et al., 2019; Liu, 2019; Lois et al., 2019;\n",
      "Wang, 2019; Xu and Yang, 2019; Yang et al., 2019).\n",
      "We excluded Dabas et al. (2020) from our survey\n",
      "because we could not determine what this paper’s\n",
      "user study on fairness was actually measuring.\n",
      "Finally, we actually categorized the motivation\n",
      "for Liu et al. (2019) (i.e., the last row in Table 3) as\n",
      "“Questionable correlations” due to a sentence else-\n",
      "where in the paper; had the paragraph we quoted\n",
      "been presented without more detail, we would have\n",
      "categorized the motivation as “Vague/unstated.”\n",
      "A.2\n",
      "Full categorization: Motivations\n",
      "Allocational harms\n",
      "Hovy and Spruit (2016);\n",
      "Caliskan et al. (2017); Madnani et al. (2017);\n",
      "Dixon et al. (2018); Kiritchenko and Mohammad\n",
      "(2018); Shen et al. (2018); Zhao et al. (2018b);\n",
      "Bhaskaran and Bhallamudi (2019); Bordia and\n",
      "Bowman (2019); Brunet et al. (2019); Chaloner\n",
      "and Maldonado (2019); De-Arteaga et al. (2019);\n",
      "Dev and Phillips (2019); Font and Costa-jussà\n",
      "(2019); James-Sorenson and Alvarez-Melis (2019);\n",
      "Kurita et al. (2019); Mayﬁeld et al. (2019); Pu-\n",
      "jari et al. (2019); Romanov et al. (2019); Ruane\n",
      "et al. (2019); Sedoc and Ungar (2019); Sun et al.\n",
      "(2019); Zmigrod et al. (2019); Hutchinson et al.\n",
      "(2020); Papakyriakopoulos et al. (2020); Ravfo-\n",
      "gel et al. (2020); Strengers et al. (2020); Sweeney\n",
      "and Najaﬁan (2020); Tan et al. (2020); Zhang et al.\n",
      "(2020b).\n",
      "Stereotyping\n",
      "Bolukbasi\n",
      "et\n",
      "al.\n",
      "(2016a,b);\n",
      "Caliskan et al. (2017); McCurdy and Serbetçi\n",
      "(2017); Rudinger et al. (2017); Zhao et al. (2017);\n",
      "Curry and Rieser (2018); Díaz et al. (2018);\n",
      "Santana et al. (2018); Sutton et al. (2018); Zhao\n",
      "et al. (2018a,b); Agarwal et al. (2019); Basta et al.\n",
      "(2019); Bhaskaran and Bhallamudi (2019); Bordia\n",
      "and Bowman (2019); Brunet et al. (2019); Cao\n",
      "and Daumé (2019); Chaloner and Maldonado\n",
      "(2019); Cho et al. (2019); Dev and Phillips (2019);\n",
      "Font and Costa-jussà (2019); Gonen and Goldberg\n",
      "(2019); James-Sorenson and Alvarez-Melis (2019);\n",
      "Kaneko and Bollegala (2019); Karve et al. (2019);\n",
      "Kurita et al. (2019); Lauscher and Glavaš (2019);\n",
      "Lee et al. (2019); Manzini et al. (2019); Mayﬁeld\n",
      "\n",
      "Categories\n",
      "NLP task\n",
      "Stated motivation\n",
      "Motivations\n",
      "Techniques\n",
      "Language\n",
      "modeling\n",
      "(Bordia and\n",
      "Bowman,\n",
      "2019)\n",
      "“Existing biases in data can be ampliﬁed by models and the\n",
      "resulting output consumed by the public can inﬂuence them, en-\n",
      "courage and reinforce harmful stereotypes, or distort the truth.\n",
      "Automated systems that depend on these models can take prob-\n",
      "lematic actions based on biased proﬁling of individuals.”\n",
      "Allocational\n",
      "harms,\n",
      "stereotyping\n",
      "Questionable\n",
      "correlations\n",
      "Sentiment\n",
      "analysis\n",
      "(Kiritchenko\n",
      "and\n",
      "Mohammad,\n",
      "2018)\n",
      "“Other biases can be inappropriate and result in negative ex-\n",
      "periences for some groups of people. Examples include, loan\n",
      "eligibility and crime recidivism prediction systems...and resumé\n",
      "sorting systems that believe that men are more qualiﬁed to be\n",
      "programmers than women (Bolukbasi et al., 2016). Similarly,\n",
      "sentiment and emotion analysis systems can also perpetuate and\n",
      "accentuate inappropriate human biases, e.g., systems that consider\n",
      "utterances from one race or gender to be less positive simply be-\n",
      "cause of their race or gender, or customer support systems that\n",
      "prioritize a call from an angry male over a call from the equally\n",
      "angry female.”\n",
      "Allocational\n",
      "harms, other\n",
      "representational\n",
      "harms (system\n",
      "performance\n",
      "differences w.r.t.\n",
      "text written by\n",
      "different social\n",
      "groups)\n",
      "Questionable\n",
      "correlations\n",
      "(differences in\n",
      "sentiment\n",
      "intensity scores\n",
      "w.r.t. text about\n",
      "different social\n",
      "groups)\n",
      "Machine\n",
      "translation\n",
      "(Cho et al.,\n",
      "2019)\n",
      "“[MT training] may incur an association of gender-speciﬁed pro-\n",
      "nouns (in the target) and gender-neutral ones (in the source) for\n",
      "lexicon pairs that frequently collocate in the corpora. We claim\n",
      "that this kind of phenomenon seriously threatens the fairness of a\n",
      "translation system, in the sense that it lacks generality and inserts\n",
      "social bias to the inference. Moreover, the input is not fully cor-\n",
      "rect (considering gender-neutrality) and might offend the users\n",
      "who expect fairer representations.”\n",
      "Questionable\n",
      "correlations,\n",
      "other\n",
      "representational\n",
      "harms\n",
      "Questionable\n",
      "correlations\n",
      "Machine\n",
      "translation\n",
      "(Stanovsky\n",
      "et al., 2019)\n",
      "“Learned models exhibit social bias when their training data\n",
      "encode stereotypes not relevant for the task, but the correlations\n",
      "are picked up anyway.”\n",
      "Stereotyping,\n",
      "questionable\n",
      "correlations\n",
      "Stereotyping,\n",
      "other\n",
      "representational\n",
      "harms (system\n",
      "performance\n",
      "differences),\n",
      "questionable\n",
      "correlations\n",
      "Type-level\n",
      "embeddings\n",
      "(Zhao et al.,\n",
      "2018b)\n",
      "“However, embeddings trained on human-generated corpora have\n",
      "been demonstrated to inherit strong gender stereotypes that re-\n",
      "ﬂect social constructs....Such a bias substantially affects down-\n",
      "stream applications....This concerns the practitioners who use\n",
      "the embedding model to build gender-sensitive applications such\n",
      "as a resume ﬁltering system or a job recommendation system as\n",
      "the automated system may discriminate candidates based on their\n",
      "gender, as reﬂected by their name. Besides, biased embeddings\n",
      "may implicitly affect downstream applications used in our daily\n",
      "lives. For example, when searching for ‘computer scientist’ using\n",
      "a search engine...a search algorithm using an embedding model in\n",
      "the backbone tends to rank male scientists higher than females’\n",
      "[sic], hindering women from being recognized and further exac-\n",
      "erbating the gender inequality in the community.”\n",
      "Allocational\n",
      "harms,\n",
      "stereotyping,\n",
      "other\n",
      "representational\n",
      "harms\n",
      "Stereotyping\n",
      "Type-level\n",
      "and contextu-\n",
      "alized\n",
      "embeddings\n",
      "(May et al.,\n",
      "2019)\n",
      "“[P]rominent word embeddings such as word2vec (Mikolov et\n",
      "al., 2013) and GloVe (Pennington et al., 2014) encode systematic\n",
      "biases against women and black people (Bolukbasi et al., 2016;\n",
      "Garg et al., 2018), implicating many NLP systems in scaling up\n",
      "social injustice.”\n",
      "Vague\n",
      "Stereotyping\n",
      "Dialogue\n",
      "generation\n",
      "(Liu et al.,\n",
      "2019)\n",
      "“Since the goal of dialogue systems is to talk with users...if the\n",
      "systems show discriminatory behaviors in the interactions, the\n",
      "user experience will be adversely affected. Moreover, public com-\n",
      "mercial chatbots can get resisted for their improper speech.”\n",
      "Vague/unstated\n",
      "Stereotyping,\n",
      "other\n",
      "representational\n",
      "harms,\n",
      "questionable\n",
      "correlations\n",
      "Table 3: Examples of the categories into which the papers’ motivations and proposed quantitative techniques for\n",
      "measuring or mitigating “bias” fall. Bold text in the quotes denotes the content that yields our categorizations.\n",
      "\n",
      "et al. (2019); Précenth (2019); Pujari et al. (2019);\n",
      "Ruane et al. (2019); Stanovsky et al. (2019);\n",
      "Sun et al. (2019); Tan and Celis (2019); Webster\n",
      "et al. (2019); Zmigrod et al. (2019); Gyamﬁet al.\n",
      "(2020); Hube et al. (2020); Hutchinson et al.\n",
      "(2020); Kim et al. (2020); Nadeem et al. (2020);\n",
      "Papakyriakopoulos et al. (2020); Ravfogel et al.\n",
      "(2020); Rozado (2020); Sen and Ganguly (2020);\n",
      "Shin et al. (2020); Strengers et al. (2020).\n",
      "Other representational harms\n",
      "Hovy and Sø-\n",
      "gaard (2015); Blodgett et al. (2016); Bolukbasi\n",
      "et al. (2016b); Hovy and Spruit (2016); Blodgett\n",
      "and O’Connor (2017); Larson (2017); Schnoebelen\n",
      "(2017); Blodgett et al. (2018); Curry and Rieser\n",
      "(2018); Díaz et al. (2018); Dixon et al. (2018); Kir-\n",
      "itchenko and Mohammad (2018); Park et al. (2018);\n",
      "Shen et al. (2018); Thelwall (2018); Zhao et al.\n",
      "(2018b); Badjatiya et al. (2019); Bagdasaryan et al.\n",
      "(2019); Bamman et al. (2019); Cao and Daumé\n",
      "(2019); Chaloner and Maldonado (2019); Cho et al.\n",
      "(2019); Davidson et al. (2019); De-Arteaga et al.\n",
      "(2019); Fisher (2019); Font and Costa-jussà (2019);\n",
      "Garimella et al. (2019); Loukina et al. (2019); May-\n",
      "ﬁeld et al. (2019); Mehrabi et al. (2019); Nozza\n",
      "et al. (2019); Prabhakaran et al. (2019); Romanov\n",
      "et al. (2019); Ruane et al. (2019); Sap et al. (2019);\n",
      "Sheng et al. (2019); Sun et al. (2019); Sweeney\n",
      "and Najaﬁan (2019); Vaidya et al. (2019); Gaut\n",
      "et al. (2020); Gencoglu (2020); Hovy et al. (2020);\n",
      "Hutchinson et al. (2020); Kim et al. (2020); Peng\n",
      "et al. (2020); Rios (2020); Sap et al. (2020); Shah\n",
      "et al. (2020); Sheng et al. (2020); Tan et al. (2020);\n",
      "Zhang et al. (2020a,b).\n",
      "Questionable\n",
      "correlations\n",
      "Jørgensen\n",
      "et\n",
      "al.\n",
      "(2015); Hovy and Spruit (2016); Madnani et al.\n",
      "(2017); Rudinger et al. (2017); Zhao et al. (2017);\n",
      "Burns et al. (2018); Dixon et al. (2018); Kir-\n",
      "itchenko and Mohammad (2018); Lu et al. (2018);\n",
      "Park et al. (2018); Shen et al. (2018); Zhang\n",
      "et al. (2018); Badjatiya et al. (2019); Bhargava\n",
      "and Forsyth (2019); Cao and Daumé (2019); Cho\n",
      "et al. (2019); Davidson et al. (2019); Dev et al.\n",
      "(2019); Garimella et al. (2019); Garg et al. (2019);\n",
      "Huang et al. (2019); James-Sorenson and Alvarez-\n",
      "Melis (2019); Kaneko and Bollegala (2019); Liu\n",
      "et al. (2019); Karve et al. (2019); Nozza et al.\n",
      "(2019); Prabhakaran et al. (2019); Romanov et al.\n",
      "(2019); Sap et al. (2019); Sedoc and Ungar (2019);\n",
      "Stanovsky et al. (2019); Sweeney and Najaﬁan\n",
      "(2019); Vaidya et al. (2019); Zhiltsova et al. (2019);\n",
      "Chopra et al. (2020); Gonen and Webster (2020);\n",
      "Gyamﬁet al. (2020); Hube et al. (2020); Ravfogel\n",
      "et al. (2020); Rios (2020); Ross et al. (2020); Saun-\n",
      "ders and Byrne (2020); Sen and Ganguly (2020);\n",
      "Shah et al. (2020); Sweeney and Najaﬁan (2020);\n",
      "Yang and Feng (2020); Zhang et al. (2020a).\n",
      "Vague/unstated\n",
      "Rudinger et al. (2018); Webster\n",
      "et al. (2018); Dinan et al. (2019); Florez (2019);\n",
      "Jumelet et al. (2019); Lauscher et al. (2019); Liang\n",
      "et al. (2019); Maudslay et al. (2019); May et al.\n",
      "(2019); Prates et al. (2019); Prost et al. (2019);\n",
      "Qian et al. (2019); Swinger et al. (2019); Zhao\n",
      "et al. (2019); Zhou et al. (2019); Ethayarajh (2020);\n",
      "Huang et al. (2020); Jia et al. (2020); Popovi´c et al.\n",
      "(2020); Pryzant et al. (2020); Vig et al. (2020);\n",
      "Wang et al. (2020); Zhao et al. (2020).\n",
      "Surveys,\n",
      "frameworks,\n",
      "and\n",
      "meta-analyses\n",
      "Hovy and Spruit (2016); Larson (2017); McCurdy\n",
      "and Serbetçi (2017); Schnoebelen (2017); Basta\n",
      "et al. (2019); Ethayarajh et al. (2019); Gonen and\n",
      "Goldberg (2019); Lauscher and Glavaš (2019);\n",
      "Loukina et al. (2019); Mayﬁeld et al. (2019);\n",
      "Mirzaev et al. (2019); Prabhumoye et al. (2019);\n",
      "Ruane et al. (2019); Sedoc and Ungar (2019); Sun\n",
      "et al. (2019); Nissim et al. (2020); Rozado (2020);\n",
      "Shah et al. (2020); Strengers et al. (2020); Wright\n",
      "et al. (2020).\n",
      "B\n",
      "Full categorization: Techniques\n",
      "Allocational harms\n",
      "De-Arteaga et al. (2019);\n",
      "Prost et al. (2019); Romanov et al. (2019); Zhao\n",
      "et al. (2020).\n",
      "Stereotyping\n",
      "Bolukbasi\n",
      "et\n",
      "al.\n",
      "(2016a,b);\n",
      "Caliskan et al. (2017); McCurdy and Serbetçi\n",
      "(2017); Díaz et al. (2018); Santana et al. (2018);\n",
      "Sutton et al. (2018); Zhang et al. (2018); Zhao\n",
      "et al. (2018a,b); Agarwal et al. (2019); Basta et al.\n",
      "(2019); Bhaskaran and Bhallamudi (2019); Brunet\n",
      "et al. (2019); Cao and Daumé (2019); Chaloner\n",
      "and Maldonado (2019); Dev and Phillips (2019);\n",
      "Ethayarajh et al. (2019); Gonen and Goldberg\n",
      "(2019); James-Sorenson and Alvarez-Melis (2019);\n",
      "Jumelet et al. (2019); Kaneko and Bollegala\n",
      "(2019); Karve et al. (2019); Kurita et al. (2019);\n",
      "Lauscher and Glavaš (2019); Lauscher et al.\n",
      "(2019); Lee et al. (2019); Liang et al. (2019); Liu\n",
      "et al. (2019); Manzini et al. (2019); Maudslay et al.\n",
      "(2019); May et al. (2019); Mirzaev et al. (2019);\n",
      "Prates et al. (2019); Précenth (2019); Prost et al.\n",
      "(2019); Pujari et al. (2019); Qian et al. (2019);\n",
      "\n",
      "Sedoc and Ungar (2019); Stanovsky et al. (2019);\n",
      "Tan and Celis (2019); Zhao et al. (2019); Zhou\n",
      "et al. (2019); Chopra et al. (2020); Gyamﬁet al.\n",
      "(2020); Nadeem et al. (2020); Nissim et al. (2020);\n",
      "Papakyriakopoulos et al. (2020); Popovi´c et al.\n",
      "(2020); Ravfogel et al. (2020); Ross et al. (2020);\n",
      "Rozado (2020); Saunders and Byrne (2020); Shin\n",
      "et al. (2020); Vig et al. (2020); Wang et al. (2020);\n",
      "Yang and Feng (2020); Zhao et al. (2020).\n",
      "Other representational harms\n",
      "Jørgensen et al.\n",
      "(2015); Hovy and Søgaard (2015); Blodgett et al.\n",
      "(2016); Blodgett and O’Connor (2017); Blodgett\n",
      "et al. (2018); Curry and Rieser (2018); Dixon et al.\n",
      "(2018); Park et al. (2018); Thelwall (2018); Web-\n",
      "ster et al. (2018); Badjatiya et al. (2019); Bag-\n",
      "dasaryan et al. (2019); Bamman et al. (2019); Bhar-\n",
      "gava and Forsyth (2019); Cao and Daumé (2019);\n",
      "Font and Costa-jussà (2019); Garg et al. (2019);\n",
      "Garimella et al. (2019); Liu et al. (2019); Louk-\n",
      "ina et al. (2019); Mehrabi et al. (2019); Nozza\n",
      "et al. (2019); Sap et al. (2019); Sheng et al. (2019);\n",
      "Stanovsky et al. (2019); Vaidya et al. (2019);\n",
      "Webster et al. (2019); Ethayarajh (2020); Gaut\n",
      "et al. (2020); Gencoglu (2020); Hovy et al. (2020);\n",
      "Huang et al. (2020); Kim et al. (2020); Peng et al.\n",
      "(2020); Ravfogel et al. (2020); Rios (2020); Sap\n",
      "et al. (2020); Saunders and Byrne (2020); Sheng\n",
      "et al. (2020); Sweeney and Najaﬁan (2020); Tan\n",
      "et al. (2020); Zhang et al. (2020a,b).\n",
      "Questionable correlations\n",
      "Jurgens et al. (2017);\n",
      "Madnani et al. (2017); Rudinger et al. (2017);\n",
      "Zhao et al. (2017); Burns et al. (2018); Díaz\n",
      "et al. (2018); Kiritchenko and Mohammad (2018);\n",
      "Lu et al. (2018); Rudinger et al. (2018); Shen\n",
      "et al. (2018); Bordia and Bowman (2019); Cao\n",
      "and Daumé (2019); Cho et al. (2019); David-\n",
      "son et al. (2019); Dev et al. (2019); Dinan et al.\n",
      "(2019); Fisher (2019); Florez (2019); Font and\n",
      "Costa-jussà (2019); Garg et al. (2019); Huang et al.\n",
      "(2019); Liu et al. (2019); Nozza et al. (2019);\n",
      "Prabhakaran et al. (2019); Qian et al. (2019); Sap\n",
      "et al. (2019); Stanovsky et al. (2019); Sweeney and\n",
      "Najaﬁan (2019); Swinger et al. (2019); Zhiltsova\n",
      "et al. (2019); Zmigrod et al. (2019); Hube et al.\n",
      "(2020); Hutchinson et al. (2020); Jia et al. (2020);\n",
      "Papakyriakopoulos et al. (2020); Popovi´c et al.\n",
      "(2020); Pryzant et al. (2020); Saunders and Byrne\n",
      "(2020); Sen and Ganguly (2020); Shah et al. (2020);\n",
      "Sweeney and Najaﬁan (2020); Zhang et al. (2020b).\n",
      "Vague/unstated\n",
      "None.\n",
      "Surveys,\n",
      "frameworks,\n",
      "and\n",
      "meta-analyses\n",
      "Hovy and Spruit (2016); Larson (2017); McCurdy\n",
      "and Serbetçi (2017); Schnoebelen (2017); Basta\n",
      "et al. (2019); Ethayarajh et al. (2019); Gonen and\n",
      "Goldberg (2019); Lauscher and Glavaš (2019);\n",
      "Loukina et al. (2019); Mayﬁeld et al. (2019);\n",
      "Mirzaev et al. (2019); Prabhumoye et al. (2019);\n",
      "Ruane et al. (2019); Sedoc and Ungar (2019); Sun\n",
      "et al. (2019); Nissim et al. (2020); Rozado (2020);\n",
      "Shah et al. (2020); Strengers et al. (2020); Wright\n",
      "et al. (2020).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b687850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdfs(pdf_folder: str) -> list[dict]:\n",
    "    texts = []\n",
    "    for pdf_file in Path(pdf_folder).glob(\"*.pdf\"):\n",
    "        doc = fitz.open(pdf_file)\n",
    "        text = \"\\n\".join(page.get_text() for page in doc)\n",
    "        texts.append({\"source\": str(pdf_file), \"text\": text})\n",
    "    return texts\n",
    "\n",
    "\n",
    "def chunk_extracted_text(extracted_data):\n",
    "    \"\"\"Splits large texts into smaller, overlapping chunks.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    all_chunks = []\n",
    "    for item in extracted_data:\n",
    "        source_text = item[\"text\"]\n",
    "        chunks = text_splitter.create_documents([source_text])\n",
    "        \n",
    "        for chunk in chunks:\n",
    "             all_chunks.append({\n",
    "                 \"source\": item[\"source\"],\n",
    "                 \"text\": chunk.page_content\n",
    "             })\n",
    "             \n",
    "    print(f\"-> Successfully created {len(all_chunks)} text chunks.\")\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_positive_pairs(chunks: List[str]) -> List[InputExample]:\n",
    "    return [\n",
    "        InputExample(texts=[chunks[i], chunks[i+1]], label=1.0)\n",
    "        for i in range(len(chunks) - 1)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d8ecb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae88ee65e05d46a083d6e80e2a949311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af5d56f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = extract_text_from_pdfs(PDF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d1df98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Successfully created 242 text chunks.\n"
     ]
    }
   ],
   "source": [
    "chunked_text = chunk_extracted_text(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1f2acbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunked_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4e2d526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLP systems, ﬁnding that their motivations\\nare often vague, inconsistent, and lacking\\nin normative reasoning, despite the fact that\\nanalyzing “bias” is an inherently normative\\nprocess.\\nWe further ﬁnd that these papers’\\nproposed quantitative techniques for measur-\\ning or mitigating “bias” are poorly matched to\\ntheir motivations and do not engage with the\\nrelevant literature outside of NLP. Based on\\nthese ﬁndings, we describe the beginnings of a\\npath forward by proposing three recommenda-'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_text[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37363a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = create_positive_pairs(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a1f1881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/Combining-image--voice--and-the-patient-s-questionna_2010_Artificial-Intelli.pdf', 'text': 'Combining image, voice, and the patient’s questionnaire data to categorize\\nlaryngeal disorders\\nAntanas Verikas a,b,*, Adas Gelzinis a, Marija Bacauskiene a, Magnus Ha˚ llander b, Virgilijus Uloza c,\\nMarius Kaseta c\\na Department of Electrical & Control Equipment, Kaunas University of Technology, Studentu 50, LT-51368, Kaunas, Lithuania\\nb Intelligent Systems Laboratory, Halmstad University, Box 823, S 301 18 Halmstad, Sweden\\nc Department of Otolaryngology, Kaunas University of Medicine, Eiveniu 2, LT-50009 Kaunas, Lithuania\\n1. Introduction\\nIn clinical practice, the diagnostic procedure of laryngeal\\ndiseases is based on evaluation of patient’s complaints, history,\\nand data of instrumental as well as histological examination.\\nDuring the last years a variety of techniques for examination of the\\nlarynx and objective measurements of voice quality have been\\ndeveloped [1,2]. Evaluation of larynx has improved signiﬁcantly\\nwith the establishment of the computer tomography (CT) and\\nmagnetic resonance imaging (MRI), as the technologies provide\\ninsights into the endoscopically blind areas and reveal depth of\\ntumour inﬁltration. The technologies may be beneﬁcial in staging\\nlarynx carcinoma and planning the most appropriate surgical\\nprocedure [3–6]. Ultrasonography is useful in cases of larger\\nlaryngeal lesions and may have some role in screening unilateral\\nvocal fold pathologies. At the same time, further ﬁne-tuning of the\\ntechnique may be necessary [7,8].\\nWhen resorting to automated characterization of human\\nlarynx, laryngeal images, voice signal and patient’s questionnaire\\ndata can be considered as the main information sources for the\\ncharacterization.\\nNowadays,\\nautomated\\nanalysis\\nof\\nvoice\\nis\\nArtiﬁcial Intelligence in Medicine 49 (2010) 43–50\\nA R T I C L E\\nI N F O\\nArticle history:\\nReceived 16 September 2008\\nReceived in revised form 19 January 2010\\nAccepted 16 February 2010\\nKeywords:\\nClassiﬁcation committee\\nSupport vector machine\\nMultiple feature sets\\nVariable selection\\nLarynx pathology\\nA B S T R A C T\\nObjective: This paper is concerned with soft computing techniques for categorizing laryngeal disorders\\nbased on information extracted from an image of patient’s vocal folds, a voice signal, and questionnaire\\ndata.\\nMethods: Multiple feature sets are exploited to characterize images and voice signals. To characterize\\ncolour, texture, and geometry of biological structures seen in colour images of vocal folds, eight feature\\nsets are used. Twelve feature sets are used to obtain a comprehensive characterization of a voice signal\\n(the sustained phonation of the vowel sound /a/). Answers to 14 questions constitute the questionnaire\\nfeature set. A committee of support vector machines is designed for categorizing the image, voice, and\\nquery data represented by the multiple feature sets into the healthy, nodular and diffuse classes. Five\\nalternatives to aggregate separate SVMs into a committee are explored. Feature selection and classiﬁer\\ndesign are combined into the same learning process based on genetic search.\\nResults: Data of all the three modalities were available from 240 patients. Among those, 151 patients\\nbelong to the nodular class, 64 to the diffuse class and 25 to the healthy class. When using a single feature\\nset to characterize each modality, the test set data classiﬁcation accuracy of 75.0%, 72.1%, and 85.0% was\\nobtained for the image, voice and questionnaire data, respectively. The use of multiple feature sets\\nallowed to increase the accuracy to 89.5% and 87.7% for the image and voice data, respectively. The test\\nset data classiﬁcation accuracy of over 98.0% was obtained from a committee exploiting multiple feature\\nsets from all the three modalities. The highest classiﬁcation accuracy was achieved when using the SVM-\\nbased aggregation with hyper parameters of the SVM determined by genetic search. Bearing in mind the\\ndifﬁculty of the task, the obtained classiﬁcation accuracy is rather encouraging.\\nConclusions: Combination of both multiple feature sets characterizing a single modality and the three\\nmodalities allowed to substantially improve the classiﬁcation accuracy if compared to the highest\\naccuracy obtained from a single feature set and a single modality. In spite of the unbalanced data sets\\nused, the error rates obtained for the three classes were rather similar.\\n\\x02 2010 Elsevier B.V. All rights reserved.\\n* Corresponding author at: Intelligent Systems Laboratory, Halmstad University,\\nBox 823, S 301 18 Halmstad, Sweden. Tel.: +46 35 167140; fax: +46 35 216724.\\nE-mail addresses: antanas.verikas@hh.se (A. Verikas), adas.gelzinis@ktu.lt\\n(A. Gelzinis), marija.bacauskiene@ktu.lt (M. Bacauskiene), magnus.hallander@hh.se\\n(M. Ha˚ llander), virgilijus.uloza@kmuk.lt (V. Uloza), marius.kaseta@kmuk.lt\\n(M. Kaseta).\\nContents lists available at ScienceDirect\\nArtificial Intelligence in Medicine\\njournal homepage: www.elsevier.com/locate/aiim\\n0933-3657/$ – see front matter \\x02 2010 Elsevier B.V. All rights reserved.\\ndoi:10.1016/j.artmed.2010.02.002\\n\\nincreasingly used for detecting and screening laryngeal patholo-\\ngies [9–16]. It was demonstrated that even telephone-based voice\\nmay lend itself for screening laryngeal disorders [11]. According to\\nHadjitodorov and Mitev, depending on the disease and its stage,\\nthe following changes can be observed in the vocalized voice signal\\nin pathological cases [10]:\\ni Signiﬁcant cycle-to-cycle pitch and amplitude perturbations;\\nii Decrease of the voice signal fundamental frequency and\\namplitude;\\niii Dominance of the ﬁrst harmonic in the signal spectrum;\\niv Presence of a turbulent noise;\\nv Decrease or loss of the harmonics over 1 kHz and presence of\\nsub-harmonics;\\nvi Pauses in the pitch period generation.\\nThere were very few attempts to create systems for automated\\nanalysis of colour laryngeal images. In [17], a technique for\\nautomated categorization of manually marked suspect lesions into\\nhealthy and diseased classes was presented. The categorization is\\nbased on textural features extracted from co-occurrence matrices\\ncomputed from manually marked areas of vocal fold images. The\\nclassiﬁcation accuracy of 81.4% was reported when testing the\\nsystem on a very small set of 35 images. A much larger set of\\nlaryngeal images has been used in studies presented in [18,19]. The\\nalgorithms developed exploit features of various types and do not\\nrequire any manual marking.\\nAttempts to exploit the patient’s questionnaire data for\\nscreening laryngeal disorders are even more scarce. The ques-\\ntionnaire data may carry information, which is not present in the\\nacoustic or visual modalities. In [20], a genetic search and support\\nvector machine (SVM) based technique to categorize the patient’s\\nquestionnaire data was presented. The categorization results\\nprovide an indication on usefulness of the data for screening\\nlaryngeal pathologies.\\nThe long-term goal of this work is a decision support system for\\ndiagnostics of laryngeal diseases. A voice signal, colour images of\\nvocal folds, and questionnaire data are the information sources\\nused in the analysis. This paper is concerned with exploiting the\\nthree information sources mentioned above for categorizing\\nlaryngeal diseases. An SVM is used as classiﬁer to make the\\ncategorization. Variable selection and classiﬁer design is integrated\\ninto the same learning process based on genetic search.\\n2. The data\\nThe mixed gender local database has been used in this study.\\nThe medical task considered in this paper concerns the laryngeal\\ncolour images, voice signal, and the query data based automated\\ncategorization of laryngeal disorders into three decision classes:\\nhealthy and two pathological classes, namely diffuse and nodular\\nmass lesions of vocal folds [18]. The pathological classes can be\\ncharacterized as follows. A rather common, clinically discrimina-\\ntive group of laryngeal diseases was chosen for the analysis, i.e.\\nmass lesions of vocal folds. Mass lesions of vocal folds could be\\ncategorized into six classes, namely, polypus, papillomata, carci-\\nnoma, cysts, keratosis, and nodules. This categorization is based on\\nclinical signs and histological structure of the mass lesions of vocal\\nfolds. We distinguished two groups of mass lesions of vocal folds,\\ni.e. nodular lesions (localized thickenings) – nodules, polyps, and\\ncysts– and diffuse lesions—papillomata, hyperplastic laryngitis with\\nkeratosis, and carcinoma. Clinically, nodular lesions (localized\\nthickenings) visually appear as single lesions of various sizes with a\\nsmooth, regular surface and distinct margins surrounded by a\\nnormal tissue of the vocal fold. Respectively, diffuse lesions\\nvisually appear as irregular, rough, multiple thickenings without\\ndistinct margins, often surrounded by an inﬂamed tissue. It is\\nworth stressing that according to the task of the study, the\\ncategorization into the nodular and diffuse classes was based on\\nvisual appearance of vocal fold mass lesions, evaluated under\\ndirect micro-laryngoscopy. However, the ﬁnal diagnosis was\\nconﬁrmed by histological examination of laryngeal specimens\\nremoved during endolaryngeal microsurgical intervention.\\nLaryngeal images have been recorded at the Department of\\nOtolaryngology, Kaunas University of Medicine, Lithuania. The\\nimages were acquired during routine direct micro-laryngoscopy\\nemploying the Moller-Wedel Universa 300 surgical microscope.\\nThe 3-CCD Elmo colour video camera of 768 \\x02 576 pixels was used\\nto record the images. To lessen the inﬂuence of variation of the\\nimage capturing conditions on image appearance, we apply the\\nmulti-scale\\nretinex\\ntheory-based\\ncolour\\nimage\\nenhancement\\n[21,22]. Details on how the enhancement has been applied can\\nbe found in [18].\\nVoice recordings of the sustained phonation of the vowel\\nsound /a/ (as in the English word ‘‘large’’) are the voice signals\\nutilized. There are three voice recordings from each subject. The\\naverage length of each recording is 2.4 s. The recordings are\\nmade in the ‘‘wav’’ ﬁle format at 44,100 samples/s rate. There\\nare 16 bits allocated for one sample. During preprocessing, the\\nbeginning and the end of each recording was eliminated. The\\nD60S Dynamics Vocal microphone has been used to make the\\nrecordings.\\nThere are fourteen questions in the questionnaire utilized in\\nthis study.\\n3. Feature sets\\n3.1. Features extracted from colour images\\nFig. 1 presents characteristic examples from the three decision\\nclasses considered, namely, nodular, diffuse, and healthy. However,\\nit is worth noting that due to the large variety of appearance of\\nvocal fold mass lesions, the classiﬁcation task can sometimes be\\ndifﬁcult even for a trained physician [23,24].\\nEight types of features are used to characterize colour, texture,\\nand geometry of biological structures seen in colour images of\\nvocal folds [19,18,25]. The list of feature types used is given below.\\nThe features are given by the ﬁrst kernel principal components\\nextracted from each type of measurements. The number of\\ncomponents (features) used is such that 99.5% of variance is\\naccounted for by the components utilized. In the parentheses, the\\nnumber of features used is provided.\\nFig. 1. Images from the nodular (left), diffuse (middle), and healthy (right) classes.\\nA. Verikas et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 43–50\\n44\\n\\n(i) The probability distribution of colour represented by the 3D\\ncolour histogram, (80) [18].\\n(ii) Features calculated from the co-occurrence matrices. A\\npolynomial pðxÞ of degree n\\npðxÞ ¼ p0 þ p1x þ \\x03 \\x03 \\x03 þ pn\\x041xn\\x041 þ pnxn\\n(1)\\nhas been ﬁtted to the values of each of the 14 Haralick’s\\ncoefﬁcients [26] calculated from the co-occurrence matrices\\nevaluated for several distance parameter values. Kernel\\nprincipal components are then extracted from the para-\\nmeters of the polynomials and used as features, (33) [27].\\n(iii) The distribution of responses obtained from the multi-channel\\nGabor ﬁltering. An image is ﬁltered by a bank of Gabor ﬁlters of\\nfrequency f and orientation u. Having the ﬁltered image, a 24-\\nbin histogram of the image is calculated. Thus, having N f\\nfrequencies and Nu orientations, N f \\x02 Nu of such histograms\\nare obtained from one image. Leaving the ﬁrst bin of the\\nhistogramsaside, the other binsare concatenatedintoone long\\nvector. Features are then given by the ﬁrst kernel principal\\ncomponents of the vector, (182) [18].\\n(iv) Fourier spectrum based features characterizing the distribu-\\ntion of image frequencies in frequency rings. The frequency\\nplane is divided into several rings Ri of different average\\nfrequency. The Chi-square xi and the entropy Mi of the\\nFourier power are then computed in each of the rings and\\nused to extract the image frequency content based features\\n(F2), (17) [19].\\n(v) Fourier spectrum based features characterizing the distribu-\\ntion of image frequencies in frequency wedges. To compute\\nthe feature vector, the upper part of the frequency plane is\\ndivided into M equidistant wedges Wi and the average power\\nis computed in each of the wedges. The average power values\\nare used to extract features (F1), (75) [19].\\n(vi) The distribution of the image intensity gradient direction. We\\nuse a histogram to represent the distribution of the gradient\\nangle. The histogram vector is then projected onto the space\\nspanned by the ﬁrst eigenvectors of the kernel covariance\\nmatrix. The vector of the kernel principal components is\\nutilized as a feature vector of this type, (65) [19].\\n(vii) The run-length matrices based features. Seven features,\\nshort-run emphasis, long-run emphasis, grey-level non-uni-\\nformity, run-length non-uniformity, run percentage, low grey\\nlevel run emphasis, and high grey level run emphasis [28] have\\nbeen extracted based on the run-length matrices. Since red\\ncolour dominates in the vocal fold images, the a\\x05ðx; yÞ (red-\\ngreen) image component (L\\x05a\\x05b\\x05 colour space) has been\\nemployed for extracting the run-length matrices based\\nfeatures (27) [25].\\n(viii) Geometrical features characterizing the shape of the edges of\\nvocal folds. Three polynomial curves given by Eq. (1) – one of\\nthe ﬁrst, one of the second, and one of the third order – were\\nﬁtted to the lower part of edges of vocal folds. Thus, in total,\\nwe have 18 parameters pi characterizing the six curves. Fig. 2\\npresents two examples of laryngeal images coming from the\\nhealthy and nodular classes along with the third order\\npolynomial curves found.\\nTo extract geometrical features, two more geometrical\\nmeasurements\\nare\\nmade.\\nA\\nvocal\\nfold\\nimage\\nis\\nﬁrst\\nsegmented into a set of homogenous regions. Two lines,\\nascending in the left-hand part and descending in the right-\\nhand part of the image are then drawn in such a way as to\\nmaximize the number of segmentation boundary points\\nintersecting the lines. Fig. 3 presents two examples of the\\nsegmentation boundaries found and the two lines drawn\\naccording to the determined directions. The ﬁrst geometrical\\nmeasurement is then given by the sum of the squared\\nnumber of the boundary points intersecting the two lines.\\nThe second geometrical measurement is obtained in the same\\nway, except that colour edge points are utilized instead of the\\nsegmentation\\nboundary\\npoints.\\nThese\\ntwo\\ngeometrical\\nmeasurements together with the 18 parameters mentioned\\nabove are then subjected to the kernel principal component\\nanalysis and used as features, (40) [19,25].\\n3.2. Features extracted from a voice signal\\nIn this study, we used 12 different feature sets, presented in the\\nlist below. In the parentheses, the number of features of each type\\nis provided. A comprehensive description of the ﬁrst eleven feature\\nsets can be found in [16].\\n1. Pitch and amplitude perturbation measures, (24).\\n2. Frequency features, (100).\\n3. Mel-frequency features, (35).\\n4. Cepstral energy features, (100).\\n5. Mel-frequency cepstral coefﬁcients, (35).\\n6. Autocorrelation features, (80).\\n7. Harmonics to noise ratio in spectral domain, (11).\\n8. Harmonics to noise ratio in cepstral domain, (11).\\n9. Linear prediction coefﬁcients. It is known that different\\nnumber of coefﬁcients p is required to model male and female\\nvoices [29,30]. According to [29], p ¼ 33 for female and p ¼ 44\\nfor male voices. Since we are dealing with a mixed gender\\ndatabase, we modeled the voice signals two times using p ¼ 33\\nand p ¼ 44. Thus, we have 77 features in total, (77).\\n10. Linear prediction cosine transform coefﬁcients, (77).\\n11. Feature set used in the commercial ‘‘Dr.Speech’’ software, (23).\\n12. Signal shape. Several periods of a voice signal are averaged and\\nrepresented by the signal amplitude at 128 equally spaced\\npoints. Features are given by the amplitude values. Fig. 4\\npresents an example of the voice signal, (128).\\n3.3. Query features\\nThe query data are represented by the following features\\n(components xi of the data vector x):\\nFig. 2. Laryngeal images coming from the nodular (left) and healthy (right) classes\\nalong with two third order curves used to calculate the geometrical features.\\nFig. 3. Vocal cord images coming from the nodular (left) and the healthy (right)\\nclasses along with two lines used to calculate the geometrical feature.\\nA. Verikas et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 43–50\\n45\\n\\n1. Patient’s age;\\n2. Duration of voice disorder (months);\\n3. Patient’s education (ﬁve grades);\\n4. Average duration of intensive speech use (h/day);\\n5. Number of days of intensive speech use (days/week);\\n6. Smoking (yes/no);\\n7. Smoked cigarets/day;\\n8. Smoking duration (years);\\n9. Subjective voice function assessment by the patient on Visual\\nAnalogue Scale. The patients were asked to rate hoarseness on\\nthis scale, ranging from 0 (‘‘no hoarseness’’) to 100 (‘‘severe\\nhoarseness’’);\\n10. Maximum phonation time (s);\\n11. Functional domain of the voice handicap index (VHI) (F);\\n12. Emotional domain of VHI (E);\\n13. Physical domain of VHI (P);\\n14. Voice Handicap Index (VHI) (the maximum value is 120),\\nassessed from answers to questions from a specially designed\\nquestionnaire and was used in this study to evaluate person’s\\nlevel of handicap resulting from a voice disorder [31].\\nThus, in total, the data are represented by 21 feature sets, 8\\nfeature sets are extracted from an image, 12 from a voice signal and\\n1 from a questionnaire.\\n4. The classiﬁer\\n4.1. The basic classiﬁer\\nA support vector machine is used as the basic classiﬁer in this\\nwork. Depending on the deﬁnition of the optimization problem,\\nseveral forms of SVM can be distinguished, for example, 1-norm or\\n2-norm SVM. Since there are examples demonstrating that the 1-\\nnorm SVM outperforms the 2-norm, especially if there are\\nredundant noise features [32], the 1-norm SVM is used in this\\nwork. Assuming that FðxÞ is the non-linear mapping of the data\\npoint x into the new space, the 1-norm soft margin SVM can be\\nconstructed by solving the following minimization problem [33]:\\nmin\\nw;b;g;j \\x04g þ C\\nX\\nN\\ni¼1\\nji\\n(2)\\nsubject to\\nyiðhw;FðxiÞi þ bÞ \\x06g \\x04 ji;ji \\x06 0; kwk2 ¼ 1;\\ni ¼ 1; . . . ; N\\n(3)\\nwhere w is the weight vector, yi ¼ \\x071 is the desired output (\\x071), N\\nis the number of training data points, h i stands for the inner\\nproduct, g is the margin, ji are the slack variables, b is the\\nthreshold, and C is the regularization constant controlling the\\ntrade-off between the margin and the slack variables. The\\ndiscriminant function for a new data point x is given by\\nfðxÞ ¼ H\\nX\\nN\\ni¼1\\na\\x05\\ni y jkðx; xiÞ þ b\\x05\\n\"\\n#\\n;\\n(4)\\nwhere kðx; xiÞ stands for the kernel and the Heaviside function\\nH½yðxÞ\\x08 ¼ \\x041, if yðxÞ \\t 0 and H½yðxÞ\\x08 ¼ 1 otherwise. In this work,\\nthe Gaussian kernel, kðxi; x jÞ ¼ exp f\\x04jjxi \\x04 x jjj2=sg, has been\\nused. The optimal values a\\x05\\ni ; b\\x05 of the parameters ai and b are found\\nduring training. The salient variables xi, the regularization constant\\nC and the Gaussian width s have been found by the genetic search.\\nSince an SVM is a binary classiﬁer while the task is to\\ndistinguish between three classes, the one-against-one scheme is\\nused to make the categorization. To make a decision, outputs of the\\nbinary SVM are converted into probabilities and the class\\ncorresponding to the highest probability is chosen.\\n4.2. The committee\\nA separate SVM is used for each set of features. Decisions\\nobtained from the separate SVMs are then combined into a\\ncommittee decision. Five aggregation alternatives, discussed in\\nSection 6.2, have been studied. SVM committees have been\\nsuccessfully used in several studies, for instance concerning facial\\nexpression\\nrecognition\\n[34,35].\\nOne\\ncan\\nalso\\nconsider\\nthe\\nAdaBoost-based approach to committee design. However, the\\nAdaBoost tends to over-ﬁtting for higher noise levels [36].\\n5. Feature selection and classiﬁer design\\nThe issue of selecting an optimal subset of relevant features\\nplays also an important role in successful design of a pattern\\nrecognition system. Some of features, that can be measured in\\nmany pattern recognition applications, may be redundant or even\\nirrelevant. Usually better performance may be achieved by\\ndiscarding such features. Moreover, as the number of features\\nused grows, the number of training samples required grows\\nexponentially. Therefore, in many practical applications we need\\nto reduce the dimensionality of the data.\\nFeature selection in general is a difﬁcult problem. In a general\\ncase, only an exhaustive search can guarantee an optimal solution.\\nA large variety of feature selection techniques that result in a sub-\\noptimal feature set have been proposed [37,38], ranging from the\\nsequential forward and backward selection to sequential forward\\nﬂoating selection [39], genetic [40], tabu [41], or branch and bound\\nalgorithm-based search [42]. Genetic search-based selection is one\\nof the most promising approaches to feature selection. However,\\ngenetic search procedures are rather time consuming. Therefore,\\nwhen selecting features for individual classiﬁers (SVMs), we\\nreduced the number of features based on the feature saliency-\\nbased ranking ﬁrst and then applied genetic search to the\\nremaining set of features. Such feature reduction was applied to\\nthe feature sets containing more than 32 features only. This value\\nwas found experimentally. When designing a classiﬁer not only a\\nfeature set, but also the hyper-parameter values of the classiﬁer are\\nto be selected. Since we use an SVM with the Gaussian kernel as a\\nbasic classiﬁer, values of two hyper-parameters, namely the kernel\\nwidth s and the regularization constant C are to be selected.\\nGenetic search has been used to ﬁnd values of these parameters.\\nWe combine selection of features and values of the hyper\\nparameters into one learning process based on genetic search.\\nFig. 4. Several superimposed periods and the averaged (bold line) voice signal.\\nA. Verikas et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 43–50\\n46\\n\\n5.1. Feature saliency\\nThe feature saliency measure used in this work is based on two\\nfactors, namely, the fuzzy derivative of the classiﬁer output with\\nrespect to the feature and the similarity between the feature and\\nthe feature set [43]. Having a feature set F, the following feature\\nsaliency score G i is assigned to the i th feature [43]:\\nG i ¼\\nb\\x02i\\nmax j¼1;...;N\\x02 j\\nþ ð1 \\x04 bÞ ¯li;F\\n(5)\\nwhere \\x02i is the classiﬁer output sensitivity-based feature saliency\\nmeasure, ¯li;F stands for the average similarity between the i th\\nfeature and the set F, N is the number of features and b is a\\nparameter.\\nThe classiﬁer output sensitivity based saliency measure for the i\\nth feature is given by [43]\\n\\x02i ¼ 1\\nPQ\\nX\\nP\\np¼1\\nX\\nQ\\nj¼1\\nj ¯y0\\njð ˜Di pÞj\\n(6)\\nwhere P is the number of data points, ˜Di p is the p th fuzzy location\\nfor the i th feature, Q is the number of classes (outputs) and ¯y0\\njð ˜Di pÞ\\nis the defuzziﬁed value of the derivative y0\\njð ˜Di pÞ of the j th output\\nwith respect to the input feature xi at the fuzzy location ˜Di p. The\\nderivative y0\\njð ˜Di pÞ is a fuzzy set. The average similarity ¯li;F is\\ncalculated as follows [43]. Let for two features i and j lði; jÞ be\\ngiven by [44]\\nlði; jÞ ¼ 1\\n2 ½varðiÞ þ varð jÞ\\n\\x04\\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\\n½varðiÞ þ varð jÞ\\x082 \\x04 4varðiÞvarð jÞ½1 \\x04 rði; jÞ2\\x08\\nq\\n\\x08\\n(7)\\nwhere varðiÞ stands for the feature i variance and rði; jÞ is the\\ncoefﬁcient of correlation between the features i and j.\\nIn the multi-class case, we calculate the average value:\\n¯lði; jÞ ¼ 1\\nQ\\nX\\nQ\\nk¼1\\nlkði; jÞ\\n(8)\\nwhere Q is the number of classes and lkði; jÞ stands for the\\nmeasure value calculated using data coming from the k th class.\\nThe measure is normalized:\\n¯lnði; jÞ ¼\\n¯lði; jÞ\\nmax i; j 2 F ¯lði; jÞ\\n(9)\\nwith F being a feature set. The similarity between the feature i and\\nthe set F is then given by\\n¯li;F ¼ min\\nj 2 F\\n¯lnði; jÞ\\n(10)\\n5.2. Genetic search\\nThe most important issues to consider when solving a problem\\nby genetic search are encoding of the problem into a chromosome\\nand evaluation, where the genetic representation of the problem\\nand the ﬁtness function for evaluating the suggested solution are\\ndeﬁned [45]. Once the encoding and evaluation are deﬁned, GA\\nrandomly generates the population of the probable solutions in the\\nform of chromosomes. Members of the population are evaluated\\nusing the ﬁtness function and, based on the evaluation results, a\\nportion of the members are selected for subjection to the genetic\\noperations. The higher the ﬁtness function value, the higher is the\\nselection probability. Crossover and mutation are the genetic\\noperations applied. In crossover, pairs of parents are combined to\\ncreate new chromosomes called offsprings. In mutation, random\\nchanges on the genes are introduced. Thus, information repre-\\nsentation in a chromosome, generation of initial population,\\nevaluation of population members, selection, crossover, mutation,\\nand reproduction (survival) are the issues to consider when\\ndesigning a genetic search algorithm.\\nIn our case, a chromosome contains all the information needed\\nto build an SVM classiﬁer. We divide the chromosome into three\\nparts. One part encodes the regularization constant C, one the\\nkernel width parameter s, and the third one encodes the inclusion/\\nnoninclusion of features. To generate the initial population, the\\nfeatures are masked randomly and values of the parameters C and\\ns are chosen randomly from the interval ½C0 \\x04 DC; C0 þ DC\\x08 and\\n½s0 \\x04 Ds;s0 þ Ds\\x08, respectively, where C0 and s0 are the very\\napproximate parameter values obtained from the experiment. The\\nﬁtness function used to evaluate chromosomes is given by the\\nclassiﬁcation accuracy of the validation set data.\\nThe selection process of a new population is governed by the\\nﬁtness values. The selection probability of the i th chromosome pi\\nis given by\\npi ¼\\nri\\nPM\\nj¼1 r j\\n(11)\\nwhere ri is the correct classiﬁcation rate obtained from the\\nclassiﬁer encoded in the i th chromosome and M is the population\\nsize.\\nThe crossover operation for two selected chromosomes is\\nexecuted with the probability of crossover pc. If a generated\\nrandom number from the interval [0,1] is smaller than the\\ncrossover probability pc, the crossover operation is executed.\\nCrossover is performed separately in each part of a chromosome.\\nThe crossover point is randomly chosen in the ‘‘feature mask’’ part\\nand two parameter parts. The corresponding parts of two\\nchromosomes selected for the crossover operation are exchanged\\nat the chosen points.\\nThe mutation operation adopted is such that each gene is\\nselected for mutation with the probability pm. The mutation\\noperation is executed independently in each chromosome part. If\\nthe gene selected for mutation is in the feature part of the\\nchromosome, the value of the bit representing the feature in the\\nfeature mask (0 or 1) is reversed. To execute mutation in the\\nparameter part of the chromosome, the value of the offspring\\nparameter determined by the selected gene is mutated by \\x07Dg,\\nwhere g stands for C or s, as the case may be. The mutation sign is\\ndetermined by the ﬁtness values of the two chromosomes, namely\\nthe sign resulting into a higher ﬁtness value is chosen. The way of\\ndetermining the mutation amplitude Dg is somewhat similar to\\nthat used in [46] and is given by\\nDg ¼ wbðmax ðjg \\x04 g p1j; jg \\x04 g p2jÞÞ\\n(12)\\nwhere g is the actual parameter value of the offspring, p1 and p2\\nstand for parents, b 2 ½0; 1\\x08 is a random number, and w is the weight\\ndecaying with the iteration number:\\nw ¼ kat\\n(13)\\nwhere t is the iteration number, a ¼ 0:95 and k is a constant. The\\nconstant k deﬁnes the initial mutation amplitude. The value of\\nk ¼ 0:4 worked well in our tests.\\nIn the reproduction process, the newly generated offspring\\nreplaces the chromosome with the smallest ﬁtness value in the\\ncurrent population, if a generated random number from the\\ninterval [0,1] is smaller than the reproduction probability pr or if\\nthe ﬁtness value of the offspring is larger than that of the\\nchromosome with the smallest ﬁtness value.\\nA. Verikas et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 43–50\\n47\\n\\n6. Experimental investigations\\n6.1. Experimental setup\\nData of all the three types were available from 240 patients.\\nAmongst those, 151 patients belong to the nodular class, 64 to the\\ndiffuse class and 25 to the healthy class. The data were randomly\\nsplit into the learning set Sl containing data of 200 patients and the\\ntest set St with data of 40 patients. In all the tests involving\\nestimation of the classiﬁcation accuracy, we run an experiment 25\\ntimes with different random split of the data set into the learning\\nand tests subsets. The results presented here are average values\\ncalculated from such 25 runs. In addition to this set of 240 patients,\\nthere were available data from patients characterized by only one\\nor two data modalities, out of the three. These data have been used\\nfor training only. Several voice and image recordings were\\navailable from one patient. Table 1 presents statistics of the data\\nused in the experiments. The data used were normalized to zero\\nmean and variance one.\\nThe genetic search lasted for 80 generations with the following\\nparameters: the population size was set to 75, the number of\\noffsprings produced for creating the next population was equal to\\n40, and the probability of including a variable into the initial\\nchromosome was set to 0.3. The values of crossover, mutation and\\nreproduction probabilities were found experimentally. The follow-\\ning values worked well in the tests: pc ¼ 0:95, pm ¼ 0:02, and\\npr ¼ 0:05. The appropriate b value in Eq. (5) has been found to be\\nb ¼ 0:6.\\n6.2. Results\\nIn the ﬁrst experiment, a separate SVM was used for each type\\nof features. The genetic search was applied for ﬁnding both values\\nof the hyper-parameters C and s, and relevant features. Features in\\nthe sets containing more than 32 features were ﬁrst ranked using\\nthe saliency score G i and then the ﬁrst 32 features were subjected\\nto the genetic search. Table 2 summarizes results of the tests. In\\nTable 2, shown is the number of features providing the best\\nperformance along with the test data set classiﬁcation accuracy.\\nSurprisingly enough, the questionnaire features exhibited the\\nhighest classiﬁcation accuracy. It seems that features extracted\\nfrom images of vocal folds are more informative than those\\ncharacterizing voice signals.\\nIn the next experiment, multiple feature sets have been used to\\nsolve the classiﬁcation task. The a posteriori probabilities obtained\\nfrom the separate SVMs were aggregated in different ways. Five\\naggregation alternatives presented below have been considered.\\n1. The class a posteriori probabilities obtained using a separate\\nfeature set for each SVM (previous experiment) are averaged\\nexploiting all the separate classiﬁers.\\n2. The average class a posteriori probabilities are calculated\\nusing not all, but selected separate classiﬁers. A technique of\\nsequential forward member inclusion was applied. The ﬁrst\\nmember selected for classiﬁcation was the one providing the\\nhighest classiﬁcation accuracy. The classiﬁer included into the\\ncommittee in the j step of the designing procedure was that\\nproviding the highest classiﬁcation accuracy of the commit-\\ntee. The process continued until all the members were\\nincluded into the committee. Thus, a series of committees\\nwith the increasing number of members was created. The\\nﬁnal committee chosen was that providing the highest\\nperformance.\\n3. The a posteriori probabilities obtained from the separate SVMs\\nare considered as new variables. Outputs of the separate SVMs\\nare\\naggregated\\ninto\\na\\ncommittee\\noutput\\nvia\\nthe\\nk-NN\\nclassiﬁcation rule applied to these variables. The Euclidean\\ndistance measure is used to ﬁnd the nearest neighbors. All three\\noutputs of all the separate SVMs are used to calculate the\\ndistance.\\n4. The same as the third alternative, except that the sequential\\nforward variable selection is used to determine the variables\\nused to calculate the distance.\\nTable 3\\nThe average test set data classiﬁcation accuracy (%) obtained from the ﬁve aggregation alternatives, along with the average number of new variables (given in the\\nparentheses) used in the classiﬁcation.\\nFeatures\\nVoice\\nImage\\nV + I\\nAll\\nAggregation\\n1\\n69:7 \\x07 2:6 (36.0)\\n71:3 \\x07 2:7 (24.0)\\n69:7 \\x07 2:9 (60.0)\\n71:6 \\x07 2:9 (63.0)\\n2\\n80:4 \\x07 1:9 (9.6)\\n81:3 \\x07 2:1 (9.0)\\n86:0 \\x07 2:0 (13.5)\\n91:7 \\x07 1:7 (10.5)\\n3\\n74:8 \\x07 2:2 (36.0)\\n78:6 \\x07 2:8 (24.0)\\n83:6 \\x07 2:5 (60.0)\\n87:6 \\x07 2:9 (63.0)\\n4\\n86:4 \\x07 1:9 (9.1)\\n87:0 \\x07 2:5 (6.3)\\n91:1 \\x07 1:9 (6.4)\\n95:7 \\x07 1:2 (5.9)\\n5\\n87:7 \\x07 1:8 (14.5)\\n89:5 \\x07 2:4 (6.9)\\n94:5 \\x07 1:7 (22.6)\\n98:5 \\x07 0:8 (22.6)\\nTable 1\\nThe statistics of the data used in the experiments.\\nData type\\nPatients\\nNodular\\nDiffuse\\nHealthy\\nRecords\\nVoice\\n316\\n151\\n64\\n101\\n948\\nImage\\n270\\n157\\n78\\n35\\n1349\\nQuestionnaire\\n260\\n157\\n78\\n25\\n260\\nTable 2\\nThe test set data classiﬁcation accuracy obtained from the separate classiﬁers.\\nFeature type\\nN # features\\nClassiﬁcation accuracy (%)\\nVoice\\nPerturbation\\n13\\n65.5\\nFrequency\\n17\\n66.0\\nMel-frequency\\n3\\n61.7\\nCepstrum\\n45\\n69.0\\nMel-coefﬁcients\\n9\\n67.0\\nAutocorrelation\\n5\\n61.6\\nHNR-spectral\\n4\\n60.0\\nHNR-cepstral\\n4\\n60.0\\nLP-coefﬁcients\\n14\\n67.9\\nLPCT-coefﬁcients\\n22\\n72.1\\nDrSpeech\\n11\\n66.4\\nSignal shape\\n21\\n69.9\\nImage\\nColour\\n13\\n69.6\\nCo-occurrence\\n15\\n75.0\\nGabor\\n19\\n65.6\\nFrequency, F2\\n9\\n63.3\\nFrequency, F1\\n21\\n69.8\\nGradient\\n14\\n71.7\\nRun-length\\n10\\n68.9\\nGeometrical\\n13\\n76.4\\nQuestionnaire\\nQuestionnaire\\n8\\n85.0\\nA. Verikas et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 43–50\\n48\\n\\n5. Aggregation by an SVM classiﬁer. The new variables are used as\\nfeatures for the classiﬁer. Both the hyper-parameters and\\nfeatures are selected via the genetic search. Features are selected\\nfrom all the new variables.\\nTable 3 presents the average test set data classiﬁcation accuracy\\nobtained from the ﬁve aggregation alternatives, where the 95%\\nconﬁdence interval is also provided. In the parentheses, the\\naverage number of new variables used in the classiﬁcation is\\npresented.\\nAs can be seen from Table 3, the classiﬁcation accuracy greatly\\ndepends on the type of aggregation applied. Simple averaging\\n(alternative 1) even deteriorates the classiﬁcation accuracy if\\ncompared to the best single classiﬁer (see Table 2). It is well known,\\nthat averaging is a robust technique to aggregate decisions and\\nvery often improves the classiﬁcation accuracy [47,48]. In our case,\\nhowever, some single classiﬁers provide a very low classiﬁcation\\naccuracy. Therefore, the simple aggregation does not improve\\nclassiﬁcation\\naccuracy\\nover\\nthe\\nbest\\nsingle\\nclassiﬁer.\\nThe\\nclassiﬁcation accuracy is improved considerably when aggregating\\nthe most appropriate selected classiﬁers; the alternative 2 in\\nTable 3. The same pattern of behavior is observed when using the\\nk-NN\\nclassiﬁer;\\nthe\\n3rd\\nand\\n4th\\nalternatives.\\nThe\\nhighest\\nclassiﬁcation accuracy is achieved when using the genetic search\\nbased aggregation exploiting the space of all the new variables (the\\nclass a posteriori probabilities). However, the rather simple\\naggregation by the k-NN classiﬁer is also very effective.\\nAs can be seen from Tables 2 and 3, the classiﬁcation accuracies\\nobtained using the three modalities (voice, image, and ques-\\ntionnaire data) are rather similar; 87.7%, 89.5% and 85.0%,\\nrespectively. However, when exploiting all the three modalities,\\na considerable improvement in the accuracy is obtained.\\nTo further explore the classiﬁcation results, we present two\\nconfusion matrices for two aggregation alternatives providing the\\nhighest classiﬁcation accuracy (alternative 4 and 5 for the all\\nfeatures case). The matrices were computed for the test set data\\nand are given in Tables 4 and 5. We have 40 test data points in one\\nexperiment. However, we run the experiment 25 times using\\ndifferent random split of the available data into the learning and\\ntest sets. Thus, the numbers presented in Tables 4 and 5 are the\\naverage numbers calculated from these 25 trials. The confusion\\nmatrices reveal that, in spite of the unbalanced data sets, the error\\nrates obtained for the three classes are rather similar. For example,\\nwhen using the 5th aggregation alternative, correctly classiﬁed are\\n99.1%, 97.1%, and 98.5% of the test data coming from the nodular,\\ndiffuse, and the healthy classes, respectively.\\nThe developed algorithms were implemented using MATLAB\\nand C++ programming languages. The C++ based implementation is\\nused in the operating phase (after training). Feature extraction is\\nthe most time-consuming part of the analysis in both image and\\nvoice processing. In the operating phase, processing of one image\\nand one voice record take approximately 4 and 1 s, respectively,\\nwhen using 2.0 GHz Laptop. By optimizing code of several feature\\nextraction subroutines, the processing time can be reduced to\\nabout 1 s. The time required to process query data is negligible if\\ncompared to the image or voice processing. Currently, the analysis\\nis performed off-line, since different hardware is used to record\\nimage and voice data.\\n7. Conclusions\\nA collection of soft computing techniques was developed for\\ncategorizing laryngeal disorders based on information extracted\\nfrom three modalities: an image of patient’s vocal folds, a voice\\nsignal and questionnaire data. The data represented by multiple\\nfeature sets were categorized into the healthy, nodular and diffuse\\nclasses. The effectiveness of single SVM classiﬁers as well as\\ncommittees of classiﬁers was studied.\\nIt was found that features extracted from images of vocal folds\\nare more informative than those characterizing voice signals.\\nHowever, surprisingly enough, the questionnaire features exhib-\\nited the highest classiﬁcation accuracy. Five alternatives to\\naggregate information available from multiple feature sets were\\nconsidered. The classiﬁcation accuracy obtained greatly depended\\non the type of aggregation applied. Due to classiﬁers of rather low\\naccuracy,\\nsimple\\naveraging\\nof\\nall\\navailable\\nclassiﬁers\\neven\\ndeteriorated the classiﬁcation accuracy if compared to the best\\nsingle classiﬁer. The classiﬁcation accuracy improved considerably\\nwhen aggregating the most appropriate selected classiﬁers. The\\nhighest classiﬁcation accuracy was achieved when using the SVM\\nand genetic search based aggregation exploiting the space the class\\na posteriori probabilities. The SVM based aggregation provided a\\nhigher classiﬁcation accuracy than the k-NN approach.\\nWhen\\nexploiting\\nmultiple\\nfeature\\nsets,\\na\\nrather\\nsimilar\\nclassiﬁcation accuracy was obtained from the three modalities.\\nCombining information from all the three modalities a consider-\\nable improvement in classiﬁcation accuracy was obtained. When\\ntesting the developed tools on the set of data collected from 240\\npatients, the classiﬁcation accuracy of over 98.0% was achieved.\\nBearing in mind the ambiguity of the classes, the obtained\\nclassiﬁcation accuracy is rather encouraging.\\nAcknowledgement\\nWe acknowledge the support from the agency for international\\nscience and technology development programmes in Lithuania\\n(COST Action 2103).\\nReferences\\n[1] Mafee MF, Valvassori GE, Becker M. Imaging of the head and neck. Thieme;\\n2005.\\n[2] Uloza V, Saferis V, Uloziene I. Perceptual and acoustic assessment of voice\\npathology and the efﬁcacy of endolaryngeal phonomicrosurgery. Journal of\\nVoice 2005;19(1):138–45.\\n[3] Rumboldt Z, Gordon L, Ackermann RBS. Imaging in head and neck cancer.\\nCurrent Treatment Options in Oncology 2006;7(1):23–34.\\n[4] Rufﬁng S, Struffert T, Reith AGW. Imaging diagnostics of the pharynx and\\nlarynx. Radiologe 2005;45(9):828–36.\\nTable 4\\nConfusion matrix for the ‘All features’’ case and the 4th aggregation alternative\\nproviding the classiﬁcation accuracy of 95:7 \\x07 1:2%.\\nPrediction\\nTrue\\nNodular\\nDiffuse\\nHealthy\\nTotal\\nNodular\\n23.40\\n1.06\\n0.09\\n24.55\\nDiffuse\\n0.58\\n9.77\\n0\\n10.35\\nHealthy\\n0\\n0\\n5.09\\n5.09\\nTotal\\n23.98\\n10.84\\n5.18\\n40.00\\nTable 5\\nConfusion matrix for the ‘‘All features’’ case and the 5th aggregation alternative\\nproviding the classiﬁcation accuracy of 98:5 \\x07 0:8%.\\nPrediction\\nTrue\\nNodular\\nDiffuse\\nHealthy\\nTotal\\nNodular\\n23.77\\n0.29\\n0.08\\n24.15\\nDiffuse\\n0.21\\n10.53\\n0\\n10.74\\nHealthy\\n0\\n0.009\\n5.10\\n5.11\\nTotal\\n23.98\\n10.84\\n5.18\\n40.00\\nA. Verikas et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 43–50\\n49\\n\\n[5] Hasso AN, Tang T. Magnetic resonance imaging of the pharynx and larynx.\\nTopics in Magnetic Resonance Imaging 1994;6(4):224–40.\\n[6] Hoorweg JJ, Kruijt RH, Heijboer RJ, Eijkemans MJ, Kerrebijn JD. Reliability of\\ninterpretation of CT examination of the larynx in patients with glottic lar-\\nyngeal\\ncarcinoma.\\nArchives\\nof\\nOtolaryngology-Head\\n&\\nNeck\\nSurgery\\n2006;135(1):129–34.\\n[7] Rubin JS, Lee S, McGuinness J, Hore I, Hill D, Berger L. The potential role of\\nultrasound in differentiating solid and cystic swellings of the true vocal fold.\\nJournal of Voice 2004;18(2):231–5.\\n[8] Schade G, Kothe C, Leuwer R. Sonography of the larynx—an alternative to\\nlaryngoscopy? HNO 2003;51(7):585–90.\\n[9] Boyanov B, Hadjitodorov S. Acoustic analysis of pathological voices. A voice\\nanalysis system for the screening of laryngeal diseases. IEEE Engineering in\\nMedicine and Biology Magazine 1997;16:74–82.\\n[10] Hadjitodorov S, Mitev P. A computer system for acoustic analysis of patho-\\nlogical voices and laryngeal diseases screening. Medical Engineering & Physics\\n2002;24:419–29.\\n[11] Moran RJ, Reilly RB, de Chazal P, Lacy PD. Telephony-based voice pathology\\nassessment using automated speech analysis. IEEE Transaction on Biomedical\\nEngineering 2006;53(3):468–77.\\n[12] Umapathy K, Krishnan S, Parsa V, Jamieson DG. Discrimination of pathological\\nvoices using a time-frequency approach. IEEE Transaction on Biomedical\\nEngineering 2005;52(3):421–30.\\n[13] Hadjitodorov S, Boyanov B, Teston B. Laryngeal pathology detection by means\\nof class-speciﬁc neural maps. IEEE Transaction on Information Technology in\\nBiomedicine 2000;4(1):68–73.\\n[14] Godino-Llorente JI, Gomez-Vilda P. Automatic detection of voice impairments\\nby means of short-term cepstral parameters and neural network based\\ndetectors. IEEE Transaction on Biomedical Engineering 2004;51(2):380–4.\\n[15] de Oliveira Rosa M, Pereira JC, Grellet M. Adaptive estimation of residue signal\\nfor voice pathology diagnosis. IEEE Transaction on Biomedical Engineering\\n2000;47(1):96–104.\\n[16] Gelzinis A, Verikas A, Bacauskiene M. Automated speech analysis applied to\\nlaryngeal disease categorization. Computer Methods and Programs in Biome-\\ndicine 2008;91(1):36–47.\\n[17] Ilgner JFR, Palm C, Schutz AG, Spitzer K, Westhofen M, Lehmann TM. Colour\\ntexture analysis for quantitative laryngoscopy. Acta Oto-Laryngologica\\n2003;123(6):730–4.\\n[18] Verikas A, Gelzinis A, Bacauskiene M, Uloza V. Towards a computer-aided\\ndiagnosis system for vocal cord diseases. Artiﬁcial Intelligence in Medicine\\n2006;36(1):71–84.\\n[19] Verikas A, Gelzinis A, Valincius D, Bacauskiene M, Uloza V. Multiple feature\\nsets based categorization of laryngeal images. Computer Methods and Pro-\\ngrams in Biomedicine 2007;85(3):257–66.\\n[20] Verikas A, Gelzinis A, Bacauskiene M, Uloza V, Kaseta M. Using the patient’s\\nquestionnaire data to screen laryngeal disorders. Computers in Biology and\\nMedicine 2009;39(2):148–55.\\n[21] Jobson DJ, Rahaman Z, Woodell GA. Properties and performance of a center/\\nsurround retinex. IEEE Transaction on Image Processing 1997;6(3):451–62.\\n[22] Rahman Z, Jobson DJ, Woodell GA. Retinex processing for automatic image\\nenhancement. Journal of Electronic Imaging 2004;13(1):100–10.\\n[23] Dikkers FG, Nikkels PG. Benign lesions of the vocal folds: histopathology and\\nphonotrauma. Annals of Otology Rhinology and Laryngology 1995;104(9/\\n1):698–703.\\n[24] Poels PJP, de Jong FICRS, Schutte HK. Consistency of the preoperative and\\nintraoperative diagnosis of benign vocal fold lesions. Journal of Voice\\n2003;17(3):425–33.\\n[25] Verikas A, Gelzinis A, Bacauskiene M, Uloza V. Intelligent vocal cord image\\nanalysis for categorizing laryngeal diseases.\\nIn: Ali M, Esposito F, editors.\\nLecture notes in artiﬁcial intelligence, vol. 3533. Berlin/Heidelberg: Springer;\\n2005. p. 69–78.\\n[26] Haralick RM, Shanmugam K, Dinstein I. Textural features for image classiﬁca-\\ntion. IEEE Transaction on System Man and Cybernetics 1973;3(6):610–21.\\n[27] Gelzinis A, Verikas A, Bacauskiene M. Increasing the discrimination power of\\nthe co-occurrence matrix-based features. Pattern Recognition 2007;40(9):\\n2367–72.\\n[28] Galloway MM. Texture analysis using gray level run lengths. Computer\\nGraphics and Image Processing 1975;4:172–9.\\n[29] Markel JD, Gray AH. Linear prediction of speech. Berlin: Springer; 1976.\\n[30] Manfredi C, Peretti G. A new insight into post-surgical objective voice\\nquality evaluation. IEEE Transaction on Biomedical Engineering 2006;53:\\n442–51.\\n[31] Jacobson B, Jonhson A, Grywalski C, Silbergleit A. The Voice Handicap Index\\n(VHI): development and validation. American Journal of Speech-Language\\nPathology 1997;6(1):66–9.\\n[32] Zhu J, Hastie SRT, Tibshirani R. 1-Norm support vector machines. In: Thrun S,\\nSaul LK, Scholkopf B, editors. Advances in neural information processing\\nsystems, vol. 16. Cambridge, MA, USA: MIT Press; 2004. p. 49–56.\\n[33] Shawe-Taylor J, Cristianini N. Kernel methods for pattern analysis. Cambridge,\\nUK: Cambridge University Press; 2004.\\n[34] Hernandez B, Olague G, Hammoud RI, Trujillo L, Romero E. Visual learning of\\ntexture descriptors for facial expression recognition in thermal imagery.\\nComputer Vision and Image Understanding 2007;106(2–3):258–69.\\n[35] Olague G, Hammoud R, Trujillo L, Hernandez B, Romero E. Facial expression\\nrecognition in nonvisual imagery. In: Hammoud RI, editor. Augmented vision\\nperception in infrared. London: Springer; 2009. p. 213–39.\\n[36] Ratsch G, Onoda T, Muller KR. Soft margins for adaboost. Machine Learning\\n2001;42(3):287–320.\\n[37] Kudo M, Sklansky J. Comparison of algorithms that select features for pattern\\nclassiﬁers. Pattern Recognition 2000;33(1):25–41.\\n[38] Verikas A, Bacauskiene M. Feature selection with neural networks. Pattern\\nRecognition Letters 2002;23(11):1323–35.\\n[39] Pudil P, Novovicova J, Somol P. Feature selection toolbox software package.\\nPattern Recognition Letters 2002;23:487–92.\\n[40] Yu S, Backer SG, Scheunders P. Genetic feature selection combined with\\ncomposite fuzzy nearest neighbor classiﬁers for hyperspectral satellite ima-\\ngery. Pattern Recognition Letters 2002;23(1–3):183–90.\\n[41] Zhang H, Sun G. Feature selection using tabu search method. Pattern Recogni-\\ntion 2002;35:701–11.\\n[42] Chen XW. An improved branch and bound algorithm for feature selection.\\nPattern Recognition Letters 2003;24(12):1925–33.\\n[43] Verikas A, Bacauskiene M, Valincius D, Gelzinis A. Predictor output sensitivity\\nand\\nfeature\\nsimilarity-based\\nfeature\\nselection.\\nFuzzy\\nSets\\n&\\nSystems\\n2008;159:422–34.\\n[44] Mitra P, Murthy CA, Pal SK. Unsupervised feature selection using feature\\nsimilarity.\\nIEEE\\nTransaction\\non\\nPattern\\nAnalysis\\nMachine\\nIntelligence\\n2002;24(3):301–12.\\n[45] Konak A, Coit DW, Smith AE. Multi-objective optimization using genetic\\nalgorithms:\\na\\ntutorial.\\nReliability\\nEngineering\\nand\\nSystem\\nSafety\\n2006;91(9):992–1007.\\n[46] Leung KF, Leung FHF, Lam HK, Ling SH. Application of a modiﬁed neural fuzzy\\nnetwork and an improved genetic algorithm to speech recognition. Neural\\nComputing & Applications 2007;16(4/5):419–31.\\n[47] Taniguchi M, Tresp V. Averaging regularized estimators. Neural Computation\\n1997;9:1163–78.\\n[48] Verikas A, Lipnickas A, Malmqvist K, Bacauskiene M, Gelzinis A. Soft combina-\\ntion of neural classiﬁers: a comparative study. Pattern Recognition Letters\\n1999;20:429–44.\\nA. Verikas et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 43–50\\n50\\n'}, {'source': '/Users/sir/Downloads/Data/PDF/Introduction to API for Data Science.pdf', 'text': \"Home\\nData Mining\\nAn Introduction to APIs (Application Programming Interfaces) &...\\n 8 min read\\nIntroduction\\nIf you are in tech domain, you will invariably bump in references to something called an “API”. You\\njust can’t skip it – if you do, you are bound\\xa0 to hear it again. APIs\\xa0 are being used almost\\neverywhere. But, if you have ever wondered what exactly is an API? or\\xa0Why are they important?\\nor How do they help? this article will help you out.\\nIn this article, I will explain what is an API in simple terms. I will tell various categories / types of\\nAPI. Then I will be introducing you to the different APIs which you commonly encounter in your\\nday to day life. To add more value, I have listed down 5 useful projects you can work on using an\\nAPI. I bet you will be tempted to try your hands on at least one of them.\\nLet’s get started!\\n\\xa0\\nTable of Contents\\n\\ue072\\ue094 Overview\\n\\ue073\\ue094 Categories of API\\n\\ue074\\ue094 Difference between an API and a Library\\n\\ue075\\ue094 Walk through an example\\n\\ue076\\ue094 5 APIs every Data Scientists should know\\n\\ue077\\ue094 List of 5 cool data science projects using APIs\\n\\ue078\\ue094 Welcome to the new playground\\n\\xa0\\n1. Overview\\nIn simple words, an API is a (hypothetical) contract between 2 softwares saying if the user\\nsoftware provides input in a pre-defined format, the later with extend its functionality and\\nprovide the outcome to the user software. Think of it like this,\\xa0Graphical user interface (GUI) or\\ncommand line interface (CLI) allows humans to Interact with code, where as an\\xa0 Application\\nprogrammable interface (API) allows one piece of code to interact with other code.\\nOne of the most common use case for APIs is on the web. If you have spent a few hours on\\ninternet, you have certainly used APIs. Sharing things on social media, making payments over the\\nweb, displaying list of tweets through a social handle – all of these services use API at the back.\\nAPIs are widely used by developers for implementing various features in their software. They\\nsimply use a simple API call within their software to implement complex features instead of\\nhaving to code it by themselves.\\nLet’s try and understand it better with the help of an example:\\nPokemon Go has been one of the most popular smartphone games. But in order to build such a\\ngame taking in account the large ecosystem, one requires complete information of routes and\\nroads across the globe. I’m sure the developers of the Pokemon Go must have faced a dilemma if\\nthey should code the maps of the entire world or use the \\xa0existing Google maps to build their\\napplication on top of it. They choose the latter, simply because it’s practically \\xa0not possible to\\ncreate something similar to Google maps in a short span of time.\\nThis is just one example. There are a lot of developers using various APIs to implement complex\\nfeatures into their applications instead of coding it themselves. Therefore, API provides a very\\nconvenient way of making code reusable.\\n\\xa0\\nBasic elements of an API:\\nAn API has\\xa0three primary elements:\\nAccess: is the user or who is allowed to ask for data or services?\\nRequest: is the actual data or service being asked for (e.g., if I give you current location from\\nmy game (Pokemon Go), tell me the map around that place). \\xa0A Request has two main parts:\\nMethods: i.e. the questions you can ask, assuming you have access (it also defines the\\ntype of responses available).\\nParameters: additional details you can include in the question or response.\\nResponse: the data or service as a result of your request.\\n\\xa0\\n2. Categories of API\\nWeb-based system\\nA web API is an interface to either a web server or a web browser. These APIs are used\\nextensively for the\\xa0 development of web applications. These APIs work at either the server\\nend or the client end. Companies like Google, Amazon, eBay all provide web-based API.\\nSome popular examples of web based API are Twitter REST API, Facebook Graph API,\\nAmazon S3 REST API, etc.\\nOperating system\\nThere are multiple OS based API that offers the functionality of various OS features that can\\nbe incorporated in creating windows or mac applications.\\nSome of the examples of OS based API are Cocoa, Carbon, WinAPI, etc.\\nDatabase system\\nInteraction with most of the database is done using the API calls to the database. These APIs\\nare defined in a manner to pass out the requested data in a predefined format that is\\nunderstandable by the requesting client.\\nThis makes the process of interaction with databases generalised and thereby enhancing the\\ncompatibility of applications with the \\xa0various database. They are very robust and provide a\\nstructured interface to database.\\nSome popular examples are Drupal 7 Database API, Drupal 8 Database API, Django API.\\nHardware System\\nThese APIs allows access to the various hardware components of a system. They are\\nextremely crucial for establishing communication to the hardware. Due to which it makes\\npossible for a range of functions from the collection of sensor data to even display on your\\nscreens.\\nFor example, the Google PowerMeter API will allow device manufacturers to build home\\nenergy monitoring devices that work\\xa0with Google PowerMeter.\\nSome other examples of Hardware APIs are: QUANT Electronic, WareNet CheckWare,OpenVX\\nHardware Acceleration, CubeSensore, etc.\\n\\xa0\\n3. Difference between an API and a Library\\nAt this point, I believe you might be scratching your head and confusing APIs with libraries. Let\\nme simplify it for you, an application programming interface (API) is an interface that defines the\\nway by which an application program may request service from the \\xa0libraries.\\nAn API is a set of rules with which the interaction between various entities is defined. We are\\nspecifically talking about interaction between two software.\\nEven a library also has an API which denotes the area of the library which is actually accessible\\nto the user from outside.\\n\\xa0\\n4. Walk through an example\\nIBM Watson has made certain data science APIs public for people like us to build amazing\\nprojects with only a few lines of code. Here, we’ll be looking at one such amazing API offered by\\nIBM called Personality Insights.\\nThis API takes as input in JSON, HTML or simple text format. The input contains text related to\\nthe person whose personality interests you. It can be anything like tweets, daily experiences,\\napplications, opinion, etc of that person.\\nThe output generated by the API is in the standard format of JSON or CSV file that contains the\\ninformation on various social traits of that person. And the developer only needs to display this\\ngenerated file to the user instead of coding the whole functionality yourself.\\nThere is also a demo on the IBM website that can be accessed here. You can choose either the\\ntweets or replies of few famous personalities to analyze their personality traits. The text can also\\nbe customized based on what input you want to provide and analyze the personality traits of\\nthat person.\\nYou can integrate this API in your code as well and build an application on top of this API.\\n\\xa0\\n5. 5 APIs every Data Scientists should know\\nFacebook API\\nFacebook API provides an interface to a\\xa0 large amount of data generated everyday. The\\ninnumerable post, comments and shares in various groups & pages produces massive data.\\nAnd this massive public data provides a\\xa0 large number of opportunities for analyzing the\\ncrowd.\\nIt is also incredibly convenient to use Facebook Graph API with both R and python to extract\\ndata. To read more about the Facebook API, click here.\\nGoogle Map API\\nGoogle Map API is one of the commonly used API. Its applications vary from integration in a\\ncab service application to the popular Pokemon Go.\\nYou can retrieve all the information like location coordinates, distances between locations,\\nroutes etc. The fun part is that you can also use this API for creating the distance feature in\\nyour datasets as well.\\xa0Read here to find out its complete implementation.\\nTwitter API\\nJust like Facebook Graph API, Twitter data can be accessed using the Twitter API as well.\\nYou can access all the data like tweets made by any user, the tweets containing a particular\\nterm or even a combination of terms, tweets done on the topic in a particular date range, etc.\\nTwitter data is a great resource for performing the tasks like opinion mining, sentiment\\nanalysis. For detailed usage of twitter API,\\xa0read here.\\nIBM Watson API\\nIBM Watson offers a set of APIs for performing a host of complex tasks such as Tone\\nanalyzer, document conversion, personality insights, visual recognition, text to speech,\\nspeech to text, etc by using just few lines of code.\\nThis set of APIs differ from the other APIs discussed so far, as they provide service for\\nmanipulating and deriving insights from the data.\\xa0To know indepth details about this API, read\\nhere.\\nQuandl API\\nQuandl lets you invoke the time series information of a large number of stocks for the\\nspecified date range. The setting up of Quandl API is very easy and provides a great resource\\nfor projects like Stock price prediction, stock profiling, etc. Click here, to read more details\\nabout\\xa0Quandl API.\\n\\xa0\\n6. List of 5 cool data science projects using API\\nI am sure that you are fascinated after reading about the above APIs, but wondering if you could\\na create project using these APIs which will be great value add to your CV? \\xa0Well, here’s the list\\nof ideas you can start with. You can either use these APIs to retrieve data & manipulate it to\\nextract insights from it or pass the data to these APIs & perform complex functions.\\nHere are the list of the projects for you. I’ll leave the execution of these ideas to you.\\nSocial Media Sentiment Analysis : By using data from Twitter and Facebook API.\\nOpinion Mining : By using data from Twitter and Facebook API.\\nStock Prediction : By using data from Yahoo Stock API and Quandl API.\\nMost Popular languages on Github : By using data from Github API.\\nMicrosoft Face Sentiment Recognition : By using Microsoft face API.\\nFurther reads:\\nUsing the meetup API to find right meetup groups\\nMining YouTube for social media analysis\\nSubject extraction using Google API\\nUse Google Maps API to create distance features\\n\\xa0\\n7. Welcome to the new Playground\\nTake a step back – \\xa0 you have just got a glimpse of an entirely new world. Think of all the\\npossibilities it enables – need face recognition on your mobile application – no worries! just\\ninvoke Google Face recognition API. Need to translate documents in Japanese to English – why\\nnot try Google Translate! The possibilities are limitless!\\nA few things to keep in mind while you think of building and using APIs:\\nAPIs are a robust way to integrate applications. You can theoretically do integrations\\nwithout APIs in some cases, but that often lands you in difficulty. For example, you want to\\npull 5 latest tweets from a user. You can do this easily through Twitter API and irrespective of\\nchanges happening at the front end of Twitter your results would be correct. If you would\\nhave done this using web scrapping, you would need to make changes with every change in\\nweb site layout. You would need to worry in case Twitter throws an advertisement in\\nbetween the last 5 tweets and filter it out!\\nAPIs might come with a limit on usage. This could be in form of daily limits, number of\\nrequests in a time frame etc. This is to ensure that the servers of the service providers are\\nunder predictable loads. If you need higher access, check if there is a paid access or talk to\\nthe service provider.\\nWhile designing your application, think of what is the best way to use an API. For example,\\nyou can send a single request to YouTube and get the top videos with some statistics. On the\\nother hand, you can send one request for each video and get more detailed statistics. What\\ndo you need for your application – if you can do what is needed in one call – do it with one\\ncall!\\n\\xa0\\n\\xa0\\nEnd Notes\\nAfter going through this article, I believe you would have acquired a better understanding of APIs\\nand how helpful they can be. I have only mentioned some of the popular APIs but the list is\\nendless. If you would like to add to the list of APIs, please share them in the comments below.\\nI’ll encourage you to pick up any of the suggested projects and work on it. I bet you’ll be shocked\\nby the power and ease with which you’ll be able to perform a complex task that would otherwise\\nhave been difficult to implement yourself.\\nDid you enjoyed reading this article? \\xa0Do share your views in the comment section below.\\nYou can\\xa0test your skills and knowledge.\\xa0Check\\nout\\xa0Live\\xa0Competitions\\xa0and compete with best\\xa0Data Scientists from all\\nover the world.\\nAPI\\nAPI Based Projects\\nAPIS\\nApplication Programming Interface\\nData Science\\nData Science Projects\\nDatabase API\\nFacebook API\\nGoogle Map API\\nHardware API\\nIBM Watson API\\nOperating System API\\nQuandl API\\nTwitter API\\nWeb API\\nSaurav is a Data Science enthusiast, currently in the final year of his graduation at MAIT, New\\nDelhi. He loves to use machine learning and analytics to solve complex data problems.\\nData Mining\\nIntermediate\\nListicle\\nMachine Learning\\nResearch & Technology\\nResponses From Readers\\nWhat are your thoughts?...\\nSubmit reply\\nWrite for us \\nWrite, captivate, and earn accolades and rewards for your work\\nCompany\\nAbout Us\\nContact Us\\nCareers\\nDiscover\\nBlogs\\nExpert session\\nPodcasts\\nComprehensive Guides\\nLearn\\nFree courses\\nLearning path\\nBlackBelt program\\nGen AI\\nEngage\\nCommunity\\nHackathons\\nEvents\\nDaily challenges\\nContribute\\nContribute & win\\nBecome a speaker\\nBecome a mentor\\nBecome an instructor\\nEnterprise\\nOur offerings\\nCase studies\\nIndustry report\\nquexto.ai\\nDownload App\\nTerms & conditions \\n Refund Policy \\n Privacy Policy \\n Cookies Policy \\n© Analytics Vidhya 2024.All rights reserved.\\nMaster Generative AI: Your step-by-step guide to become a certified GenAI expert\\nDownload Roadmap\\nSauravkaushik8 Kaushik\\n05 Jul, 2020\\nprogeCAD USA\\nFree 30 Day progeCAD Trial\\nDOWNLOAD\\nSauravkaushik8 Kaushik\\n05 Jul, 2020\\nRECOMMENDED ARTICLES\\nBuilding additional features &\\nvariables thro...\\nYou Cannot Miss These 8\\nPython Libraries\\n6 Deep Learning Applications\\na beginner can build ...\\nHow to Fetch Data using API\\nand SQL databases!\\nAdd Shine to your Data\\nScience Resume with these\\n8...\\n6 Exciting Open Source Data\\nScience Projects you S...\\nJacques Gouimenou\\n18 Nov, 2016\\nThat's fine! good informative article. Thanks for sharing your knowlegde with everyone.\\nShow 1 reply\\n 1\\nWang Hao\\n18 Nov, 2016\\nThanks for your sharing. Learned a lot~\\nShow 1 reply\\n 1\\nJames\\n18 Nov, 2016\\nThank you for the insightful post on API. Ihave actually book marked it for my future. I am actually\\nstudying MATLAB and before study ML with the same language sometimes later. Are these APIs\\napplicable to MATLAB, Specially for web-based API because MATLAB is not extensively used with\\nthe web (I might be wrong, because I am only a beginner).\\nShow 1 reply\\n1\\nReach a Global Audience\\nGet Expert Feedback\\nBuild Your Brand & Audience\\nCash In on Your Knowledge\\nJoin a Thriving Community\\nLevel Up Your Data Science\\nGame\\nRahul Shah\\n27\\nSion Chakrabarti\\n16\\nCHIRAG GOYAL\\n87\\nBarney Darlington\\n5\\nExplore \\nLogin\\nAn Introduction to APIs (Application Programming Interfaces) & 5 APIs a Data Scientist must know!\\nWe use cookies on Analytics Vidhya websites to deliver our services, analyze web traffic, and improve your experience on the site. By using Analytics Vidhya, you agree to our Privacy Policy and Terms of Use.\\nAccept\\nSign in with Google\\nUse your Google Account\\nto sign in to\\nanalyticsvidhya.com\\nNo more passwords to\\nremember. Signing in is fast,\\nsimple and secure.\\nContinue\\n\"}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/Introduction to API for Data Science.pdf', 'text': \"Home\\nData Mining\\nAn Introduction to APIs (Application Programming Interfaces) &...\\n 8 min read\\nIntroduction\\nIf you are in tech domain, you will invariably bump in references to something called an “API”. You\\njust can’t skip it – if you do, you are bound\\xa0 to hear it again. APIs\\xa0 are being used almost\\neverywhere. But, if you have ever wondered what exactly is an API? or\\xa0Why are they important?\\nor How do they help? this article will help you out.\\nIn this article, I will explain what is an API in simple terms. I will tell various categories / types of\\nAPI. Then I will be introducing you to the different APIs which you commonly encounter in your\\nday to day life. To add more value, I have listed down 5 useful projects you can work on using an\\nAPI. I bet you will be tempted to try your hands on at least one of them.\\nLet’s get started!\\n\\xa0\\nTable of Contents\\n\\ue072\\ue094 Overview\\n\\ue073\\ue094 Categories of API\\n\\ue074\\ue094 Difference between an API and a Library\\n\\ue075\\ue094 Walk through an example\\n\\ue076\\ue094 5 APIs every Data Scientists should know\\n\\ue077\\ue094 List of 5 cool data science projects using APIs\\n\\ue078\\ue094 Welcome to the new playground\\n\\xa0\\n1. Overview\\nIn simple words, an API is a (hypothetical) contract between 2 softwares saying if the user\\nsoftware provides input in a pre-defined format, the later with extend its functionality and\\nprovide the outcome to the user software. Think of it like this,\\xa0Graphical user interface (GUI) or\\ncommand line interface (CLI) allows humans to Interact with code, where as an\\xa0 Application\\nprogrammable interface (API) allows one piece of code to interact with other code.\\nOne of the most common use case for APIs is on the web. If you have spent a few hours on\\ninternet, you have certainly used APIs. Sharing things on social media, making payments over the\\nweb, displaying list of tweets through a social handle – all of these services use API at the back.\\nAPIs are widely used by developers for implementing various features in their software. They\\nsimply use a simple API call within their software to implement complex features instead of\\nhaving to code it by themselves.\\nLet’s try and understand it better with the help of an example:\\nPokemon Go has been one of the most popular smartphone games. But in order to build such a\\ngame taking in account the large ecosystem, one requires complete information of routes and\\nroads across the globe. I’m sure the developers of the Pokemon Go must have faced a dilemma if\\nthey should code the maps of the entire world or use the \\xa0existing Google maps to build their\\napplication on top of it. They choose the latter, simply because it’s practically \\xa0not possible to\\ncreate something similar to Google maps in a short span of time.\\nThis is just one example. There are a lot of developers using various APIs to implement complex\\nfeatures into their applications instead of coding it themselves. Therefore, API provides a very\\nconvenient way of making code reusable.\\n\\xa0\\nBasic elements of an API:\\nAn API has\\xa0three primary elements:\\nAccess: is the user or who is allowed to ask for data or services?\\nRequest: is the actual data or service being asked for (e.g., if I give you current location from\\nmy game (Pokemon Go), tell me the map around that place). \\xa0A Request has two main parts:\\nMethods: i.e. the questions you can ask, assuming you have access (it also defines the\\ntype of responses available).\\nParameters: additional details you can include in the question or response.\\nResponse: the data or service as a result of your request.\\n\\xa0\\n2. Categories of API\\nWeb-based system\\nA web API is an interface to either a web server or a web browser. These APIs are used\\nextensively for the\\xa0 development of web applications. These APIs work at either the server\\nend or the client end. Companies like Google, Amazon, eBay all provide web-based API.\\nSome popular examples of web based API are Twitter REST API, Facebook Graph API,\\nAmazon S3 REST API, etc.\\nOperating system\\nThere are multiple OS based API that offers the functionality of various OS features that can\\nbe incorporated in creating windows or mac applications.\\nSome of the examples of OS based API are Cocoa, Carbon, WinAPI, etc.\\nDatabase system\\nInteraction with most of the database is done using the API calls to the database. These APIs\\nare defined in a manner to pass out the requested data in a predefined format that is\\nunderstandable by the requesting client.\\nThis makes the process of interaction with databases generalised and thereby enhancing the\\ncompatibility of applications with the \\xa0various database. They are very robust and provide a\\nstructured interface to database.\\nSome popular examples are Drupal 7 Database API, Drupal 8 Database API, Django API.\\nHardware System\\nThese APIs allows access to the various hardware components of a system. They are\\nextremely crucial for establishing communication to the hardware. Due to which it makes\\npossible for a range of functions from the collection of sensor data to even display on your\\nscreens.\\nFor example, the Google PowerMeter API will allow device manufacturers to build home\\nenergy monitoring devices that work\\xa0with Google PowerMeter.\\nSome other examples of Hardware APIs are: QUANT Electronic, WareNet CheckWare,OpenVX\\nHardware Acceleration, CubeSensore, etc.\\n\\xa0\\n3. Difference between an API and a Library\\nAt this point, I believe you might be scratching your head and confusing APIs with libraries. Let\\nme simplify it for you, an application programming interface (API) is an interface that defines the\\nway by which an application program may request service from the \\xa0libraries.\\nAn API is a set of rules with which the interaction between various entities is defined. We are\\nspecifically talking about interaction between two software.\\nEven a library also has an API which denotes the area of the library which is actually accessible\\nto the user from outside.\\n\\xa0\\n4. Walk through an example\\nIBM Watson has made certain data science APIs public for people like us to build amazing\\nprojects with only a few lines of code. Here, we’ll be looking at one such amazing API offered by\\nIBM called Personality Insights.\\nThis API takes as input in JSON, HTML or simple text format. The input contains text related to\\nthe person whose personality interests you. It can be anything like tweets, daily experiences,\\napplications, opinion, etc of that person.\\nThe output generated by the API is in the standard format of JSON or CSV file that contains the\\ninformation on various social traits of that person. And the developer only needs to display this\\ngenerated file to the user instead of coding the whole functionality yourself.\\nThere is also a demo on the IBM website that can be accessed here. You can choose either the\\ntweets or replies of few famous personalities to analyze their personality traits. The text can also\\nbe customized based on what input you want to provide and analyze the personality traits of\\nthat person.\\nYou can integrate this API in your code as well and build an application on top of this API.\\n\\xa0\\n5. 5 APIs every Data Scientists should know\\nFacebook API\\nFacebook API provides an interface to a\\xa0 large amount of data generated everyday. The\\ninnumerable post, comments and shares in various groups & pages produces massive data.\\nAnd this massive public data provides a\\xa0 large number of opportunities for analyzing the\\ncrowd.\\nIt is also incredibly convenient to use Facebook Graph API with both R and python to extract\\ndata. To read more about the Facebook API, click here.\\nGoogle Map API\\nGoogle Map API is one of the commonly used API. Its applications vary from integration in a\\ncab service application to the popular Pokemon Go.\\nYou can retrieve all the information like location coordinates, distances between locations,\\nroutes etc. The fun part is that you can also use this API for creating the distance feature in\\nyour datasets as well.\\xa0Read here to find out its complete implementation.\\nTwitter API\\nJust like Facebook Graph API, Twitter data can be accessed using the Twitter API as well.\\nYou can access all the data like tweets made by any user, the tweets containing a particular\\nterm or even a combination of terms, tweets done on the topic in a particular date range, etc.\\nTwitter data is a great resource for performing the tasks like opinion mining, sentiment\\nanalysis. For detailed usage of twitter API,\\xa0read here.\\nIBM Watson API\\nIBM Watson offers a set of APIs for performing a host of complex tasks such as Tone\\nanalyzer, document conversion, personality insights, visual recognition, text to speech,\\nspeech to text, etc by using just few lines of code.\\nThis set of APIs differ from the other APIs discussed so far, as they provide service for\\nmanipulating and deriving insights from the data.\\xa0To know indepth details about this API, read\\nhere.\\nQuandl API\\nQuandl lets you invoke the time series information of a large number of stocks for the\\nspecified date range. The setting up of Quandl API is very easy and provides a great resource\\nfor projects like Stock price prediction, stock profiling, etc. Click here, to read more details\\nabout\\xa0Quandl API.\\n\\xa0\\n6. List of 5 cool data science projects using API\\nI am sure that you are fascinated after reading about the above APIs, but wondering if you could\\na create project using these APIs which will be great value add to your CV? \\xa0Well, here’s the list\\nof ideas you can start with. You can either use these APIs to retrieve data & manipulate it to\\nextract insights from it or pass the data to these APIs & perform complex functions.\\nHere are the list of the projects for you. I’ll leave the execution of these ideas to you.\\nSocial Media Sentiment Analysis : By using data from Twitter and Facebook API.\\nOpinion Mining : By using data from Twitter and Facebook API.\\nStock Prediction : By using data from Yahoo Stock API and Quandl API.\\nMost Popular languages on Github : By using data from Github API.\\nMicrosoft Face Sentiment Recognition : By using Microsoft face API.\\nFurther reads:\\nUsing the meetup API to find right meetup groups\\nMining YouTube for social media analysis\\nSubject extraction using Google API\\nUse Google Maps API to create distance features\\n\\xa0\\n7. Welcome to the new Playground\\nTake a step back – \\xa0 you have just got a glimpse of an entirely new world. Think of all the\\npossibilities it enables – need face recognition on your mobile application – no worries! just\\ninvoke Google Face recognition API. Need to translate documents in Japanese to English – why\\nnot try Google Translate! The possibilities are limitless!\\nA few things to keep in mind while you think of building and using APIs:\\nAPIs are a robust way to integrate applications. You can theoretically do integrations\\nwithout APIs in some cases, but that often lands you in difficulty. For example, you want to\\npull 5 latest tweets from a user. You can do this easily through Twitter API and irrespective of\\nchanges happening at the front end of Twitter your results would be correct. If you would\\nhave done this using web scrapping, you would need to make changes with every change in\\nweb site layout. You would need to worry in case Twitter throws an advertisement in\\nbetween the last 5 tweets and filter it out!\\nAPIs might come with a limit on usage. This could be in form of daily limits, number of\\nrequests in a time frame etc. This is to ensure that the servers of the service providers are\\nunder predictable loads. If you need higher access, check if there is a paid access or talk to\\nthe service provider.\\nWhile designing your application, think of what is the best way to use an API. For example,\\nyou can send a single request to YouTube and get the top videos with some statistics. On the\\nother hand, you can send one request for each video and get more detailed statistics. What\\ndo you need for your application – if you can do what is needed in one call – do it with one\\ncall!\\n\\xa0\\n\\xa0\\nEnd Notes\\nAfter going through this article, I believe you would have acquired a better understanding of APIs\\nand how helpful they can be. I have only mentioned some of the popular APIs but the list is\\nendless. If you would like to add to the list of APIs, please share them in the comments below.\\nI’ll encourage you to pick up any of the suggested projects and work on it. I bet you’ll be shocked\\nby the power and ease with which you’ll be able to perform a complex task that would otherwise\\nhave been difficult to implement yourself.\\nDid you enjoyed reading this article? \\xa0Do share your views in the comment section below.\\nYou can\\xa0test your skills and knowledge.\\xa0Check\\nout\\xa0Live\\xa0Competitions\\xa0and compete with best\\xa0Data Scientists from all\\nover the world.\\nAPI\\nAPI Based Projects\\nAPIS\\nApplication Programming Interface\\nData Science\\nData Science Projects\\nDatabase API\\nFacebook API\\nGoogle Map API\\nHardware API\\nIBM Watson API\\nOperating System API\\nQuandl API\\nTwitter API\\nWeb API\\nSaurav is a Data Science enthusiast, currently in the final year of his graduation at MAIT, New\\nDelhi. He loves to use machine learning and analytics to solve complex data problems.\\nData Mining\\nIntermediate\\nListicle\\nMachine Learning\\nResearch & Technology\\nResponses From Readers\\nWhat are your thoughts?...\\nSubmit reply\\nWrite for us \\nWrite, captivate, and earn accolades and rewards for your work\\nCompany\\nAbout Us\\nContact Us\\nCareers\\nDiscover\\nBlogs\\nExpert session\\nPodcasts\\nComprehensive Guides\\nLearn\\nFree courses\\nLearning path\\nBlackBelt program\\nGen AI\\nEngage\\nCommunity\\nHackathons\\nEvents\\nDaily challenges\\nContribute\\nContribute & win\\nBecome a speaker\\nBecome a mentor\\nBecome an instructor\\nEnterprise\\nOur offerings\\nCase studies\\nIndustry report\\nquexto.ai\\nDownload App\\nTerms & conditions \\n Refund Policy \\n Privacy Policy \\n Cookies Policy \\n© Analytics Vidhya 2024.All rights reserved.\\nMaster Generative AI: Your step-by-step guide to become a certified GenAI expert\\nDownload Roadmap\\nSauravkaushik8 Kaushik\\n05 Jul, 2020\\nprogeCAD USA\\nFree 30 Day progeCAD Trial\\nDOWNLOAD\\nSauravkaushik8 Kaushik\\n05 Jul, 2020\\nRECOMMENDED ARTICLES\\nBuilding additional features &\\nvariables thro...\\nYou Cannot Miss These 8\\nPython Libraries\\n6 Deep Learning Applications\\na beginner can build ...\\nHow to Fetch Data using API\\nand SQL databases!\\nAdd Shine to your Data\\nScience Resume with these\\n8...\\n6 Exciting Open Source Data\\nScience Projects you S...\\nJacques Gouimenou\\n18 Nov, 2016\\nThat's fine! good informative article. Thanks for sharing your knowlegde with everyone.\\nShow 1 reply\\n 1\\nWang Hao\\n18 Nov, 2016\\nThanks for your sharing. Learned a lot~\\nShow 1 reply\\n 1\\nJames\\n18 Nov, 2016\\nThank you for the insightful post on API. Ihave actually book marked it for my future. I am actually\\nstudying MATLAB and before study ML with the same language sometimes later. Are these APIs\\napplicable to MATLAB, Specially for web-based API because MATLAB is not extensively used with\\nthe web (I might be wrong, because I am only a beginner).\\nShow 1 reply\\n1\\nReach a Global Audience\\nGet Expert Feedback\\nBuild Your Brand & Audience\\nCash In on Your Knowledge\\nJoin a Thriving Community\\nLevel Up Your Data Science\\nGame\\nRahul Shah\\n27\\nSion Chakrabarti\\n16\\nCHIRAG GOYAL\\n87\\nBarney Darlington\\n5\\nExplore \\nLogin\\nAn Introduction to APIs (Application Programming Interfaces) & 5 APIs a Data Scientist must know!\\nWe use cookies on Analytics Vidhya websites to deliver our services, analyze web traffic, and improve your experience on the site. By using Analytics Vidhya, you agree to our Privacy Policy and Terms of Use.\\nAccept\\nSign in with Google\\nUse your Google Account\\nto sign in to\\nanalyticsvidhya.com\\nNo more passwords to\\nremember. Signing in is fast,\\nsimple and secure.\\nContinue\\n\"}, {'source': '/Users/sir/Downloads/Data/PDF/Fuzzy-Arden-Syntax--A-fuzzy-programming-langu_2010_Artificial-Intelligence-i.pdf', 'text': 'Fuzzy Arden Syntax: A fuzzy programming language for medicine\\nThomas Vetterlein a,*, Harald Mandl b, Klaus-Peter Adlassnig a,b\\na Section for Medical Expert and Knowledge-Based Systems, Medical University of Vienna, Spitalgasse 23, 1090 Vienna, Austria\\nb Medexter Healthcare GmbH, Borschkegasse 7/5, 1090 Vienna, Austria\\n1. Introduction\\nIn medical computer science, intelligent management of clinical\\ndata is the objective of what is known as clinical decision support, or\\nCDS for short. The performance of a CDS system (CDSS) clearly\\nextends the mere storage and user-speciﬁedmanipulation ofpatient\\ndata. Its essential feature is the ability to provide consequences of\\nthe digitally handled medical information, derived on the basis of a\\nmedical knowledge base. A logical inference module provides\\nmethods of deducing consequences from the available data in\\naccordance with coded interrelations between medical facts.\\nWe deal here with an established programming language\\ndesigned especially for computerised analysis of medical data.\\nThe language is named Arden Syntax and has been used for ﬁfteen\\nyears now; it was ﬁrst published by the American Society for Testing\\nand Materials in 1992. The language has been developed since. The\\npresent paper refers to Arden Syntax 2.5, which was published\\nby Health Level Seven, Inc., in 2005. Complete speciﬁcations can\\nbe found in [1], and further information on the subject is available\\non the Web, e.g., at http://www.hl7.org/implement/standards/\\nardensyntax.cfm (accessed on 10 October 2008).\\nArden Syntax is a convenient tool for the implementation of a\\nCDSS. The aim of an Arden-Syntax-based CDSS is to derive, from a\\npatient’s electronic record, information suitable to facilitate or\\nimprove clinical decision-making. The user might employ the CDSS\\nby speciﬁcally calling some of its functions. However, in order to\\nachieve optimal performance the system should run permanently in\\nthe background and notice in real time whether the available data\\nimply the necessity of a consequence. In this case, a proposal or if\\nnecessary a warning is communicated to the clinical personnel. A\\nArtiﬁcial Intelligence in Medicine 49 (2010) 1–10\\nA R T I C L E\\nI N F O\\nArticle history:\\nReceived 6 March 2009\\nReceived in revised form 15 December 2009\\nAccepted 8 January 2010\\nKeywords:\\nClinical decision support system\\nArden Syntax\\nFuzzy Arden Syntax\\nFuzzy set theory\\nA B S T R A C T\\nObjective: The programming language Arden Syntax has been optimised for use in clinical decision\\nsupport systems. We describe an extension of this language named Fuzzy Arden Syntax, whose original\\nversion was introduced in S. Tiffe’s dissertation on ‘‘Fuzzy Arden Syntax: Representation and\\nInterpretation of Vague Medical Knowledge by Fuzziﬁed Arden Syntax’’ (Vienna University of\\nTechnology, 2003). The primary aim is to provide an easy means of processing vague or uncertain\\ndata, which frequently appears in medicine.\\nMethods: For both propositional and number data types, fuzzy equivalents have been added to Arden\\nSyntax. The Boolean data type was generalised to represent any truth degree between the two extremes\\n0 (falsity) and 1 (truth); fuzzy data types were introduced to represent fuzzy sets. The operations on truth\\nvalues and real numbers were generalised accordingly. As the conditions to decide whether a certain\\nprogramme unit is executed or not may be indeterminate, a Fuzzy Arden Syntax programme may split.\\nThe data in the different branches may be optionally aggregated subsequently.\\nResults: Fuzzy Arden Syntax offers the possibility to formulate conveniently Medical Logic Modules\\n(MLMs) based on the principle of a continuously graded applicability of statements. Furthermore, ad hoc\\ndecisions about sharp value boundaries can be avoided. As an illustrative example shows, an MLM\\nmaking use of the features of Fuzzy Arden Syntax is not signiﬁcantly more complex than its Arden Syntax\\nequivalent; in the ideal case, a programme handling crisp data remains practically unchanged when\\ncompared to its fuzziﬁed version. In the latter case, the output data, which can be a set of weighted\\nalternatives, typically depends continuously from the input data.\\nConclusion: In typical applications an Arden Syntax MLM can produce a different output after only slight\\nchanges of the input; discontinuities are in fact unavoidable when the input varies continuously but the\\noutput is taken from a discrete set of possibilities. This inconvenience can, however, be attenuated by\\nmeans of certain mechanisms on which the programme ﬂow under Fuzzy Arden Syntax is based. To write\\na programme making use of these possibilities is not signiﬁcantly more difﬁcult than to write a\\nprogramme according to the usual practice.\\n\\x02 2010 Elsevier B.V. All rights reserved.\\n* Corresponding author. Tel.: +43 1 40400 6665; fax: +43 1 40400 6667.\\nE-mail address: Thomas.Vetterlein@meduniwien.ac.at (T. Vetterlein).\\nContents lists available at ScienceDirect\\nArtificial Intelligence in Medicine\\njournal homepage: www.elsevier.com/locate/aiim\\n0933-3657/$ – see front matter \\x02 2010 Elsevier B.V. All rights reserved.\\ndoi:10.1016/j.artmed.2010.01.003\\n\\nCDSSshouldnotmerelyperformroutinetasks, butgivediscretehints\\nastowhatshouldorshouldnotbedone.Oneoftheprimaryobjectives\\nis to avoid the mistakes that become evident from the available data,\\nprovided the appropriate knowledge has been entered previously.\\nIn\\nprinciple\\nan\\ninference\\nmechanism\\nbased\\non\\nmedical\\nknowledge may be coded in any programming language. However,\\nArden Syntax is equipped with several features which were\\nspeciﬁcally chosen for the intended application: (i) a syntax close\\nto natural language, which renders a programme to a large extent\\nreadable by the non-expert in computer science; (ii) thorough\\nchoice of just a few data types typically needed for medical\\ndocumentation, including time and duration; (iii) the possibility to\\nprocess\\nand\\nto\\ncommunicate\\nreal-time\\nevents,\\ndetermined\\naccording to the host system speciﬁcations; (iv) easy handling\\nof temporal relations, supported in particular by a time component\\nattached to all simple data types; (v) high error tolerance of Arden\\nSyntax programmes, selecting, in case of doubt, the option most\\nlikely meant by the user, rather than interrupting the work ﬂow\\nevery time by an error message.\\nThe function of an Arden-Syntax-based system is to check a\\npatient’saccessible digitaldatawithregardtospeciﬁcconditions and\\nreact appropriately if the conditions apply. Needless to say, medical\\ndata do not always permit a clear distinction between whether a\\nspeciﬁc condition is fulﬁlled or not; borderline cases defy this\\nstipulation. When the assumptions of an implication are approxi-\\nmately but not entirely fulﬁlled, it is, from a strict point of view, not\\npossible to draw any conclusion. However, a conclusion can well be\\ndesirable. This situation suggests the implementation of methods\\nprovided by fuzzy logic. In fuzzy logic, a conclusion is permitted,\\nprovided thatappropriatemodiﬁcationsare appliedtothe result:the\\noutgoing statement is weakened with regards to content. The\\nsimplest way to realise this idea is to assign a weight to the outcome.\\nIn case of more than one possible outcome, several distinct outputs\\nmight result, all weighted with a value strictly smaller than 1.\\nBased on these considerations, a conservative extension of Arden\\nSyntax was proposed by S. Tiffe in his Ph.D. thesis [2]. The principle is\\nto generalise classical two-valued logic to a many-valued logic; the\\nextension is named Fuzzy Arden Syntax. The work presented here\\nhas taken up this line and led toan alternative version of Fuzzy Arden\\nSyntax, whose full speciﬁcation can be found in [3]. A comparison to\\nTiffe’s work is contained in Section 11.\\n2. Fuzzy logic to process vague information\\nThe features which have been added to Arden Syntax aim at\\nsimplifying programmes which process indeterminate data by\\nmeans of fuzzy logic. Let us review very shortly the signiﬁcance of\\nthis methodology.\\nAs a starting point for the development of fuzzy logic, often\\nZadeh’s article in 1965 is seen [4]. The ﬁeld has since then\\ndeveloped enormously and into several different directions.\\nAmong the numerous general monographs on the topic we may\\nrecommend [5].\\nThe main idea is to generalise the notion of a truth value. Rather\\nthan using two truth values 0 and 1, modelling ‘‘false’’ and ‘‘true’’,\\nrespectively, typically the elements of the entire real unit interval\\n½0; 1\\x02 are used. When a truth value is associated with a vague\\nstatement, it expresses the degree to which a proposition is true. In\\nparticular, 0 expresses the clear falsity while 1 expresses the clear\\ntruth of a proposition; anything between the two just expresses a\\ntendency.\\nFuzzy logic is at present widely used for controlling tasks and in\\nautomated decision support. Its use is implied in all cases in which\\nthe precise speciﬁcation of a problem is not possible or too\\ncomplex, where, however, a speciﬁcation in vague terms is\\navailable and assumed to be fully sufﬁcient for the practical\\nneeds. In controlling applications, for example, instruction sets are\\noften available in natural language. A standard method to use these\\ninstructions directly, dealing with the involved vagueness in an\\namazingly efﬁcient way, is due to Mamdani and Assilian [6].\\nVariants of the Mamdani–Assilian controller exist, according to\\none of which, e.g., the system FuzzyKBWean [7] is based, which\\nprovides recommendations to the clinical personnel of an optimal\\nchoice of instrument parameters when weaning off an intensive\\ncare patient from intubation. In CDS, a problem may be to specify a\\npathological state on base of the lower and upper boundaries of\\nlaboratory values. The use of fuzzy sets rather than sharp intervals\\nprevents the otherwise inevitable discontinuities of such speciﬁ-\\ncations. Boundary cases are treated appropriately and jumps of the\\noutput by arbitrary small change of the input are avoided. The\\nmedical expert systems CADIAG are an example; see, e.g., [8].\\nBy default, fuzzy logic is rather speciﬁc about the type of truth\\ndegrees dealt with. A truth value in fuzzy logic does not represent\\nuncertainty in the sense of lack of knowledge. It rather represents to\\nwhat degree some well-speciﬁed fact, represented, e.g., by a real\\nnumber, is in accordance with a vague notion, like, e.g., a natural\\nlanguage expression. For instance, a body temperature of 38.5\\x03 is in\\npartial, but not full accordance with the concept of ‘‘high fever’’, and\\ncould be described by a value, say, 0:3. The statement ‘‘high fever’’\\nassociated with a truth value 0:3 is sharply to be distinguished from\\nthe statement that the patient has high fever with the probability\\n0:3; in the former case, uncertainty is not involved.\\nIn the present context, however, we will adopt a broader\\nviewpoint about the nature of truth degrees. In medicine, values of\\nthe real unit interval are frequently used not only to express\\ndegrees of applicability of a vague concept, but also of uncertainty,\\nor more speciﬁcally the probability of some event. In Fuzzy Arden\\nSyntax, the meaning of truth values is not predeﬁned; we do not\\nintend to prevent the user from using a probabilistic interpretation\\nor an interpretation according to some more general framework of\\nuncertainty management. We will actually in the sequel explicitly\\ninclude this possibility. We note, however, that Fuzzy Arden\\nSyntax does not contain special functions supporting reasoning\\nunder uncertainty; after all its design aims at supporting the\\nprocession of vague rather than uncertain information. To include\\nprobability theory or some kind of plausibility logic could motivate\\na further extension of the language.\\nWe note that fuzzy analoga have been proposed for several\\nprogramming languages. The aim has been to simplify inferences\\nbased on methods of fuzzy logic. In particular, the language HALO\\n[9] has been described as ‘‘fuzzy Pascal’’. The language FRIL [10]\\nextends Prolog, so as to deal with uncertainty of data and rules;\\nsimilar aims are followed in [11]. The programming language L[12]\\ndoes not refer to an existing language, but takes up the concept of\\nW-recursiveness, which is an adaption of the notion of recursive-\\nness for fuzzy functions. Active research is still being conducted in\\nthis ﬁeld; a recent paper [13] could well be added to the list.\\nOn the very basic level, Fuzzy Arden Syntax is comparable to\\nthese programming languages. Apart from that, however, the\\nintentions are not comparable. Recall that Arden Syntax provides\\nprogrammes which read, in the ideal case, like a speciﬁcation of a\\nmedical procedure proposed under certain circumstances. Our aim\\nwas not only to provide fuzzy equivalents for the standard data\\ntypes found in any language; we were primarily interested in\\nrendering more ﬂexible those components which were introduced\\ninto Arden Syntax in order to adapt it for use in medicine.\\n3. Arden Syntax\\nFuzzy Arden Syntax is based on Arden Syntax and every\\nprogramme written in Arden Syntax runs under Fuzzy Arden\\nSyntax without any alteration of its effect. That is, Fuzzy Arden\\nT. Vetterlein et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 1–10\\n2\\n\\nSyntax is backward compatible with Arden Syntax. We tried to\\nkeep all changes to the necessary minimum. The ofﬁcial Arden\\nSyntax speciﬁcations can be found in [1]. It would be impossible to\\nenumerate them here. Nevertheless, we intend to make the main\\nfeatures of Fuzzy Arden Syntax comprehensible even to those\\nwithout a profound knowledge of HL7’s Arden Syntax. To this end\\nwe will explain a few basic principles of Arden Syntax and provide\\na sample programme. Moreover, when presenting Fuzzy Arden\\nSyntax we will provide integrated speciﬁcations.\\nAs mentioned earlier, Arden Syntax is typically used to realise a\\nCDSS. An Arden-Syntax-based CDSS is highly modular; it is\\ncomposed of a possibly high number of programming units named\\nMedical Logic Modules, or MLMs for short. The set of MLMs is not\\nhierarchically structured; each MLM may be considered and can be\\nread and understood independently. However, MLMs may call\\neach other, such that some MLMs may be considered subroutines\\nof other MLMs. An MLM may take over data and return data. The\\nexecution of an MLM may be dependent on a speciﬁc event\\ncommunicated from the outside.\\nA host system provides the user interface, access to the patient\\ndata base, and control over the execution of MLMs. All requests\\noriginating from the user or an MLM are processed by the host. The\\nhost speciﬁes a set of events and communicates the occurrence of\\nany event to the MLMs concerned. An event may reﬂect, for\\ninstance, a speciﬁc change in the patient database.\\nA typical MLM refers to a speciﬁc event which may take place at\\nany time in the database. Whenever this event occurs, the MLM is\\ntriggered and causes, or reminds the users to take, the appropriate\\naction.\\nThis phenomenon is demonstrated by one of the sample MLMs\\nfrom [1]. The MLM Penicillin_Allergy is executed whenever a\\npenicillin is prescribed at the hospital where the system is in use or,\\nmore precisely, when the corresponding change is made in the\\ndatabase. A warning is issued when an allergy against penicillin is\\nregistered for the respective patient. What follows is a brief version\\nof the MLM which can be found in [1, X3.3].\\nAn MLM consists of three main parts, named categories, each of\\nwhich consists of several entries, named slots. The ﬁrst two\\ncategories, namely maintenance and library, contain metadata,\\nof which only the content of three slots are of interest to the\\nprogrammer. These are shown in the above example and are\\npresumably self-explanatory.\\nThe programme itself is located in the last category named\\nknowledge. In the knowledge category, there are always a data, a\\nlogic, and an action slot, which are executed in this order, as well as\\nan evoke slot. In the data slot of our example, we ﬁrst ﬁnd the event\\nPenicillin_Order deﬁned, which occurs whenever penicillin is\\nordered. The deﬁnition of an event is given within curly brackets;\\nthis part is interpreted by the host system and the syntax is speciﬁc\\nfor\\nthe\\nmedical\\ninstitution\\nmentioned\\nin\\nthe\\nmaintenance\\ncategory. The next line contains a database query; the read\\ncommand causes a list to be read containing the notiﬁcations of an\\nallergy of the patient to penicillin. The result is stored in the\\nvariable Penicillin_Allergy. The effect of the key word last is\\nthat all entries but the last one are deleted from the list. Note that\\nin the present case, only one entry is needed.\\nThe events evoking this MLM are speciﬁed in the evoke slot. In\\nthis example, it is the event Penicillin_Order, which has been\\ndeﬁned in the data slot.\\nThe logic slot contains the part of the programme whose\\npurpose is to decide whether the subsequent action slot will be\\nexecuted or not. A command of the form conclude ActionCon-\\ndition, where ActionCondition is some Boolean variable,\\ncontains the decision: if ActionCondition is true, the action slot\\nis performed; otherwise the MLM is terminated at this point.\\nIn the above example, the programme ﬁrst checks whether the\\nlist Penicillin_Allergy is empty. If this is not the case the\\ncommand\\nconclude\\ntrue\\nis\\nexecuted,\\nmeaning\\nthat\\nthe\\nprogramme jumps to the beginning of the action slot.\\nThe action slot deﬁnes the action to be taken, provided that the\\nneed for an action has been identiﬁed in the logic slot. In the above\\nexample, a warning message is sent to the host, which will be\\ndisplayed on the user’s interface.\\n4. The concept underlying Fuzzy Arden Syntax\\nFuzzy Arden Syntax incorporates new concepts into Arden\\nSyntax in order to assist in processing information that may not be\\ncompletely determinate. It is based on the observation that, in\\nmedicine, we typically draw conclusions from real parameters in a\\nway that the exact values do not really matter. Besides, we\\nfrequently infer information from facts which are not seen to be\\nclearly true, but are considered true to a certain degree which high\\nenough to allow a conclusion.\\nIn contrast, a typical MLM written in Arden Syntax does depend\\non exact values and only deals with the two truth values ‘‘true’’ and\\n‘‘false’’. Consequently, a small change of a parameter may cause a\\nsharp change in an outgoing recommendation; for, if a recommen-\\ndation depends on a real value, a limit value must be chosen and\\nthis limit is typically chosen ad hoc because a precise limit point is\\nrarely available.\\nA solution would be to specify a limit point in a rough manner\\nrather than precisely, and issue differentiated recommendations in\\nborderline cases. Similarly, statements which merely express a\\ntendency rather than a simple ‘‘true’’ or ‘‘false’’ should also be\\npermitted. The new features of Fuzzy Arden Syntax simplify the\\nintegration of such elements.\\nTo illustrate the problem, let us consider a further example of an\\nMLM written in Arden Syntax. The MLM UTI_SUTI is based on the\\nspeciﬁcation of nosocomial infections by the US-American Centers\\nfor Disease Control and Prevention [14]; it determines whether a\\nhospitalised patient has a symptomatic urinary tract infection, or\\nT. Vetterlein et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 1–10\\n3\\n\\nSUTI for short. This MLM, which is part of the MONI system [15], is\\ndesigned as a subroutine of another MLM; it expects two input\\nparameters, namely a code for the patient’s hospital stay, which is\\nstored in the variable Stay, and a date, stored in the variable Day. It\\nyields a Boolean truth value indicating whether a SUTI is present or\\nnot.\\nThe result, which indicates whether a UTI-SUTI infection is\\npresent, depends on two real parameters and four yes–no\\nquestions. Vagueness is involved in all cases. For instance, in\\norder to check whether Fever is true, the patient’s body\\ntemperature must be compared with a sharp value simply because\\nthis variable is two-valued; the limit value depends on an ad hoc\\nchoice. With regard to the variable Dysuria, the patient is\\nrequired to specify whether or not the condition is present and this\\nmay be difﬁcult in the borderline case. The ﬁnal result may be\\nnegative but would well have been positive in case of a slight\\nchange in the manner of determining the input parameters.\\nThe aim underlying the conception of Fuzzy Arden Syntax is to\\nprocess, in a convenient way, statements that are not necessarily\\nentirely false or entirely true, and thus render it unnecessary for an\\nMLM’s author or user to make odd decisions about borderline\\ncases. The following principle is essential. To write or to\\nunderstand a Fuzzy Arden Syntax programme should not cause\\nessential difﬁculties for those already familiar with Arden Syntax.\\nCertainly, the user must be aware of certain modiﬁcations. The\\nprogram text should, however, remain practically unchanged\\nwhen based on fuzzy logic rather than classical logic.\\nThe main decisions underlying Fuzzy Arden Syntax have been\\nmade in accordance with fuzzy set theory and fuzzy logics. In\\nparticular, as a set of truth values, all (representable) reals between\\n0 and 1 are used. The MLM UTI_SUTI illustrates the usefulness of\\nextending the set of truth values. We argue that for input\\nparameters like Dysuria which are subjective in nature, it is more\\nappropriate to use generalised truth values rather than two-valued\\nones. Similarly, for those input parameters which result from a\\nmeasurement, such as Temperature, derived propositions like\\nFever should be considered fuzzy and assigned non-sharp truth\\nvalues in borderline cases. To what extent does the MLM needs to\\nbe modiﬁed under Fuzzy Arden Syntax in order to incorporate\\nthese demands? As a matter of fact the programme can remain\\npractically unchanged. All that needs to be speciﬁed is the degree\\nof impreciseness, which is easily done by formulating the\\ncomparison involving body temperature as follows: ‘‘Tempera-\\nture >= 38.5 fuzziﬁed by 0.5’’. In Section 9 we will show what\\nthis example, when fuzziﬁed, looks like in Arden Syntax and in\\nFuzzy Arden Syntax.\\nLet us now indicate the contexts in which fuzziness appears in\\nFuzzy Arden Syntax. The former data type boolean now takes\\nvalues within the real unit interval [0,1]. If an if-then-else\\ncommand\\ndepends\\non\\na\\nvariable\\ncontaining,\\nsay,\\n0.7,\\nthe\\nprogramme splits. Both the then-block and the else-block are\\nexecuted: the former weighted 0.7, the latter weighted 0.3. The\\ntwo branches may be reuniﬁed if the programmer wishes to do so.\\nThe variables are then aggregated, in a way that the weight of each\\nbranch is taken into account.\\nFor every data type involving real values – number, time,\\nduration – a fuzzy counterpart is added. Piecewise linear fuzzy sets\\nover the respective base set can be represented. Distinctions like\\n‘‘the patient is young’’ or ‘‘middle-aged’’ or ‘‘old’’ can be made in a\\nconvenient way, namely without the necessity to specify sharp\\nborders. Moreover, a fuzzy set can be defuzziﬁed afterwards by\\nmeans of a single command. For example, the behaviour of a\\nMamdani–Assilian controller can be achieved without noteworthy\\neffort.\\nFinally, all non-compound data types are endowed with an\\nadditional component called the degree of applicability. This\\ncomponent stores a truth value expressing, for instance, the degree\\nto which it would be reasonable to use the value in the variable’s\\nmain component. It is 1 by default, and whenever the programme\\nbranches it is reduced automatically according to the weight\\nassigned to the branch. The programmer may decide to make\\nexplicit use of this component but is not required to do so.\\nWhen processing vague data, one general problem inevitably\\nencountered in fuzzy logic is the fact that there is no canonical\\nchoice for the connectives on the extended set of truth values. As\\nwe also cannot solve this problem, the programmer may choose\\nbetween several options in Fuzzy Arden Syntax. Non-default\\noptions must be speciﬁed within a newly introduced category,\\nnamed fuzzy options. The available options will be explained in\\nthe relevant context.\\nLet us ﬁnally consider the question how well a Fuzzy-Arden-\\nSyntax-based CDSS might be accepted by the user. As a matter of\\nfact, clinicians supporting the formulation of MLMs or using a\\nfuzzy-logic-based environment will be asked for a slightly\\ndifferent point of view than before. In the process of writing an\\nMLM, clinicians are asked to provide data together with an explicit\\nT. Vetterlein et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 1–10\\n4\\n\\nindeterminacy, like, e.g., fuzzy boundaries of laboratory para-\\nmeters. Furthermore, the user of an MLM will be faced with the fact\\nthat in general more differentiated information is provided in the\\noutput than before. In practice this can mean that a set of weighted\\nalternatives is presented as result, a possibly uncommon situation.\\nNote that both mentioned points actually represent a progress.\\nAdhocdecisionsaboutexactboundaries ofthenormalrangeofsome\\nparameters are no longer necessary. Furthermore, complex results\\njust occur in borderline situations; to provide a simpliﬁed output in\\nsuch cases would hide the actual complexity of the situation.\\nIn both cases, however, a minimal understanding of the nature\\nof continuous truth degrees must be assumed. We are decidedly\\noptimistic in this respect. We can rely on a rich experience in the\\ncooperation with clinicians; the concept of a graded applicability is\\nusually quite easily understood and the acceptance is high.\\n5. Data types in Fuzzy Arden Syntax\\nWe will now begin describing the features of Fuzzy Arden Syntax\\nin a systematic way. For additional information, we refer to [3].\\nOur ﬁrst issue is to explain the data types. To maintain the\\nclarity of the presentation, the data types incorporated already in\\nArden Syntax will not be separately marked.\\nThere are simple and compound data types. In general, a\\nvariable is bound to a data type by assignment and a declaration is\\nnot necessary; thus Fuzzy Arden Syntax is loosely typed as is Arden\\nSyntax. Furthermore, a variable may contain the value null. This is\\nto reﬂect that the variable is undeﬁned, as for instance when the\\ndeﬁning expression is erroneous.\\nEvery simple data type contains three components: the main\\nvalue, the primary time, and the degree of applicability. The\\nfollowing explanations refer to the main component; the meaning\\nof the other two will be explained later. Neither the second nor the\\nthird component needs to be explicitly addressed at any time; both\\ncan safely be ignored if not needed.\\nIn Fuzzy Arden Syntax, the data type of propositional variables\\nis denoted by truth value or – for reasons of backwards\\ncompatibility – equivalently boolean. A variable of this type\\nstores real numbers between 0 and 1. We may write:\\nA variable of type number or, equivalently, crisp number\\nstores real numbers which can be represented in the available\\nﬂoating point format. A variable of type time or crisp time stores\\na date, a time, and optionally the referred time zone. A variable of\\ntype duration or crisp duration stores a length in time. For\\nexample:\\nWe next turn to the fuzzy data types. The data type fuzzy\\nnumber is dedicated to fuzzy sets over the reals. Needless to say,\\nfuzzy sets cannot be allowed to have an arbitrarily complicated\\nstructure. This is no essential restriction. In medicine, as in most\\nother applications, speciﬁc truth values do not have a speciﬁc\\nmeaning. Consequently, we may conﬁne ourselves to fuzzy sets of\\na simple form. We will assume that we may partition the reals into\\na ﬁnite number of (possibly unbounded) intervals on each of which\\nthe fuzzy set is linear and continuous.\\nFormally, a fuzzy set u : R ! ½0; 1\\x02 can be stored in a variable of\\ntype fuzzy number if the following condition is met: There are\\na1 < a2 < . . . < ak, k \\x04 1, in R such that u is linear on each open\\ninterval ða1; a2Þ; . . . ; ðak\\x051; akÞ, u is constant on ð\\x051; a1Þ and ðak; 1Þ,\\nand for each x 2 R, uðxÞ coincides either with the left limit or the\\nright limit of u at x. If u is continuous, we then deﬁne\\nFuzzyset_u:= fuzzy set ða1; t1Þ; ða2; t2Þ; . . . ; ðak; tkÞ;\\nwhere ti ¼ uðaiÞ for i ¼ 1; . . . ; k. Thus, for instance, the fuzzy set\\nR ! ½0; 1\\x02;\\ns 7!\\n1\\nif\\ns < 150\\n1\\n10ð160 \\x05 sÞ\\nif\\n150 \\x06 s < 160\\n0\\nif\\ns \\x04 160;\\n8\\n<\\n:\\nexpressing the predicate ‘‘small’’ for body size in centimetres, is\\ndeﬁned by:\\nSmall:= fuzzy set (150, 1), (160, 0);\\nDiscontinuities are, as usual, allowed to include the character-\\nistic functions, which are not likely to be required in applications,\\nbut should at least be deﬁnable. (Recall that characteristic\\nfunctions correspond to subsets; it maps all elements of a set A\\nto 1 and all elements of the complement of A to 0.)\\nAt discontinuity points we denote the left as well as the right\\nlimit. The ﬁrst assignment is taken to be the value at that point,\\nunless the second one appears twice. For instance,\\ngives the characteristic function of the set ½2; 3\\x02 \\x07 R.\\nFurthermore, the triangular normal fuzzy sets are likely to\\nappear frequently. This refers to those fuzzy sets whose graph\\nforms a symmetrical triangle around one point, which is mapped to\\n1. A simpliﬁed notation is permitted for these: an expression of the\\nform fuzzy set (a \\x05 b, 0), (a, 1), (a þ b, 0), where a; b 2 R and b > 0,\\nmay also be written as:\\na fuzziﬁed by b\\nAll deﬁnitions concerning the data type fuzzy number apply\\nmutatis mutandis to the data types fuzzy time and fuzzy\\nduration as well. The fuzziﬁed by operator, for instance, may be\\nused for pairs of a time and a duration; e.g., when referring to the\\ntime period approximately three days before the current time we\\nmay use the expression:\\n3 days ago fuzziﬁed by 12 h\\nFor the sake of completeness we mention that one more simple\\ndata type named string exists. A variable of this type stores text.\\nAs mentioned earlier, two additional components are included\\nin each simple data type. The so-called primary time is stored in\\nthe second component. This is typically the time at which the value\\nemerged, which could be the time the value was measured. When\\nreading data from the patient database this component may be\\nﬁlled automatically with a provided value; no explicit command is\\nneeded. Details are implementation-dependent, and the primary\\ntime is null if not speciﬁed.\\nThe third component contains what we call the degree of\\napplicability, or applicability for short. This is a truth value\\nexpressing the degree to which the main value may be considered\\napplicable. Typical examples would be:\\n(i) We wish to derive a statement about the development of a\\nquantity within the last 24 h. The limit of 24 h is chosen ad hoc\\nand should be considered fuzzy. We may collect the values\\nfrom the last, say, 28 h, and assign to each value a degree of\\nT. Vetterlein et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 1–10\\n5\\n\\napplicability of 1 if it originates from the preceding 24 h, else\\nsuccessively smaller values. The applicabilities may then be\\nused to weight the values accordingly in the subsequent\\ncalculation.\\n(ii) InFuzzyArdenSyntaxaprogrammearrivingatapointwherethe\\ncontinuation depends on a condition which is neither clearly\\nfalse nor clearly true, will split. Before executing one of the\\nbranches the set of variables is duplicated and the applicabilities\\nare reduced according to the weights of the branches.\\n(iii) The degree of applicability is also at the user’s free disposal. For\\ninstance, it may be attached to a variable in order to express\\nthat this value is not fully reliable.\\nThe third component of a variable Var can be read and changed\\nthrough the expression applicability of Var. By default it is set\\nto 1 and it is never undeﬁned.\\nTwo compound data types exist: list and object. A list\\nvariable stores a sequence of n values of a simple data type, where\\nn \\x04 0 is dynamic. An object variable stores a ﬁxed number of values\\nof a simple data type; the components are denoted by speciﬁc\\nidentiﬁers, which must be declared in advance. For reasons which\\nwill become apparent in Section 10 below, the key word object\\nmay be replaced by linguistic variable.\\n6. Operations in Fuzzy Arden Syntax\\nThe question as to how one should interpret the logical\\nconnectives for generalised truth values, has always been a\\ndelicate one. A generally valid recommendation cannot be made. In\\nfact, it would even not be permissible to say that a speciﬁc type of\\napplication requires a speciﬁc set of connectives.\\nOnly a few basic properties are usually required, namely those\\nwhich are found to be natural for the respective connective. The\\nconjunction\\nis\\ncommonly\\ninterpreted\\nby\\na\\nfunction\\n\\x08 :\\n½0; 1\\x022 ! ½0; 1\\x02 which is associative, commutative, neutral with\\nrespect to 1, and in both arguments isotone. Such a function is\\ncalled a t-norm [16]. The disjunction is usually taken as the\\ncorresponding t-conorm, and the negation as the subtraction from\\n1. This standard is assumed in Fuzzy Arden Syntax as well;\\nhowever, subroutines to interpret the connectives may also be\\ndeﬁned by the programmer.\\nThe three basic operations to combine truth values, are and, or,\\nand not. To interpret and, the user may opt for the Łukasiewicz t-\\nnorm \\x08”, the product t-norm \\x08P, or the Go¨del t-norm \\x08G, where\\na\\x08”b ¼ max fa þ b \\x05 1; 0g;\\na\\x08Pb ¼ a \\t b;\\na\\x08Gb ¼ min fa; bg\\nfor a; b 2 ½0; 1\\x02. The choice is realised by an option in the fuzzy\\noptions category. By default, the Go¨del t-norm is used. Further-\\nmore, the and connective may be speciﬁed by the user, in which\\ncase the command\\nconjunction by\\nMLM ‘UserspeciﬁedConjunction’;\\nmust appear. Here, UserspeciﬁedConjunction must be the name\\nof an MLM which accepts two truth values as its input and returns one\\ntruth value as output. It is presumed that this MLM encodes a t-norm.\\nAs usual in Arden Syntax, however, it is not checked whether this\\nMLM fulﬁls any of the special properties which are expected from a\\nconjunction.\\nThe interpretation of the disjunction and the negation may be\\nchosen in a similar fashion, independent of the conjunction.\\nHowever, we presume the user will make use of this freedom only\\nin the exceptional case. By default the conjunction is the t-conorm\\n\\n associated to the t-norm \\x08 in use, that is\\n\\n : ½0; 1\\x022 ! ½0; 1\\x02;\\nða; bÞ 7! 1 \\x05 ðð1 \\x05 aÞ\\x08ð1 \\x05 bÞÞ:\\nThus, in case that the default t-norm is used the maximum of the\\ntwo truth values is taken. Moreover, the negation is, by default, the\\nstandard negation\\n½0; 1\\x02 ! ½0; 1\\x02;\\nt 7! 1 \\x05 t:\\nNote that if one of the three standard t-norms, its corresponding\\nt-conorm and the standard negation is chosen, then the sharp truth\\nvalues 0 and 1 are connected as in classical two-valued logic. Thus,\\nthe compatibility with Arden Syntax is ensured in this case, the\\nspecial treatment of null included.\\nIn the present context two further connectives, denoted at\\nleast and at most, are important. In medical literature, when\\nlisting symptom combinations specifying a situation in which the\\npresence of a certain disease is assumed we frequently encounter\\nphrases like ‘‘at least two of the following conditions must be met:\\n. . .’’. By default these connectives are deﬁned by the basic ones. For\\ninstance, let List be a list of truth values; then the expression\\nat least n of List\\nis the disjunction of all conjunctions of exactly n entries in List. We\\neasily check that if the default connectives are used, the displayed\\nexpression returns the n-th largest value in List. The connective at\\nmost is speciﬁed similarly.\\nHowever, there are further reasonable possibilities to interpret\\nat least. In Fuzzy Arden Syntax, a user-deﬁned deﬁnition can be\\ngiven, which need not be related to the chosen t-norm. For\\ninstance, the following interpretation has been proposed to\\ninterpret at least n of List:\\nmin fv1 þ \\t \\t \\t þ vk; ng\\nn\\n;\\nwhere v1; . . . ; vk are the truth values contained in List.\\nA few other logical operations are provided. For instance, any\\nof List is the disjunction of the truth values contained in the list\\nList. These operations depend on the three main connectives\\nmentioned, and we will not enumerate them.\\nWe next turn to the operations with numbers. Number\\nvariables may be connected by the basic arithmetic operations\\nþ; \\x05; $; =. A few other common functions are available. For time\\nand duration variables these operations are also deﬁned whenever\\nit makes sense.\\nFor variables storing fuzzy sets, addition and subtraction as well\\nas multiplication with, and division by, positive crisp reals are\\ndeﬁned according to Zadeh’s extension principle, provided that the\\nrespective operation is well-deﬁned. Zadeh’s extension principle\\n[4] is the canonical way to extend operations on real numbers to\\nfuzzy sets. For instance, the sum u þ v of fuzzy sets u and v over R is\\ndeﬁned by\\nðu þ vÞðxÞ ¼ sup fuðyÞ ^ vðzÞ : y þ z ¼ xg\\nfor x 2 R.\\nFinally, numbers as well as times and durations may be\\ncompared with respect to their natural order. The comparison of\\ntwo crisp numbers yields a crisp truth value.\\nFurthermore, fuzzy sets being available, we need a way to query\\nthe compatibility of crisp values with properties modelled by fuzzy\\nsets. A crisp number r, contained, say, in Var, may be correlated to a\\nfuzzy number u contained, say, in FuzzyVar, by the expression\\nT. Vetterlein et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 1–10\\n6\\n\\nVar is FuzzyVar\\nwhich simply gives the value of u at r, that is, uðrÞ. Moreover, the\\nexpression Var <= FuzzyVar returns sup fuðxÞ : r \\x06 xg, and similarly,\\nVar >= FuzzyVar returns sup fuðxÞ : r \\x04 xg. Analogous deﬁnitions\\napply for fuzzy times and fuzzy durations.\\nFinally, if FuzzyVar stores a fuzzy set, we may call a\\ndefuzziﬁcation function by\\ndefuzziﬁed Fuzzyset\\nreturning an element of the respective domain. Centre-of-gravity\\ndefuzziﬁcation and mean-of-maximum defuzziﬁcation are prede-\\nﬁned (for these methods, see, e.g., [5]). A user-deﬁned function\\nmay be chosen in the fuzzy options category as well.\\nOne operation is especially important in Arden Syntax and has\\nbeen modiﬁed in Fuzzy Arden Syntax. Given a list, we may form a\\nsublist by means of the operator where. We may actually think of\\nthe where operator as an operator forming a subset. In Fuzzy Arden\\nSyntax this operator should serve to form a fuzzy subset. Let a list\\nList:= Value1, . . ., Valuen;\\nbe given, and let Condition(\\t) be an expression of type truth value\\nwith one free variable. Then\\nList where Condition(it)\\narises from List as follows: For each i, the degree of applicability of\\nValuei is connected with Condition(Valuei) by the and-operator.\\nThe result, if deﬁned and > 0, is stored as the new applicability of\\nValuei; otherwise, the entry Valuei is removed from the list.\\nTo see how this command works, let us reconsider the above\\nexample. We may produce a list with a patient’s body temperature\\nfrom the last approximately 24 h using the following:\\nTempatureList:= read {temperature} where\\nit occurred within the past 24 hours\\nfuzziﬁed by 4 hours;\\nThis leads, just as proposed above, to a list of values from the\\nlast 28 h where the applicability of the values from the time period\\nbetween 28 h ago and 24 h ago is reduced: The older the value is,\\nthe smaller is its applicability.\\n7. Conditional statements and branching in Fuzzy Arden\\nSyntax\\nConditions in Fuzzy Arden Syntax may be indeterminate. The\\ntruth values vary continuously from 0 to 1. Therefore, a command\\ndirecting the programme ﬂow into one of two or more blocks must\\nbe\\ncarefully interpreted,\\ndepending on\\nthe\\ncontent of the\\nrespective propositional variable.\\nIn full accordance with what we are familiar with, an if-then-\\nelse statement in Fuzzy Arden Syntax would be as follows:\\nwhere Condition is an expression of type truth value. However,\\nits manner of execution is one of the main differences between\\nArden and Fuzzy Arden Syntax.\\nThe command is executed as follows. If Condition is 1, block 1\\nis executed; if Condition is 0 or null, block 2 is executed. If,\\nhowever, Condition is t 2 ð0; 1Þ, the programme splits: block 1\\nand block 2, named programme branches in the sequel, will be\\nexecuted in parallel. To this end, each branch is provided with its\\nown set of variables which, accordingly, are duplicated. Moreover,\\nthe degree of applicability of each variable is in case of block 1\\nmultiplied by t, in case of block 2 multiplied by 1 \\x05 t. t and 1 \\x05 t are\\ncalled the relative weights of block 1 and block 2, respectively.\\nThe programme may branch several times. Each command\\nexecuted during the run of the programme is assigned a weight in\\nthe straightforward manner. The weight is 1 as long as the\\nprogramme does not split; when the weight is w and the\\nprogramme enters a branch with relative weight t, the weight\\nwill be reduced to w \\t t.\\nIn a branch of weight w, the range of the degree of applicability\\nof any variable is ½0; w\\x02. Whenever the content of a variable is\\nchanged its applicability will be reduced to w if necessary.\\nThe number of branches into which the programme may split at\\na time is not limited to two. Branching into n þ 1 blocks is coded as\\nfollows:\\nIn this case the relative weight ti of the i-th branch is given by\\nConditioni, where i ¼ 1; . . . ; n. The case that Conditioni is\\nundeﬁned is treated like ti ¼ 0, in which case the branch is not\\nexecuted. Moreover, if the sum of the ti is strictly smaller than 1,\\nthe relative weight of block n þ 1 will be 1 \\x05 t1 \\x05 . . . \\x05 tn, else this\\nblock is skipped.\\nThe possibility of letting the programme branch into more than\\ntwo programme blocks is one of the signiﬁcant features of Fuzzy\\nArden Syntax. Quite often we have to distinguish between\\nconditions of the form Var is FuzzySet1, . . ., Var is FuzzySetn,\\nwhere Var is a crisp value and FuzzySet1, . . . are fuzzy values. We\\nallow an abbreviating syntax for this case, namely,\\nThe same is possible if the conditions are of the form Var =\\nValue1, . . . for any data type of the involved variables.\\nAn example will follow in Section 10. In the following we will\\ndescribe how one proceeds after completion of an if-then-else\\ncommand.\\n8. Conditional statements and aggregation in Fuzzy Arden\\nSyntax\\nOnce all branches of a programme have completed their\\nexecution in parallel because of an unsharp condition, it is difﬁcult\\nto issue a general recommendation as to how one should proceed.\\nTwo possibilities exist:\\n(A) The programme remains split, that is, all subsequent com-\\nmands are executed in parallel as well, the action slot included.\\n(B) The programme reuniﬁes. The multiplied variables are merged\\ninto single ones.\\nBoth options are available in Fuzzy Arden Syntax; possibility (A)\\nis the default. The more appropriate option in the individual\\nsituation should be decided on the basis of the speciﬁc application.\\nIf (A) is selected the MLM’s results will be provided by each\\nbranch separately. The unit to which the results are sent – the host\\nT. Vetterlein et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 1–10\\n7\\n\\nsystem or the calling MLM – must be prepared to deal with the\\nsituation.\\nIf the MLM sends information to the host, the host system has to\\nprocess possibly divergent information. The key tool to be used is\\nthe third component of the data: the degree of applicability must\\nthen be interpreted by the host in order to conclude the relevance\\nof the received data. For instance, when displaying the information\\nto the user a clear statement as to the applicability must be added\\nor the information must be modiﬁed in another appropriate way.\\nThe user in turn has to understand that the data, in case of low\\napplicability, is to be interpreted as one of several possibilities.\\nIf the MLM is called by another MLM and returns data the\\ncalling MLM splits accordingly as well.\\nThe possibility (B) implies that the task of combining divergent\\npieces of information is executed within the MLM itself. To opt for\\n(B), the ﬁnal line of an if-then-else statement is modiﬁed: after the\\nkey word endif or endswitch, respectively, the key word\\naggregate is added. Thus, when writing\\nthe two branches unify after their execution. The programme\\nweight is then set to the sum of the weight of the branches, i.e., to\\nthe same value as before.\\nMoreover, corresponding variables are aggregated. Let Var be a\\nvariable deﬁned in at least one branch. As far as the main\\ncomponent is concerned, the procedure is as follows. If the content\\nof Var is the same in each branch, the content is taken over.\\nOtherwise, if Var is deﬁned in all branches and of the same simple\\ndata type except string, the contents are aggregated according to\\na predeﬁned method. If Var is of the same compound type in all\\nbranches, we proceed successively with the components in the\\nsame manner.\\nIn the remaining cases Var is set to null. For example, this is\\napplicable when a string variable is assigned a different text in two\\nbranches.\\nNo\\nmethod\\nis\\ncurrently\\nable\\nto\\naggregate\\ntext\\nautomatically or make a canonical choice.\\nThe aggregation method may be speciﬁed in the fuzzy options\\nslot, separately for crisp and fuzzy data. Numerous methods are\\navailable to aggregate data and the implementation of a user-\\ndeﬁned function is possible. By default the weighted mean is\\ncalculated. Thus, e.g., the values r1; . . . ; rn\\nwith degrees of\\napplicability t1; . . . ; tn, respectively, are aggregated to\\nt1r1 þ \\t \\t \\t þ tnrn\\nt1 þ \\t \\t \\t þ tn\\n;\\nthe same formula is used for crisp and fuzzy data. In the fuzzy case\\nanother predeﬁned method is available, namely, the supremum of\\nthe ri, cut off at height ti, may be taken:\\nðr1 ^ ¯t1Þ _ . . . _ ðrn ^ ¯tnÞ;\\n(1)\\nhere, ¯ti is the constant ti function, ^ connects two fuzzy sets by the\\npointwise minimum, and _ connects two fuzzy sets by the\\npointwise maximum.\\nTo aggregate the contents of variables with respect to the\\nremaining two components is straightforward. The primary time of\\nVar is taken over if coincident in all branches. If distinct times\\nappear we can no longer assume that these times are related to the\\ntime at which the value emerged; the primary time will be set to\\nnull in this case.\\nFurthermore, as might be expected, the degrees of applicability\\nare added. Thus, if left unchanged during the execution of all\\nbranches, the applicabilities prior to the execution of the if-then-\\nelse statement, will be restored.\\nThe conclude command is treated exactly in the same manner\\nas the if-then statement. Thus, the command\\nconclude Condition;\\nis interpreted as ‘‘if Condition applies, then jump to the action part,\\nelse quit the programme’’. In other words, if Condition yields a value\\n> 0, the applicabilities of all variables are multiplied by this value, and\\nthe action slot is executed. If Condition is 0 or undeﬁned, the\\nprogramme or this branch of the programme is terminated.\\n9. Fuzzy Arden Syntax versus Arden Syntax\\nThis section will deal with the example from Section 4: the\\nMLM UTI_SUTI. Let us see how a ‘‘fuzziﬁed’’ version of this MLM\\nlooks in Arden Syntax on the one hand and in Fuzzy Arden Syntax\\non the other hand.\\nIn Arden Syntax the result would look as follows, provided the\\nlogical connectives are chosen as the Fuzzy Arden Syntax default\\nconnectives.\\nIt is not necessary to print the MLM UTI_SUTI from Section 4\\nagain in order to show what the fuzzy version looks alike under\\nT. Vetterlein et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 1–10\\n8\\n\\nFuzzy Arden Syntax. The following changes must be made: (i)\\nreplace Temperature > 39 by Temperature >= 39 fuzziﬁed by\\n1; (ii) replace Organ_urine_culture >= 1e5 by Organ_ur-\\nine_culture >= 1e5 fuzziﬁed by 5e4.\\nFuzzy logical calculations have to be stated explicitly in Arden\\nSyntax. This makes the programme more cumbersome. The only\\nadvantage might be transparency.In FuzzyArden Syntax,practically\\nnothing changes in the programme text. The user who is familiar\\nwith Arden Syntax, however, must have some knowledge of the\\ndifferent ways in which the programme is interpreted.\\nNote that in case of emulation under Arden Syntax as well as in\\ncase of Fuzzy Arden Syntax, further processing of the result is not\\nincluded in the MLMs. However, as the input values are provided in\\na more differentiated way, the result has to be treated in a more\\ndifferentiated way as well. Considerably more effort is needed to\\ncommunicate a result to the user if vagueness or uncertainty are\\ninvolved. In the example, there will be more than the possibilities\\nof ‘‘a SUTI applies’’ and ‘‘there is no evidence of a SUTI’’. The result is\\na continuous truth degree, to which one message from a larger set\\nof messages should be associated, the two mentioned ones\\nrepresenting the extreme cases.\\n10. Fuzzy inference with Fuzzy Arden Syntax\\nThis section illustrates how fuzzy inference may be realised in\\nFuzzy Arden Syntax.\\nIn applications, fuzzy sets usually appear when partitioning a\\ndomain of values into subdomains without sharp borders and\\ntypically a conditioning according to this partition follows. A fuzzy\\npartition over a domain M is a partition of unity over M, that is, a\\nﬁnite set u1; . . . ; un of fuzzy sets such that u1ðxÞ þ \\t \\t \\t þ unðxÞ ¼ 1\\nfor each x 2 M. In Fuzzy Arden Syntax, a fuzzy partition is\\nconveniently stored in an object variable; we recall that instead\\nof the key word object, we may equivalently use the key word\\nlinguistic variable.\\nLet us consider the following declaration:\\nConsequently,\\nwe\\nhave\\ncreated\\nthree\\nvariables,\\nnamely\\nAge.Young, Age.Middle_Aged, and Age.Old. Each of these\\nvariables is ready to be used like any variable of a simple data type.\\nWe proceed by assigning these three variable fuzzy sets which\\nform a partition of unity over the positive durations:\\nWe may now use the blurred age ranges for distinctions as\\nfollows:\\nThen the variable DoseforPatient will contain a weighted\\nmean of the three values LowDose, MiddleDose, and HighDose,\\nwhere the weights reﬂect the, possibly partial, compatibility of\\nAgeofPatient with the three age ranges.\\nThe example may be modiﬁed to include the case that the doses\\nare fuzzy as well. If Dose.Low, Dose.Middle, Dose.High is a\\nfurther partition of unity we may write as follows:\\nIn this case DoseforPatient contains the result of the\\naggregation of three fuzzy sets rather than the aggregation of\\nthree crisp values. The outcoming fuzzy set is defuzziﬁed to the\\ncrisp value DoseTobetaken.\\nNote that, when choosing the supremum as the aggregation\\nmethod for fuzzy sets, our last sample programme imitates a\\nMamdani–Assilian controller. Namely, it ﬁrst determines to which\\nextend the input value, i.e., the age, is compatible with each of the\\nthree conditions; let t1; t2; t3 be the resulting truth values. Then,\\nthe fuzzy sets modelling a low, middle, and high dose, say r1; r2; r3,\\nrespectively, are aggregated to\\nðr1 ^ ¯t1Þ _ ðr2 ^ ¯t2Þ _ ðr3 ^ ¯t3Þ:\\nFinally, the last command defuzziﬁes this fuzzy set and provides a\\nsharp output value. For an explanation of a Mamdani–Assilian\\ncontroller, see, e.g., [5]. Note that the text of the programme is not\\nmore complicated than the text of if-then rules.\\n11. Fuzzy Arden Syntax: a view back and forward\\nOur work relies on ideas which were developed in the Ph.D.\\nThesis of S. Tiffe [2] in 2003. When comparing our proposal to\\nTiffe’s, we see that most elements were modiﬁed. Some of the\\nchanges are due to the fact that [2] is based on the older Arden\\nSyntax version 2.1 and several improvements in Arden Syntax have\\nbeen effected since that time. To explain all differences would\\nexceed the present framework; the interested reader is referred\\ndirectly to [2]. However, we shall list some facts.\\n\\x0b Tiffe’s degree of presence and degree of applicability were replaced\\nby\\nour\\ndegree\\nof\\napplicability\\nand\\nthe\\nprogram\\nweight,\\nrespectively. The function of these values and their mutual\\nrelationship between these two concepts has been newly\\ndeﬁned. Furthermore, a truth value describing the applicability\\nis associated with any data type and can be manually modiﬁed.\\n\\x0b In [2], a canonical way how to proceed after programme\\nbranching is not proposed; instead, several different possibilities\\nare presented. In contrast, we have decided to let always all\\nbranches of a split programme be executed in parallel and we\\nrequire the user to specify how the results are recombined.\\nThe difference is most evident in case of different texts sent to\\nthe host by different branches. If, for instance, there are results\\n‘‘Give medicament M immediately’’ and ‘‘To give medicament M\\nis not recommended’’ from two program branches, it seems\\ninappropriate to select one of these texts on the basis of their\\napplicabilities. The user should rather specify how to deal with\\ncontradictory recommendations.\\n\\x0b We do not fuzzify while- or for-loops. Actually, in [2] a warning\\nagainst while-loops is already contained.\\nT. Vetterlein et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 1–10\\n9\\n\\n\\x0b To process continuous truth values, we have generalised the\\nexisting boolean data type rather than introducing a new data\\ntype.\\nFurthermore, we allow to associate the value null to a\\nvariable storing a truth value. It is a difference not to specify a\\ntruth degree or to use the value 0.\\n\\x0b We have strictly simpliﬁed the usage of linguistic variables. In\\nparticular, the use of external MLMs to deﬁne fuzzy sets are not\\nnecessary. The new features of Arden Syntax version 2.5\\ncompared to 2.1 made these improvements possible.\\nAn implementation of Fuzzy Arden Syntax according to the\\nspeciﬁcation [3] is in progress. Whereas for the user, the transition\\nfrom Arden to Fuzzy Arden Syntax might not be serious, the effort\\nneeded for an implementation of the extended language is quite\\nhigh. The implementation of a Fuzzy Arden Syntax compiler will\\npresumably be ﬁnished in the last quarter of 2009. Its subsequent\\nuse within the new data management system of the Vienna\\nGeneral Hospital is scheduled. Afterwards, a systematic test of the\\nfuzzy extension will be undertaken.\\n12. Conclusion\\nWe have outlined the speciﬁcation of an extension of the\\nprogramming language Arden Syntax, which is designed for CDS\\napplications in medicine. Our primary reason for elaborating Tiffe’s\\nFuzzy Arden Syntax is to incorporate the possibility to deal with\\nindeterminate data, i.e., when the processed information is vague\\nor uncertain.\\nWe focused on keeping Fuzzy Arden as simple as Arden Syntax.\\nThe syntax is kept as close to natural language as is the case with\\nArden Syntax and ensures easy comprehension of an MLM’s\\ncontents. Indeed a programme text, when fuzzy rather than crisp\\ndata is processed, remains practically the same in typical\\napplications. When a Mamdani–Assilian inference is realised the\\nprogramme text is closely correlated to the text of the if-then rules\\non which the inference is based.\\nThe advantage of Fuzzy Arden Syntax when compared to Arden\\nSyntax is evident. The actual beneﬁts of Fuzzy Arden Syntax will\\nhave to be proven in clinical practice.\\nReferences\\n[1] Arden Syntax for Medical Logic Systems, Version 2.5, Health Level Seven; 2005.\\n[2] Tiffe S. Fuzzy Arden Syntax: representation and interpretation of vague\\nmedical knowledge by Fuzziﬁed Arden Syntax. Ph.D. thesis. Vienna: Technical\\nUniversity Vienna; 2003.\\n[3] Vetterlein T, Mandl H, Adlassnig KP. Vorschla¨ge zur Speziﬁkation der Pro-\\ngrammiersprache Fuzzy Arden Syntax (Proposal of a speciﬁcation of the\\nprogramming language Fuzzy Arden Syntax—in German), technical report.\\nVienna: Vienna University of Technology; 2008. Available at http://www.\\nmeduniwien.ac.at/user/thomas.vetterlein/articles/FuzzyArdenSpezif.pdf (last\\naccessed: 20 October, 2009).\\n[4] Zadeh LA. Fuzzy sets. Inf Control 1965;8:338–53.\\n[5] Nguyen HT, Walker EA. A ﬁrst course in fuzzy logic. Boca Raton: Chapman &\\nHall/CRC; 2006.\\n[6] Mamdani EH, Assilian S. An experiment in linguistic synthesis of fuzzy con-\\ntrollers. Int J Man-Mach Stud 1975;7:1–13.\\n[7] Schuh C, Hiesmayr M, Ehrengruber T, Katz E, Neugebauer T, Adlassnig KP, et al.\\nFuzzy knowledge-based weaning from artiﬁcial ventilation (FuzzyKBWean).\\nIn: Jamshidi M, Fahti M, Pierrot F, editors. Proceedings of the world automation\\ncongress 1996. 1996. p. 583–8.\\n[8] Adlassnig K-P, Kolarz G. CADIAG-2: computer-assisted medical diagnosis\\nusing fuzzy subsets. In: Gupta MM, Sanchez E, editors. Approximate reasoning\\nin decision analysis. Amsterdam: North-Holland Publ. Comp.; 1982. p. 219–47.\\n[9] Clark DF, Kandel A. HALO—a fuzzy programming language. Fuzzy Sets Syst\\n1991;44:199–208.\\n[10] Baldwin JF, Martin TP, Pilsworth BW. Fril: fuzzy and evidential reasoning in\\nartiﬁcial intelligence. New York: John Wiley & Sons; 1995.\\n[11] Munakata T. Notes on implementing fuzzy sets in Prolog. Fuzzy Sets Syst\\n1996;98:311–7.\\n[12] Morales-Bueno R, Conejo R, Pe´rez de la Cruz JL, Clares B. An elementary fuzzy\\nprogramming language. Fuzzy Sets Syst 1993;57:55–73.\\n[13] Zhao X, Li F. Denotational semantics of dynamic fuzzy logic programming\\nlanguage. In: Zhang YQ, Lin YT, editors. Proceedings of the IEEE international\\nconference 2006 on granular computing. 2007. p. 409–12.\\n[14] Garner JS, Jarvis WR, Emori TG, Horan TC, Hughes JM. CDC deﬁnitions for\\nnosocomial infections.\\nIn: Olmsted RN, editor. APIC infection control and\\napplied epidemiology: principles and practice. Mosby: St. Louis; 1996. p. A1–\\n20.\\n[15] Adlassnig KP, Blacky A, Koller W. Artiﬁcial-intelligence-based hospital-ac-\\nquired infection control. Stud Health Technol Inf 2009;149:103–10.\\n[16] Klement EP, Mesiar R, Pap E. Triangular norms. Dordrecht: Kluwer Acad. Publ.;\\n2000.\\nT. Vetterlein et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 1–10\\n10\\n'}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/Fuzzy-Arden-Syntax--A-fuzzy-programming-langu_2010_Artificial-Intelligence-i.pdf', 'text': 'Fuzzy Arden Syntax: A fuzzy programming language for medicine\\nThomas Vetterlein a,*, Harald Mandl b, Klaus-Peter Adlassnig a,b\\na Section for Medical Expert and Knowledge-Based Systems, Medical University of Vienna, Spitalgasse 23, 1090 Vienna, Austria\\nb Medexter Healthcare GmbH, Borschkegasse 7/5, 1090 Vienna, Austria\\n1. Introduction\\nIn medical computer science, intelligent management of clinical\\ndata is the objective of what is known as clinical decision support, or\\nCDS for short. The performance of a CDS system (CDSS) clearly\\nextends the mere storage and user-speciﬁedmanipulation ofpatient\\ndata. Its essential feature is the ability to provide consequences of\\nthe digitally handled medical information, derived on the basis of a\\nmedical knowledge base. A logical inference module provides\\nmethods of deducing consequences from the available data in\\naccordance with coded interrelations between medical facts.\\nWe deal here with an established programming language\\ndesigned especially for computerised analysis of medical data.\\nThe language is named Arden Syntax and has been used for ﬁfteen\\nyears now; it was ﬁrst published by the American Society for Testing\\nand Materials in 1992. The language has been developed since. The\\npresent paper refers to Arden Syntax 2.5, which was published\\nby Health Level Seven, Inc., in 2005. Complete speciﬁcations can\\nbe found in [1], and further information on the subject is available\\non the Web, e.g., at http://www.hl7.org/implement/standards/\\nardensyntax.cfm (accessed on 10 October 2008).\\nArden Syntax is a convenient tool for the implementation of a\\nCDSS. The aim of an Arden-Syntax-based CDSS is to derive, from a\\npatient’s electronic record, information suitable to facilitate or\\nimprove clinical decision-making. The user might employ the CDSS\\nby speciﬁcally calling some of its functions. However, in order to\\nachieve optimal performance the system should run permanently in\\nthe background and notice in real time whether the available data\\nimply the necessity of a consequence. In this case, a proposal or if\\nnecessary a warning is communicated to the clinical personnel. A\\nArtiﬁcial Intelligence in Medicine 49 (2010) 1–10\\nA R T I C L E\\nI N F O\\nArticle history:\\nReceived 6 March 2009\\nReceived in revised form 15 December 2009\\nAccepted 8 January 2010\\nKeywords:\\nClinical decision support system\\nArden Syntax\\nFuzzy Arden Syntax\\nFuzzy set theory\\nA B S T R A C T\\nObjective: The programming language Arden Syntax has been optimised for use in clinical decision\\nsupport systems. We describe an extension of this language named Fuzzy Arden Syntax, whose original\\nversion was introduced in S. Tiffe’s dissertation on ‘‘Fuzzy Arden Syntax: Representation and\\nInterpretation of Vague Medical Knowledge by Fuzziﬁed Arden Syntax’’ (Vienna University of\\nTechnology, 2003). The primary aim is to provide an easy means of processing vague or uncertain\\ndata, which frequently appears in medicine.\\nMethods: For both propositional and number data types, fuzzy equivalents have been added to Arden\\nSyntax. The Boolean data type was generalised to represent any truth degree between the two extremes\\n0 (falsity) and 1 (truth); fuzzy data types were introduced to represent fuzzy sets. The operations on truth\\nvalues and real numbers were generalised accordingly. As the conditions to decide whether a certain\\nprogramme unit is executed or not may be indeterminate, a Fuzzy Arden Syntax programme may split.\\nThe data in the different branches may be optionally aggregated subsequently.\\nResults: Fuzzy Arden Syntax offers the possibility to formulate conveniently Medical Logic Modules\\n(MLMs) based on the principle of a continuously graded applicability of statements. Furthermore, ad hoc\\ndecisions about sharp value boundaries can be avoided. As an illustrative example shows, an MLM\\nmaking use of the features of Fuzzy Arden Syntax is not signiﬁcantly more complex than its Arden Syntax\\nequivalent; in the ideal case, a programme handling crisp data remains practically unchanged when\\ncompared to its fuzziﬁed version. In the latter case, the output data, which can be a set of weighted\\nalternatives, typically depends continuously from the input data.\\nConclusion: In typical applications an Arden Syntax MLM can produce a different output after only slight\\nchanges of the input; discontinuities are in fact unavoidable when the input varies continuously but the\\noutput is taken from a discrete set of possibilities. This inconvenience can, however, be attenuated by\\nmeans of certain mechanisms on which the programme ﬂow under Fuzzy Arden Syntax is based. To write\\na programme making use of these possibilities is not signiﬁcantly more difﬁcult than to write a\\nprogramme according to the usual practice.\\n\\x02 2010 Elsevier B.V. All rights reserved.\\n* Corresponding author. Tel.: +43 1 40400 6665; fax: +43 1 40400 6667.\\nE-mail address: Thomas.Vetterlein@meduniwien.ac.at (T. Vetterlein).\\nContents lists available at ScienceDirect\\nArtificial Intelligence in Medicine\\njournal homepage: www.elsevier.com/locate/aiim\\n0933-3657/$ – see front matter \\x02 2010 Elsevier B.V. All rights reserved.\\ndoi:10.1016/j.artmed.2010.01.003\\n\\nCDSSshouldnotmerelyperformroutinetasks, butgivediscretehints\\nastowhatshouldorshouldnotbedone.Oneoftheprimaryobjectives\\nis to avoid the mistakes that become evident from the available data,\\nprovided the appropriate knowledge has been entered previously.\\nIn\\nprinciple\\nan\\ninference\\nmechanism\\nbased\\non\\nmedical\\nknowledge may be coded in any programming language. However,\\nArden Syntax is equipped with several features which were\\nspeciﬁcally chosen for the intended application: (i) a syntax close\\nto natural language, which renders a programme to a large extent\\nreadable by the non-expert in computer science; (ii) thorough\\nchoice of just a few data types typically needed for medical\\ndocumentation, including time and duration; (iii) the possibility to\\nprocess\\nand\\nto\\ncommunicate\\nreal-time\\nevents,\\ndetermined\\naccording to the host system speciﬁcations; (iv) easy handling\\nof temporal relations, supported in particular by a time component\\nattached to all simple data types; (v) high error tolerance of Arden\\nSyntax programmes, selecting, in case of doubt, the option most\\nlikely meant by the user, rather than interrupting the work ﬂow\\nevery time by an error message.\\nThe function of an Arden-Syntax-based system is to check a\\npatient’saccessible digitaldatawithregardtospeciﬁcconditions and\\nreact appropriately if the conditions apply. Needless to say, medical\\ndata do not always permit a clear distinction between whether a\\nspeciﬁc condition is fulﬁlled or not; borderline cases defy this\\nstipulation. When the assumptions of an implication are approxi-\\nmately but not entirely fulﬁlled, it is, from a strict point of view, not\\npossible to draw any conclusion. However, a conclusion can well be\\ndesirable. This situation suggests the implementation of methods\\nprovided by fuzzy logic. In fuzzy logic, a conclusion is permitted,\\nprovided thatappropriatemodiﬁcationsare appliedtothe result:the\\noutgoing statement is weakened with regards to content. The\\nsimplest way to realise this idea is to assign a weight to the outcome.\\nIn case of more than one possible outcome, several distinct outputs\\nmight result, all weighted with a value strictly smaller than 1.\\nBased on these considerations, a conservative extension of Arden\\nSyntax was proposed by S. Tiffe in his Ph.D. thesis [2]. The principle is\\nto generalise classical two-valued logic to a many-valued logic; the\\nextension is named Fuzzy Arden Syntax. The work presented here\\nhas taken up this line and led toan alternative version of Fuzzy Arden\\nSyntax, whose full speciﬁcation can be found in [3]. A comparison to\\nTiffe’s work is contained in Section 11.\\n2. Fuzzy logic to process vague information\\nThe features which have been added to Arden Syntax aim at\\nsimplifying programmes which process indeterminate data by\\nmeans of fuzzy logic. Let us review very shortly the signiﬁcance of\\nthis methodology.\\nAs a starting point for the development of fuzzy logic, often\\nZadeh’s article in 1965 is seen [4]. The ﬁeld has since then\\ndeveloped enormously and into several different directions.\\nAmong the numerous general monographs on the topic we may\\nrecommend [5].\\nThe main idea is to generalise the notion of a truth value. Rather\\nthan using two truth values 0 and 1, modelling ‘‘false’’ and ‘‘true’’,\\nrespectively, typically the elements of the entire real unit interval\\n½0; 1\\x02 are used. When a truth value is associated with a vague\\nstatement, it expresses the degree to which a proposition is true. In\\nparticular, 0 expresses the clear falsity while 1 expresses the clear\\ntruth of a proposition; anything between the two just expresses a\\ntendency.\\nFuzzy logic is at present widely used for controlling tasks and in\\nautomated decision support. Its use is implied in all cases in which\\nthe precise speciﬁcation of a problem is not possible or too\\ncomplex, where, however, a speciﬁcation in vague terms is\\navailable and assumed to be fully sufﬁcient for the practical\\nneeds. In controlling applications, for example, instruction sets are\\noften available in natural language. A standard method to use these\\ninstructions directly, dealing with the involved vagueness in an\\namazingly efﬁcient way, is due to Mamdani and Assilian [6].\\nVariants of the Mamdani–Assilian controller exist, according to\\none of which, e.g., the system FuzzyKBWean [7] is based, which\\nprovides recommendations to the clinical personnel of an optimal\\nchoice of instrument parameters when weaning off an intensive\\ncare patient from intubation. In CDS, a problem may be to specify a\\npathological state on base of the lower and upper boundaries of\\nlaboratory values. The use of fuzzy sets rather than sharp intervals\\nprevents the otherwise inevitable discontinuities of such speciﬁ-\\ncations. Boundary cases are treated appropriately and jumps of the\\noutput by arbitrary small change of the input are avoided. The\\nmedical expert systems CADIAG are an example; see, e.g., [8].\\nBy default, fuzzy logic is rather speciﬁc about the type of truth\\ndegrees dealt with. A truth value in fuzzy logic does not represent\\nuncertainty in the sense of lack of knowledge. It rather represents to\\nwhat degree some well-speciﬁed fact, represented, e.g., by a real\\nnumber, is in accordance with a vague notion, like, e.g., a natural\\nlanguage expression. For instance, a body temperature of 38.5\\x03 is in\\npartial, but not full accordance with the concept of ‘‘high fever’’, and\\ncould be described by a value, say, 0:3. The statement ‘‘high fever’’\\nassociated with a truth value 0:3 is sharply to be distinguished from\\nthe statement that the patient has high fever with the probability\\n0:3; in the former case, uncertainty is not involved.\\nIn the present context, however, we will adopt a broader\\nviewpoint about the nature of truth degrees. In medicine, values of\\nthe real unit interval are frequently used not only to express\\ndegrees of applicability of a vague concept, but also of uncertainty,\\nor more speciﬁcally the probability of some event. In Fuzzy Arden\\nSyntax, the meaning of truth values is not predeﬁned; we do not\\nintend to prevent the user from using a probabilistic interpretation\\nor an interpretation according to some more general framework of\\nuncertainty management. We will actually in the sequel explicitly\\ninclude this possibility. We note, however, that Fuzzy Arden\\nSyntax does not contain special functions supporting reasoning\\nunder uncertainty; after all its design aims at supporting the\\nprocession of vague rather than uncertain information. To include\\nprobability theory or some kind of plausibility logic could motivate\\na further extension of the language.\\nWe note that fuzzy analoga have been proposed for several\\nprogramming languages. The aim has been to simplify inferences\\nbased on methods of fuzzy logic. In particular, the language HALO\\n[9] has been described as ‘‘fuzzy Pascal’’. The language FRIL [10]\\nextends Prolog, so as to deal with uncertainty of data and rules;\\nsimilar aims are followed in [11]. The programming language L[12]\\ndoes not refer to an existing language, but takes up the concept of\\nW-recursiveness, which is an adaption of the notion of recursive-\\nness for fuzzy functions. Active research is still being conducted in\\nthis ﬁeld; a recent paper [13] could well be added to the list.\\nOn the very basic level, Fuzzy Arden Syntax is comparable to\\nthese programming languages. Apart from that, however, the\\nintentions are not comparable. Recall that Arden Syntax provides\\nprogrammes which read, in the ideal case, like a speciﬁcation of a\\nmedical procedure proposed under certain circumstances. Our aim\\nwas not only to provide fuzzy equivalents for the standard data\\ntypes found in any language; we were primarily interested in\\nrendering more ﬂexible those components which were introduced\\ninto Arden Syntax in order to adapt it for use in medicine.\\n3. Arden Syntax\\nFuzzy Arden Syntax is based on Arden Syntax and every\\nprogramme written in Arden Syntax runs under Fuzzy Arden\\nSyntax without any alteration of its effect. That is, Fuzzy Arden\\nT. Vetterlein et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 1–10\\n2\\n\\nSyntax is backward compatible with Arden Syntax. We tried to\\nkeep all changes to the necessary minimum. The ofﬁcial Arden\\nSyntax speciﬁcations can be found in [1]. It would be impossible to\\nenumerate them here. Nevertheless, we intend to make the main\\nfeatures of Fuzzy Arden Syntax comprehensible even to those\\nwithout a profound knowledge of HL7’s Arden Syntax. To this end\\nwe will explain a few basic principles of Arden Syntax and provide\\na sample programme. Moreover, when presenting Fuzzy Arden\\nSyntax we will provide integrated speciﬁcations.\\nAs mentioned earlier, Arden Syntax is typically used to realise a\\nCDSS. An Arden-Syntax-based CDSS is highly modular; it is\\ncomposed of a possibly high number of programming units named\\nMedical Logic Modules, or MLMs for short. The set of MLMs is not\\nhierarchically structured; each MLM may be considered and can be\\nread and understood independently. However, MLMs may call\\neach other, such that some MLMs may be considered subroutines\\nof other MLMs. An MLM may take over data and return data. The\\nexecution of an MLM may be dependent on a speciﬁc event\\ncommunicated from the outside.\\nA host system provides the user interface, access to the patient\\ndata base, and control over the execution of MLMs. All requests\\noriginating from the user or an MLM are processed by the host. The\\nhost speciﬁes a set of events and communicates the occurrence of\\nany event to the MLMs concerned. An event may reﬂect, for\\ninstance, a speciﬁc change in the patient database.\\nA typical MLM refers to a speciﬁc event which may take place at\\nany time in the database. Whenever this event occurs, the MLM is\\ntriggered and causes, or reminds the users to take, the appropriate\\naction.\\nThis phenomenon is demonstrated by one of the sample MLMs\\nfrom [1]. The MLM Penicillin_Allergy is executed whenever a\\npenicillin is prescribed at the hospital where the system is in use or,\\nmore precisely, when the corresponding change is made in the\\ndatabase. A warning is issued when an allergy against penicillin is\\nregistered for the respective patient. What follows is a brief version\\nof the MLM which can be found in [1, X3.3].\\nAn MLM consists of three main parts, named categories, each of\\nwhich consists of several entries, named slots. The ﬁrst two\\ncategories, namely maintenance and library, contain metadata,\\nof which only the content of three slots are of interest to the\\nprogrammer. These are shown in the above example and are\\npresumably self-explanatory.\\nThe programme itself is located in the last category named\\nknowledge. In the knowledge category, there are always a data, a\\nlogic, and an action slot, which are executed in this order, as well as\\nan evoke slot. In the data slot of our example, we ﬁrst ﬁnd the event\\nPenicillin_Order deﬁned, which occurs whenever penicillin is\\nordered. The deﬁnition of an event is given within curly brackets;\\nthis part is interpreted by the host system and the syntax is speciﬁc\\nfor\\nthe\\nmedical\\ninstitution\\nmentioned\\nin\\nthe\\nmaintenance\\ncategory. The next line contains a database query; the read\\ncommand causes a list to be read containing the notiﬁcations of an\\nallergy of the patient to penicillin. The result is stored in the\\nvariable Penicillin_Allergy. The effect of the key word last is\\nthat all entries but the last one are deleted from the list. Note that\\nin the present case, only one entry is needed.\\nThe events evoking this MLM are speciﬁed in the evoke slot. In\\nthis example, it is the event Penicillin_Order, which has been\\ndeﬁned in the data slot.\\nThe logic slot contains the part of the programme whose\\npurpose is to decide whether the subsequent action slot will be\\nexecuted or not. A command of the form conclude ActionCon-\\ndition, where ActionCondition is some Boolean variable,\\ncontains the decision: if ActionCondition is true, the action slot\\nis performed; otherwise the MLM is terminated at this point.\\nIn the above example, the programme ﬁrst checks whether the\\nlist Penicillin_Allergy is empty. If this is not the case the\\ncommand\\nconclude\\ntrue\\nis\\nexecuted,\\nmeaning\\nthat\\nthe\\nprogramme jumps to the beginning of the action slot.\\nThe action slot deﬁnes the action to be taken, provided that the\\nneed for an action has been identiﬁed in the logic slot. In the above\\nexample, a warning message is sent to the host, which will be\\ndisplayed on the user’s interface.\\n4. The concept underlying Fuzzy Arden Syntax\\nFuzzy Arden Syntax incorporates new concepts into Arden\\nSyntax in order to assist in processing information that may not be\\ncompletely determinate. It is based on the observation that, in\\nmedicine, we typically draw conclusions from real parameters in a\\nway that the exact values do not really matter. Besides, we\\nfrequently infer information from facts which are not seen to be\\nclearly true, but are considered true to a certain degree which high\\nenough to allow a conclusion.\\nIn contrast, a typical MLM written in Arden Syntax does depend\\non exact values and only deals with the two truth values ‘‘true’’ and\\n‘‘false’’. Consequently, a small change of a parameter may cause a\\nsharp change in an outgoing recommendation; for, if a recommen-\\ndation depends on a real value, a limit value must be chosen and\\nthis limit is typically chosen ad hoc because a precise limit point is\\nrarely available.\\nA solution would be to specify a limit point in a rough manner\\nrather than precisely, and issue differentiated recommendations in\\nborderline cases. Similarly, statements which merely express a\\ntendency rather than a simple ‘‘true’’ or ‘‘false’’ should also be\\npermitted. The new features of Fuzzy Arden Syntax simplify the\\nintegration of such elements.\\nTo illustrate the problem, let us consider a further example of an\\nMLM written in Arden Syntax. The MLM UTI_SUTI is based on the\\nspeciﬁcation of nosocomial infections by the US-American Centers\\nfor Disease Control and Prevention [14]; it determines whether a\\nhospitalised patient has a symptomatic urinary tract infection, or\\nT. Vetterlein et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 1–10\\n3\\n\\nSUTI for short. This MLM, which is part of the MONI system [15], is\\ndesigned as a subroutine of another MLM; it expects two input\\nparameters, namely a code for the patient’s hospital stay, which is\\nstored in the variable Stay, and a date, stored in the variable Day. It\\nyields a Boolean truth value indicating whether a SUTI is present or\\nnot.\\nThe result, which indicates whether a UTI-SUTI infection is\\npresent, depends on two real parameters and four yes–no\\nquestions. Vagueness is involved in all cases. For instance, in\\norder to check whether Fever is true, the patient’s body\\ntemperature must be compared with a sharp value simply because\\nthis variable is two-valued; the limit value depends on an ad hoc\\nchoice. With regard to the variable Dysuria, the patient is\\nrequired to specify whether or not the condition is present and this\\nmay be difﬁcult in the borderline case. The ﬁnal result may be\\nnegative but would well have been positive in case of a slight\\nchange in the manner of determining the input parameters.\\nThe aim underlying the conception of Fuzzy Arden Syntax is to\\nprocess, in a convenient way, statements that are not necessarily\\nentirely false or entirely true, and thus render it unnecessary for an\\nMLM’s author or user to make odd decisions about borderline\\ncases. The following principle is essential. To write or to\\nunderstand a Fuzzy Arden Syntax programme should not cause\\nessential difﬁculties for those already familiar with Arden Syntax.\\nCertainly, the user must be aware of certain modiﬁcations. The\\nprogram text should, however, remain practically unchanged\\nwhen based on fuzzy logic rather than classical logic.\\nThe main decisions underlying Fuzzy Arden Syntax have been\\nmade in accordance with fuzzy set theory and fuzzy logics. In\\nparticular, as a set of truth values, all (representable) reals between\\n0 and 1 are used. The MLM UTI_SUTI illustrates the usefulness of\\nextending the set of truth values. We argue that for input\\nparameters like Dysuria which are subjective in nature, it is more\\nappropriate to use generalised truth values rather than two-valued\\nones. Similarly, for those input parameters which result from a\\nmeasurement, such as Temperature, derived propositions like\\nFever should be considered fuzzy and assigned non-sharp truth\\nvalues in borderline cases. To what extent does the MLM needs to\\nbe modiﬁed under Fuzzy Arden Syntax in order to incorporate\\nthese demands? As a matter of fact the programme can remain\\npractically unchanged. All that needs to be speciﬁed is the degree\\nof impreciseness, which is easily done by formulating the\\ncomparison involving body temperature as follows: ‘‘Tempera-\\nture >= 38.5 fuzziﬁed by 0.5’’. In Section 9 we will show what\\nthis example, when fuzziﬁed, looks like in Arden Syntax and in\\nFuzzy Arden Syntax.\\nLet us now indicate the contexts in which fuzziness appears in\\nFuzzy Arden Syntax. The former data type boolean now takes\\nvalues within the real unit interval [0,1]. If an if-then-else\\ncommand\\ndepends\\non\\na\\nvariable\\ncontaining,\\nsay,\\n0.7,\\nthe\\nprogramme splits. Both the then-block and the else-block are\\nexecuted: the former weighted 0.7, the latter weighted 0.3. The\\ntwo branches may be reuniﬁed if the programmer wishes to do so.\\nThe variables are then aggregated, in a way that the weight of each\\nbranch is taken into account.\\nFor every data type involving real values – number, time,\\nduration – a fuzzy counterpart is added. Piecewise linear fuzzy sets\\nover the respective base set can be represented. Distinctions like\\n‘‘the patient is young’’ or ‘‘middle-aged’’ or ‘‘old’’ can be made in a\\nconvenient way, namely without the necessity to specify sharp\\nborders. Moreover, a fuzzy set can be defuzziﬁed afterwards by\\nmeans of a single command. For example, the behaviour of a\\nMamdani–Assilian controller can be achieved without noteworthy\\neffort.\\nFinally, all non-compound data types are endowed with an\\nadditional component called the degree of applicability. This\\ncomponent stores a truth value expressing, for instance, the degree\\nto which it would be reasonable to use the value in the variable’s\\nmain component. It is 1 by default, and whenever the programme\\nbranches it is reduced automatically according to the weight\\nassigned to the branch. The programmer may decide to make\\nexplicit use of this component but is not required to do so.\\nWhen processing vague data, one general problem inevitably\\nencountered in fuzzy logic is the fact that there is no canonical\\nchoice for the connectives on the extended set of truth values. As\\nwe also cannot solve this problem, the programmer may choose\\nbetween several options in Fuzzy Arden Syntax. Non-default\\noptions must be speciﬁed within a newly introduced category,\\nnamed fuzzy options. The available options will be explained in\\nthe relevant context.\\nLet us ﬁnally consider the question how well a Fuzzy-Arden-\\nSyntax-based CDSS might be accepted by the user. As a matter of\\nfact, clinicians supporting the formulation of MLMs or using a\\nfuzzy-logic-based environment will be asked for a slightly\\ndifferent point of view than before. In the process of writing an\\nMLM, clinicians are asked to provide data together with an explicit\\nT. Vetterlein et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 1–10\\n4\\n\\nindeterminacy, like, e.g., fuzzy boundaries of laboratory para-\\nmeters. Furthermore, the user of an MLM will be faced with the fact\\nthat in general more differentiated information is provided in the\\noutput than before. In practice this can mean that a set of weighted\\nalternatives is presented as result, a possibly uncommon situation.\\nNote that both mentioned points actually represent a progress.\\nAdhocdecisionsaboutexactboundaries ofthenormalrangeofsome\\nparameters are no longer necessary. Furthermore, complex results\\njust occur in borderline situations; to provide a simpliﬁed output in\\nsuch cases would hide the actual complexity of the situation.\\nIn both cases, however, a minimal understanding of the nature\\nof continuous truth degrees must be assumed. We are decidedly\\noptimistic in this respect. We can rely on a rich experience in the\\ncooperation with clinicians; the concept of a graded applicability is\\nusually quite easily understood and the acceptance is high.\\n5. Data types in Fuzzy Arden Syntax\\nWe will now begin describing the features of Fuzzy Arden Syntax\\nin a systematic way. For additional information, we refer to [3].\\nOur ﬁrst issue is to explain the data types. To maintain the\\nclarity of the presentation, the data types incorporated already in\\nArden Syntax will not be separately marked.\\nThere are simple and compound data types. In general, a\\nvariable is bound to a data type by assignment and a declaration is\\nnot necessary; thus Fuzzy Arden Syntax is loosely typed as is Arden\\nSyntax. Furthermore, a variable may contain the value null. This is\\nto reﬂect that the variable is undeﬁned, as for instance when the\\ndeﬁning expression is erroneous.\\nEvery simple data type contains three components: the main\\nvalue, the primary time, and the degree of applicability. The\\nfollowing explanations refer to the main component; the meaning\\nof the other two will be explained later. Neither the second nor the\\nthird component needs to be explicitly addressed at any time; both\\ncan safely be ignored if not needed.\\nIn Fuzzy Arden Syntax, the data type of propositional variables\\nis denoted by truth value or – for reasons of backwards\\ncompatibility – equivalently boolean. A variable of this type\\nstores real numbers between 0 and 1. We may write:\\nA variable of type number or, equivalently, crisp number\\nstores real numbers which can be represented in the available\\nﬂoating point format. A variable of type time or crisp time stores\\na date, a time, and optionally the referred time zone. A variable of\\ntype duration or crisp duration stores a length in time. For\\nexample:\\nWe next turn to the fuzzy data types. The data type fuzzy\\nnumber is dedicated to fuzzy sets over the reals. Needless to say,\\nfuzzy sets cannot be allowed to have an arbitrarily complicated\\nstructure. This is no essential restriction. In medicine, as in most\\nother applications, speciﬁc truth values do not have a speciﬁc\\nmeaning. Consequently, we may conﬁne ourselves to fuzzy sets of\\na simple form. We will assume that we may partition the reals into\\na ﬁnite number of (possibly unbounded) intervals on each of which\\nthe fuzzy set is linear and continuous.\\nFormally, a fuzzy set u : R ! ½0; 1\\x02 can be stored in a variable of\\ntype fuzzy number if the following condition is met: There are\\na1 < a2 < . . . < ak, k \\x04 1, in R such that u is linear on each open\\ninterval ða1; a2Þ; . . . ; ðak\\x051; akÞ, u is constant on ð\\x051; a1Þ and ðak; 1Þ,\\nand for each x 2 R, uðxÞ coincides either with the left limit or the\\nright limit of u at x. If u is continuous, we then deﬁne\\nFuzzyset_u:= fuzzy set ða1; t1Þ; ða2; t2Þ; . . . ; ðak; tkÞ;\\nwhere ti ¼ uðaiÞ for i ¼ 1; . . . ; k. Thus, for instance, the fuzzy set\\nR ! ½0; 1\\x02;\\ns 7!\\n1\\nif\\ns < 150\\n1\\n10ð160 \\x05 sÞ\\nif\\n150 \\x06 s < 160\\n0\\nif\\ns \\x04 160;\\n8\\n<\\n:\\nexpressing the predicate ‘‘small’’ for body size in centimetres, is\\ndeﬁned by:\\nSmall:= fuzzy set (150, 1), (160, 0);\\nDiscontinuities are, as usual, allowed to include the character-\\nistic functions, which are not likely to be required in applications,\\nbut should at least be deﬁnable. (Recall that characteristic\\nfunctions correspond to subsets; it maps all elements of a set A\\nto 1 and all elements of the complement of A to 0.)\\nAt discontinuity points we denote the left as well as the right\\nlimit. The ﬁrst assignment is taken to be the value at that point,\\nunless the second one appears twice. For instance,\\ngives the characteristic function of the set ½2; 3\\x02 \\x07 R.\\nFurthermore, the triangular normal fuzzy sets are likely to\\nappear frequently. This refers to those fuzzy sets whose graph\\nforms a symmetrical triangle around one point, which is mapped to\\n1. A simpliﬁed notation is permitted for these: an expression of the\\nform fuzzy set (a \\x05 b, 0), (a, 1), (a þ b, 0), where a; b 2 R and b > 0,\\nmay also be written as:\\na fuzziﬁed by b\\nAll deﬁnitions concerning the data type fuzzy number apply\\nmutatis mutandis to the data types fuzzy time and fuzzy\\nduration as well. The fuzziﬁed by operator, for instance, may be\\nused for pairs of a time and a duration; e.g., when referring to the\\ntime period approximately three days before the current time we\\nmay use the expression:\\n3 days ago fuzziﬁed by 12 h\\nFor the sake of completeness we mention that one more simple\\ndata type named string exists. A variable of this type stores text.\\nAs mentioned earlier, two additional components are included\\nin each simple data type. The so-called primary time is stored in\\nthe second component. This is typically the time at which the value\\nemerged, which could be the time the value was measured. When\\nreading data from the patient database this component may be\\nﬁlled automatically with a provided value; no explicit command is\\nneeded. Details are implementation-dependent, and the primary\\ntime is null if not speciﬁed.\\nThe third component contains what we call the degree of\\napplicability, or applicability for short. This is a truth value\\nexpressing the degree to which the main value may be considered\\napplicable. Typical examples would be:\\n(i) We wish to derive a statement about the development of a\\nquantity within the last 24 h. The limit of 24 h is chosen ad hoc\\nand should be considered fuzzy. We may collect the values\\nfrom the last, say, 28 h, and assign to each value a degree of\\nT. Vetterlein et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 1–10\\n5\\n\\napplicability of 1 if it originates from the preceding 24 h, else\\nsuccessively smaller values. The applicabilities may then be\\nused to weight the values accordingly in the subsequent\\ncalculation.\\n(ii) InFuzzyArdenSyntaxaprogrammearrivingatapointwherethe\\ncontinuation depends on a condition which is neither clearly\\nfalse nor clearly true, will split. Before executing one of the\\nbranches the set of variables is duplicated and the applicabilities\\nare reduced according to the weights of the branches.\\n(iii) The degree of applicability is also at the user’s free disposal. For\\ninstance, it may be attached to a variable in order to express\\nthat this value is not fully reliable.\\nThe third component of a variable Var can be read and changed\\nthrough the expression applicability of Var. By default it is set\\nto 1 and it is never undeﬁned.\\nTwo compound data types exist: list and object. A list\\nvariable stores a sequence of n values of a simple data type, where\\nn \\x04 0 is dynamic. An object variable stores a ﬁxed number of values\\nof a simple data type; the components are denoted by speciﬁc\\nidentiﬁers, which must be declared in advance. For reasons which\\nwill become apparent in Section 10 below, the key word object\\nmay be replaced by linguistic variable.\\n6. Operations in Fuzzy Arden Syntax\\nThe question as to how one should interpret the logical\\nconnectives for generalised truth values, has always been a\\ndelicate one. A generally valid recommendation cannot be made. In\\nfact, it would even not be permissible to say that a speciﬁc type of\\napplication requires a speciﬁc set of connectives.\\nOnly a few basic properties are usually required, namely those\\nwhich are found to be natural for the respective connective. The\\nconjunction\\nis\\ncommonly\\ninterpreted\\nby\\na\\nfunction\\n\\x08 :\\n½0; 1\\x022 ! ½0; 1\\x02 which is associative, commutative, neutral with\\nrespect to 1, and in both arguments isotone. Such a function is\\ncalled a t-norm [16]. The disjunction is usually taken as the\\ncorresponding t-conorm, and the negation as the subtraction from\\n1. This standard is assumed in Fuzzy Arden Syntax as well;\\nhowever, subroutines to interpret the connectives may also be\\ndeﬁned by the programmer.\\nThe three basic operations to combine truth values, are and, or,\\nand not. To interpret and, the user may opt for the Łukasiewicz t-\\nnorm \\x08”, the product t-norm \\x08P, or the Go¨del t-norm \\x08G, where\\na\\x08”b ¼ max fa þ b \\x05 1; 0g;\\na\\x08Pb ¼ a \\t b;\\na\\x08Gb ¼ min fa; bg\\nfor a; b 2 ½0; 1\\x02. The choice is realised by an option in the fuzzy\\noptions category. By default, the Go¨del t-norm is used. Further-\\nmore, the and connective may be speciﬁed by the user, in which\\ncase the command\\nconjunction by\\nMLM ‘UserspeciﬁedConjunction’;\\nmust appear. Here, UserspeciﬁedConjunction must be the name\\nof an MLM which accepts two truth values as its input and returns one\\ntruth value as output. It is presumed that this MLM encodes a t-norm.\\nAs usual in Arden Syntax, however, it is not checked whether this\\nMLM fulﬁls any of the special properties which are expected from a\\nconjunction.\\nThe interpretation of the disjunction and the negation may be\\nchosen in a similar fashion, independent of the conjunction.\\nHowever, we presume the user will make use of this freedom only\\nin the exceptional case. By default the conjunction is the t-conorm\\n\\n associated to the t-norm \\x08 in use, that is\\n\\n : ½0; 1\\x022 ! ½0; 1\\x02;\\nða; bÞ 7! 1 \\x05 ðð1 \\x05 aÞ\\x08ð1 \\x05 bÞÞ:\\nThus, in case that the default t-norm is used the maximum of the\\ntwo truth values is taken. Moreover, the negation is, by default, the\\nstandard negation\\n½0; 1\\x02 ! ½0; 1\\x02;\\nt 7! 1 \\x05 t:\\nNote that if one of the three standard t-norms, its corresponding\\nt-conorm and the standard negation is chosen, then the sharp truth\\nvalues 0 and 1 are connected as in classical two-valued logic. Thus,\\nthe compatibility with Arden Syntax is ensured in this case, the\\nspecial treatment of null included.\\nIn the present context two further connectives, denoted at\\nleast and at most, are important. In medical literature, when\\nlisting symptom combinations specifying a situation in which the\\npresence of a certain disease is assumed we frequently encounter\\nphrases like ‘‘at least two of the following conditions must be met:\\n. . .’’. By default these connectives are deﬁned by the basic ones. For\\ninstance, let List be a list of truth values; then the expression\\nat least n of List\\nis the disjunction of all conjunctions of exactly n entries in List. We\\neasily check that if the default connectives are used, the displayed\\nexpression returns the n-th largest value in List. The connective at\\nmost is speciﬁed similarly.\\nHowever, there are further reasonable possibilities to interpret\\nat least. In Fuzzy Arden Syntax, a user-deﬁned deﬁnition can be\\ngiven, which need not be related to the chosen t-norm. For\\ninstance, the following interpretation has been proposed to\\ninterpret at least n of List:\\nmin fv1 þ \\t \\t \\t þ vk; ng\\nn\\n;\\nwhere v1; . . . ; vk are the truth values contained in List.\\nA few other logical operations are provided. For instance, any\\nof List is the disjunction of the truth values contained in the list\\nList. These operations depend on the three main connectives\\nmentioned, and we will not enumerate them.\\nWe next turn to the operations with numbers. Number\\nvariables may be connected by the basic arithmetic operations\\nþ; \\x05; $; =. A few other common functions are available. For time\\nand duration variables these operations are also deﬁned whenever\\nit makes sense.\\nFor variables storing fuzzy sets, addition and subtraction as well\\nas multiplication with, and division by, positive crisp reals are\\ndeﬁned according to Zadeh’s extension principle, provided that the\\nrespective operation is well-deﬁned. Zadeh’s extension principle\\n[4] is the canonical way to extend operations on real numbers to\\nfuzzy sets. For instance, the sum u þ v of fuzzy sets u and v over R is\\ndeﬁned by\\nðu þ vÞðxÞ ¼ sup fuðyÞ ^ vðzÞ : y þ z ¼ xg\\nfor x 2 R.\\nFinally, numbers as well as times and durations may be\\ncompared with respect to their natural order. The comparison of\\ntwo crisp numbers yields a crisp truth value.\\nFurthermore, fuzzy sets being available, we need a way to query\\nthe compatibility of crisp values with properties modelled by fuzzy\\nsets. A crisp number r, contained, say, in Var, may be correlated to a\\nfuzzy number u contained, say, in FuzzyVar, by the expression\\nT. Vetterlein et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 1–10\\n6\\n\\nVar is FuzzyVar\\nwhich simply gives the value of u at r, that is, uðrÞ. Moreover, the\\nexpression Var <= FuzzyVar returns sup fuðxÞ : r \\x06 xg, and similarly,\\nVar >= FuzzyVar returns sup fuðxÞ : r \\x04 xg. Analogous deﬁnitions\\napply for fuzzy times and fuzzy durations.\\nFinally, if FuzzyVar stores a fuzzy set, we may call a\\ndefuzziﬁcation function by\\ndefuzziﬁed Fuzzyset\\nreturning an element of the respective domain. Centre-of-gravity\\ndefuzziﬁcation and mean-of-maximum defuzziﬁcation are prede-\\nﬁned (for these methods, see, e.g., [5]). A user-deﬁned function\\nmay be chosen in the fuzzy options category as well.\\nOne operation is especially important in Arden Syntax and has\\nbeen modiﬁed in Fuzzy Arden Syntax. Given a list, we may form a\\nsublist by means of the operator where. We may actually think of\\nthe where operator as an operator forming a subset. In Fuzzy Arden\\nSyntax this operator should serve to form a fuzzy subset. Let a list\\nList:= Value1, . . ., Valuen;\\nbe given, and let Condition(\\t) be an expression of type truth value\\nwith one free variable. Then\\nList where Condition(it)\\narises from List as follows: For each i, the degree of applicability of\\nValuei is connected with Condition(Valuei) by the and-operator.\\nThe result, if deﬁned and > 0, is stored as the new applicability of\\nValuei; otherwise, the entry Valuei is removed from the list.\\nTo see how this command works, let us reconsider the above\\nexample. We may produce a list with a patient’s body temperature\\nfrom the last approximately 24 h using the following:\\nTempatureList:= read {temperature} where\\nit occurred within the past 24 hours\\nfuzziﬁed by 4 hours;\\nThis leads, just as proposed above, to a list of values from the\\nlast 28 h where the applicability of the values from the time period\\nbetween 28 h ago and 24 h ago is reduced: The older the value is,\\nthe smaller is its applicability.\\n7. Conditional statements and branching in Fuzzy Arden\\nSyntax\\nConditions in Fuzzy Arden Syntax may be indeterminate. The\\ntruth values vary continuously from 0 to 1. Therefore, a command\\ndirecting the programme ﬂow into one of two or more blocks must\\nbe\\ncarefully interpreted,\\ndepending on\\nthe\\ncontent of the\\nrespective propositional variable.\\nIn full accordance with what we are familiar with, an if-then-\\nelse statement in Fuzzy Arden Syntax would be as follows:\\nwhere Condition is an expression of type truth value. However,\\nits manner of execution is one of the main differences between\\nArden and Fuzzy Arden Syntax.\\nThe command is executed as follows. If Condition is 1, block 1\\nis executed; if Condition is 0 or null, block 2 is executed. If,\\nhowever, Condition is t 2 ð0; 1Þ, the programme splits: block 1\\nand block 2, named programme branches in the sequel, will be\\nexecuted in parallel. To this end, each branch is provided with its\\nown set of variables which, accordingly, are duplicated. Moreover,\\nthe degree of applicability of each variable is in case of block 1\\nmultiplied by t, in case of block 2 multiplied by 1 \\x05 t. t and 1 \\x05 t are\\ncalled the relative weights of block 1 and block 2, respectively.\\nThe programme may branch several times. Each command\\nexecuted during the run of the programme is assigned a weight in\\nthe straightforward manner. The weight is 1 as long as the\\nprogramme does not split; when the weight is w and the\\nprogramme enters a branch with relative weight t, the weight\\nwill be reduced to w \\t t.\\nIn a branch of weight w, the range of the degree of applicability\\nof any variable is ½0; w\\x02. Whenever the content of a variable is\\nchanged its applicability will be reduced to w if necessary.\\nThe number of branches into which the programme may split at\\na time is not limited to two. Branching into n þ 1 blocks is coded as\\nfollows:\\nIn this case the relative weight ti of the i-th branch is given by\\nConditioni, where i ¼ 1; . . . ; n. The case that Conditioni is\\nundeﬁned is treated like ti ¼ 0, in which case the branch is not\\nexecuted. Moreover, if the sum of the ti is strictly smaller than 1,\\nthe relative weight of block n þ 1 will be 1 \\x05 t1 \\x05 . . . \\x05 tn, else this\\nblock is skipped.\\nThe possibility of letting the programme branch into more than\\ntwo programme blocks is one of the signiﬁcant features of Fuzzy\\nArden Syntax. Quite often we have to distinguish between\\nconditions of the form Var is FuzzySet1, . . ., Var is FuzzySetn,\\nwhere Var is a crisp value and FuzzySet1, . . . are fuzzy values. We\\nallow an abbreviating syntax for this case, namely,\\nThe same is possible if the conditions are of the form Var =\\nValue1, . . . for any data type of the involved variables.\\nAn example will follow in Section 10. In the following we will\\ndescribe how one proceeds after completion of an if-then-else\\ncommand.\\n8. Conditional statements and aggregation in Fuzzy Arden\\nSyntax\\nOnce all branches of a programme have completed their\\nexecution in parallel because of an unsharp condition, it is difﬁcult\\nto issue a general recommendation as to how one should proceed.\\nTwo possibilities exist:\\n(A) The programme remains split, that is, all subsequent com-\\nmands are executed in parallel as well, the action slot included.\\n(B) The programme reuniﬁes. The multiplied variables are merged\\ninto single ones.\\nBoth options are available in Fuzzy Arden Syntax; possibility (A)\\nis the default. The more appropriate option in the individual\\nsituation should be decided on the basis of the speciﬁc application.\\nIf (A) is selected the MLM’s results will be provided by each\\nbranch separately. The unit to which the results are sent – the host\\nT. Vetterlein et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 1–10\\n7\\n\\nsystem or the calling MLM – must be prepared to deal with the\\nsituation.\\nIf the MLM sends information to the host, the host system has to\\nprocess possibly divergent information. The key tool to be used is\\nthe third component of the data: the degree of applicability must\\nthen be interpreted by the host in order to conclude the relevance\\nof the received data. For instance, when displaying the information\\nto the user a clear statement as to the applicability must be added\\nor the information must be modiﬁed in another appropriate way.\\nThe user in turn has to understand that the data, in case of low\\napplicability, is to be interpreted as one of several possibilities.\\nIf the MLM is called by another MLM and returns data the\\ncalling MLM splits accordingly as well.\\nThe possibility (B) implies that the task of combining divergent\\npieces of information is executed within the MLM itself. To opt for\\n(B), the ﬁnal line of an if-then-else statement is modiﬁed: after the\\nkey word endif or endswitch, respectively, the key word\\naggregate is added. Thus, when writing\\nthe two branches unify after their execution. The programme\\nweight is then set to the sum of the weight of the branches, i.e., to\\nthe same value as before.\\nMoreover, corresponding variables are aggregated. Let Var be a\\nvariable deﬁned in at least one branch. As far as the main\\ncomponent is concerned, the procedure is as follows. If the content\\nof Var is the same in each branch, the content is taken over.\\nOtherwise, if Var is deﬁned in all branches and of the same simple\\ndata type except string, the contents are aggregated according to\\na predeﬁned method. If Var is of the same compound type in all\\nbranches, we proceed successively with the components in the\\nsame manner.\\nIn the remaining cases Var is set to null. For example, this is\\napplicable when a string variable is assigned a different text in two\\nbranches.\\nNo\\nmethod\\nis\\ncurrently\\nable\\nto\\naggregate\\ntext\\nautomatically or make a canonical choice.\\nThe aggregation method may be speciﬁed in the fuzzy options\\nslot, separately for crisp and fuzzy data. Numerous methods are\\navailable to aggregate data and the implementation of a user-\\ndeﬁned function is possible. By default the weighted mean is\\ncalculated. Thus, e.g., the values r1; . . . ; rn\\nwith degrees of\\napplicability t1; . . . ; tn, respectively, are aggregated to\\nt1r1 þ \\t \\t \\t þ tnrn\\nt1 þ \\t \\t \\t þ tn\\n;\\nthe same formula is used for crisp and fuzzy data. In the fuzzy case\\nanother predeﬁned method is available, namely, the supremum of\\nthe ri, cut off at height ti, may be taken:\\nðr1 ^ ¯t1Þ _ . . . _ ðrn ^ ¯tnÞ;\\n(1)\\nhere, ¯ti is the constant ti function, ^ connects two fuzzy sets by the\\npointwise minimum, and _ connects two fuzzy sets by the\\npointwise maximum.\\nTo aggregate the contents of variables with respect to the\\nremaining two components is straightforward. The primary time of\\nVar is taken over if coincident in all branches. If distinct times\\nappear we can no longer assume that these times are related to the\\ntime at which the value emerged; the primary time will be set to\\nnull in this case.\\nFurthermore, as might be expected, the degrees of applicability\\nare added. Thus, if left unchanged during the execution of all\\nbranches, the applicabilities prior to the execution of the if-then-\\nelse statement, will be restored.\\nThe conclude command is treated exactly in the same manner\\nas the if-then statement. Thus, the command\\nconclude Condition;\\nis interpreted as ‘‘if Condition applies, then jump to the action part,\\nelse quit the programme’’. In other words, if Condition yields a value\\n> 0, the applicabilities of all variables are multiplied by this value, and\\nthe action slot is executed. If Condition is 0 or undeﬁned, the\\nprogramme or this branch of the programme is terminated.\\n9. Fuzzy Arden Syntax versus Arden Syntax\\nThis section will deal with the example from Section 4: the\\nMLM UTI_SUTI. Let us see how a ‘‘fuzziﬁed’’ version of this MLM\\nlooks in Arden Syntax on the one hand and in Fuzzy Arden Syntax\\non the other hand.\\nIn Arden Syntax the result would look as follows, provided the\\nlogical connectives are chosen as the Fuzzy Arden Syntax default\\nconnectives.\\nIt is not necessary to print the MLM UTI_SUTI from Section 4\\nagain in order to show what the fuzzy version looks alike under\\nT. Vetterlein et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 1–10\\n8\\n\\nFuzzy Arden Syntax. The following changes must be made: (i)\\nreplace Temperature > 39 by Temperature >= 39 fuzziﬁed by\\n1; (ii) replace Organ_urine_culture >= 1e5 by Organ_ur-\\nine_culture >= 1e5 fuzziﬁed by 5e4.\\nFuzzy logical calculations have to be stated explicitly in Arden\\nSyntax. This makes the programme more cumbersome. The only\\nadvantage might be transparency.In FuzzyArden Syntax,practically\\nnothing changes in the programme text. The user who is familiar\\nwith Arden Syntax, however, must have some knowledge of the\\ndifferent ways in which the programme is interpreted.\\nNote that in case of emulation under Arden Syntax as well as in\\ncase of Fuzzy Arden Syntax, further processing of the result is not\\nincluded in the MLMs. However, as the input values are provided in\\na more differentiated way, the result has to be treated in a more\\ndifferentiated way as well. Considerably more effort is needed to\\ncommunicate a result to the user if vagueness or uncertainty are\\ninvolved. In the example, there will be more than the possibilities\\nof ‘‘a SUTI applies’’ and ‘‘there is no evidence of a SUTI’’. The result is\\na continuous truth degree, to which one message from a larger set\\nof messages should be associated, the two mentioned ones\\nrepresenting the extreme cases.\\n10. Fuzzy inference with Fuzzy Arden Syntax\\nThis section illustrates how fuzzy inference may be realised in\\nFuzzy Arden Syntax.\\nIn applications, fuzzy sets usually appear when partitioning a\\ndomain of values into subdomains without sharp borders and\\ntypically a conditioning according to this partition follows. A fuzzy\\npartition over a domain M is a partition of unity over M, that is, a\\nﬁnite set u1; . . . ; un of fuzzy sets such that u1ðxÞ þ \\t \\t \\t þ unðxÞ ¼ 1\\nfor each x 2 M. In Fuzzy Arden Syntax, a fuzzy partition is\\nconveniently stored in an object variable; we recall that instead\\nof the key word object, we may equivalently use the key word\\nlinguistic variable.\\nLet us consider the following declaration:\\nConsequently,\\nwe\\nhave\\ncreated\\nthree\\nvariables,\\nnamely\\nAge.Young, Age.Middle_Aged, and Age.Old. Each of these\\nvariables is ready to be used like any variable of a simple data type.\\nWe proceed by assigning these three variable fuzzy sets which\\nform a partition of unity over the positive durations:\\nWe may now use the blurred age ranges for distinctions as\\nfollows:\\nThen the variable DoseforPatient will contain a weighted\\nmean of the three values LowDose, MiddleDose, and HighDose,\\nwhere the weights reﬂect the, possibly partial, compatibility of\\nAgeofPatient with the three age ranges.\\nThe example may be modiﬁed to include the case that the doses\\nare fuzzy as well. If Dose.Low, Dose.Middle, Dose.High is a\\nfurther partition of unity we may write as follows:\\nIn this case DoseforPatient contains the result of the\\naggregation of three fuzzy sets rather than the aggregation of\\nthree crisp values. The outcoming fuzzy set is defuzziﬁed to the\\ncrisp value DoseTobetaken.\\nNote that, when choosing the supremum as the aggregation\\nmethod for fuzzy sets, our last sample programme imitates a\\nMamdani–Assilian controller. Namely, it ﬁrst determines to which\\nextend the input value, i.e., the age, is compatible with each of the\\nthree conditions; let t1; t2; t3 be the resulting truth values. Then,\\nthe fuzzy sets modelling a low, middle, and high dose, say r1; r2; r3,\\nrespectively, are aggregated to\\nðr1 ^ ¯t1Þ _ ðr2 ^ ¯t2Þ _ ðr3 ^ ¯t3Þ:\\nFinally, the last command defuzziﬁes this fuzzy set and provides a\\nsharp output value. For an explanation of a Mamdani–Assilian\\ncontroller, see, e.g., [5]. Note that the text of the programme is not\\nmore complicated than the text of if-then rules.\\n11. Fuzzy Arden Syntax: a view back and forward\\nOur work relies on ideas which were developed in the Ph.D.\\nThesis of S. Tiffe [2] in 2003. When comparing our proposal to\\nTiffe’s, we see that most elements were modiﬁed. Some of the\\nchanges are due to the fact that [2] is based on the older Arden\\nSyntax version 2.1 and several improvements in Arden Syntax have\\nbeen effected since that time. To explain all differences would\\nexceed the present framework; the interested reader is referred\\ndirectly to [2]. However, we shall list some facts.\\n\\x0b Tiffe’s degree of presence and degree of applicability were replaced\\nby\\nour\\ndegree\\nof\\napplicability\\nand\\nthe\\nprogram\\nweight,\\nrespectively. The function of these values and their mutual\\nrelationship between these two concepts has been newly\\ndeﬁned. Furthermore, a truth value describing the applicability\\nis associated with any data type and can be manually modiﬁed.\\n\\x0b In [2], a canonical way how to proceed after programme\\nbranching is not proposed; instead, several different possibilities\\nare presented. In contrast, we have decided to let always all\\nbranches of a split programme be executed in parallel and we\\nrequire the user to specify how the results are recombined.\\nThe difference is most evident in case of different texts sent to\\nthe host by different branches. If, for instance, there are results\\n‘‘Give medicament M immediately’’ and ‘‘To give medicament M\\nis not recommended’’ from two program branches, it seems\\ninappropriate to select one of these texts on the basis of their\\napplicabilities. The user should rather specify how to deal with\\ncontradictory recommendations.\\n\\x0b We do not fuzzify while- or for-loops. Actually, in [2] a warning\\nagainst while-loops is already contained.\\nT. Vetterlein et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 1–10\\n9\\n\\n\\x0b To process continuous truth values, we have generalised the\\nexisting boolean data type rather than introducing a new data\\ntype.\\nFurthermore, we allow to associate the value null to a\\nvariable storing a truth value. It is a difference not to specify a\\ntruth degree or to use the value 0.\\n\\x0b We have strictly simpliﬁed the usage of linguistic variables. In\\nparticular, the use of external MLMs to deﬁne fuzzy sets are not\\nnecessary. The new features of Arden Syntax version 2.5\\ncompared to 2.1 made these improvements possible.\\nAn implementation of Fuzzy Arden Syntax according to the\\nspeciﬁcation [3] is in progress. Whereas for the user, the transition\\nfrom Arden to Fuzzy Arden Syntax might not be serious, the effort\\nneeded for an implementation of the extended language is quite\\nhigh. The implementation of a Fuzzy Arden Syntax compiler will\\npresumably be ﬁnished in the last quarter of 2009. Its subsequent\\nuse within the new data management system of the Vienna\\nGeneral Hospital is scheduled. Afterwards, a systematic test of the\\nfuzzy extension will be undertaken.\\n12. Conclusion\\nWe have outlined the speciﬁcation of an extension of the\\nprogramming language Arden Syntax, which is designed for CDS\\napplications in medicine. Our primary reason for elaborating Tiffe’s\\nFuzzy Arden Syntax is to incorporate the possibility to deal with\\nindeterminate data, i.e., when the processed information is vague\\nor uncertain.\\nWe focused on keeping Fuzzy Arden as simple as Arden Syntax.\\nThe syntax is kept as close to natural language as is the case with\\nArden Syntax and ensures easy comprehension of an MLM’s\\ncontents. Indeed a programme text, when fuzzy rather than crisp\\ndata is processed, remains practically the same in typical\\napplications. When a Mamdani–Assilian inference is realised the\\nprogramme text is closely correlated to the text of the if-then rules\\non which the inference is based.\\nThe advantage of Fuzzy Arden Syntax when compared to Arden\\nSyntax is evident. The actual beneﬁts of Fuzzy Arden Syntax will\\nhave to be proven in clinical practice.\\nReferences\\n[1] Arden Syntax for Medical Logic Systems, Version 2.5, Health Level Seven; 2005.\\n[2] Tiffe S. Fuzzy Arden Syntax: representation and interpretation of vague\\nmedical knowledge by Fuzziﬁed Arden Syntax. Ph.D. thesis. Vienna: Technical\\nUniversity Vienna; 2003.\\n[3] Vetterlein T, Mandl H, Adlassnig KP. Vorschla¨ge zur Speziﬁkation der Pro-\\ngrammiersprache Fuzzy Arden Syntax (Proposal of a speciﬁcation of the\\nprogramming language Fuzzy Arden Syntax—in German), technical report.\\nVienna: Vienna University of Technology; 2008. Available at http://www.\\nmeduniwien.ac.at/user/thomas.vetterlein/articles/FuzzyArdenSpezif.pdf (last\\naccessed: 20 October, 2009).\\n[4] Zadeh LA. Fuzzy sets. Inf Control 1965;8:338–53.\\n[5] Nguyen HT, Walker EA. A ﬁrst course in fuzzy logic. Boca Raton: Chapman &\\nHall/CRC; 2006.\\n[6] Mamdani EH, Assilian S. An experiment in linguistic synthesis of fuzzy con-\\ntrollers. Int J Man-Mach Stud 1975;7:1–13.\\n[7] Schuh C, Hiesmayr M, Ehrengruber T, Katz E, Neugebauer T, Adlassnig KP, et al.\\nFuzzy knowledge-based weaning from artiﬁcial ventilation (FuzzyKBWean).\\nIn: Jamshidi M, Fahti M, Pierrot F, editors. Proceedings of the world automation\\ncongress 1996. 1996. p. 583–8.\\n[8] Adlassnig K-P, Kolarz G. CADIAG-2: computer-assisted medical diagnosis\\nusing fuzzy subsets. In: Gupta MM, Sanchez E, editors. Approximate reasoning\\nin decision analysis. Amsterdam: North-Holland Publ. Comp.; 1982. p. 219–47.\\n[9] Clark DF, Kandel A. HALO—a fuzzy programming language. Fuzzy Sets Syst\\n1991;44:199–208.\\n[10] Baldwin JF, Martin TP, Pilsworth BW. Fril: fuzzy and evidential reasoning in\\nartiﬁcial intelligence. New York: John Wiley & Sons; 1995.\\n[11] Munakata T. Notes on implementing fuzzy sets in Prolog. Fuzzy Sets Syst\\n1996;98:311–7.\\n[12] Morales-Bueno R, Conejo R, Pe´rez de la Cruz JL, Clares B. An elementary fuzzy\\nprogramming language. Fuzzy Sets Syst 1993;57:55–73.\\n[13] Zhao X, Li F. Denotational semantics of dynamic fuzzy logic programming\\nlanguage. In: Zhang YQ, Lin YT, editors. Proceedings of the IEEE international\\nconference 2006 on granular computing. 2007. p. 409–12.\\n[14] Garner JS, Jarvis WR, Emori TG, Horan TC, Hughes JM. CDC deﬁnitions for\\nnosocomial infections.\\nIn: Olmsted RN, editor. APIC infection control and\\napplied epidemiology: principles and practice. Mosby: St. Louis; 1996. p. A1–\\n20.\\n[15] Adlassnig KP, Blacky A, Koller W. Artiﬁcial-intelligence-based hospital-ac-\\nquired infection control. Stud Health Technol Inf 2009;149:103–10.\\n[16] Klement EP, Mesiar R, Pap E. Triangular norms. Dordrecht: Kluwer Acad. Publ.;\\n2000.\\nT. Vetterlein et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 1–10\\n10\\n'}, {'source': '/Users/sir/Downloads/Data/PDF/A_Critical_Survey_of_Bias_in_NLP.pdf', 'text': 'Language (Technology) is Power: A Critical Survey of “Bias” in NLP\\nSu Lin Blodgett\\nCollege of Information and Computer Sciences\\nUniversity of Massachusetts Amherst\\nblodgett@cs.umass.edu\\nSolon Barocas\\nMicrosoft Research\\nCornell University\\nsolon@microsoft.com\\nHal Daumé III\\nMicrosoft Research\\nUniversity of Maryland\\nme@hal3.name\\nHanna Wallach\\nMicrosoft Research\\nwallach@microsoft.com\\nAbstract\\nWe survey 146 papers analyzing “bias” in\\nNLP systems, ﬁnding that their motivations\\nare often vague, inconsistent, and lacking\\nin normative reasoning, despite the fact that\\nanalyzing “bias” is an inherently normative\\nprocess.\\nWe further ﬁnd that these papers’\\nproposed quantitative techniques for measur-\\ning or mitigating “bias” are poorly matched to\\ntheir motivations and do not engage with the\\nrelevant literature outside of NLP. Based on\\nthese ﬁndings, we describe the beginnings of a\\npath forward by proposing three recommenda-\\ntions that should guide work analyzing “bias”\\nin NLP systems. These recommendations rest\\non a greater recognition of the relationships\\nbetween\\nlanguage\\nand\\nsocial\\nhierarchies,\\nencouraging\\nresearchers\\nand\\npractitioners\\nto\\narticulate\\ntheir\\nconceptualizations\\nof\\n“bias”—i.e., what kinds of system behaviors\\nare harmful, in what ways, to whom, and why,\\nas well as the normative reasoning underlying\\nthese statements—and to center work around\\nthe lived experiences of members of commu-\\nnities affected by NLP systems, while inter-\\nrogating and reimagining the power relations\\nbetween technologists and such communities.\\n1\\nIntroduction\\nA large body of work analyzing “bias” in natural\\nlanguage processing (NLP) systems has emerged\\nin recent years, including work on “bias” in embed-\\nding spaces (e.g., Bolukbasi et al., 2016a; Caliskan\\net al., 2017; Gonen and Goldberg, 2019; May\\net al., 2019) as well as work on “bias” in systems\\ndeveloped for a breadth of tasks including language\\nmodeling (Lu et al., 2018; Bordia and Bowman,\\n2019), coreference resolution (Rudinger et al.,\\n2018; Zhao et al., 2018a), machine translation (Van-\\nmassenhove et al., 2018; Stanovsky et al., 2019),\\nsentiment analysis (Kiritchenko and Mohammad,\\n2018), and hate speech/toxicity detection (e.g.,\\nPark et al., 2018; Dixon et al., 2018), among others.\\nAlthough these papers have laid vital ground-\\nwork by illustrating some of the ways that NLP\\nsystems can be harmful, the majority of them fail\\nto engage critically with what constitutes “bias”\\nin the ﬁrst place. Despite the fact that analyzing\\n“bias” is an inherently normative process—in\\nwhich some system behaviors are deemed good\\nand others harmful—papers on “bias” in NLP\\nsystems are rife with unstated assumptions about\\nwhat kinds of system behaviors are harmful, in\\nwhat ways, to whom, and why. Indeed, the term\\n“bias” (or “gender bias” or “racial bias”) is used\\nto describe a wide range of system behaviors, even\\nthough they may be harmful in different ways, to\\ndifferent groups, or for different reasons. Even\\npapers analyzing “bias” in NLP systems developed\\nfor the same task often conceptualize it differently.\\nFor example, the following system behaviors\\nare all understood to be self-evident statements of\\n“racial bias”: (a) embedding spaces in which embed-\\ndings for names associated with African Americans\\nare closer (compared to names associated with\\nEuropean Americans) to unpleasant words than\\npleasant words (Caliskan et al., 2017); (b) senti-\\nment analysis systems yielding different intensity\\nscores for sentences containing names associated\\nwith African Americans and sentences containing\\nnames associated with European Americans (Kir-\\nitchenko and Mohammad, 2018); and (c) toxicity\\narXiv:2005.14050v2  [cs.CL]  29 May 2020\\n\\ndetection systems scoring tweets containing fea-\\ntures associated with African-American English as\\nmore offensive than tweets without these features\\n(Davidson et al., 2019; Sap et al., 2019). Moreover,\\nsome of these papers focus on “racial bias”\\nexpressed in written text, while others focus on\\n“racial bias” against authors. This use of imprecise\\nterminology obscures these important differences.\\nWe survey 146 papers analyzing “bias” in NLP\\nsystems, ﬁnding that their motivations are often\\nvague and inconsistent. Many lack any normative\\nreasoning for why the system behaviors that are\\ndescribed as “bias” are harmful, in what ways, and\\nto whom. Moreover, the vast majority of these\\npapers do not engage with the relevant literature\\noutside of NLP to ground normative concerns when\\nproposing quantitative techniques for measuring\\nor mitigating “bias.” As a result, we ﬁnd that many\\nof these techniques are poorly matched to their\\nmotivations, and are not comparable to one another.\\nWe then describe the beginnings of a path\\nforward by proposing three recommendations\\nthat should guide work analyzing “bias” in NLP\\nsystems. We argue that such work should examine\\nthe relationships between language and social hi-\\nerarchies; we call on researchers and practitioners\\nconducting such work to articulate their conceptu-\\nalizations of “bias” in order to enable conversations\\nabout what kinds of system behaviors are harmful,\\nin what ways, to whom, and why; and we recom-\\nmend deeper engagements between technologists\\nand communities affected by NLP systems. We\\nalso provide several concrete research questions\\nthat are implied by each of our recommendations.\\n2\\nMethod\\nOur survey includes all papers known to us\\nanalyzing “bias” in NLP systems—146 papers in\\ntotal. We omitted papers about speech, restricting\\nour survey to papers about written text only. To\\nidentify the 146 papers, we ﬁrst searched the ACL\\nAnthology1 for all papers with the keywords “bias”\\nor “fairness” that were made available prior to May\\n2020. We retained all papers about social “bias,”\\nand discarded all papers about other deﬁnitions of\\nthe keywords (e.g., hypothesis-only bias, inductive\\nbias, media bias). We also discarded all papers us-\\ning “bias” in NLP systems to measure social “bias”\\nin text or the real world (e.g., Garg et al., 2018).\\nTo ensure that we did not exclude any relevant\\n1https://www.aclweb.org/anthology/\\nNLP task\\nPapers\\nEmbeddings (type-level or contextualized)\\n54\\nCoreference resolution\\n20\\nLanguage modeling or dialogue generation\\n17\\nHate-speech detection\\n17\\nSentiment analysis\\n15\\nMachine translation\\n8\\nTagging or parsing\\n5\\nSurveys, frameworks, and meta-analyses\\n20\\nOther\\n22\\nTable 1: The NLP tasks covered by the 146 papers.\\npapers without the keywords “bias” or “fairness,”\\nwe also traversed the citation graph of our initial\\nset of papers, retaining any papers analyzing “bias”\\nin NLP systems that are cited by or cite the papers\\nin our initial set. Finally, we manually inspected\\nany papers analyzing “bias” in NLP systems from\\nleading machine learning, human–computer inter-\\naction, and web conferences and workshops, such\\nas ICML, NeurIPS, AIES, FAccT, CHI, and WWW,\\nalong with any relevant papers that were made\\navailable in the “Computation and Language” and\\n“Computers and Society” categories on arXiv prior\\nto May 2020, but found that they had already been\\nidentiﬁed via our traversal of the citation graph. We\\nprovide a list of all 146 papers in the appendix. In\\nTable 1, we provide a breakdown of the NLP tasks\\ncovered by the papers. We note that counts do not\\nsum to 146, because some papers cover multiple\\ntasks. For example, a paper might test the efﬁcacy\\nof a technique for mitigating “bias” in embed-\\nding spaces in the context of sentiment analysis.\\nOnce identiﬁed, we then read each of the 146 pa-\\npers with the goal of categorizing their motivations\\nand their proposed quantitative techniques for mea-\\nsuring or mitigating “bias.” We used a previously\\ndeveloped taxonomy of harms for this categoriza-\\ntion, which differentiates between so-called alloca-\\ntional and representational harms (Barocas et al.,\\n2017; Crawford, 2017). Allocational harms arise\\nwhen an automated system allocates resources (e.g.,\\ncredit) or opportunities (e.g., jobs) unfairly to dif-\\nferent social groups; representational harms arise\\nwhen a system (e.g., a search engine) represents\\nsome social groups in a less favorable light than\\nothers, demeans them, or fails to recognize their\\nexistence altogether. Adapting and extending this\\ntaxonomy, we categorized the 146 papers’ motiva-\\ntions and techniques into the following categories:\\n▷Allocational harms.\\n\\nPapers\\nCategory\\nMotivation\\nTechnique\\nAllocational harms\\n30\\n4\\nStereotyping\\n50\\n58\\nOther representational harms\\n52\\n43\\nQuestionable correlations\\n47\\n42\\nVague/unstated\\n23\\n0\\nSurveys, frameworks, and\\nmeta-analyses\\n20\\n20\\nTable 2: The categories into which the 146 papers fall.\\n▷Representational harms:2\\n▷Stereotyping that propagates negative gen-\\neralizations about particular social groups.\\n▷Differences in system performance for dif-\\nferent social groups, language that misrep-\\nresents the distribution of different social\\ngroups in the population, or language that\\nis denigrating to particular social groups.\\n▷Questionable correlations between system be-\\nhavior and features of language that are typi-\\ncally associated with particular social groups.\\n▷Vague descriptions of “bias” (or “gender\\nbias” or “racial bias”) or no description at all.\\n▷Surveys, frameworks, and meta-analyses.\\nIn Table 2 we provide counts for each of the\\nsix categories listed above. (We also provide a\\nlist of the papers that fall into each category in the\\nappendix.) Again, we note that the counts do not\\nsum to 146, because some papers state multiple\\nmotivations, propose multiple techniques, or pro-\\npose a single technique for measuring or mitigating\\nmultiple harms. Table 3, which is in the appendix,\\ncontains examples of the papers’ motivations and\\ntechniques across a range of different NLP tasks.\\n3\\nFindings\\nCategorizing the 146 papers’ motivations and pro-\\nposed quantitative techniques for measuring or miti-\\ngating “bias” into the six categories listed above en-\\nabled us to identify several commonalities, which\\nwe present below, along with illustrative quotes.\\n2We grouped several types of representational harms into\\ntwo categories to reﬂect that the main point of differentiation\\nbetween the 146 papers’ motivations and proposed quantitative\\ntechniques for measuring or mitigating “bias” is whether or not\\nthey focus on stereotyping. Among the papers that do not fo-\\ncus on stereotyping, we found that most lack sufﬁciently clear\\nmotivations and techniques to reliably categorize them further.\\n3.1\\nMotivations\\nPapers state a wide range of motivations,\\nmultiple motivations, vague motivations, and\\nsometimes no motivations at all.\\nWe found that\\nthe papers’ motivations span all six categories, with\\nseveral papers falling into each one. Appropriately,\\npapers that provide surveys or frameworks for an-\\nalyzing “bias” in NLP systems often state multiple\\nmotivations (e.g., Hovy and Spruit, 2016; Bender,\\n2019; Sun et al., 2019; Rozado, 2020; Shah et al.,\\n2020). However, as the examples in Table 3 (in the\\nappendix) illustrate, many other papers (33%) do\\nso as well. Some papers (16%) state only vague\\nmotivations or no motivations at all. For example,\\n“[N]o human should be discriminated on the basis\\nof demographic attributes by an NLP system.”\\n—Kaneko and Bollegala (2019)\\n“[P]rominent word embeddings [...] encode\\nsystematic biases against women and black people\\n[...] implicating many NLP systems in scaling up\\nsocial injustice.”\\n—May et al. (2019)\\nThese examples leave unstated what it might mean\\nfor an NLP system to “discriminate,” what con-\\nstitutes “systematic biases,” or how NLP systems\\ncontribute to “social injustice” (itself undeﬁned).\\nPapers’ motivations sometimes include no nor-\\nmative reasoning.\\nWe found that some papers\\n(32%) are not motivated by any apparent normative\\nconcerns, often focusing instead on concerns about\\nsystem performance. For example, the ﬁrst quote\\nbelow includes normative reasoning—namely that\\nmodels should not use demographic information\\nto make predictions—while the other focuses on\\nlearned correlations impairing system performance.\\n“In [text classiﬁcation], models are expected to\\nmake predictions with the semantic information\\nrather than with the demographic group identity\\ninformation (e.g., ‘gay’, ‘black’) contained in the\\nsentences.”\\n—Zhang et al. (2020a)\\n“An over-prevalence of some gendered forms in the\\ntraining data leads to translations with identiﬁable\\nerrors. Translations are better for sentences\\ninvolving men and for sentences containing\\nstereotypical gender roles.”\\n—Saunders and Byrne (2020)\\nEven when papers do state clear motivations,\\nthey are often unclear about why the system be-\\nhaviors that are described as “bias” are harm-\\nful, in what ways, and to whom.\\nWe found that\\neven papers with clear motivations often fail to ex-\\nplain what kinds of system behaviors are harmful,\\nin what ways, to whom, and why. For example,\\n\\n“Deploying these word embedding algorithms in\\npractice, for example in automated translation\\nsystems or as hiring aids, runs the serious risk of\\nperpetuating problematic biases in important\\nsocietal contexts.”\\n—Brunet et al. (2019)\\n“[I]f the systems show discriminatory behaviors in\\nthe interactions, the user experience will be\\nadversely affected.”\\n—Liu et al. (2019)\\nThese examples leave unstated what “problematic\\nbiases” or non-ideal user experiences might look\\nlike, how the system behaviors might result in\\nthese things, and who the relevant stakeholders\\nor users might be. In contrast, we ﬁnd that papers\\nthat provide surveys or frameworks for analyzing\\n“bias” in NLP systems often name who is harmed,\\nacknowledging that different social groups may\\nexperience these systems differently due to their\\ndifferent relationships with NLP systems or\\ndifferent social positions. For example, Ruane\\net al. (2019) argue for a “deep understanding of\\nthe user groups [sic] characteristics, contexts, and\\ninterests” when designing conversational agents.\\nPapers about NLP systems developed for the\\nsame task often conceptualize “bias” differ-\\nently.\\nEven papers that cover the same NLP task\\noften conceptualize “bias” in ways that differ sub-\\nstantially and are sometimes inconsistent. Rows 3\\nand 4 of Table 3 (in the appendix) contain machine\\ntranslation papers with different conceptualizations\\nof “bias,” leading to different proposed techniques,\\nwhile rows 5 and 6 contain papers on “bias” in em-\\nbedding spaces that state different motivations, but\\npropose techniques for quantifying stereotyping.\\nPapers’ motivations conﬂate allocational and\\nrepresentational harms.\\nWe found that the pa-\\npers’ motivations sometimes (16%) name imme-\\ndiate representational harms, such as stereotyping,\\nalongside more distant allocational harms, which,\\nin the case of stereotyping, are usually imagined as\\ndownstream effects of stereotypes on résumé ﬁlter-\\ning. Many of these papers use the imagined down-\\nstream effects to justify focusing on particular sys-\\ntem behaviors, even when the downstream effects\\nare not measured. Papers on “bias” in embedding\\nspaces are especially likely to do this because em-\\nbeddings are often used as input to other systems:\\n“However, none of these papers [on embeddings]\\nhave recognized how blatantly sexist the\\nembeddings are and hence risk introducing biases\\nof various types into real-world systems.”\\n—Bolukbasi et al. (2016a)\\n“It is essential to quantify and mitigate gender bias\\nin these embeddings to avoid them from affecting\\ndownstream applications.”\\n—Zhou et al. (2019)\\nIn contrast, papers that provide surveys or frame-\\nworks for analyzing “bias” in NLP systems treat\\nrepresentational harms as harmful in their own\\nright. For example, Mayﬁeld et al. (2019) and\\nRuane et al. (2019) cite the harmful reproduction\\nof dominant linguistic norms by NLP systems (a\\npoint to which we return in section 4), while Bender\\n(2019) outlines a range of harms, including seeing\\nstereotypes in search results and being made invis-\\nible to search engines due to language practices.\\n3.2\\nTechniques\\nPapers’ techniques are not well grounded in the\\nrelevant literature outside of NLP.\\nPerhaps un-\\nsurprisingly given that the papers’ motivations are\\noften vague, inconsistent, and lacking in normative\\nreasoning, we also found that the papers’ proposed\\nquantitative techniques for measuring or mitigating\\n“bias” do not effectively engage with the relevant\\nliterature outside of NLP. Papers on stereotyping\\nare a notable exception: the Word Embedding\\nAssociation Test (Caliskan et al., 2017) draws on\\nthe Implicit Association Test (Greenwald et al.,\\n1998) from the social psychology literature, while\\nseveral techniques operationalize the well-studied\\n“Angry Black Woman” stereotype (Kiritchenko\\nand Mohammad, 2018; May et al., 2019; Tan\\nand Celis, 2019) and the “double bind” faced by\\nwomen (May et al., 2019; Tan and Celis, 2019), in\\nwhich women who succeed at stereotypically male\\ntasks are perceived to be less likable than similarly\\nsuccessful men (Heilman et al., 2004). Tan and\\nCelis (2019) also examine the compounding effects\\nof race and gender, drawing on Black feminist\\nscholarship on intersectionality (Crenshaw, 1989).\\nPapers’ techniques are poorly matched to their\\nmotivations.\\nWe found that although 21% of the\\npapers include allocational harms in their motiva-\\ntions, only four papers actually propose techniques\\nfor measuring or mitigating allocational harms.\\nPapers focus on a narrow range of potential\\nsources of “bias.”\\nWe found that nearly all of the\\npapers focus on system predictions as the potential\\nsources of “bias,” with many additionally focusing\\non “bias” in datasets (e.g., differences in the\\nnumber of gendered pronouns in the training data\\n(Zhao et al., 2019)). Most papers do not interrogate\\n\\nthe normative implications of other decisions made\\nduring the development and deployment lifecycle—\\nperhaps unsurprising given that their motivations\\nsometimes include no normative reasoning.\\nA\\nfew papers are exceptions, illustrating the impacts\\nof task deﬁnitions, annotation guidelines, and\\nevaluation metrics: Cao and Daumé (2019) study\\nhow folk conceptions of gender (Keyes, 2018) are\\nreproduced in coreference resolution systems that\\nassume a strict gender dichotomy, thereby main-\\ntaining cisnormativity; Sap et al. (2019) focus on\\nthe effect of priming annotators with information\\nabout possible dialectal differences when asking\\nthem to apply toxicity labels to sample tweets, ﬁnd-\\ning that annotators who are primed are signiﬁcantly\\nless likely to label tweets containing features asso-\\nciated with African-American English as offensive.\\n4\\nA path forward\\nWe now describe how researchers and practitioners\\nconducting work analyzing “bias” in NLP systems\\nmight avoid the pitfalls presented in the previous\\nsection—the beginnings of a path forward. We\\npropose three recommendations that should guide\\nsuch work, and, for each, provide several concrete\\nresearch questions. We emphasize that these ques-\\ntions are not comprehensive, and are intended to\\ngenerate further questions and lines of engagement.\\nOur three recommendations are as follows:\\n(R1) Ground work analyzing “bias” in NLP sys-\\ntems in the relevant literature outside of NLP\\nthat explores the relationships between lan-\\nguage and social hierarchies. Treat represen-\\ntational harms as harmful in their own right.\\n(R2) Provide explicit statements of why the\\nsystem behaviors that are described as “bias”\\nare harmful, in what ways, and to whom.\\nBe forthright about the normative reasoning\\n(Green, 2019) underlying these statements.\\n(R3) Examine language use in practice by engag-\\ning with the lived experiences of members of\\ncommunities affected by NLP systems. Inter-\\nrogate and reimagine the power relations be-\\ntween technologists and such communities.\\n4.1\\nLanguage and social hierarchies\\nTurning ﬁrst to (R1), we argue that work analyzing\\n“bias” in NLP systems will paint a much fuller pic-\\nture if it engages with the relevant literature outside\\nof NLP that explores the relationships between\\nlanguage and social hierarchies. Many disciplines,\\nincluding sociolinguistics, linguistic anthropology,\\nsociology, and social psychology, study how\\nlanguage takes on social meaning and the role that\\nlanguage plays in maintaining social hierarchies.\\nFor example, language is the means through which\\nsocial groups are labeled and one way that beliefs\\nabout social groups are transmitted (e.g., Maass,\\n1999; Beukeboom and Burgers, 2019).\\nGroup\\nlabels can serve as the basis of stereotypes and thus\\nreinforce social inequalities: “[T]he label content\\nfunctions to identify a given category of people,\\nand thereby conveys category boundaries and a\\nposition in a hierarchical taxonomy” (Beukeboom\\nand Burgers, 2019).\\nSimilarly, “controlling\\nimages,” such as stereotypes of Black women,\\nwhich are linguistically and visually transmitted\\nthrough literature, news media, television, and so\\nforth, provide “ideological justiﬁcation” for their\\ncontinued oppression (Collins, 2000, Chapter 4).\\nAs a result, many groups have sought to bring\\nabout social changes through changes in language,\\ndisrupting patterns of oppression and marginal-\\nization via so-called “gender-fair” language\\n(Sczesny et al., 2016; Menegatti and Rubini, 2017),\\nlanguage that is more inclusive to people with\\ndisabilities (ADA, 2018), and language that is less\\ndehumanizing (e.g., abandoning the use of the term\\n“illegal” in everyday discourse on immigration in\\nthe U.S. (Rosa, 2019)). The fact that group labels\\nare so contested is evidence of how deeply inter-\\ntwined language and social hierarchies are. Taking\\n“gender-fair” language as an example, the hope\\nis that reducing asymmetries in language about\\nwomen and men will reduce asymmetries in their\\nsocial standing. Meanwhile, struggles over lan-\\nguage use often arise from dominant social groups’\\ndesire to “control both material and symbolic\\nresources”—i.e., “the right to decide what words\\nwill mean and to control those meanings”—as was\\nthe case in some white speakers’ insistence on\\nusing offensive place names against the objections\\nof Indigenous speakers (Hill, 2008, Chapter 3).\\nSociolinguists and linguistic anthropologists\\nhave also examined language attitudes and lan-\\nguage ideologies, or people’s metalinguistic beliefs\\nabout language: Which language varieties or prac-\\ntices are taken as standard, ordinary, or unmarked?\\nWhich are considered correct, prestigious, or ap-\\npropriate for public use, and which are considered\\nincorrect, uneducated, or offensive (e.g., Campbell-\\n\\nKibler, 2009; Preston, 2009; Loudermilk, 2015;\\nLanehart and Malik, 2018)? Which are rendered in-\\nvisible (Roche, 2019)?3 Language ideologies play\\na vital role in reinforcing and justifying social hi-\\nerarchies because beliefs about language varieties\\nor practices often translate into beliefs about their\\nspeakers (e.g. Alim et al., 2016; Rosa and Flores,\\n2017; Craft et al., 2020). For example, in the U.S.,\\nthe portrayal of non-white speakers’ language\\nvarieties and practices as linguistically deﬁcient\\nhelped to justify violent European colonialism, and\\ntoday continues to justify enduring racial hierar-\\nchies by maintaining views of non-white speakers\\nas lacking the language “required for complex\\nthinking processes and successful engagement\\nin the global economy” (Rosa and Flores, 2017).\\nRecognizing the role that language plays in\\nmaintaining social hierarchies is critical to the\\nfuture of work analyzing “bias” in NLP systems.\\nFirst, it helps to explain why representational\\nharms are harmful in their own right. Second, the\\ncomplexity of the relationships between language\\nand social hierarchies illustrates why studying\\n“bias” in NLP systems is so challenging, suggesting\\nthat researchers and practitioners will need to move\\nbeyond existing algorithmic fairness techniques.\\nWe argue that work must be grounded in the\\nrelevant literature outside of NLP that examines\\nthe relationships between language and social\\nhierarchies; without this grounding, researchers\\nand practitioners risk measuring or mitigating\\nonly what is convenient to measure or mitigate,\\nrather than what is most normatively concerning.\\nMore speciﬁcally, we recommend that work\\nanalyzing “bias” in NLP systems be reoriented\\naround the following question: How are social\\nhierarchies, language ideologies, and NLP systems\\ncoproduced? This question mirrors Benjamin’s\\n(2020) call to examine how “race and technology\\nare coproduced”—i.e., how racial hierarchies, and\\nthe ideologies and discourses that maintain them,\\ncreate and are re-created by technology. We recom-\\nmend that researchers and practitioners similarly\\nask how existing social hierarchies and language\\nideologies drive the development and deployment\\nof NLP systems, and how these systems therefore\\nreproduce these hierarchies and ideologies. As\\na starting point for reorienting work analyzing\\n“bias” in NLP systems around this question, we\\n3Language ideologies encompass much more than this; see,\\ne.g., Lippi-Green (2012), Alim et al. (2016), Rosa and Flores\\n(2017), Rosa and Burdick (2017), and Charity Hudley (2017).\\nprovide the following concrete research questions:\\n▷How do social hierarchies and language\\nideologies inﬂuence the decisions made during\\nthe development and deployment lifecycle?\\nWhat kinds of NLP systems do these decisions\\nresult in, and what kinds do they foreclose?\\n⋄General assumptions: To which linguistic\\nnorms do NLP systems adhere (Bender,\\n2019; Ruane et al., 2019)? Which language\\npractices are implicitly assumed to be\\nstandard, ordinary, correct, or appropriate?\\n⋄Task deﬁnition:\\nFor which speakers\\nare NLP systems (and NLP resources)\\ndeveloped? (See Joshi et al. (2020) for\\na discussion.)\\nHow do task deﬁnitions\\ndiscretize the world? For example, how\\nare social groups delineated when deﬁning\\ndemographic attribute prediction tasks\\n(e.g., Koppel et al., 2002; Rosenthal and\\nMcKeown, 2011; Nguyen et al., 2013)?\\nWhat about languages in native language\\nprediction tasks (Tetreault et al., 2013)?\\n⋄Data: How are datasets collected, prepro-\\ncessed, and labeled or annotated? What are\\nthe impacts of annotation guidelines, anno-\\ntator assumptions and perceptions (Olteanu\\net al., 2019; Sap et al., 2019; Geiger et al.,\\n2020), and annotation aggregation pro-\\ncesses (Pavlick and Kwiatkowski, 2019)?\\n⋄Evaluation: How are NLP systems evalu-\\nated? What are the impacts of evaluation\\nmetrics (Olteanu et al., 2017)? Are any\\nnon-quantitative evaluations performed?\\n▷How do NLP systems reproduce or transform\\nlanguage ideologies? Which language varieties\\nor practices come to be deemed good or bad?\\nMight “good” language simply mean language\\nthat is easily handled by existing NLP sys-\\ntems? For example, linguistic phenomena aris-\\ning from many language practices (Eisenstein,\\n2013) are described as “noisy text” and often\\nviewed as a target for “normalization.” How\\ndo the language ideologies that are reproduced\\nby NLP systems maintain social hierarchies?\\n▷Which\\nrepresentational\\nharms\\nare\\nbeing\\nmeasured or mitigated? Are these the most\\nnormatively concerning harms, or merely\\nthose that are well handled by existing algo-\\nrithmic fairness techniques? Are there other\\nrepresentational harms that might be analyzed?\\n\\n4.2\\nConceptualizations of “bias”\\nTurning now to (R2), we argue that work analyzing\\n“bias” in NLP systems should provide explicit\\nstatements of why the system behaviors that are\\ndescribed as “bias” are harmful, in what ways,\\nand to whom, as well as the normative reasoning\\nunderlying these statements.\\nIn other words,\\nresearchers and practitioners should articulate their\\nconceptualizations of “bias.”\\nAs we described\\nabove, papers often contain descriptions of system\\nbehaviors that are understood to be self-evident\\nstatements of “bias.”\\nThis use of imprecise\\nterminology has led to papers all claiming to\\nanalyze “bias” in NLP systems, sometimes even\\nin systems developed for the same task, but with\\ndifferent or even inconsistent conceptualizations of\\n“bias,” and no explanations for these differences.\\nYet analyzing “bias” is an inherently normative\\nprocess—in which some system behaviors are\\ndeemed good and others harmful—even if assump-\\ntions about what kinds of system behaviors are\\nharmful, in what ways, for whom, and why are\\nnot stated. We therefore echo calls by Bardzell and\\nBardzell (2011), Keyes et al. (2019), and Green\\n(2019) for researchers and practitioners to make\\ntheir normative reasoning explicit by articulating\\nthe social values that underpin their decisions to\\ndeem some system behaviors as harmful, no matter\\nhow obvious such values appear to be. We further\\nargue that this reasoning should take into account\\nthe relationships between language and social\\nhierarchies that we described above. First, these\\nrelationships provide a foundation from which to\\napproach the normative reasoning that we recom-\\nmend making explicit. For example, some system\\nbehaviors might be harmful precisely because\\nthey maintain social hierarchies. Second, if work\\nanalyzing “bias” in NLP systems is reoriented\\nto understand how social hierarchies, language\\nideologies, and NLP systems are coproduced, then\\nthis work will be incomplete if we fail to account\\nfor the ways that social hierarchies and language\\nideologies determine what we mean by “bias” in\\nthe ﬁrst place. As a starting point, we therefore\\nprovide the following concrete research questions:\\n▷What kinds of system behaviors are described\\nas “bias”? What are their potential sources (e.g.,\\ngeneral assumptions, task deﬁnition, data)?\\n▷In what ways are these system behaviors harm-\\nful, to whom are they harmful, and why?\\n▷What are the social values (obvious or not) that\\nunderpin this conceptualization of “bias?”\\n4.3\\nLanguage use in practice\\nFinally, we turn to (R3). Our perspective, which\\nrests on a greater recognition of the relationships\\nbetween language and social hierarchies, suggests\\nseveral directions for examining language use in\\npractice. Here, we focus on two. First, because lan-\\nguage is necessarily situated, and because different\\nsocial groups have different lived experiences due\\nto their different social positions (Hanna et al.,\\n2020)—particularly groups at the intersections\\nof multiple axes of oppression—we recommend\\nthat researchers and practitioners center work\\nanalyzing “bias” in NLP systems around the lived\\nexperiences of members of communities affected\\nby these systems. Second, we recommend that\\nthe power relations between technologists and\\nsuch communities be interrogated and reimagined.\\nResearchers have pointed out that algorithmic\\nfairness techniques, by proposing incremental\\ntechnical mitigations—e.g., collecting new datasets\\nor training better models—maintain these power\\nrelations by (a) assuming that automated systems\\nshould continue to exist, rather than asking\\nwhether they should be built at all, and (b) keeping\\ndevelopment and deployment decisions in the\\nhands of technologists (Bennett and Keyes, 2019;\\nCifor et al., 2019; Green, 2019; Katell et al., 2020).\\nThere are many disciplines for researchers and\\npractitioners to draw on when pursuing these\\ndirections.\\nFor example, in human–computer\\ninteraction, Hamidi et al. (2018) study transgender\\npeople’s experiences with automated gender\\nrecognition systems in order to uncover how\\nthese systems reproduce structures of transgender\\nexclusion by redeﬁning what it means to perform\\ngender “normally.” Value-sensitive design provides\\na framework for accounting for the values of differ-\\nent stakeholders in the design of technology (e.g.,\\nFriedman et al., 2006; Friedman and Hendry, 2019;\\nLe Dantec et al., 2009; Yoo et al., 2019), while\\nparticipatory design seeks to involve stakeholders\\nin the design process itself (Sanders, 2002; Muller,\\n2007; Simonsen and Robertson, 2013; DiSalvo\\net al., 2013). Participatory action research in educa-\\ntion (Kemmis, 2006) and in language documenta-\\ntion and reclamation (Junker, 2018) is also relevant.\\nIn particular, work on language reclamation to\\nsupport decolonization and tribal sovereignty\\n(Leonard, 2012) and work in sociolinguistics focus-\\n\\ning on developing co-equal research relationships\\nwith community members and supporting linguis-\\ntic justice efforts (e.g., Bucholtz et al., 2014, 2016,\\n2019) provide examples of more emancipatory rela-\\ntionships with communities. Finally, several work-\\nshops and events have begun to explore how to em-\\npower stakeholders in the development and deploy-\\nment of technology (Vaccaro et al., 2019; Givens\\nand Morris, 2020; Sassaman et al., 2020)4 and how\\nto help researchers and practitioners consider when\\nnot to build systems at all (Barocas et al., 2020).\\nAs a starting point for engaging with commu-\\nnities affected by NLP systems, we therefore\\nprovide the following concrete research questions:\\n▷How do communities become aware of NLP\\nsystems? Do they resist them, and if so, how?\\n▷What additional costs are borne by communi-\\nties for whom NLP systems do not work well?\\n▷Do NLP systems shift power toward oppressive\\ninstitutions (e.g., by enabling predictions that\\ncommunities do not want made, linguistically\\nbased unfair allocation of resources or oppor-\\ntunities (Rosa and Flores, 2017), surveillance,\\nor censorship), or away from such institutions?\\n▷Who is involved in the development and\\ndeployment of NLP systems?\\nHow do\\ndecision-making processes maintain power re-\\nlations between technologists and communities\\naffected by NLP systems?\\nCan these pro-\\ncesses be changed to reimagine these relations?\\n5\\nCase study\\nTo illustrate our recommendations, we present a\\ncase study covering work on African-American\\nEnglish (AAE).5 Work analyzing “bias” in the con-\\ntext of AAE has shown that part-of-speech taggers,\\nlanguage identiﬁcation systems, and dependency\\nparsers all work less well on text containing\\nfeatures associated with AAE than on text without\\nthese features (Jørgensen et al., 2015, 2016; Blod-\\ngett et al., 2016, 2018), and that toxicity detection\\nsystems score tweets containing features associated\\nwith AAE as more offensive than tweets with-\\nout them (Davidson et al., 2019; Sap et al., 2019).\\nThese papers have been critical for highlighting\\nAAE as a language variety for which existing NLP\\n4Also https://participatoryml.github.io/\\n5This language variety has had many different names\\nover the years,\\nbut is now generally called African-\\nAmerican English (AAE), African-American Vernacular En-\\nglish (AAVE), or African-American Language (AAL) (Green,\\n2002; Wolfram and Schilling, 2015; Rickford and King, 2016).\\nsystems may not work, illustrating their limitations.\\nHowever, they do not conceptualize “racial bias” in\\nthe same way. The ﬁrst four of these papers simply\\nfocus on system performance differences between\\ntext containing features associated with AAE and\\ntext without these features. In contrast, the last\\ntwo papers also focus on such system performance\\ndifferences, but motivate this focus with the fol-\\nlowing additional reasoning: If tweets containing\\nfeatures associated with AAE are scored as more\\noffensive than tweets without these features, then\\nthis might (a) yield negative perceptions of AAE;\\n(b) result in disproportionate removal of tweets\\ncontaining these features, impeding participation\\nin online platforms and reducing the space avail-\\nable online in which speakers can use AAE freely;\\nand (c) cause AAE speakers to incur additional\\ncosts if they have to change their language practices\\nto avoid negative perceptions or tweet removal.\\nMore importantly, none of these papers engage\\nwith the literature on AAE, racial hierarchies in the\\nU.S., and raciolinguistic ideologies. By failing to\\nengage with this literature—thereby treating AAE\\nsimply as one of many non-Penn Treebank vari-\\neties of English or perhaps as another challenging\\ndomain—work analyzing “bias” in NLP systems\\nin the context of AAE fails to situate these systems\\nin the world. Who are the speakers of AAE? How\\nare they viewed? We argue that AAE as a language\\nvariety cannot be separated from its speakers—\\nprimarily Black people in the U.S., who experience\\nsystemic anti-Black racism—and the language ide-\\nologies that reinforce and justify racial hierarchies.\\nEven after decades of sociolinguistic efforts to\\nlegitimize AAE, it continues to be viewed as “bad”\\nEnglish and its speakers continue to be viewed as\\nlinguistically inadequate—a view called the deﬁcit\\nperspective (Alim et al., 2016; Rosa and Flores,\\n2017). This perspective persists despite demon-\\nstrations that AAE is rule-bound and grammatical\\n(Mufwene et al., 1998; Green, 2002), in addition\\nto ample evidence of its speakers’ linguistic adroit-\\nness (e.g., Alim, 2004; Rickford and King, 2016).\\nThis perspective belongs to a broader set of raciolin-\\nguistic ideologies (Rosa and Flores, 2017), which\\nalso produce allocational harms; speakers of AAE\\nare frequently penalized for not adhering to domi-\\nnant language practices, including in the education\\nsystem (Alim, 2004; Terry et al., 2010), when\\nseeking housing (Baugh, 2018), and in the judicial\\nsystem, where their testimony is misunderstood or,\\n\\nworse yet, disbelieved (Rickford and King, 2016;\\nJones et al., 2019). These raciolinguistic ideologies\\nposition\\nracialized\\ncommunities\\nas\\nneeding\\nlinguistic intervention, such as language education\\nprograms, in which these and other harms can be\\nreduced if communities accommodate to domi-\\nnant language practices (Rosa and Flores, 2017).\\nIn the technology industry, speakers of AAE are\\noften not considered consumers who matter. For\\nexample, Benjamin (2019) recounts an Apple em-\\nployee who worked on speech recognition for Siri:\\n“As they worked on different English dialects —\\nAustralian, Singaporean, and Indian English — [the\\nemployee] asked his boss: ‘What about African\\nAmerican English?’ To this his boss responded:\\n‘Well, Apple products are for the premium market.”’\\nThe reality, of course, is that speakers of AAE tend\\nnot to represent the “premium market” precisely be-\\ncause of institutions and policies that help to main-\\ntain racial hierarchies by systematically denying\\nthem the opportunities to develop wealth that are\\navailable to white Americans (Rothstein, 2017)—\\nan exclusion that is reproduced in technology by\\ncountless decisions like the one described above.\\nEngaging with the literature outlined above\\nsituates the system behaviors that are described\\nas “bias,” providing a foundation for normative\\nreasoning. Researchers and practitioners should\\nbe concerned about “racial bias” in toxicity\\ndetection systems not only because performance\\ndifferences impair system performance,\\nbut\\nbecause they reproduce longstanding injustices of\\nstigmatization and disenfranchisement for speakers\\nof AAE. In re-stigmatizing AAE, they reproduce\\nlanguage ideologies in which AAE is viewed as\\nungrammatical, uneducated, and offensive. These\\nideologies, in turn, enable linguistic discrimination\\nand justify enduring racial hierarchies (Rosa and\\nFlores, 2017). Our perspective, which understands\\nracial hierarchies and raciolinguistic ideologies as\\nstructural conditions that govern the development\\nand deployment of technology,\\nimplies that\\ntechniques for measuring or mitigating “bias”\\nin NLP systems will necessarily be incomplete\\nunless they interrogate and dismantle these\\nstructural conditions, including the power relations\\nbetween technologists and racialized communities.\\nWe emphasize that engaging with the literature\\non AAE, racial hierarchies in the U.S., and\\nraciolinguistic ideologies can generate new lines of\\nengagement. These lines include work on the ways\\nthat the decisions made during the development\\nand deployment of NLP systems produce stigmati-\\nzation and disenfranchisement, and work on AAE\\nuse in practice, such as the ways that speakers\\nof AAE interact with NLP systems that were not\\ndesigned for them. This literature can also help re-\\nsearchers and practitioners address the allocational\\nharms that may be produced by NLP systems, and\\nensure that even well-intentioned NLP systems\\ndo not position racialized communities as needing\\nlinguistic intervention or accommodation to\\ndominant language practices. Finally, researchers\\nand practitioners wishing to design better systems\\ncan also draw on a growing body of work on\\nanti-racist language pedagogy that challenges the\\ndeﬁcit perspective of AAE and other racialized\\nlanguage practices (e.g. Flores and Chaparro, 2018;\\nBaker-Bell, 2019; Martínez and Mejía, 2019), as\\nwell as the work that we described in section 4.3\\non reimagining the power relations between tech-\\nnologists and communities affected by technology.\\n6\\nConclusion\\nBy surveying 146 papers analyzing “bias” in NLP\\nsystems, we found that (a) their motivations are\\noften vague, inconsistent, and lacking in norma-\\ntive reasoning; and (b) their proposed quantitative\\ntechniques for measuring or mitigating “bias” are\\npoorly matched to their motivations and do not en-\\ngage with the relevant literature outside of NLP.\\nTo help researchers and practitioners avoid these\\npitfalls, we proposed three recommendations that\\nshould guide work analyzing “bias” in NLP sys-\\ntems, and, for each, provided several concrete re-\\nsearch questions. These recommendations rest on\\na greater recognition of the relationships between\\nlanguage and social hierarchies—a step that we\\nsee as paramount to establishing a path forward.\\nAcknowledgments\\nThis paper is based upon work supported by the\\nNational Science Foundation Graduate Research\\nFellowship under Grant No. 1451512. Any opin-\\nion, ﬁndings, and conclusions or recommendations\\nexpressed in this material are those of the authors\\nand do not necessarily reﬂect the views of the Na-\\ntional Science Foundation. We thank the reviewers\\nfor their useful feedback, especially the sugges-\\ntion to include additional details about our method.\\n\\nReferences\\nArtem Abzaliev. 2019.\\nOn GAP coreference resolu-\\ntion shared task: insights from the 3rd place solution.\\nIn Proceedings of the Workshop on Gender Bias in\\nNatural Language Processing, pages 107–112, Flo-\\nrence, Italy.\\nADA. 2018.\\nGuidelines for Writing About Peo-\\nple With Disabilities.\\nADA National Network.\\nhttps://bit.ly/2KREbkB.\\nOshin Agarwal, Funda Durupinar, Norman I. Badler,\\nand Ani Nenkova. 2019. Word embeddings (also)\\nencode human personality stereotypes. In Proceed-\\nings of the Joint Conference on Lexical and Com-\\nputational Semantics, pages 205–211, Minneapolis,\\nMN.\\nH. Samy Alim. 2004. You Know My Steez: An Ethno-\\ngraphic and Sociolinguistic Study of Styleshifting in\\na Black American Speech Community. American Di-\\nalect Society.\\nH. Samy Alim, John R. Rickford, and Arnetha F. Ball,\\neditors. 2016.\\nRaciolinguistics:\\nHow Language\\nShapes Our Ideas About Race. Oxford University\\nPress.\\nSandeep Attree. 2019. Gendered ambiguous pronouns\\nshared task: Boosting model conﬁdence by evidence\\npooling. In Proceedings of the Workshop on Gen-\\nder Bias in Natural Language Processing, Florence,\\nItaly.\\nPinkesh Badjatiya,\\nManish Gupta,\\nand Vasudeva\\nVarma. 2019.\\nStereotypical bias removal for hate\\nspeech detection task using knowledge-based gen-\\neralizations.\\nIn Proceedings of the International\\nWorld Wide Web Conference, pages 49–59, San Fran-\\ncisco, CA.\\nEugene Bagdasaryan, Omid Poursaeed, and Vitaly\\nShmatikov. 2019.\\nDifferential Privacy Has Dis-\\nparate Impact on Model Accuracy. In Proceedings\\nof the Conference on Neural Information Processing\\nSystems, Vancouver, Canada.\\nApril Baker-Bell. 2019.\\nDismantling anti-black lin-\\nguistic racism in English language arts classrooms:\\nToward an anti-racist black language pedagogy. The-\\nory Into Practice.\\nDavid Bamman, Sejal Popat, and Sheng Shen. 2019.\\nAn annotated dataset of literary entities. In Proceed-\\nings of the North American Association for Com-\\nputational Linguistics (NAACL), pages 2138–2144,\\nMinneapolis, MN.\\nXingce Bao and Qianqian Qiao. 2019. Transfer Learn-\\ning from Pre-trained BERT for Pronoun Resolution.\\nIn Proceedings of the Workshop on Gender Bias\\nin Natural Language Processing, pages 82–88, Flo-\\nrence, Italy.\\nShaowen Bardzell and Jeffrey Bardzell. 2011. Towards\\na Feminist HCI Methodology: Social Science, Femi-\\nnism, and HCI. In Proceedings of the Conference on\\nHuman Factors in Computing Systems (CHI), pages\\n675–684, Vancouver, Canada.\\nSolon Barocas, Asia J. Biega, Benjamin Fish, J˛edrzej\\nNiklas, and Luke Stark. 2020.\\nWhen Not to De-\\nsign, Build, or Deploy. In Proceedings of the Confer-\\nence on Fairness, Accountability, and Transparency,\\nBarcelona, Spain.\\nSolon Barocas, Kate Crawford, Aaron Shapiro, and\\nHanna Wallach. 2017. The Problem With Bias: Al-\\nlocative Versus Representational Harms in Machine\\nLearning. In Proceedings of SIGCIS, Philadelphia,\\nPA.\\nChristine Basta, Marta R. Costa-jussà, and Noe Casas.\\n2019. Evaluating the underlying gender bias in con-\\ntextualized word embeddings.\\nIn Proceedings of\\nthe Workshop on Gender Bias for Natural Language\\nProcessing, pages 33–39, Florence, Italy.\\nJohn Baugh. 2018.\\nLinguistics in Pursuit of Justice.\\nCambridge University Press.\\nEmily M. Bender. 2019. A typology of ethical risks\\nin language technology with an eye towards where\\ntransparent documentation can help.\\nPresented at\\nThe Future of Artiﬁcial Intelligence:\\nLanguage,\\nEthics, Technology Workshop. https://bit.ly/\\n2P9t9M6.\\nRuha Benjamin. 2019. Race After Technology: Aboli-\\ntionist Tools for the New Jim Code. John Wiley &\\nSons.\\nRuha Benjamin. 2020. 2020 Vision: Reimagining the\\nDefault Settings of Technology & Society. Keynote\\nat ICLR.\\nCynthia L. Bennett and Os Keyes. 2019. What is the\\nPoint of Fairness?\\nDisability, AI, and The Com-\\nplexity of Justice.\\nIn Proceedings of the ASSETS\\nWorkshop on AI Fairness for People with Disabili-\\nties, Pittsburgh, PA.\\nCamiel J. Beukeboom and Christian Burgers. 2019.\\nHow Stereotypes Are Shared Through Language: A\\nReview and Introduction of the Social Categories\\nand Stereotypes Communication (SCSC) Frame-\\nwork. Review of Communication Research, 7:1–37.\\nShruti Bhargava and David Forsyth. 2019.\\nExpos-\\ning and Correcting the Gender Bias in Image\\nCaptioning Datasets and Models.\\narXiv preprint\\narXiv:1912.00578.\\nJayadev Bhaskaran and Isha Bhallamudi. 2019. Good\\nSecretaries, Bad Truck Drivers? Occupational Gen-\\nder Stereotypes in Sentiment Analysis. In Proceed-\\nings of the Workshop on Gender Bias in Natural Lan-\\nguage Processing, pages 62–68, Florence, Italy.\\n\\nSu Lin Blodgett, Lisa Green, and Brendan O’Connor.\\n2016.\\nDemographic Dialectal Variation in Social\\nMedia: A Case Study of African-American English.\\nIn Proceedings of Empirical Methods in Natural\\nLanguage Processing (EMNLP), pages 1119–1130,\\nAustin, TX.\\nSu Lin Blodgett and Brendan O’Connor. 2017. Racial\\nDisparity in Natural Language Processing: A Case\\nStudy of Social Media African-American English.\\nIn Proceedings of the Workshop on Fairness, Ac-\\ncountability, and Transparency in Machine Learning\\n(FAT/ML), Halifax, Canada.\\nSu Lin Blodgett, Johnny Wei, and Brendan O’Connor.\\n2018.\\nTwitter Universal Dependency Parsing for\\nAfrican-American and Mainstream American En-\\nglish. In Proceedings of the Association for Compu-\\ntational Linguistics (ACL), pages 1415–1425, Mel-\\nbourne, Australia.\\nTolga\\nBolukbasi,\\nKai-Wei\\nChang,\\nJames\\nZou,\\nVenkatesh Saligrama,\\nand Adam Kalai. 2016a.\\nMan is to Computer Programmer as Woman is to\\nHomemaker? Debiasing Word Embeddings. In Pro-\\nceedings of the Conference on Neural Information\\nProcessing Systems, pages 4349–4357, Barcelona,\\nSpain.\\nTolga\\nBolukbasi,\\nKai-Wei\\nChang,\\nJames\\nZou,\\nVenkatesh Saligrama, and Adam Kalai. 2016b.\\nQuantifying and reducing stereotypes in word\\nembeddings. In Proceedings of the ICML Workshop\\non #Data4Good: Machine Learning in Social Good\\nApplications, pages 41–45, New York, NY.\\nShikha Bordia and Samuel R. Bowman. 2019. Identify-\\ning and reducing gender bias in word-level language\\nmodels. In Proceedings of the NAACL Student Re-\\nsearch Workshop, pages 7–15, Minneapolis, MN.\\nMarc-Etienne Brunet, Colleen Alkalay-Houlihan, Ash-\\nton Anderson, and Richard Zemel. 2019.\\nUnder-\\nstanding the Origins of Bias in Word Embeddings.\\nIn Proceedings of the International Conference on\\nMachine Learning, pages 803–811, Long Beach,\\nCA.\\nMary Bucholtz, Dolores Inés Casillas, and Jin Sook\\nLee. 2016.\\nBeyond Empowerment: Accompani-\\nment and Sociolinguistic Justice in a Youth Research\\nProgram. In Robert Lawson and Dave Sayers, edi-\\ntors, Sociolinguistic Research: Application and Im-\\npact, pages 25–44. Routledge.\\nMary Bucholtz, Dolores Inés Casillas, and Jin Sook\\nLee. 2019.\\nCalifornia Latinx Youth as Agents of\\nSociolinguistic Justice. In Netta Avineri, Laura R.\\nGraham, Eric J. Johnson, Robin Conley Riner, and\\nJonathan Rosa, editors, Language and Social Justice\\nin Practice, pages 166–175. Routledge.\\nMary Bucholtz, Audrey Lopez, Allina Mojarro, Elena\\nSkapoulli, Chris VanderStouwe, and Shawn Warner-\\nGarcia. 2014. Sociolinguistic Justice in the Schools:\\nStudent Researchers as Linguistic Experts.\\nLan-\\nguage and Linguistics Compass, 8:144–157.\\nKaylee Burns, Lisa Anne Hendricks, Kate Saenko,\\nTrevor Darrell, and Anna Rohrbach. 2018. Women\\nalso Snowboard: Overcoming Bias in Captioning\\nModels. In Procedings of the European Conference\\non Computer Vision (ECCV), pages 793–811, Mu-\\nnich, Germany.\\nAylin\\nCaliskan,\\nJoanna\\nJ.\\nBryson,\\nand\\nArvind\\nNarayanan. 2017. Semantics derived automatically\\nfrom language corpora contain human-like biases.\\nScience, 356(6334).\\nKathryn Campbell-Kibler. 2009.\\nThe nature of so-\\nciolinguistic perception.\\nLanguage Variation and\\nChange, 21(1):135–156.\\nYang Trista Cao and Hal Daumé, III. 2019.\\nTo-\\nward gender-inclusive coreference resolution. arXiv\\npreprint arXiv:1910.13913.\\nRakesh Chada. 2019. Gendered pronoun resolution us-\\ning bert and an extractive question answering formu-\\nlation. In Proceedings of the Workshop on Gender\\nBias in Natural Language Processing, pages 126–\\n133, Florence, Italy.\\nKaytlin Chaloner and Alfredo Maldonado. 2019. Mea-\\nsuring Gender Bias in Word Embedding across Do-\\nmains and Discovering New Gender Bias Word Cat-\\negories. In Proceedings of the Workshop on Gender\\nBias in Natural Language Processing, pages 25–32,\\nFlorence, Italy.\\nAnne H. Charity Hudley. 2017. Language and Racial-\\nization. In Ofelia García, Nelson Flores, and Mas-\\nsimiliano Spotti, editors, The Oxford Handbook of\\nLanguage and Society. Oxford University Press.\\nWon Ik Cho, Ji Won Kim, Seok Min Kim, and\\nNam Soo Kim. 2019. On measuring gender bias in\\ntranslation of gender-neutral pronouns. In Proceed-\\nings of the Workshop on Gender Bias in Natural Lan-\\nguage Processing, pages 173–181, Florence, Italy.\\nShivang Chopra, Ramit Sawhney, Puneet Mathur, and\\nRajiv Ratn Shah. 2020. Hindi-English Hate Speech\\nDetection: Author Proﬁling, Debiasing, and Practi-\\ncal Perspectives. In Proceedings of the AAAI Con-\\nference on Artiﬁcial Intelligence (AAAI), New York,\\nNY.\\nMarika Cifor, Patricia Garcia, T.L. Cowan, Jasmine\\nRault, Tonia Sutherland, Anita Say Chan, Jennifer\\nRode, Anna Lauren Hoffmann, Niloufar Salehi, and\\nLisa Nakamura. 2019.\\nFeminist Data Manifest-\\nNo. Retrieved from https://www.manifestno.\\ncom/.\\nPatricia Hill Collins. 2000.\\nBlack Feminist Thought:\\nKnowledge, Consciousness, and the Politics of Em-\\npowerment. Routledge.\\n\\nJustin T. Craft, Kelly E. Wright, Rachel Elizabeth\\nWeissler, and Robin M. Queen. 2020.\\nLanguage\\nand Discrimination: Generating Meaning, Perceiv-\\ning Identities, and Discriminating Outcomes.\\nAn-\\nnual Review of Linguistics, 6(1).\\nKate Crawford. 2017. The Trouble with Bias. Keynote\\nat NeurIPS.\\nKimberle Crenshaw. 1989. Demarginalizing the Inter-\\nsection of Race and Sex: A Black Feminist Critique\\nof Antidiscrmination Doctrine, Feminist Theory and\\nAntiracist Politics. University of Chicago Legal Fo-\\nrum.\\nAmanda Cercas Curry and Verena Rieser. 2018.\\n#MeToo: How Conversational Systems Respond to\\nSexual Harassment. In Proceedings of the Workshop\\non Ethics in Natural Language Processing, pages 7–\\n14, New Orleans, LA.\\nKaran Dabas, Nishtha Madaan, Gautam Singh, Vi-\\njay Arya, Sameep Mehta, and Tanmoy Chakraborty.\\n2020. Fair Transfer of Multiple Style Attributes in\\nText. arXiv preprint arXiv:2001.06693.\\nThomas Davidson, Debasmita Bhattacharya, and Ing-\\nmar Weber. 2019. Racial bias in hate speech and\\nabusive language detection datasets. In Proceedings\\nof the Workshop on Abusive Language Online, pages\\n25–35, Florence, Italy.\\nMaria De-Arteaga, Alexey Romanov, Hanna Wal-\\nlach, Jennifer Chayes, Christian Borgs, Alexandra\\nChouldechova, Sahin Geyik, Krishnaram Kentha-\\npadi, and Adam Tauman Kalai. 2019. Bias in bios:\\nA case study of semantic representation bias in a\\nhigh-stakes setting. In Proceedings of the Confer-\\nence on Fairness, Accountability, and Transparency,\\npages 120–128, Atlanta, GA.\\nSunipa Dev, Tao Li, Jeff Phillips, and Vivek Sriku-\\nmar. 2019.\\nOn Measuring and Mitigating Biased\\nInferences of Word Embeddings.\\narXiv preprint\\narXiv:1908.09369.\\nSunipa Dev and Jeff Phillips. 2019. Attenuating Bias in\\nWord Vectors. In Proceedings of the International\\nConference on Artiﬁcial Intelligence and Statistics,\\npages 879–887, Naha, Japan.\\nMark Díaz, Isaac Johnson, Amanda Lazar, Anne Marie\\nPiper, and Darren Gergle. 2018.\\nAddressing age-\\nrelated bias in sentiment analysis. In Proceedings\\nof the Conference on Human Factors in Computing\\nSystems (CHI), Montréal, Canada.\\nEmily Dinan, Angela Fan, Adina Williams, Jack Ur-\\nbanek, Douwe Kiela, and Jason Weston. 2019.\\nQueens are Powerful too:\\nMitigating Gender\\nBias in Dialogue Generation.\\narXiv preprint\\narXiv:1911.03842.\\nCarl DiSalvo, Andrew Clement, and Volkmar Pipek.\\n2013. Communities: Participatory Design for, with\\nand by communities. In Jesper Simonsen and Toni\\nRobertson, editors, Routledge International Hand-\\nbook of Participatory Design, pages 182–209. Rout-\\nledge.\\nLucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain,\\nand Lucy Vasserman. 2018. Measuring and mitigat-\\ning unintended bias in text classiﬁcation.\\nIn Pro-\\nceedings of the Conference on Artiﬁcial Intelligence,\\nEthics, and Society (AIES), New Orleans, LA.\\nJacob Eisenstein. 2013.\\nWhat to do about bad lan-\\nguage on the Internet. In Proceedings of the North\\nAmerican Association for Computational Linguistics\\n(NAACL), pages 359–369.\\nKawin Ethayarajh. 2020. Is Your Classiﬁer Actually\\nBiased? Measuring Fairness under Uncertainty with\\nBernstein Bounds. In Proceedings of the Associa-\\ntion for Computational Linguistics (ACL).\\nKawin Ethayarajh, David Duvenaud, and Graeme Hirst.\\n2019. Understanding Undesirable Word Embedding\\nAssocations. In Proceedings of the Association for\\nComputational Linguistics (ACL), pages 1696–1705,\\nFlorence, Italy.\\nJoseph Fisher. 2019.\\nMeasuring social bias in\\nknowledge graph embeddings.\\narXiv preprint\\narXiv:1912.02761.\\nNelson Flores and Soﬁa Chaparro. 2018. What counts\\nas language education policy? Developing a materi-\\nalist Anti-racist approach to language activism. Lan-\\nguage Policy, 17(3):365–384.\\nOmar U. Florez. 2019. On the Unintended Social Bias\\nof Training Language Generation Models with Data\\nfrom Local Media. In Proceedings of the NeurIPS\\nWorkshop on Human-Centric Machine Learning,\\nVancouver, Canada.\\nJoel Escudé Font and Marta R. Costa-jussà. 2019.\\nEqualizing gender biases in neural machine trans-\\nlation with word embeddings techniques.\\nIn Pro-\\nceedings of the Workshop on Gender Bias for Natu-\\nral Language Processing, pages 147–154, Florence,\\nItaly.\\nBatya Friedman and David G. Hendry. 2019.\\nValue\\nSensitive Design: Shaping Technology with Moral\\nImagination. MIT Press.\\nBatya Friedman, Peter H. Kahn Jr., and Alan Borning.\\n2006. Value Sensitive Design and Information Sys-\\ntems. In Dennis Galletta and Ping Zhang, editors,\\nHuman-Computer Interaction in Management Infor-\\nmation Systems: Foundations, pages 348–372. M.E.\\nSharpe.\\nNikhil Garg, Londa Schiebinger, Dan Jurafsky, and\\nJames Zou. 2018. Word Embeddings Quantify 100\\nYears of Gender and Ethnic Stereotypes. Proceed-\\nings of the National Academy of Sciences, 115(16).\\n\\nSahaj Garg, Vincent Perot, Nicole Limtiaco, Ankur\\nTaly, Ed H. Chi, and Alex Beutel. 2019. Counter-\\nfactual fairness in text classiﬁcation through robust-\\nness. In Proceedings of the Conference on Artiﬁcial\\nIntelligence, Ethics, and Society (AIES), Honolulu,\\nHI.\\nAparna Garimella, Carmen Banea, Dirk Hovy, and\\nRada Mihalcea. 2019. Women’s syntactic resilience\\nand men’s grammatical luck: Gender bias in part-of-\\nspeech tagging and dependency parsing data. In Pro-\\nceedings of the Association for Computational Lin-\\nguistics (ACL), pages 3493–3498, Florence, Italy.\\nAndrew Gaut, Tony Sun, Shirlyn Tang, Yuxin Huang,\\nJing Qian,\\nMai ElSherief,\\nJieyu Zhao,\\nDiba\\nMirza, Elizabeth Belding, Kai-Wei Chang, and\\nWilliam Yang Wang. 2020.\\nTowards Understand-\\ning Gender Bias in Relation Extraction. In Proceed-\\nings of the Association for Computational Linguis-\\ntics (ACL).\\nR. Stuart Geiger, Kevin Yu, Yanlai Yang, Mindy Dai,\\nJie Qiu, Rebekah Tang, and Jenny Huang. 2020.\\nGarbage In, Garbage Out?\\nDo Machine Learn-\\ning Application Papers in Social Computing Report\\nWhere Human-Labeled Training Data Comes From?\\nIn Proceedings of the Conference on Fairness, Ac-\\ncountability, and Transparency, pages 325–336.\\nOguzhan Gencoglu. 2020.\\nCyberbullying Detec-\\ntion with Fairness Constraints.\\narXiv preprint\\narXiv:2005.06625.\\nAlexandra Reeve Givens and Meredith Ringel Morris.\\n2020. Centering Disability Perspecives in Algorith-\\nmic Fairness, Accountability, and Transparency. In\\nProceedings of the Conference on Fairness, Account-\\nability, and Transparency, Barcelona, Spain.\\nHila Gonen and Yoav Goldberg. 2019. Lipstick on a\\nPig: Debiasing Methods Cover up Systematic Gen-\\nder Biases in Word Embeddings But do not Remove\\nThem. In Proceedings of the North American As-\\nsociation for Computational Linguistics (NAACL),\\npages 609–614, Minneapolis, MN.\\nHila Gonen and Kellie Webster. 2020.\\nAuto-\\nmatically Identifying Gender Issues in Machine\\nTranslation using Perturbations.\\narXiv preprint\\narXiv:2004.14065.\\nBen Green. 2019. “Good” isn’t good enough. In Pro-\\nceedings of the AI for Social Good Workshop, Van-\\ncouver, Canada.\\nLisa J. Green. 2002. African American English: A Lin-\\nguistic Introduction. Cambridge University Press.\\nAnthony G. Greenwald, Debbie E. McGhee, and Jor-\\ndan L.K. Schwartz. 1998. Measuring individual dif-\\nferences in implicit cognition: The implicit associa-\\ntion test. Journal of Personality and Social Psychol-\\nogy, 74(6):1464–1480.\\nEnoch Opanin Gyamﬁ, Yunbo Rao, Miao Gou, and\\nYanhua Shao. 2020. deb2viz: Debiasing gender in\\nword embedding data using subspace visualization.\\nIn Proceedings of the International Conference on\\nGraphics and Image Processing.\\nFoad Hamidi, Morgan Klaus Scheuerman, and Stacy M.\\nBranham. 2018. Gender Recognition or Gender Re-\\nductionism? The Social Implications of Automatic\\nGender Recognition Systems. In Proceedings of the\\nConference on Human Factors in Computing Sys-\\ntems (CHI), Montréal, Canada.\\nAlex Hanna, Emily Denton, Andrew Smart, and Jamila\\nSmith-Loud. 2020. Towards a Critical Race Method-\\nology in Algorithmic Fairness. In Proceedings of the\\nConference on Fairness, Accountability, and Trans-\\nparency, pages 501–512, Barcelona, Spain.\\nMadeline E. Heilman, Aaaron S. Wallen, Daniella\\nFuchs, and Melinda M. Tamkins. 2004. Penalties\\nfor Success: Reactions to Women Who Succeed at\\nMale Gender-Typed Tasks. Journal of Applied Psy-\\nchology, 89(3):416–427.\\nJane H. Hill. 2008. The Everyday Language of White\\nRacism. Wiley-Blackwell.\\nDirk Hovy, Federico Bianchi, and Tommaso Fornaciari.\\n2020. Can You Translate that into Man? Commer-\\ncial Machine Translation Systems Include Stylistic\\nBiases. In Proceedings of the Association for Com-\\nputational Linguistics (ACL).\\nDirk Hovy and Anders Søgaard. 2015. Tagging Per-\\nformance Correlates with Author Age. In Proceed-\\nings of the Association for Computational Linguis-\\ntics and the International Joint Conference on Nat-\\nural Language Processing, pages 483–488, Beijing,\\nChina.\\nDirk Hovy and Shannon L. Spruit. 2016. The social\\nimpact of natural language processing. In Proceed-\\nings of the Association for Computational Linguis-\\ntics (ACL), pages 591–598, Berlin, Germany.\\nPo-Sen Huang, Huan Zhang, Ray Jiang, Robert\\nStanforth, Johannes Welbl, Jack W. Rae, Vishal\\nMaini, Dani Yogatama, and Pushmeet Kohli. 2019.\\nReducing Sentiment Bias in Language Models\\nvia Counterfactual Evaluation.\\narXiv preprint\\narXiv:1911.03064.\\nXiaolei Huang, Linzi Xing, Franck Dernoncourt, and\\nMichael J. Paul. 2020.\\nMultilingual Twitter Cor-\\npus and Baselines for Evaluating Demographic Bias\\nin Hate Speech Recognition.\\nIn Proceedings of\\nthe Language Resources and Evaluation Conference\\n(LREC), Marseille, France.\\nChristoph Hube, Maximilian Idahl, and Besnik Fetahu.\\n2020. Debiasing Word Embeddings from Sentiment\\nAssociations in Names. In Proceedings of the Inter-\\nnational Conference on Web Search and Data Min-\\ning, pages 259–267, Houston, TX.\\n\\nBen Hutchinson, Vinodkumar Prabhakaran, Emily\\nDenton, Kellie Webster, Yu Zhong, and Stephen De-\\nnuyl. 2020. Social Biases in NLP Models as Barriers\\nfor Persons with Disabilities. In Proceedings of the\\nAssociation for Computational Linguistics (ACL).\\nMatei Ionita, Yury Kashnitsky, Ken Krige, Vladimir\\nLarin, Dennis Logvinenko, and Atanas Atanasov.\\n2019.\\nResolving gendered ambiguous pronouns\\nwith BERT.\\nIn Proceedings of the Workshop on\\nGender Bias in Natural Language Processing, pages\\n113–119, Florence, Italy.\\nHailey James-Sorenson and David Alvarez-Melis.\\n2019. Probabilistic Bias Mitigation in Word Embed-\\ndings. In Proceedings of the Workshop on Human-\\nCentric Machine Learning, Vancouver, Canada.\\nShengyu Jia, Tao Meng, Jieyu Zhao, and Kai-Wei\\nChang. 2020. Mitigating Gender Bias Ampliﬁcation\\nin Distribution by Posterior Regularization. In Pro-\\nceedings of the Association for Computational Lin-\\nguistics (ACL).\\nTaylor Jones, Jessica Rose Kalbfeld, Ryan Hancock,\\nand Robin Clark. 2019. Testifying while black: An\\nexperimental study of court reporter accuracy in tran-\\nscription of African American English. Language,\\n95(2).\\nAnna Jørgensen, Dirk Hovy, and Anders Søgaard. 2015.\\nChallenges of studying and processing dialects in\\nsocial media. In Proceedings of the Workshop on\\nNoisy User-Generated Text, pages 9–18, Beijing,\\nChina.\\nAnna Jørgensen, Dirk Hovy, and Anders Søgaard. 2016.\\nLearning a POS tagger for AAVE-like language. In\\nProceedings of the North American Association for\\nComputational Linguistics (NAACL), pages 1115–\\n1120, San Diego, CA.\\nPratik Joshi, Sebastian Santy, Amar Budhiraja, Kalika\\nBali, and Monojit Choudhury. 2020. The State and\\nFate of Linguistic Diversity and Inclusion in the\\nNLP World. In Proceedings of the Association for\\nComputational Linguistics (ACL).\\nJaap Jumelet, Willem Zuidema, and Dieuwke Hupkes.\\n2019. Analysing Neural Language Models: Contex-\\ntual Decomposition Reveals Default Reasoning in\\nNumber and Gender Assignment. In Proceedings\\nof the Conference on Natural Language Learning,\\nHong Kong, China.\\nMarie-Odile Junker. 2018.\\nParticipatory action re-\\nsearch for Indigenous linguistics in the digital age.\\nIn Shannon T. Bischoff and Carmen Jany, editors,\\nInsights from Practices in Community-Based Re-\\nsearch, pages 164–175. De Gruyter Mouton.\\nDavid Jurgens, Yulia Tsvetkov, and Dan Jurafsky. 2017.\\nIncorporating Dialectal Variability for Socially Equi-\\ntable Language Identiﬁcation. In Proceedings of the\\nAssociation for Computational Linguistics (ACL),\\npages 51–57, Vancouver, Canada.\\nMasahiro Kaneko and Danushka Bollegala. 2019.\\nGender-preserving debiasing for pre-trained word\\nembeddings. In Proceedings of the Association for\\nComputational Linguistics (ACL), pages 1641–1650,\\nFlorence, Italy.\\nSaket Karve, Lyle Ungar, and João Sedoc. 2019. Con-\\nceptor debiasing of word representations evaluated\\non WEAT. In Proceedings of the Workshop on Gen-\\nder Bias in Natural Language Processing, pages 40–\\n48, Florence, Italy.\\nMichael Katell, Meg Young, Dharma Dailey, Bernease\\nHerman, Vivian Guetler, Aaron Tam, Corinne Bintz,\\nDanielle Raz, and P.M. Krafft. 2020.\\nToward sit-\\nuated interventions for algorithmic equity: lessons\\nfrom the ﬁeld. In Proceedings of the Conference on\\nFairness, Accountability, and Transparency, pages\\n45–55, Barcelona, Spain.\\nStephen Kemmis. 2006. Participatory action research\\nand the public sphere. Educational Action Research,\\n14(4):459–476.\\nOs Keyes. 2018.\\nThe Misgendering Machines:\\nTrans/HCI\\nImplications\\nof\\nAutomatic\\nGender\\nRecognition. Proceedings of the ACM on Human-\\nComputer Interaction, 2(CSCW).\\nOs Keyes, Josephine Hoy, and Margaret Drouhard.\\n2019. Human-Computer Insurrection: Notes on an\\nAnarchist HCI. In Proceedings of the Conference on\\nHuman Factors in Computing Systems (CHI), Glas-\\ngow, Scotland, UK.\\nJae Yeon Kim, Carlos Ortiz, Sarah Nam, Sarah Santi-\\nago, and Vivek Datta. 2020. Intersectional Bias in\\nHate Speech and Abusive Language Datasets.\\nIn\\nProceedings of the Association for Computational\\nLinguistics (ACL).\\nSvetlana Kiritchenko and Saif M. Mohammad. 2018.\\nExamining Gender and Race Bias in Two Hundred\\nSentiment Analysis Systems. In Proceedings of the\\nJoint Conference on Lexical and Computational Se-\\nmantics, pages 43–53, New Orleans, LA.\\nMoshe Koppel, Shlomo Argamon, and Anat Rachel\\nShimoni. 2002.\\nAutomatically Categorizing Writ-\\nten Texts by Author Gender. Literary and Linguistic\\nComputing, 17(4):401–412.\\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W.\\nBlack, and Yulia Tsvetkov. 2019. Measuring bias\\nin contextualized word representations. In Proceed-\\nings of the Workshop on Gender Bias for Natu-\\nral Language Processing, pages 166–172, Florence,\\nItaly.\\nSonja L. Lanehart and Ayesha M. Malik. 2018. Black\\nIs, Black Isn’t: Perceptions of Language and Black-\\nness. In Jeffrey Reaser, Eric Wilbanks, Karissa Woj-\\ncik, and Walt Wolfram, editors, Language Variety in\\nthe New South. University of North Carolina Press.\\n\\nBrian N. Larson. 2017. Gender as a variable in natural-\\nlanguage processing: Ethical considerations. In Pro-\\nceedings of the Workshop on Ethics in Natural Lan-\\nguage Processing, pages 30–40, Valencia, Spain.\\nAnne Lauscher and Goran Glavaš. 2019. Are We Con-\\nsistently Biased? Multidimensional Analysis of Bi-\\nases in Distributional Word Vectors. In Proceedings\\nof the Joint Conference on Lexical and Computa-\\ntional Semantics, pages 85–91, Minneapolis, MN.\\nAnne Lauscher, Goran Glavaš, Simone Paolo Ponzetto,\\nand Ivan Vuli´c. 2019. A General Framework for Im-\\nplicit and Explicit Debiasing of Distributional Word\\nVector Spaces. arXiv preprint arXiv:1909.06092.\\nChristopher A. Le Dantec, Erika Shehan Poole, and Su-\\nsan P. Wyche. 2009. Values as Lived Experience:\\nEvolving Value Sensitive Design in Support of Value\\nDiscovery. In Proceedings of the Conference on Hu-\\nman Factors in Computing Systems (CHI), Boston,\\nMA.\\nNayeon Lee, Andrea Madotto, and Pascale Fung. 2019.\\nExploring Social Bias in Chatbots using Stereotype\\nKnowledge.\\nIn Proceedings of the Workshop on\\nWidening NLP, pages 177–180, Florence, Italy.\\nWesley Y. Leonard. 2012. Reframing language recla-\\nmation programmes for everybody’s empowerment.\\nGender and Language, 6(2):339–367.\\nPaul Pu Liang, Irene Li, Emily Zheng, Yao Chong Lim,\\nRuslan Salakhutdinov, and Louis-Philippe Morency.\\n2019. Towards Debiasing Sentence Representations.\\nIn Proceedings of the NeurIPS Workshop on Human-\\nCentric Machine Learning, Vancouver, Canada.\\nRosina Lippi-Green. 2012.\\nEnglish with an Ac-\\ncent: Language, Ideology, and Discrimination in the\\nUnited States. Routledge.\\nBo Liu. 2019. Anonymized BERT: An Augmentation\\nApproach to the Gendered Pronoun Resolution Chal-\\nlenge. In Proceedings of the Workshop on Gender\\nBias in Natural Language Processing, pages 120–\\n125, Florence, Italy.\\nHaochen Liu, Jamell Dacon, Wenqi Fan, Hui Liu, Zi-\\ntao Liu, and Jiliang Tang. 2019. Does Gender Mat-\\nter? Towards Fairness in Dialogue Systems. arXiv\\npreprint arXiv:1910.10486.\\nFelipe Alfaro Lois, José A.R. Fonollosa, and Costa-jà.\\n2019. BERT Masked Language Modeling for Co-\\nreference Resolution. In Proceedings of the Work-\\nshop on Gender Bias in Natural Language Process-\\ning, pages 76–81, Florence, Italy.\\nBrandon C. Loudermilk. 2015. Implicit attitudes and\\nthe perception of sociolinguistic variation. In Alexei\\nPrikhodkine and Dennis R. Preston, editors, Re-\\nsponses to Language Varieties:\\nVariability, pro-\\ncesses and outcomes, pages 137–156.\\nAnastassia Loukina, Nitin Madnani, and Klaus Zech-\\nner. 2019. The many dimensions of algorithmic fair-\\nness in educational applications. In Proceedings of\\nthe Workshop on Innovative Use of NLP for Build-\\ning Educational Applications, pages 1–10, Florence,\\nItaly.\\nKaiji Lu, Peter Mardziel, Fangjing Wu, Preetam Aman-\\ncharla, and Anupam Datta. 2018.\\nGender bias in\\nneural natural language processing. arXiv preprint\\narXiv:1807.11714.\\nAnne Maass. 1999. Linguistic intergroup bias: Stereo-\\ntype perpetuation through language.\\nAdvances in\\nExperimental Social Psychology, 31:79–121.\\nNitin Madnani, Anastassia Loukina, Alina von Davier,\\nJill Burstein, and Aoife Cahill. 2017. Building Bet-\\nter Open-Source Tools to Support Fairness in Auto-\\nmated Scoring. In Proceedings of the Workshop on\\nEthics in Natural Language Processing, pages 41–\\n52, Valencia, Spain.\\nThomas Manzini, Yao Chong Lim, Yulia Tsvetkov, and\\nAlan W. Black. 2019. Black is to Criminal as Cau-\\ncasian is to Police: Detecting and Removing Multi-\\nclass Bias in Word Embeddings. In Proceedings of\\nthe North American Association for Computational\\nLinguistics (NAACL), pages 801–809, Minneapolis,\\nMN.\\nRamón Antonio Martínez and Alexander Feliciano\\nMejía. 2019.\\nLooking closely and listening care-\\nfully: A sociocultural approach to understanding\\nthe complexity of Latina/o/x students’ everyday lan-\\nguage. Theory Into Practice.\\nRowan Hall Maudslay, Hila Gonen, Ryan Cotterell,\\nand Simone Teufel. 2019. It’s All in the Name: Mit-\\nigating Gender Bias with Name-Based Counterfac-\\ntual Data Substitution. In Proceedings of Empirical\\nMethods in Natural Language Processing (EMNLP),\\npages 5270–5278, Hong Kong, China.\\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\\nBowman, and Rachel Rudinger. 2019. On Measur-\\ning Social Biases in Sentence Encoders. In Proceed-\\nings of the North American Association for Compu-\\ntational Linguistics (NAACL), pages 629–634, Min-\\nneapolis, MN.\\nElijah Mayﬁeld,\\nMichael Madaio,\\nShrimai Prab-\\nhumoye, David Gerritsen, Brittany McLaughlin,\\nEzekiel Dixon-Roman, and Alan W. Black. 2019.\\nEquity Beyond Bias in Language Technologies for\\nEducation. In Proceedings of the Workshop on Inno-\\nvative Use of NLP for Building Educational Appli-\\ncations, Florence, Italy.\\nKatherine McCurdy and O˘guz Serbetçi. 2017. Gram-\\nmatical gender associations outweigh topical gender\\nbias in crosslinguistic word embeddings.\\nIn Pro-\\nceedings of the Workshop for Women & Underrepre-\\nsented Minorities in Natural Language Processing,\\nVancouver, Canada.\\n\\nNinareh Mehrabi, Thamme Gowda, Fred Morstatter,\\nNanyun Peng, and Aram Galstyan. 2019. Man is to\\nPerson as Woman is to Location: Measuring Gender\\nBias in Named Entity Recognition. arXiv preprint\\narXiv:1910.10872.\\nMichela Menegatti and Monica Rubini. 2017. Gender\\nbias and sexism in language. In Oxford Research\\nEncyclopedia of Communication. Oxford University\\nPress.\\nInom Mirzaev, Anthony Schulte, Michael Conover, and\\nSam Shah. 2019. Considerations for the interpreta-\\ntion of bias measures of word embeddings. arXiv\\npreprint arXiv:1906.08379.\\nSalikoko S. Mufwene, Guy Bailey, and John R. Rick-\\nford, editors. 1998.\\nAfrican-American English:\\nStructure, History, and Use. Routledge.\\nMichael J. Muller. 2007.\\nParticipatory Design: The\\nThird Space in HCI. In The Human-Computer Inter-\\naction Handbook, pages 1087–1108. CRC Press.\\nMoin\\nNadeem,\\nAnna\\nBethke,\\nand\\nSiva\\nReddy.\\n2020.\\nStereoSet:\\nMeasuring stereotypical bias\\nin pretrained language models.\\narXiv preprint\\narXiv:2004.09456.\\nDong Nguyen, Rilana Gravel, Dolf Trieschnigg, and\\nTheo Meder. 2013.\\n“How Old Do You Think I\\nAm?”: A Study of Language and Age in Twitter. In\\nProceedings of the Conference on Web and Social\\nMedia (ICWSM), pages 439–448, Boston, MA.\\nMalvina Nissim, Rik van Noord, and Rob van der Goot.\\n2020. Fair is better than sensational: Man is to doc-\\ntor as woman is to doctor. Computational Linguis-\\ntics.\\nDebora Nozza, Claudia Volpetti, and Elisabetta Fersini.\\n2019. Unintended Bias in Misogyny Detection. In\\nProceedings of the Conference on Web Intelligence,\\npages 149–155.\\nAlexandra Olteanu, Carlos Castillo, Fernando Diaz,\\nand Emre Kıcıman. 2019.\\nSocial Data: Biases,\\nMethodological Pitfalls, and Ethical Boundaries.\\nFrontiers in Big Data, 2.\\nAlexandra Olteanu, Kartik Talamadupula, and Kush R.\\nVarshney. 2017. The Limits of Abstract Evaluation\\nMetrics: The Case of Hate Speech Detection.\\nIn\\nProceedings of the ACM Web Science Conference,\\nTroy, NY.\\nOrestis Papakyriakopoulos, Simon Hegelich, Juan Car-\\nlos Medina Serrano, and Fabienne Marco. 2020.\\nBias in word embeddings.\\nIn Proceedings of the\\nConference on Fairness, Accountability, and Trans-\\nparency, pages 446–457, Barcelona, Spain.\\nJi Ho Park, Jamin Shin, and Pascale Fung. 2018. Re-\\nducing Gender Bias in Abusive Language Detection.\\nIn Proceedings of Empirical Methods in Natural\\nLanguage Processing (EMNLP), pages 2799–2804,\\nBrussels, Belgium.\\nEllie Pavlick and Tom Kwiatkowski. 2019. Inherent\\nDisagreements in Human Textual Inferences. Trans-\\nactions of the Association for Computational Lin-\\nguistics, 7:677–694.\\nXiangyu Peng, Siyan Li, Spencer Frazier, and Mark\\nRiedl. 2020. Fine-Tuning a Transformer-Based Lan-\\nguage Model to Avoid Generating Non-Normative\\nText. arXiv preprint arXiv:2001.08764.\\nRadomir Popovi´c, Florian Lemmerich, and Markus\\nStrohmaier. 2020.\\nJoint Multiclass Debiasing of\\nWord Embeddings. In Proceedings of the Interna-\\ntional Symposium on Intelligent Systems, Graz, Aus-\\ntria.\\nVinodkumar Prabhakaran, Ben Hutchinson, and Mar-\\ngaret Mitchell. 2019. Perturbation Sensitivity Anal-\\nysis to Detect Unintended Model Biases.\\nIn Pro-\\nceedings of Empirical Methods in Natural Language\\nProcessing (EMNLP), pages 5744–5749,\\nHong\\nKong, China.\\nShrimai Prabhumoye, Elijah Mayﬁeld, and Alan W.\\nBlack. 2019. Principled Frameworks for Evaluating\\nEthics in NLP Systems. In Proceedings of the Work-\\nshop on Innovative Use of NLP for Building Educa-\\ntional Applications, Florence, Italy.\\nMarcelo Prates, Pedro Avelar, and Luis C. Lamb. 2019.\\nAssessing gender bias in machine translation: A\\ncase study with google translate. Neural Computing\\nand Applications.\\nRasmus Précenth. 2019. Word embeddings and gender\\nstereotypes in Swedish and English. Master’s thesis,\\nUppsala University.\\nDennis R. Preston. 2009.\\nAre you really smart (or\\nstupid, or cute, or ugly, or cool)? Or do you just talk\\nthat way? Language attitudes, standardization and\\nlanguage change. Oslo: Novus forlag, pages 105–\\n129.\\nFlavien Prost, Nithum Thain, and Tolga Bolukbasi.\\n2019. Debiasing Embeddings for Reduced Gender\\nBias in Text Classiﬁcation. In Proceedings of the\\nWorkshop on Gender Bias in Natural Language Pro-\\ncessing, pages 69–75, Florence, Italy.\\nReid Pryzant, Richard Diehl Martinez, Nathan Dass,\\nSadao Kurohashi, Dan Jurafsky, and Diyi Yang.\\n2020. Automatically Neutralizing Subjective Bias\\nin Text. In Proceedings of the AAAI Conference on\\nArtiﬁcial Intelligence (AAAI), New York, NY.\\nArun K. Pujari, Ansh Mittal, Anshuman Padhi, An-\\nshul Jain, Mukesh Jadon, and Vikas Kumar. 2019.\\nDebiasing Gender biased Hindi Words with Word-\\nembedding.\\nIn Proceedings of the International\\nConference on Algorithms, Computing and Artiﬁcial\\nIntelligence, pages 450–456.\\nYusu Qian, Urwa Muaz, Ben Zhang, and Jae Won\\nHyun. 2019.\\nReducing gender bias in word-level\\n\\nlanguage models with a gender-equalizing loss func-\\ntion. In Proceedings of the ACL Student Research\\nWorkshop, pages 223–228, Florence, Italy.\\nShauli Ravfogel, Yanai Elazar, Hila Gonen, Michael\\nTwiton, and Yoav Goldberg. 2020.\\nNull It Out:\\nGuarding Protected Attributes by Iterative Nullspace\\nProjection.\\nIn Proceedings of the Association for\\nComputational Linguistics (ACL).\\nJohn R. Rickford and Sharese King. 2016. Language\\nand linguistics on trial: Hearing Rachel Jeantel (and\\nother vernacular speakers) in the courtroom and be-\\nyond. Language, 92(4):948–988.\\nAnthony Rios. 2020. FuzzE: Fuzzy Fairness Evalua-\\ntion of Offensive Language Classiﬁers on African-\\nAmerican English. In Proceedings of the AAAI Con-\\nference on Artiﬁcial Intelligence (AAAI), New York,\\nNY.\\nGerald Roche. 2019.\\nArticulating language oppres-\\nsion: colonialism, coloniality and the erasure of Ti-\\nbetâ ˘A´Zs minority languages. Patterns of Prejudice.\\nAlexey Romanov, Maria De-Arteaga, Hanna Wal-\\nlach, Jennifer Chayes, Christian Borgs, Alexandra\\nChouldechova, Sahin Geyik, Krishnaram Kentha-\\npadi, Anna Rumshisky, and Adam Tauman Kalai.\\n2019. What’s in a Name? Reducing Bias in Bios\\nwithout Access to Protected Attributes. In Proceed-\\nings of the North American Association for Com-\\nputational Linguistics (NAACL), pages 4187–4195,\\nMinneapolis, MN.\\nJonathan Rosa. 2019. Contesting Representations of\\nMigrant “Illegality” through the Drop the I-Word\\nCampaign: Rethinking Language Change and So-\\ncial Change.\\nIn Netta Avineri, Laura R. Graham,\\nEric J. Johnson, Robin Conley Riner, and Jonathan\\nRosa, editors, Language and Social Justice in Prac-\\ntice. Routledge.\\nJonathan Rosa and Christa Burdick. 2017. Language\\nIdeologies.\\nIn Ofelia García, Nelson Flores, and\\nMassimiliano Spotti, editors, The Oxford Handbook\\nof Language and Society. Oxford University Press.\\nJonathan Rosa and Nelson Flores. 2017.\\nUnsettling\\nrace and language: Toward a raciolinguistic perspec-\\ntive. Language in Society, 46:621–647.\\nSara Rosenthal and Kathleen McKeown. 2011.\\nAge\\nPrediction in Blogs: A Study of Style, Content, and\\nOnline Behavior in Pre- and Post-Social Media Gen-\\nerations. In Proceedings of the North American As-\\nsociation for Computational Linguistics (NAACL),\\npages 763–772, Portland, OR.\\nCandace\\nRoss,\\nBoris\\nKatz,\\nand\\nAndrei\\nBarbu.\\n2020.\\nMeasuring Social Biases in Grounded Vi-\\nsion and Language Embeddings.\\narXiv preprint\\narXiv:2002.08911.\\nRichard Rothstein. 2017. The Color of Law: A For-\\ngotten History of How Our Government Segregated\\nAmerica. Liveright Publishing.\\nDavid Rozado. 2020. Wide range screening of algo-\\nrithmic bias in word embedding models using large\\nsentiment lexicons reveals underreported bias types.\\nPLOS One.\\nElayne Ruane, Abeba Birhane, and Anthony Ven-\\ntresque. 2019. Conversational AI: Social and Ethi-\\ncal Considerations. In Proceedings of the Irish Con-\\nference on Artiﬁcial Intelligence and Cognitive Sci-\\nence, Galway, Ireland.\\nRachel Rudinger,\\nChandler May,\\nand Benjamin\\nVan Durme. 2017. Social bias in elicited natural lan-\\nguage inferences. In Proceedings of the Workshop\\non Ethics in Natural Language Processing, pages\\n74–79, Valencia, Spain.\\nRachel Rudinger, Jason Naradowsky, Brian Leonard,\\nand Benjamin Van Durme. 2018.\\nGender Bias\\nin Coreference Resolution.\\nIn Proceedings of the\\nNorth American Association for Computational Lin-\\nguistics (NAACL), pages 8–14, New Orleans, LA.\\nElizabeth B.N. Sanders. 2002. From user-centered to\\nparticipatory design approaches. In Jorge Frascara,\\neditor, Design and the Social Sciences: Making Con-\\nnections, pages 18–25. CRC Press.\\nBrenda Salenave Santana, Vinicius Woloszyn, and Le-\\nandro Krug Wives. 2018. Is there gender bias and\\nstereotype in Portuguese word embeddings?\\nIn\\nProceedings of the International Conference on the\\nComputational Processing of Portuguese Student Re-\\nsearch Workshop, Canela, Brazil.\\nMaarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,\\nand Noah A. Smith. 2019. The risk of racial bias in\\nhate speech detection. In Proceedings of the Asso-\\nciation for Computational Linguistics (ACL), pages\\n1668–1678, Florence, Italy.\\nMaarten Sap, Saadia Gabriel, Lianhui Qin, Dan Juraf-\\nsky, Noah A. Smith, and Yejin Choi. 2020. Social\\nBias Frames: Reasoning about Social and Power Im-\\nplications of Language. In Proceedings of the Asso-\\nciation for Computational Linguistics (ACL).\\nHanna Sassaman, Jennifer Lee, Jenessa Irvine, and\\nShankar Narayan. 2020.\\nCreating Community-\\nBased Tech Policy: Case Studies, Lessons Learned,\\nand What Technologists and Communities Can Do\\nTogether. In Proceedings of the Conference on Fair-\\nness, Accountability, and Transparency, Barcelona,\\nSpain.\\nDanielle Saunders and Bill Byrne. 2020.\\nReducing\\nGender Bias in Neural Machine Translation as a Do-\\nmain Adaptation Problem. In Proceedings of the As-\\nsociation for Computational Linguistics (ACL).\\nTyler Schnoebelen. 2017.\\nGoal-Oriented Design for\\nEthical Machine Learning and NLP. In Proceedings\\nof the Workshop on Ethics in Natural Language Pro-\\ncessing, pages 88–93, Valencia, Spain.\\n\\nSabine Sczesny, Magda Formanowicz, and Franziska\\nMoser. 2016. Can gender-fair language reduce gen-\\nder stereotyping and discrimination?\\nFrontiers in\\nPsychology, 7.\\nJoão Sedoc and Lyle Ungar. 2019. The Role of Pro-\\ntected Class Word Lists in Bias Identiﬁcation of Con-\\ntextualized Word Representations. In Proceedings\\nof the Workshop on Gender Bias in Natural Lan-\\nguage Processing, pages 55–61, Florence, Italy.\\nProcheta Sen and Debasis Ganguly. 2020. Towards So-\\ncially Responsible AI: Cognitive Bias-Aware Multi-\\nObjective Learning.\\nIn Proceedings of the AAAI\\nConference on Artiﬁcial Intelligence (AAAI), New\\nYork, NY.\\nDeven Shah, H. Andrew Schwartz, and Dirk Hovy.\\n2020. Predictive Biases in Natural Language Pro-\\ncessing Models:\\nA Conceptual Framework and\\nOverview.\\nIn Proceedings of the Association for\\nComputational Linguistics (ACL).\\nJudy Hanwen Shen, Lauren Fratamico, Iyad Rahwan,\\nand Alexander M. Rush. 2018.\\nDarling or Baby-\\ngirl? Investigating Stylistic Bias in Sentiment Anal-\\nysis. In Proceedings of the Workshop on Fairness,\\nAccountability, and Transparency (FAT/ML), Stock-\\nholm, Sweden.\\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\\nand Nanyun Peng. 2019. The Woman Worked as\\na Babysitter: On Biases in Language Generation.\\nIn Proceedings of Empirical Methods in Natural\\nLanguage Processing (EMNLP), pages 3398–3403,\\nHong Kong, China.\\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\\nand Nanyun Peng. 2020.\\nTowards Controllable\\nBiases in Language Generation.\\narXiv preprint\\narXiv:2005.00268.\\nSeungjae Shin, Kyungwoo Song, JoonHo Jang, Hyemi\\nKim, Weonyoung Joo, and Il-Chul Moon. 2020.\\nNeutralizing Gender Bias in Word Embedding with\\nLatent Disentanglement and Counterfactual Genera-\\ntion. arXiv preprint arXiv:2004.03133.\\nJesper Simonsen and Toni Robertson, editors. 2013.\\nRoutledge International Handbook of Participatory\\nDesign. Routledge.\\nGabriel Stanovsky, Noah A. Smith, and Luke Zettle-\\nmoyer. 2019.\\nEvaluating gender bias in machine\\ntranslation. In Proceedings of the Association for\\nComputational Linguistics (ACL), pages 1679–1684,\\nFlorence, Italy.\\nYolande Strengers, Lizhe Qu, Qiongkai Xu, and Jarrod\\nKnibbe. 2020.\\nAdhering, Steering, and Queering:\\nTreatment of Gender in Natural Language Genera-\\ntion. In Proceedings of the Conference on Human\\nFactors in Computing Systems (CHI), Honolulu, HI.\\nTony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang,\\nMai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth\\nBelding, Kai-Wei Chang, and William Yang Wang.\\n2019.\\nMitigating Gender Bias in Natural Lan-\\nguage Processing: Literature Review. In Proceed-\\nings of the Association for Computational Linguis-\\ntics (ACL), pages 1630–1640, Florence, Italy.\\nAdam Sutton, Thomas Lansdall-Welfare, and Nello\\nCristianini. 2018.\\nBiased embeddings from wild\\ndata:\\nMeasuring, understanding and removing.\\nIn Proceedings of the International Symposium\\non Intelligent Data Analysis, pages 328–339, ’s-\\nHertogenbosch, Netherlands.\\nChris Sweeney and Maryam Najaﬁan. 2019. A Trans-\\nparent Framework for Evaluating Unintended De-\\nmographic Bias in Word Embeddings. In Proceed-\\nings of the Association for Computational Linguis-\\ntics (ACL), pages 1662–1667, Florence, Italy.\\nChris Sweeney and Maryam Najaﬁan. 2020. Reduc-\\ning sentiment polarity for demographic attributes\\nin word embeddings using adversarial learning. In\\nProceedings of the Conference on Fairness, Ac-\\ncountability, and Transparency, pages 359–368,\\nBarcelona, Spain.\\nNathaniel Swinger, Maria De-Arteaga, Neil Thomas\\nHeffernan, Mark D.M. Leiserson, and Adam Tau-\\nman Kalai. 2019. What are the biases in my word\\nembedding?\\nIn Proceedings of the Conference on\\nArtiﬁcial Intelligence, Ethics, and Society (AIES),\\nHonolulu, HI.\\nSamson Tan, Shaﬁq Joty, Min-Yen Kan, and Richard\\nSocher. 2020.\\nIt’s Morphin’ Time!\\nCombating\\nLinguistic Discrimination with Inﬂectional Perturba-\\ntions. In Proceedings of the Association for Compu-\\ntational Linguistics (ACL).\\nYi Chern Tan and L. Elisa Celis. 2019.\\nAssessing\\nSocial and Intersectional Biases in Contextualized\\nWord Representations. In Proceedings of the Con-\\nference on Neural Information Processing Systems,\\nVancouver, Canada.\\nJ. Michael Terry, Randall Hendrick, Evangelos Evan-\\ngelou, and Richard L. Smith. 2010.\\nVariable\\ndialect switching among African American chil-\\ndren: Inferences about working memory.\\nLingua,\\n120(10):2463–2475.\\nJoel Tetreault, Daniel Blanchard, and Aoife Cahill.\\n2013. A Report on the First Native Language Iden-\\ntiﬁcation Shared Task. In Proceedings of the Work-\\nshop on Innovative Use of NLP for Building Educa-\\ntional Applications, pages 48–57, Atlanta, GA.\\nMike Thelwall. 2018. Gender Bias in Sentiment Anal-\\nysis. Online Information Review, 42(1):45–57.\\nKristen Vaccaro, Karrie Karahalios, Deirdre K. Mul-\\nligan, Daniel Kluttz, and Tad Hirsch. 2019.\\nCon-\\ntestability in Algorithmic Systems. In Conference\\nCompanion Publication of the 2019 on Computer\\n\\nSupported Cooperative Work and Social Computing,\\npages 523–527, Austin, TX.\\nAmeya Vaidya, Feng Mai, and Yue Ning. 2019. Em-\\npirical Analysis of Multi-Task Learning for Reduc-\\ning Model Bias in Toxic Comment Detection. arXiv\\npreprint arXiv:1909.09758v2.\\nEva Vanmassenhove, Christian Hardmeier, and Andy\\nWay. 2018.\\nGetting Gender Right in Neural Ma-\\nchine Translation.\\nIn Proceedings of Empirical\\nMethods in Natural Language Processing (EMNLP),\\npages 3003–3008, Brussels, Belgium.\\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\\nSharon Qian, Daniel Nevo, Yaron Singer, and Stu-\\nart Shieber. 2020.\\nCausal Mediation Analysis for\\nInterpreting Neural NLP: The Case of Gender Bias.\\narXiv preprint arXiv:2004.12265.\\nTianlu Wang, Xi Victoria Lin, Nazneen Fatema Ra-\\njani, Bryan McCann, Vicente Ordonez, and Caim-\\ning Xiong. 2020.\\nDouble-Hard Debias: Tailoring\\nWord Embeddings for Gender Bias Mitigation. In\\nProceedings of the Association for Computational\\nLinguistics (ACL).\\nZili Wang. 2019. MSnet: A BERT-based Network for\\nGendered Pronoun Resolution.\\nIn Proceedings of\\nthe Workshop on Gender Bias in Natural Language\\nProcessing, pages 89–95, Florence, Italy.\\nKellie Webster, Marta R. Costa-jussà, Christian Hard-\\nmeier, and Will Radford. 2019. Gendered Ambigu-\\nous Pronoun (GAP) Shared Task at the Gender Bias\\nin NLP Workshop 2019. In Proceedings of the Work-\\nshop on Gender Bias in Natural Language Process-\\ning, pages 1–7, Florence, Italy.\\nKellie Webster, Marta Recasens, Vera Axelrod, and Ja-\\nson Baldridge. 2018. Mind the GAP: A balanced\\ncorpus of gendered ambiguous pronouns. Transac-\\ntions of the Association for Computational Linguis-\\ntics, 6:605–618.\\nWalt Wolfram and Natalie Schilling. 2015. American\\nEnglish: Dialects and Variation, 3 edition. Wiley\\nBlackwell.\\nAustin P. Wright, Omar Shaikh, Haekyu Park, Will Ep-\\nperson, Muhammed Ahmed, Stephane Pinel, Diyi\\nYang, and Duen Horng (Polo) Chau. 2020.\\nRE-\\nCAST: Interactive Auditing of Automatic Toxicity\\nDetection Models.\\nIn Proceedings of the Con-\\nference on Human Factors in Computing Systems\\n(CHI), Honolulu, HI.\\nYinchuan Xu and Junlin Yang. 2019. Look again at\\nthe syntax: Relational graph convolutional network\\nfor gendered ambiguous pronoun resolution. In Pro-\\nceedings of the Workshop on Gender Bias in Natu-\\nral Language Processing, pages 96–101, Florence,\\nItaly.\\nKai-Chou Yang, Timothy Niven, Tzu-Hsuan Chou, and\\nHung-Yu Kao. 2019.\\nFill the GAP: Exploiting\\nBERT for Pronoun Resolution. In Proceedings of\\nthe Workshop on Gender Bias in Natural Language\\nProcessing, pages 102–106, Florence, Italy.\\nZekun Yang and Juan Feng. 2020. A Causal Inference\\nMethod for Reducing Gender Bias in Word Embed-\\nding Relations.\\nIn Proceedings of the AAAI Con-\\nference on Artiﬁcial Intelligence (AAAI), New York,\\nNY.\\nDaisy Yoo, Anya Ernest, Soﬁa Serholt, Eva Eriksson,\\nand Peter Dalsgaard. 2019. Service Design in HCI\\nResearch: The Extended Value Co-creation Model.\\nIn Proceedings of the Halfway to the Future Sympo-\\nsium, Nottingham, United Kingdom.\\nBrian Hu Zhang,\\nBlake Lemoine,\\nand Margaret\\nMitchell. 2018. Mitigating unwanted biases with ad-\\nversarial learning. In Proceedings of the Conference\\non Artiﬁcial Intelligence, Ethics, and Society (AIES),\\nNew Orleans, LA.\\nGuanhua Zhang, Bing Bai, Junqi Zhang, Kun Bai, Con-\\nghui Zhu, and Tiejun Zhao. 2020a. Demographics\\nShould Not Be the Reason of Toxicity: Mitigating\\nDiscrimination in Text Classiﬁcations with Instance\\nWeighting.\\nIn Proceedings of the Association for\\nComputational Linguistics (ACL).\\nHaoran Zhang,\\nAmy X. Lu,\\nMohamed Abdalla,\\nMatthew\\nMcDermott,\\nand\\nMarzyeh\\nGhassemi.\\n2020b. Hurtful Words: Quantifying Biases in Clin-\\nical Contextual Word Embeddings. In Proceedings\\nof the ACM Conference on Health, Inference, and\\nLearning.\\nJieyu Zhao, Subhabrata Mukherjee, Saghar Hosseini,\\nKai-Wei Chang, and Ahmed Hassan Awadallah.\\n2020. Gender Bias in Multilingual Embeddings and\\nCross-Lingual Transfer. In Proceedings of the Asso-\\nciation for Computational Linguistics (ACL).\\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cot-\\nterell, Vicente Ordonez, and Kai-Wei Chang. 2019.\\nGender Bias in Contextualized Word Embeddings.\\nIn Proceedings of the North American Association\\nfor Computational Linguistics (NAACL), pages 629–\\n634, Minneapolis, MN.\\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\\ndonez, and Kai-Wei Chang. 2017.\\nMen also like\\nshopping: Reducing gender bias ampliﬁcation us-\\ning corpus-level constraints.\\nIn Proceedings of\\nEmpirical Methods in Natural Language Process-\\ning (EMNLP), pages 2979–2989, Copenhagen, Den-\\nmark.\\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\\ndonez, and Kai-Wei Chang. 2018a.\\nGender Bias\\nin Coreference Resolution: Evaluation and Debias-\\ning Methods. In Proceedings of the North American\\nAssociation for Computational Linguistics (NAACL),\\npages 15–20, New Orleans, LA.\\n\\nJieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai-\\nWei Chang. 2018b. Learning Gender-Neutral Word\\nEmbeddings. In Proceedings of Empirical Methods\\nin Natural Language Processing (EMNLP), pages\\n4847–4853, Brussels, Belgium.\\nAlina Zhiltsova, Simon Caton, and Catherine Mulwa.\\n2019. Mitigation of Unintended Biases against Non-\\nNative English Texts in Sentiment Analysis. In Pro-\\nceedings of the Irish Conference on Artiﬁcial Intelli-\\ngence and Cognitive Science, Galway, Ireland.\\nPei Zhou, Weijia Shi, Jieyu Zhao, Kuan-Hao Huang,\\nMuhao Chen, and Kai-Wei Chang. 2019. Examin-\\ning gender bias in languages with grammatical gen-\\nders. In Proceedings of Empirical Methods in Nat-\\nural Language Processing (EMNLP), pages 5279–\\n5287, Hong Kong, China.\\nRan Zmigrod, S. J. Mielke, Hanna Wallach, and Ryan\\nCotterell. 2019. Counterfactual data augmentation\\nfor mitigating gender stereotypes in languages with\\nrich morphology. In Proceedings of the Association\\nfor Computational Linguistics (ACL), pages 1651–\\n1661, Florence, Italy.\\nA\\nAppendix\\nIn Table 3, we provide examples of the papers’ mo-\\ntivations and techniques across several NLP tasks.\\nA.1\\nCategorization details\\nIn this section, we provide some additional details\\nabout our method—speciﬁcally, our categorization.\\nWhat counts as being covered by an NLP task?\\nWe considered a paper to cover a given NLP task if\\nit analyzed “bias” with respect to that task, but not\\nif it only evaluated overall performance on that task.\\nFor example, a paper examining the impact of miti-\\ngating “bias” in word embeddings on “bias” in sen-\\ntiment analysis would be counted as covering both\\nNLP tasks. In contrast, a paper assessing whether\\nperformance on sentiment analysis degraded after\\nmitigating “bias” in word embeddings would be\\ncounted only as focusing on embeddings.\\nWhat counts as a motivation?\\nWe considered a\\nmotivation to include any description of the prob-\\nlem that motivated the paper or proposed quantita-\\ntive technique, including any normative reasoning.\\nWe excluded from the “Vague/unstated” cate-\\ngory of motivations the papers that participated in\\nthe Gendered Ambiguous Pronoun (GAP) Shared\\nTask at the First ACL Workshop on Gender Bias in\\nNLP. In an ideal world, shared task papers would\\nengage with “bias” more critically, but given the\\nnature of shared tasks it is understandable that they\\ndo not. As a result, we excluded them from our\\ncounts for techniques as well. We cite the papers\\nhere; most propose techniques we would have cate-\\ngorized as “Questionable correlations,” with a few\\nas “Other representational harms” (Abzaliev, 2019;\\nAttree, 2019; Bao and Qiao, 2019; Chada, 2019;\\nIonita et al., 2019; Liu, 2019; Lois et al., 2019;\\nWang, 2019; Xu and Yang, 2019; Yang et al., 2019).\\nWe excluded Dabas et al. (2020) from our survey\\nbecause we could not determine what this paper’s\\nuser study on fairness was actually measuring.\\nFinally, we actually categorized the motivation\\nfor Liu et al. (2019) (i.e., the last row in Table 3) as\\n“Questionable correlations” due to a sentence else-\\nwhere in the paper; had the paragraph we quoted\\nbeen presented without more detail, we would have\\ncategorized the motivation as “Vague/unstated.”\\nA.2\\nFull categorization: Motivations\\nAllocational harms\\nHovy and Spruit (2016);\\nCaliskan et al. (2017); Madnani et al. (2017);\\nDixon et al. (2018); Kiritchenko and Mohammad\\n(2018); Shen et al. (2018); Zhao et al. (2018b);\\nBhaskaran and Bhallamudi (2019); Bordia and\\nBowman (2019); Brunet et al. (2019); Chaloner\\nand Maldonado (2019); De-Arteaga et al. (2019);\\nDev and Phillips (2019); Font and Costa-jussà\\n(2019); James-Sorenson and Alvarez-Melis (2019);\\nKurita et al. (2019); Mayﬁeld et al. (2019); Pu-\\njari et al. (2019); Romanov et al. (2019); Ruane\\net al. (2019); Sedoc and Ungar (2019); Sun et al.\\n(2019); Zmigrod et al. (2019); Hutchinson et al.\\n(2020); Papakyriakopoulos et al. (2020); Ravfo-\\ngel et al. (2020); Strengers et al. (2020); Sweeney\\nand Najaﬁan (2020); Tan et al. (2020); Zhang et al.\\n(2020b).\\nStereotyping\\nBolukbasi\\net\\nal.\\n(2016a,b);\\nCaliskan et al. (2017); McCurdy and Serbetçi\\n(2017); Rudinger et al. (2017); Zhao et al. (2017);\\nCurry and Rieser (2018); Díaz et al. (2018);\\nSantana et al. (2018); Sutton et al. (2018); Zhao\\net al. (2018a,b); Agarwal et al. (2019); Basta et al.\\n(2019); Bhaskaran and Bhallamudi (2019); Bordia\\nand Bowman (2019); Brunet et al. (2019); Cao\\nand Daumé (2019); Chaloner and Maldonado\\n(2019); Cho et al. (2019); Dev and Phillips (2019);\\nFont and Costa-jussà (2019); Gonen and Goldberg\\n(2019); James-Sorenson and Alvarez-Melis (2019);\\nKaneko and Bollegala (2019); Karve et al. (2019);\\nKurita et al. (2019); Lauscher and Glavaš (2019);\\nLee et al. (2019); Manzini et al. (2019); Mayﬁeld\\n\\nCategories\\nNLP task\\nStated motivation\\nMotivations\\nTechniques\\nLanguage\\nmodeling\\n(Bordia and\\nBowman,\\n2019)\\n“Existing biases in data can be ampliﬁed by models and the\\nresulting output consumed by the public can inﬂuence them, en-\\ncourage and reinforce harmful stereotypes, or distort the truth.\\nAutomated systems that depend on these models can take prob-\\nlematic actions based on biased proﬁling of individuals.”\\nAllocational\\nharms,\\nstereotyping\\nQuestionable\\ncorrelations\\nSentiment\\nanalysis\\n(Kiritchenko\\nand\\nMohammad,\\n2018)\\n“Other biases can be inappropriate and result in negative ex-\\nperiences for some groups of people. Examples include, loan\\neligibility and crime recidivism prediction systems...and resumé\\nsorting systems that believe that men are more qualiﬁed to be\\nprogrammers than women (Bolukbasi et al., 2016). Similarly,\\nsentiment and emotion analysis systems can also perpetuate and\\naccentuate inappropriate human biases, e.g., systems that consider\\nutterances from one race or gender to be less positive simply be-\\ncause of their race or gender, or customer support systems that\\nprioritize a call from an angry male over a call from the equally\\nangry female.”\\nAllocational\\nharms, other\\nrepresentational\\nharms (system\\nperformance\\ndifferences w.r.t.\\ntext written by\\ndifferent social\\ngroups)\\nQuestionable\\ncorrelations\\n(differences in\\nsentiment\\nintensity scores\\nw.r.t. text about\\ndifferent social\\ngroups)\\nMachine\\ntranslation\\n(Cho et al.,\\n2019)\\n“[MT training] may incur an association of gender-speciﬁed pro-\\nnouns (in the target) and gender-neutral ones (in the source) for\\nlexicon pairs that frequently collocate in the corpora. We claim\\nthat this kind of phenomenon seriously threatens the fairness of a\\ntranslation system, in the sense that it lacks generality and inserts\\nsocial bias to the inference. Moreover, the input is not fully cor-\\nrect (considering gender-neutrality) and might offend the users\\nwho expect fairer representations.”\\nQuestionable\\ncorrelations,\\nother\\nrepresentational\\nharms\\nQuestionable\\ncorrelations\\nMachine\\ntranslation\\n(Stanovsky\\net al., 2019)\\n“Learned models exhibit social bias when their training data\\nencode stereotypes not relevant for the task, but the correlations\\nare picked up anyway.”\\nStereotyping,\\nquestionable\\ncorrelations\\nStereotyping,\\nother\\nrepresentational\\nharms (system\\nperformance\\ndifferences),\\nquestionable\\ncorrelations\\nType-level\\nembeddings\\n(Zhao et al.,\\n2018b)\\n“However, embeddings trained on human-generated corpora have\\nbeen demonstrated to inherit strong gender stereotypes that re-\\nﬂect social constructs....Such a bias substantially affects down-\\nstream applications....This concerns the practitioners who use\\nthe embedding model to build gender-sensitive applications such\\nas a resume ﬁltering system or a job recommendation system as\\nthe automated system may discriminate candidates based on their\\ngender, as reﬂected by their name. Besides, biased embeddings\\nmay implicitly affect downstream applications used in our daily\\nlives. For example, when searching for ‘computer scientist’ using\\na search engine...a search algorithm using an embedding model in\\nthe backbone tends to rank male scientists higher than females’\\n[sic], hindering women from being recognized and further exac-\\nerbating the gender inequality in the community.”\\nAllocational\\nharms,\\nstereotyping,\\nother\\nrepresentational\\nharms\\nStereotyping\\nType-level\\nand contextu-\\nalized\\nembeddings\\n(May et al.,\\n2019)\\n“[P]rominent word embeddings such as word2vec (Mikolov et\\nal., 2013) and GloVe (Pennington et al., 2014) encode systematic\\nbiases against women and black people (Bolukbasi et al., 2016;\\nGarg et al., 2018), implicating many NLP systems in scaling up\\nsocial injustice.”\\nVague\\nStereotyping\\nDialogue\\ngeneration\\n(Liu et al.,\\n2019)\\n“Since the goal of dialogue systems is to talk with users...if the\\nsystems show discriminatory behaviors in the interactions, the\\nuser experience will be adversely affected. Moreover, public com-\\nmercial chatbots can get resisted for their improper speech.”\\nVague/unstated\\nStereotyping,\\nother\\nrepresentational\\nharms,\\nquestionable\\ncorrelations\\nTable 3: Examples of the categories into which the papers’ motivations and proposed quantitative techniques for\\nmeasuring or mitigating “bias” fall. Bold text in the quotes denotes the content that yields our categorizations.\\n\\net al. (2019); Précenth (2019); Pujari et al. (2019);\\nRuane et al. (2019); Stanovsky et al. (2019);\\nSun et al. (2019); Tan and Celis (2019); Webster\\net al. (2019); Zmigrod et al. (2019); Gyamﬁet al.\\n(2020); Hube et al. (2020); Hutchinson et al.\\n(2020); Kim et al. (2020); Nadeem et al. (2020);\\nPapakyriakopoulos et al. (2020); Ravfogel et al.\\n(2020); Rozado (2020); Sen and Ganguly (2020);\\nShin et al. (2020); Strengers et al. (2020).\\nOther representational harms\\nHovy and Sø-\\ngaard (2015); Blodgett et al. (2016); Bolukbasi\\net al. (2016b); Hovy and Spruit (2016); Blodgett\\nand O’Connor (2017); Larson (2017); Schnoebelen\\n(2017); Blodgett et al. (2018); Curry and Rieser\\n(2018); Díaz et al. (2018); Dixon et al. (2018); Kir-\\nitchenko and Mohammad (2018); Park et al. (2018);\\nShen et al. (2018); Thelwall (2018); Zhao et al.\\n(2018b); Badjatiya et al. (2019); Bagdasaryan et al.\\n(2019); Bamman et al. (2019); Cao and Daumé\\n(2019); Chaloner and Maldonado (2019); Cho et al.\\n(2019); Davidson et al. (2019); De-Arteaga et al.\\n(2019); Fisher (2019); Font and Costa-jussà (2019);\\nGarimella et al. (2019); Loukina et al. (2019); May-\\nﬁeld et al. (2019); Mehrabi et al. (2019); Nozza\\net al. (2019); Prabhakaran et al. (2019); Romanov\\net al. (2019); Ruane et al. (2019); Sap et al. (2019);\\nSheng et al. (2019); Sun et al. (2019); Sweeney\\nand Najaﬁan (2019); Vaidya et al. (2019); Gaut\\net al. (2020); Gencoglu (2020); Hovy et al. (2020);\\nHutchinson et al. (2020); Kim et al. (2020); Peng\\net al. (2020); Rios (2020); Sap et al. (2020); Shah\\net al. (2020); Sheng et al. (2020); Tan et al. (2020);\\nZhang et al. (2020a,b).\\nQuestionable\\ncorrelations\\nJørgensen\\net\\nal.\\n(2015); Hovy and Spruit (2016); Madnani et al.\\n(2017); Rudinger et al. (2017); Zhao et al. (2017);\\nBurns et al. (2018); Dixon et al. (2018); Kir-\\nitchenko and Mohammad (2018); Lu et al. (2018);\\nPark et al. (2018); Shen et al. (2018); Zhang\\net al. (2018); Badjatiya et al. (2019); Bhargava\\nand Forsyth (2019); Cao and Daumé (2019); Cho\\net al. (2019); Davidson et al. (2019); Dev et al.\\n(2019); Garimella et al. (2019); Garg et al. (2019);\\nHuang et al. (2019); James-Sorenson and Alvarez-\\nMelis (2019); Kaneko and Bollegala (2019); Liu\\net al. (2019); Karve et al. (2019); Nozza et al.\\n(2019); Prabhakaran et al. (2019); Romanov et al.\\n(2019); Sap et al. (2019); Sedoc and Ungar (2019);\\nStanovsky et al. (2019); Sweeney and Najaﬁan\\n(2019); Vaidya et al. (2019); Zhiltsova et al. (2019);\\nChopra et al. (2020); Gonen and Webster (2020);\\nGyamﬁet al. (2020); Hube et al. (2020); Ravfogel\\net al. (2020); Rios (2020); Ross et al. (2020); Saun-\\nders and Byrne (2020); Sen and Ganguly (2020);\\nShah et al. (2020); Sweeney and Najaﬁan (2020);\\nYang and Feng (2020); Zhang et al. (2020a).\\nVague/unstated\\nRudinger et al. (2018); Webster\\net al. (2018); Dinan et al. (2019); Florez (2019);\\nJumelet et al. (2019); Lauscher et al. (2019); Liang\\net al. (2019); Maudslay et al. (2019); May et al.\\n(2019); Prates et al. (2019); Prost et al. (2019);\\nQian et al. (2019); Swinger et al. (2019); Zhao\\net al. (2019); Zhou et al. (2019); Ethayarajh (2020);\\nHuang et al. (2020); Jia et al. (2020); Popovi´c et al.\\n(2020); Pryzant et al. (2020); Vig et al. (2020);\\nWang et al. (2020); Zhao et al. (2020).\\nSurveys,\\nframeworks,\\nand\\nmeta-analyses\\nHovy and Spruit (2016); Larson (2017); McCurdy\\nand Serbetçi (2017); Schnoebelen (2017); Basta\\net al. (2019); Ethayarajh et al. (2019); Gonen and\\nGoldberg (2019); Lauscher and Glavaš (2019);\\nLoukina et al. (2019); Mayﬁeld et al. (2019);\\nMirzaev et al. (2019); Prabhumoye et al. (2019);\\nRuane et al. (2019); Sedoc and Ungar (2019); Sun\\net al. (2019); Nissim et al. (2020); Rozado (2020);\\nShah et al. (2020); Strengers et al. (2020); Wright\\net al. (2020).\\nB\\nFull categorization: Techniques\\nAllocational harms\\nDe-Arteaga et al. (2019);\\nProst et al. (2019); Romanov et al. (2019); Zhao\\net al. (2020).\\nStereotyping\\nBolukbasi\\net\\nal.\\n(2016a,b);\\nCaliskan et al. (2017); McCurdy and Serbetçi\\n(2017); Díaz et al. (2018); Santana et al. (2018);\\nSutton et al. (2018); Zhang et al. (2018); Zhao\\net al. (2018a,b); Agarwal et al. (2019); Basta et al.\\n(2019); Bhaskaran and Bhallamudi (2019); Brunet\\net al. (2019); Cao and Daumé (2019); Chaloner\\nand Maldonado (2019); Dev and Phillips (2019);\\nEthayarajh et al. (2019); Gonen and Goldberg\\n(2019); James-Sorenson and Alvarez-Melis (2019);\\nJumelet et al. (2019); Kaneko and Bollegala\\n(2019); Karve et al. (2019); Kurita et al. (2019);\\nLauscher and Glavaš (2019); Lauscher et al.\\n(2019); Lee et al. (2019); Liang et al. (2019); Liu\\net al. (2019); Manzini et al. (2019); Maudslay et al.\\n(2019); May et al. (2019); Mirzaev et al. (2019);\\nPrates et al. (2019); Précenth (2019); Prost et al.\\n(2019); Pujari et al. (2019); Qian et al. (2019);\\n\\nSedoc and Ungar (2019); Stanovsky et al. (2019);\\nTan and Celis (2019); Zhao et al. (2019); Zhou\\net al. (2019); Chopra et al. (2020); Gyamﬁet al.\\n(2020); Nadeem et al. (2020); Nissim et al. (2020);\\nPapakyriakopoulos et al. (2020); Popovi´c et al.\\n(2020); Ravfogel et al. (2020); Ross et al. (2020);\\nRozado (2020); Saunders and Byrne (2020); Shin\\net al. (2020); Vig et al. (2020); Wang et al. (2020);\\nYang and Feng (2020); Zhao et al. (2020).\\nOther representational harms\\nJørgensen et al.\\n(2015); Hovy and Søgaard (2015); Blodgett et al.\\n(2016); Blodgett and O’Connor (2017); Blodgett\\net al. (2018); Curry and Rieser (2018); Dixon et al.\\n(2018); Park et al. (2018); Thelwall (2018); Web-\\nster et al. (2018); Badjatiya et al. (2019); Bag-\\ndasaryan et al. (2019); Bamman et al. (2019); Bhar-\\ngava and Forsyth (2019); Cao and Daumé (2019);\\nFont and Costa-jussà (2019); Garg et al. (2019);\\nGarimella et al. (2019); Liu et al. (2019); Louk-\\nina et al. (2019); Mehrabi et al. (2019); Nozza\\net al. (2019); Sap et al. (2019); Sheng et al. (2019);\\nStanovsky et al. (2019); Vaidya et al. (2019);\\nWebster et al. (2019); Ethayarajh (2020); Gaut\\net al. (2020); Gencoglu (2020); Hovy et al. (2020);\\nHuang et al. (2020); Kim et al. (2020); Peng et al.\\n(2020); Ravfogel et al. (2020); Rios (2020); Sap\\net al. (2020); Saunders and Byrne (2020); Sheng\\net al. (2020); Sweeney and Najaﬁan (2020); Tan\\net al. (2020); Zhang et al. (2020a,b).\\nQuestionable correlations\\nJurgens et al. (2017);\\nMadnani et al. (2017); Rudinger et al. (2017);\\nZhao et al. (2017); Burns et al. (2018); Díaz\\net al. (2018); Kiritchenko and Mohammad (2018);\\nLu et al. (2018); Rudinger et al. (2018); Shen\\net al. (2018); Bordia and Bowman (2019); Cao\\nand Daumé (2019); Cho et al. (2019); David-\\nson et al. (2019); Dev et al. (2019); Dinan et al.\\n(2019); Fisher (2019); Florez (2019); Font and\\nCosta-jussà (2019); Garg et al. (2019); Huang et al.\\n(2019); Liu et al. (2019); Nozza et al. (2019);\\nPrabhakaran et al. (2019); Qian et al. (2019); Sap\\net al. (2019); Stanovsky et al. (2019); Sweeney and\\nNajaﬁan (2019); Swinger et al. (2019); Zhiltsova\\net al. (2019); Zmigrod et al. (2019); Hube et al.\\n(2020); Hutchinson et al. (2020); Jia et al. (2020);\\nPapakyriakopoulos et al. (2020); Popovi´c et al.\\n(2020); Pryzant et al. (2020); Saunders and Byrne\\n(2020); Sen and Ganguly (2020); Shah et al. (2020);\\nSweeney and Najaﬁan (2020); Zhang et al. (2020b).\\nVague/unstated\\nNone.\\nSurveys,\\nframeworks,\\nand\\nmeta-analyses\\nHovy and Spruit (2016); Larson (2017); McCurdy\\nand Serbetçi (2017); Schnoebelen (2017); Basta\\net al. (2019); Ethayarajh et al. (2019); Gonen and\\nGoldberg (2019); Lauscher and Glavaš (2019);\\nLoukina et al. (2019); Mayﬁeld et al. (2019);\\nMirzaev et al. (2019); Prabhumoye et al. (2019);\\nRuane et al. (2019); Sedoc and Ungar (2019); Sun\\net al. (2019); Nissim et al. (2020); Rozado (2020);\\nShah et al. (2020); Strengers et al. (2020); Wright\\net al. (2020).\\n'}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/A_Critical_Survey_of_Bias_in_NLP.pdf', 'text': 'Language (Technology) is Power: A Critical Survey of “Bias” in NLP\\nSu Lin Blodgett\\nCollege of Information and Computer Sciences\\nUniversity of Massachusetts Amherst\\nblodgett@cs.umass.edu\\nSolon Barocas\\nMicrosoft Research\\nCornell University\\nsolon@microsoft.com\\nHal Daumé III\\nMicrosoft Research\\nUniversity of Maryland\\nme@hal3.name\\nHanna Wallach\\nMicrosoft Research\\nwallach@microsoft.com\\nAbstract\\nWe survey 146 papers analyzing “bias” in\\nNLP systems, ﬁnding that their motivations\\nare often vague, inconsistent, and lacking\\nin normative reasoning, despite the fact that\\nanalyzing “bias” is an inherently normative\\nprocess.\\nWe further ﬁnd that these papers’\\nproposed quantitative techniques for measur-\\ning or mitigating “bias” are poorly matched to\\ntheir motivations and do not engage with the\\nrelevant literature outside of NLP. Based on\\nthese ﬁndings, we describe the beginnings of a\\npath forward by proposing three recommenda-\\ntions that should guide work analyzing “bias”\\nin NLP systems. These recommendations rest\\non a greater recognition of the relationships\\nbetween\\nlanguage\\nand\\nsocial\\nhierarchies,\\nencouraging\\nresearchers\\nand\\npractitioners\\nto\\narticulate\\ntheir\\nconceptualizations\\nof\\n“bias”—i.e., what kinds of system behaviors\\nare harmful, in what ways, to whom, and why,\\nas well as the normative reasoning underlying\\nthese statements—and to center work around\\nthe lived experiences of members of commu-\\nnities affected by NLP systems, while inter-\\nrogating and reimagining the power relations\\nbetween technologists and such communities.\\n1\\nIntroduction\\nA large body of work analyzing “bias” in natural\\nlanguage processing (NLP) systems has emerged\\nin recent years, including work on “bias” in embed-\\nding spaces (e.g., Bolukbasi et al., 2016a; Caliskan\\net al., 2017; Gonen and Goldberg, 2019; May\\net al., 2019) as well as work on “bias” in systems\\ndeveloped for a breadth of tasks including language\\nmodeling (Lu et al., 2018; Bordia and Bowman,\\n2019), coreference resolution (Rudinger et al.,\\n2018; Zhao et al., 2018a), machine translation (Van-\\nmassenhove et al., 2018; Stanovsky et al., 2019),\\nsentiment analysis (Kiritchenko and Mohammad,\\n2018), and hate speech/toxicity detection (e.g.,\\nPark et al., 2018; Dixon et al., 2018), among others.\\nAlthough these papers have laid vital ground-\\nwork by illustrating some of the ways that NLP\\nsystems can be harmful, the majority of them fail\\nto engage critically with what constitutes “bias”\\nin the ﬁrst place. Despite the fact that analyzing\\n“bias” is an inherently normative process—in\\nwhich some system behaviors are deemed good\\nand others harmful—papers on “bias” in NLP\\nsystems are rife with unstated assumptions about\\nwhat kinds of system behaviors are harmful, in\\nwhat ways, to whom, and why. Indeed, the term\\n“bias” (or “gender bias” or “racial bias”) is used\\nto describe a wide range of system behaviors, even\\nthough they may be harmful in different ways, to\\ndifferent groups, or for different reasons. Even\\npapers analyzing “bias” in NLP systems developed\\nfor the same task often conceptualize it differently.\\nFor example, the following system behaviors\\nare all understood to be self-evident statements of\\n“racial bias”: (a) embedding spaces in which embed-\\ndings for names associated with African Americans\\nare closer (compared to names associated with\\nEuropean Americans) to unpleasant words than\\npleasant words (Caliskan et al., 2017); (b) senti-\\nment analysis systems yielding different intensity\\nscores for sentences containing names associated\\nwith African Americans and sentences containing\\nnames associated with European Americans (Kir-\\nitchenko and Mohammad, 2018); and (c) toxicity\\narXiv:2005.14050v2  [cs.CL]  29 May 2020\\n\\ndetection systems scoring tweets containing fea-\\ntures associated with African-American English as\\nmore offensive than tweets without these features\\n(Davidson et al., 2019; Sap et al., 2019). Moreover,\\nsome of these papers focus on “racial bias”\\nexpressed in written text, while others focus on\\n“racial bias” against authors. This use of imprecise\\nterminology obscures these important differences.\\nWe survey 146 papers analyzing “bias” in NLP\\nsystems, ﬁnding that their motivations are often\\nvague and inconsistent. Many lack any normative\\nreasoning for why the system behaviors that are\\ndescribed as “bias” are harmful, in what ways, and\\nto whom. Moreover, the vast majority of these\\npapers do not engage with the relevant literature\\noutside of NLP to ground normative concerns when\\nproposing quantitative techniques for measuring\\nor mitigating “bias.” As a result, we ﬁnd that many\\nof these techniques are poorly matched to their\\nmotivations, and are not comparable to one another.\\nWe then describe the beginnings of a path\\nforward by proposing three recommendations\\nthat should guide work analyzing “bias” in NLP\\nsystems. We argue that such work should examine\\nthe relationships between language and social hi-\\nerarchies; we call on researchers and practitioners\\nconducting such work to articulate their conceptu-\\nalizations of “bias” in order to enable conversations\\nabout what kinds of system behaviors are harmful,\\nin what ways, to whom, and why; and we recom-\\nmend deeper engagements between technologists\\nand communities affected by NLP systems. We\\nalso provide several concrete research questions\\nthat are implied by each of our recommendations.\\n2\\nMethod\\nOur survey includes all papers known to us\\nanalyzing “bias” in NLP systems—146 papers in\\ntotal. We omitted papers about speech, restricting\\nour survey to papers about written text only. To\\nidentify the 146 papers, we ﬁrst searched the ACL\\nAnthology1 for all papers with the keywords “bias”\\nor “fairness” that were made available prior to May\\n2020. We retained all papers about social “bias,”\\nand discarded all papers about other deﬁnitions of\\nthe keywords (e.g., hypothesis-only bias, inductive\\nbias, media bias). We also discarded all papers us-\\ning “bias” in NLP systems to measure social “bias”\\nin text or the real world (e.g., Garg et al., 2018).\\nTo ensure that we did not exclude any relevant\\n1https://www.aclweb.org/anthology/\\nNLP task\\nPapers\\nEmbeddings (type-level or contextualized)\\n54\\nCoreference resolution\\n20\\nLanguage modeling or dialogue generation\\n17\\nHate-speech detection\\n17\\nSentiment analysis\\n15\\nMachine translation\\n8\\nTagging or parsing\\n5\\nSurveys, frameworks, and meta-analyses\\n20\\nOther\\n22\\nTable 1: The NLP tasks covered by the 146 papers.\\npapers without the keywords “bias” or “fairness,”\\nwe also traversed the citation graph of our initial\\nset of papers, retaining any papers analyzing “bias”\\nin NLP systems that are cited by or cite the papers\\nin our initial set. Finally, we manually inspected\\nany papers analyzing “bias” in NLP systems from\\nleading machine learning, human–computer inter-\\naction, and web conferences and workshops, such\\nas ICML, NeurIPS, AIES, FAccT, CHI, and WWW,\\nalong with any relevant papers that were made\\navailable in the “Computation and Language” and\\n“Computers and Society” categories on arXiv prior\\nto May 2020, but found that they had already been\\nidentiﬁed via our traversal of the citation graph. We\\nprovide a list of all 146 papers in the appendix. In\\nTable 1, we provide a breakdown of the NLP tasks\\ncovered by the papers. We note that counts do not\\nsum to 146, because some papers cover multiple\\ntasks. For example, a paper might test the efﬁcacy\\nof a technique for mitigating “bias” in embed-\\nding spaces in the context of sentiment analysis.\\nOnce identiﬁed, we then read each of the 146 pa-\\npers with the goal of categorizing their motivations\\nand their proposed quantitative techniques for mea-\\nsuring or mitigating “bias.” We used a previously\\ndeveloped taxonomy of harms for this categoriza-\\ntion, which differentiates between so-called alloca-\\ntional and representational harms (Barocas et al.,\\n2017; Crawford, 2017). Allocational harms arise\\nwhen an automated system allocates resources (e.g.,\\ncredit) or opportunities (e.g., jobs) unfairly to dif-\\nferent social groups; representational harms arise\\nwhen a system (e.g., a search engine) represents\\nsome social groups in a less favorable light than\\nothers, demeans them, or fails to recognize their\\nexistence altogether. Adapting and extending this\\ntaxonomy, we categorized the 146 papers’ motiva-\\ntions and techniques into the following categories:\\n▷Allocational harms.\\n\\nPapers\\nCategory\\nMotivation\\nTechnique\\nAllocational harms\\n30\\n4\\nStereotyping\\n50\\n58\\nOther representational harms\\n52\\n43\\nQuestionable correlations\\n47\\n42\\nVague/unstated\\n23\\n0\\nSurveys, frameworks, and\\nmeta-analyses\\n20\\n20\\nTable 2: The categories into which the 146 papers fall.\\n▷Representational harms:2\\n▷Stereotyping that propagates negative gen-\\neralizations about particular social groups.\\n▷Differences in system performance for dif-\\nferent social groups, language that misrep-\\nresents the distribution of different social\\ngroups in the population, or language that\\nis denigrating to particular social groups.\\n▷Questionable correlations between system be-\\nhavior and features of language that are typi-\\ncally associated with particular social groups.\\n▷Vague descriptions of “bias” (or “gender\\nbias” or “racial bias”) or no description at all.\\n▷Surveys, frameworks, and meta-analyses.\\nIn Table 2 we provide counts for each of the\\nsix categories listed above. (We also provide a\\nlist of the papers that fall into each category in the\\nappendix.) Again, we note that the counts do not\\nsum to 146, because some papers state multiple\\nmotivations, propose multiple techniques, or pro-\\npose a single technique for measuring or mitigating\\nmultiple harms. Table 3, which is in the appendix,\\ncontains examples of the papers’ motivations and\\ntechniques across a range of different NLP tasks.\\n3\\nFindings\\nCategorizing the 146 papers’ motivations and pro-\\nposed quantitative techniques for measuring or miti-\\ngating “bias” into the six categories listed above en-\\nabled us to identify several commonalities, which\\nwe present below, along with illustrative quotes.\\n2We grouped several types of representational harms into\\ntwo categories to reﬂect that the main point of differentiation\\nbetween the 146 papers’ motivations and proposed quantitative\\ntechniques for measuring or mitigating “bias” is whether or not\\nthey focus on stereotyping. Among the papers that do not fo-\\ncus on stereotyping, we found that most lack sufﬁciently clear\\nmotivations and techniques to reliably categorize them further.\\n3.1\\nMotivations\\nPapers state a wide range of motivations,\\nmultiple motivations, vague motivations, and\\nsometimes no motivations at all.\\nWe found that\\nthe papers’ motivations span all six categories, with\\nseveral papers falling into each one. Appropriately,\\npapers that provide surveys or frameworks for an-\\nalyzing “bias” in NLP systems often state multiple\\nmotivations (e.g., Hovy and Spruit, 2016; Bender,\\n2019; Sun et al., 2019; Rozado, 2020; Shah et al.,\\n2020). However, as the examples in Table 3 (in the\\nappendix) illustrate, many other papers (33%) do\\nso as well. Some papers (16%) state only vague\\nmotivations or no motivations at all. For example,\\n“[N]o human should be discriminated on the basis\\nof demographic attributes by an NLP system.”\\n—Kaneko and Bollegala (2019)\\n“[P]rominent word embeddings [...] encode\\nsystematic biases against women and black people\\n[...] implicating many NLP systems in scaling up\\nsocial injustice.”\\n—May et al. (2019)\\nThese examples leave unstated what it might mean\\nfor an NLP system to “discriminate,” what con-\\nstitutes “systematic biases,” or how NLP systems\\ncontribute to “social injustice” (itself undeﬁned).\\nPapers’ motivations sometimes include no nor-\\nmative reasoning.\\nWe found that some papers\\n(32%) are not motivated by any apparent normative\\nconcerns, often focusing instead on concerns about\\nsystem performance. For example, the ﬁrst quote\\nbelow includes normative reasoning—namely that\\nmodels should not use demographic information\\nto make predictions—while the other focuses on\\nlearned correlations impairing system performance.\\n“In [text classiﬁcation], models are expected to\\nmake predictions with the semantic information\\nrather than with the demographic group identity\\ninformation (e.g., ‘gay’, ‘black’) contained in the\\nsentences.”\\n—Zhang et al. (2020a)\\n“An over-prevalence of some gendered forms in the\\ntraining data leads to translations with identiﬁable\\nerrors. Translations are better for sentences\\ninvolving men and for sentences containing\\nstereotypical gender roles.”\\n—Saunders and Byrne (2020)\\nEven when papers do state clear motivations,\\nthey are often unclear about why the system be-\\nhaviors that are described as “bias” are harm-\\nful, in what ways, and to whom.\\nWe found that\\neven papers with clear motivations often fail to ex-\\nplain what kinds of system behaviors are harmful,\\nin what ways, to whom, and why. For example,\\n\\n“Deploying these word embedding algorithms in\\npractice, for example in automated translation\\nsystems or as hiring aids, runs the serious risk of\\nperpetuating problematic biases in important\\nsocietal contexts.”\\n—Brunet et al. (2019)\\n“[I]f the systems show discriminatory behaviors in\\nthe interactions, the user experience will be\\nadversely affected.”\\n—Liu et al. (2019)\\nThese examples leave unstated what “problematic\\nbiases” or non-ideal user experiences might look\\nlike, how the system behaviors might result in\\nthese things, and who the relevant stakeholders\\nor users might be. In contrast, we ﬁnd that papers\\nthat provide surveys or frameworks for analyzing\\n“bias” in NLP systems often name who is harmed,\\nacknowledging that different social groups may\\nexperience these systems differently due to their\\ndifferent relationships with NLP systems or\\ndifferent social positions. For example, Ruane\\net al. (2019) argue for a “deep understanding of\\nthe user groups [sic] characteristics, contexts, and\\ninterests” when designing conversational agents.\\nPapers about NLP systems developed for the\\nsame task often conceptualize “bias” differ-\\nently.\\nEven papers that cover the same NLP task\\noften conceptualize “bias” in ways that differ sub-\\nstantially and are sometimes inconsistent. Rows 3\\nand 4 of Table 3 (in the appendix) contain machine\\ntranslation papers with different conceptualizations\\nof “bias,” leading to different proposed techniques,\\nwhile rows 5 and 6 contain papers on “bias” in em-\\nbedding spaces that state different motivations, but\\npropose techniques for quantifying stereotyping.\\nPapers’ motivations conﬂate allocational and\\nrepresentational harms.\\nWe found that the pa-\\npers’ motivations sometimes (16%) name imme-\\ndiate representational harms, such as stereotyping,\\nalongside more distant allocational harms, which,\\nin the case of stereotyping, are usually imagined as\\ndownstream effects of stereotypes on résumé ﬁlter-\\ning. Many of these papers use the imagined down-\\nstream effects to justify focusing on particular sys-\\ntem behaviors, even when the downstream effects\\nare not measured. Papers on “bias” in embedding\\nspaces are especially likely to do this because em-\\nbeddings are often used as input to other systems:\\n“However, none of these papers [on embeddings]\\nhave recognized how blatantly sexist the\\nembeddings are and hence risk introducing biases\\nof various types into real-world systems.”\\n—Bolukbasi et al. (2016a)\\n“It is essential to quantify and mitigate gender bias\\nin these embeddings to avoid them from affecting\\ndownstream applications.”\\n—Zhou et al. (2019)\\nIn contrast, papers that provide surveys or frame-\\nworks for analyzing “bias” in NLP systems treat\\nrepresentational harms as harmful in their own\\nright. For example, Mayﬁeld et al. (2019) and\\nRuane et al. (2019) cite the harmful reproduction\\nof dominant linguistic norms by NLP systems (a\\npoint to which we return in section 4), while Bender\\n(2019) outlines a range of harms, including seeing\\nstereotypes in search results and being made invis-\\nible to search engines due to language practices.\\n3.2\\nTechniques\\nPapers’ techniques are not well grounded in the\\nrelevant literature outside of NLP.\\nPerhaps un-\\nsurprisingly given that the papers’ motivations are\\noften vague, inconsistent, and lacking in normative\\nreasoning, we also found that the papers’ proposed\\nquantitative techniques for measuring or mitigating\\n“bias” do not effectively engage with the relevant\\nliterature outside of NLP. Papers on stereotyping\\nare a notable exception: the Word Embedding\\nAssociation Test (Caliskan et al., 2017) draws on\\nthe Implicit Association Test (Greenwald et al.,\\n1998) from the social psychology literature, while\\nseveral techniques operationalize the well-studied\\n“Angry Black Woman” stereotype (Kiritchenko\\nand Mohammad, 2018; May et al., 2019; Tan\\nand Celis, 2019) and the “double bind” faced by\\nwomen (May et al., 2019; Tan and Celis, 2019), in\\nwhich women who succeed at stereotypically male\\ntasks are perceived to be less likable than similarly\\nsuccessful men (Heilman et al., 2004). Tan and\\nCelis (2019) also examine the compounding effects\\nof race and gender, drawing on Black feminist\\nscholarship on intersectionality (Crenshaw, 1989).\\nPapers’ techniques are poorly matched to their\\nmotivations.\\nWe found that although 21% of the\\npapers include allocational harms in their motiva-\\ntions, only four papers actually propose techniques\\nfor measuring or mitigating allocational harms.\\nPapers focus on a narrow range of potential\\nsources of “bias.”\\nWe found that nearly all of the\\npapers focus on system predictions as the potential\\nsources of “bias,” with many additionally focusing\\non “bias” in datasets (e.g., differences in the\\nnumber of gendered pronouns in the training data\\n(Zhao et al., 2019)). Most papers do not interrogate\\n\\nthe normative implications of other decisions made\\nduring the development and deployment lifecycle—\\nperhaps unsurprising given that their motivations\\nsometimes include no normative reasoning.\\nA\\nfew papers are exceptions, illustrating the impacts\\nof task deﬁnitions, annotation guidelines, and\\nevaluation metrics: Cao and Daumé (2019) study\\nhow folk conceptions of gender (Keyes, 2018) are\\nreproduced in coreference resolution systems that\\nassume a strict gender dichotomy, thereby main-\\ntaining cisnormativity; Sap et al. (2019) focus on\\nthe effect of priming annotators with information\\nabout possible dialectal differences when asking\\nthem to apply toxicity labels to sample tweets, ﬁnd-\\ning that annotators who are primed are signiﬁcantly\\nless likely to label tweets containing features asso-\\nciated with African-American English as offensive.\\n4\\nA path forward\\nWe now describe how researchers and practitioners\\nconducting work analyzing “bias” in NLP systems\\nmight avoid the pitfalls presented in the previous\\nsection—the beginnings of a path forward. We\\npropose three recommendations that should guide\\nsuch work, and, for each, provide several concrete\\nresearch questions. We emphasize that these ques-\\ntions are not comprehensive, and are intended to\\ngenerate further questions and lines of engagement.\\nOur three recommendations are as follows:\\n(R1) Ground work analyzing “bias” in NLP sys-\\ntems in the relevant literature outside of NLP\\nthat explores the relationships between lan-\\nguage and social hierarchies. Treat represen-\\ntational harms as harmful in their own right.\\n(R2) Provide explicit statements of why the\\nsystem behaviors that are described as “bias”\\nare harmful, in what ways, and to whom.\\nBe forthright about the normative reasoning\\n(Green, 2019) underlying these statements.\\n(R3) Examine language use in practice by engag-\\ning with the lived experiences of members of\\ncommunities affected by NLP systems. Inter-\\nrogate and reimagine the power relations be-\\ntween technologists and such communities.\\n4.1\\nLanguage and social hierarchies\\nTurning ﬁrst to (R1), we argue that work analyzing\\n“bias” in NLP systems will paint a much fuller pic-\\nture if it engages with the relevant literature outside\\nof NLP that explores the relationships between\\nlanguage and social hierarchies. Many disciplines,\\nincluding sociolinguistics, linguistic anthropology,\\nsociology, and social psychology, study how\\nlanguage takes on social meaning and the role that\\nlanguage plays in maintaining social hierarchies.\\nFor example, language is the means through which\\nsocial groups are labeled and one way that beliefs\\nabout social groups are transmitted (e.g., Maass,\\n1999; Beukeboom and Burgers, 2019).\\nGroup\\nlabels can serve as the basis of stereotypes and thus\\nreinforce social inequalities: “[T]he label content\\nfunctions to identify a given category of people,\\nand thereby conveys category boundaries and a\\nposition in a hierarchical taxonomy” (Beukeboom\\nand Burgers, 2019).\\nSimilarly, “controlling\\nimages,” such as stereotypes of Black women,\\nwhich are linguistically and visually transmitted\\nthrough literature, news media, television, and so\\nforth, provide “ideological justiﬁcation” for their\\ncontinued oppression (Collins, 2000, Chapter 4).\\nAs a result, many groups have sought to bring\\nabout social changes through changes in language,\\ndisrupting patterns of oppression and marginal-\\nization via so-called “gender-fair” language\\n(Sczesny et al., 2016; Menegatti and Rubini, 2017),\\nlanguage that is more inclusive to people with\\ndisabilities (ADA, 2018), and language that is less\\ndehumanizing (e.g., abandoning the use of the term\\n“illegal” in everyday discourse on immigration in\\nthe U.S. (Rosa, 2019)). The fact that group labels\\nare so contested is evidence of how deeply inter-\\ntwined language and social hierarchies are. Taking\\n“gender-fair” language as an example, the hope\\nis that reducing asymmetries in language about\\nwomen and men will reduce asymmetries in their\\nsocial standing. Meanwhile, struggles over lan-\\nguage use often arise from dominant social groups’\\ndesire to “control both material and symbolic\\nresources”—i.e., “the right to decide what words\\nwill mean and to control those meanings”—as was\\nthe case in some white speakers’ insistence on\\nusing offensive place names against the objections\\nof Indigenous speakers (Hill, 2008, Chapter 3).\\nSociolinguists and linguistic anthropologists\\nhave also examined language attitudes and lan-\\nguage ideologies, or people’s metalinguistic beliefs\\nabout language: Which language varieties or prac-\\ntices are taken as standard, ordinary, or unmarked?\\nWhich are considered correct, prestigious, or ap-\\npropriate for public use, and which are considered\\nincorrect, uneducated, or offensive (e.g., Campbell-\\n\\nKibler, 2009; Preston, 2009; Loudermilk, 2015;\\nLanehart and Malik, 2018)? Which are rendered in-\\nvisible (Roche, 2019)?3 Language ideologies play\\na vital role in reinforcing and justifying social hi-\\nerarchies because beliefs about language varieties\\nor practices often translate into beliefs about their\\nspeakers (e.g. Alim et al., 2016; Rosa and Flores,\\n2017; Craft et al., 2020). For example, in the U.S.,\\nthe portrayal of non-white speakers’ language\\nvarieties and practices as linguistically deﬁcient\\nhelped to justify violent European colonialism, and\\ntoday continues to justify enduring racial hierar-\\nchies by maintaining views of non-white speakers\\nas lacking the language “required for complex\\nthinking processes and successful engagement\\nin the global economy” (Rosa and Flores, 2017).\\nRecognizing the role that language plays in\\nmaintaining social hierarchies is critical to the\\nfuture of work analyzing “bias” in NLP systems.\\nFirst, it helps to explain why representational\\nharms are harmful in their own right. Second, the\\ncomplexity of the relationships between language\\nand social hierarchies illustrates why studying\\n“bias” in NLP systems is so challenging, suggesting\\nthat researchers and practitioners will need to move\\nbeyond existing algorithmic fairness techniques.\\nWe argue that work must be grounded in the\\nrelevant literature outside of NLP that examines\\nthe relationships between language and social\\nhierarchies; without this grounding, researchers\\nand practitioners risk measuring or mitigating\\nonly what is convenient to measure or mitigate,\\nrather than what is most normatively concerning.\\nMore speciﬁcally, we recommend that work\\nanalyzing “bias” in NLP systems be reoriented\\naround the following question: How are social\\nhierarchies, language ideologies, and NLP systems\\ncoproduced? This question mirrors Benjamin’s\\n(2020) call to examine how “race and technology\\nare coproduced”—i.e., how racial hierarchies, and\\nthe ideologies and discourses that maintain them,\\ncreate and are re-created by technology. We recom-\\nmend that researchers and practitioners similarly\\nask how existing social hierarchies and language\\nideologies drive the development and deployment\\nof NLP systems, and how these systems therefore\\nreproduce these hierarchies and ideologies. As\\na starting point for reorienting work analyzing\\n“bias” in NLP systems around this question, we\\n3Language ideologies encompass much more than this; see,\\ne.g., Lippi-Green (2012), Alim et al. (2016), Rosa and Flores\\n(2017), Rosa and Burdick (2017), and Charity Hudley (2017).\\nprovide the following concrete research questions:\\n▷How do social hierarchies and language\\nideologies inﬂuence the decisions made during\\nthe development and deployment lifecycle?\\nWhat kinds of NLP systems do these decisions\\nresult in, and what kinds do they foreclose?\\n⋄General assumptions: To which linguistic\\nnorms do NLP systems adhere (Bender,\\n2019; Ruane et al., 2019)? Which language\\npractices are implicitly assumed to be\\nstandard, ordinary, correct, or appropriate?\\n⋄Task deﬁnition:\\nFor which speakers\\nare NLP systems (and NLP resources)\\ndeveloped? (See Joshi et al. (2020) for\\na discussion.)\\nHow do task deﬁnitions\\ndiscretize the world? For example, how\\nare social groups delineated when deﬁning\\ndemographic attribute prediction tasks\\n(e.g., Koppel et al., 2002; Rosenthal and\\nMcKeown, 2011; Nguyen et al., 2013)?\\nWhat about languages in native language\\nprediction tasks (Tetreault et al., 2013)?\\n⋄Data: How are datasets collected, prepro-\\ncessed, and labeled or annotated? What are\\nthe impacts of annotation guidelines, anno-\\ntator assumptions and perceptions (Olteanu\\net al., 2019; Sap et al., 2019; Geiger et al.,\\n2020), and annotation aggregation pro-\\ncesses (Pavlick and Kwiatkowski, 2019)?\\n⋄Evaluation: How are NLP systems evalu-\\nated? What are the impacts of evaluation\\nmetrics (Olteanu et al., 2017)? Are any\\nnon-quantitative evaluations performed?\\n▷How do NLP systems reproduce or transform\\nlanguage ideologies? Which language varieties\\nor practices come to be deemed good or bad?\\nMight “good” language simply mean language\\nthat is easily handled by existing NLP sys-\\ntems? For example, linguistic phenomena aris-\\ning from many language practices (Eisenstein,\\n2013) are described as “noisy text” and often\\nviewed as a target for “normalization.” How\\ndo the language ideologies that are reproduced\\nby NLP systems maintain social hierarchies?\\n▷Which\\nrepresentational\\nharms\\nare\\nbeing\\nmeasured or mitigated? Are these the most\\nnormatively concerning harms, or merely\\nthose that are well handled by existing algo-\\nrithmic fairness techniques? Are there other\\nrepresentational harms that might be analyzed?\\n\\n4.2\\nConceptualizations of “bias”\\nTurning now to (R2), we argue that work analyzing\\n“bias” in NLP systems should provide explicit\\nstatements of why the system behaviors that are\\ndescribed as “bias” are harmful, in what ways,\\nand to whom, as well as the normative reasoning\\nunderlying these statements.\\nIn other words,\\nresearchers and practitioners should articulate their\\nconceptualizations of “bias.”\\nAs we described\\nabove, papers often contain descriptions of system\\nbehaviors that are understood to be self-evident\\nstatements of “bias.”\\nThis use of imprecise\\nterminology has led to papers all claiming to\\nanalyze “bias” in NLP systems, sometimes even\\nin systems developed for the same task, but with\\ndifferent or even inconsistent conceptualizations of\\n“bias,” and no explanations for these differences.\\nYet analyzing “bias” is an inherently normative\\nprocess—in which some system behaviors are\\ndeemed good and others harmful—even if assump-\\ntions about what kinds of system behaviors are\\nharmful, in what ways, for whom, and why are\\nnot stated. We therefore echo calls by Bardzell and\\nBardzell (2011), Keyes et al. (2019), and Green\\n(2019) for researchers and practitioners to make\\ntheir normative reasoning explicit by articulating\\nthe social values that underpin their decisions to\\ndeem some system behaviors as harmful, no matter\\nhow obvious such values appear to be. We further\\nargue that this reasoning should take into account\\nthe relationships between language and social\\nhierarchies that we described above. First, these\\nrelationships provide a foundation from which to\\napproach the normative reasoning that we recom-\\nmend making explicit. For example, some system\\nbehaviors might be harmful precisely because\\nthey maintain social hierarchies. Second, if work\\nanalyzing “bias” in NLP systems is reoriented\\nto understand how social hierarchies, language\\nideologies, and NLP systems are coproduced, then\\nthis work will be incomplete if we fail to account\\nfor the ways that social hierarchies and language\\nideologies determine what we mean by “bias” in\\nthe ﬁrst place. As a starting point, we therefore\\nprovide the following concrete research questions:\\n▷What kinds of system behaviors are described\\nas “bias”? What are their potential sources (e.g.,\\ngeneral assumptions, task deﬁnition, data)?\\n▷In what ways are these system behaviors harm-\\nful, to whom are they harmful, and why?\\n▷What are the social values (obvious or not) that\\nunderpin this conceptualization of “bias?”\\n4.3\\nLanguage use in practice\\nFinally, we turn to (R3). Our perspective, which\\nrests on a greater recognition of the relationships\\nbetween language and social hierarchies, suggests\\nseveral directions for examining language use in\\npractice. Here, we focus on two. First, because lan-\\nguage is necessarily situated, and because different\\nsocial groups have different lived experiences due\\nto their different social positions (Hanna et al.,\\n2020)—particularly groups at the intersections\\nof multiple axes of oppression—we recommend\\nthat researchers and practitioners center work\\nanalyzing “bias” in NLP systems around the lived\\nexperiences of members of communities affected\\nby these systems. Second, we recommend that\\nthe power relations between technologists and\\nsuch communities be interrogated and reimagined.\\nResearchers have pointed out that algorithmic\\nfairness techniques, by proposing incremental\\ntechnical mitigations—e.g., collecting new datasets\\nor training better models—maintain these power\\nrelations by (a) assuming that automated systems\\nshould continue to exist, rather than asking\\nwhether they should be built at all, and (b) keeping\\ndevelopment and deployment decisions in the\\nhands of technologists (Bennett and Keyes, 2019;\\nCifor et al., 2019; Green, 2019; Katell et al., 2020).\\nThere are many disciplines for researchers and\\npractitioners to draw on when pursuing these\\ndirections.\\nFor example, in human–computer\\ninteraction, Hamidi et al. (2018) study transgender\\npeople’s experiences with automated gender\\nrecognition systems in order to uncover how\\nthese systems reproduce structures of transgender\\nexclusion by redeﬁning what it means to perform\\ngender “normally.” Value-sensitive design provides\\na framework for accounting for the values of differ-\\nent stakeholders in the design of technology (e.g.,\\nFriedman et al., 2006; Friedman and Hendry, 2019;\\nLe Dantec et al., 2009; Yoo et al., 2019), while\\nparticipatory design seeks to involve stakeholders\\nin the design process itself (Sanders, 2002; Muller,\\n2007; Simonsen and Robertson, 2013; DiSalvo\\net al., 2013). Participatory action research in educa-\\ntion (Kemmis, 2006) and in language documenta-\\ntion and reclamation (Junker, 2018) is also relevant.\\nIn particular, work on language reclamation to\\nsupport decolonization and tribal sovereignty\\n(Leonard, 2012) and work in sociolinguistics focus-\\n\\ning on developing co-equal research relationships\\nwith community members and supporting linguis-\\ntic justice efforts (e.g., Bucholtz et al., 2014, 2016,\\n2019) provide examples of more emancipatory rela-\\ntionships with communities. Finally, several work-\\nshops and events have begun to explore how to em-\\npower stakeholders in the development and deploy-\\nment of technology (Vaccaro et al., 2019; Givens\\nand Morris, 2020; Sassaman et al., 2020)4 and how\\nto help researchers and practitioners consider when\\nnot to build systems at all (Barocas et al., 2020).\\nAs a starting point for engaging with commu-\\nnities affected by NLP systems, we therefore\\nprovide the following concrete research questions:\\n▷How do communities become aware of NLP\\nsystems? Do they resist them, and if so, how?\\n▷What additional costs are borne by communi-\\nties for whom NLP systems do not work well?\\n▷Do NLP systems shift power toward oppressive\\ninstitutions (e.g., by enabling predictions that\\ncommunities do not want made, linguistically\\nbased unfair allocation of resources or oppor-\\ntunities (Rosa and Flores, 2017), surveillance,\\nor censorship), or away from such institutions?\\n▷Who is involved in the development and\\ndeployment of NLP systems?\\nHow do\\ndecision-making processes maintain power re-\\nlations between technologists and communities\\naffected by NLP systems?\\nCan these pro-\\ncesses be changed to reimagine these relations?\\n5\\nCase study\\nTo illustrate our recommendations, we present a\\ncase study covering work on African-American\\nEnglish (AAE).5 Work analyzing “bias” in the con-\\ntext of AAE has shown that part-of-speech taggers,\\nlanguage identiﬁcation systems, and dependency\\nparsers all work less well on text containing\\nfeatures associated with AAE than on text without\\nthese features (Jørgensen et al., 2015, 2016; Blod-\\ngett et al., 2016, 2018), and that toxicity detection\\nsystems score tweets containing features associated\\nwith AAE as more offensive than tweets with-\\nout them (Davidson et al., 2019; Sap et al., 2019).\\nThese papers have been critical for highlighting\\nAAE as a language variety for which existing NLP\\n4Also https://participatoryml.github.io/\\n5This language variety has had many different names\\nover the years,\\nbut is now generally called African-\\nAmerican English (AAE), African-American Vernacular En-\\nglish (AAVE), or African-American Language (AAL) (Green,\\n2002; Wolfram and Schilling, 2015; Rickford and King, 2016).\\nsystems may not work, illustrating their limitations.\\nHowever, they do not conceptualize “racial bias” in\\nthe same way. The ﬁrst four of these papers simply\\nfocus on system performance differences between\\ntext containing features associated with AAE and\\ntext without these features. In contrast, the last\\ntwo papers also focus on such system performance\\ndifferences, but motivate this focus with the fol-\\nlowing additional reasoning: If tweets containing\\nfeatures associated with AAE are scored as more\\noffensive than tweets without these features, then\\nthis might (a) yield negative perceptions of AAE;\\n(b) result in disproportionate removal of tweets\\ncontaining these features, impeding participation\\nin online platforms and reducing the space avail-\\nable online in which speakers can use AAE freely;\\nand (c) cause AAE speakers to incur additional\\ncosts if they have to change their language practices\\nto avoid negative perceptions or tweet removal.\\nMore importantly, none of these papers engage\\nwith the literature on AAE, racial hierarchies in the\\nU.S., and raciolinguistic ideologies. By failing to\\nengage with this literature—thereby treating AAE\\nsimply as one of many non-Penn Treebank vari-\\neties of English or perhaps as another challenging\\ndomain—work analyzing “bias” in NLP systems\\nin the context of AAE fails to situate these systems\\nin the world. Who are the speakers of AAE? How\\nare they viewed? We argue that AAE as a language\\nvariety cannot be separated from its speakers—\\nprimarily Black people in the U.S., who experience\\nsystemic anti-Black racism—and the language ide-\\nologies that reinforce and justify racial hierarchies.\\nEven after decades of sociolinguistic efforts to\\nlegitimize AAE, it continues to be viewed as “bad”\\nEnglish and its speakers continue to be viewed as\\nlinguistically inadequate—a view called the deﬁcit\\nperspective (Alim et al., 2016; Rosa and Flores,\\n2017). This perspective persists despite demon-\\nstrations that AAE is rule-bound and grammatical\\n(Mufwene et al., 1998; Green, 2002), in addition\\nto ample evidence of its speakers’ linguistic adroit-\\nness (e.g., Alim, 2004; Rickford and King, 2016).\\nThis perspective belongs to a broader set of raciolin-\\nguistic ideologies (Rosa and Flores, 2017), which\\nalso produce allocational harms; speakers of AAE\\nare frequently penalized for not adhering to domi-\\nnant language practices, including in the education\\nsystem (Alim, 2004; Terry et al., 2010), when\\nseeking housing (Baugh, 2018), and in the judicial\\nsystem, where their testimony is misunderstood or,\\n\\nworse yet, disbelieved (Rickford and King, 2016;\\nJones et al., 2019). These raciolinguistic ideologies\\nposition\\nracialized\\ncommunities\\nas\\nneeding\\nlinguistic intervention, such as language education\\nprograms, in which these and other harms can be\\nreduced if communities accommodate to domi-\\nnant language practices (Rosa and Flores, 2017).\\nIn the technology industry, speakers of AAE are\\noften not considered consumers who matter. For\\nexample, Benjamin (2019) recounts an Apple em-\\nployee who worked on speech recognition for Siri:\\n“As they worked on different English dialects —\\nAustralian, Singaporean, and Indian English — [the\\nemployee] asked his boss: ‘What about African\\nAmerican English?’ To this his boss responded:\\n‘Well, Apple products are for the premium market.”’\\nThe reality, of course, is that speakers of AAE tend\\nnot to represent the “premium market” precisely be-\\ncause of institutions and policies that help to main-\\ntain racial hierarchies by systematically denying\\nthem the opportunities to develop wealth that are\\navailable to white Americans (Rothstein, 2017)—\\nan exclusion that is reproduced in technology by\\ncountless decisions like the one described above.\\nEngaging with the literature outlined above\\nsituates the system behaviors that are described\\nas “bias,” providing a foundation for normative\\nreasoning. Researchers and practitioners should\\nbe concerned about “racial bias” in toxicity\\ndetection systems not only because performance\\ndifferences impair system performance,\\nbut\\nbecause they reproduce longstanding injustices of\\nstigmatization and disenfranchisement for speakers\\nof AAE. In re-stigmatizing AAE, they reproduce\\nlanguage ideologies in which AAE is viewed as\\nungrammatical, uneducated, and offensive. These\\nideologies, in turn, enable linguistic discrimination\\nand justify enduring racial hierarchies (Rosa and\\nFlores, 2017). Our perspective, which understands\\nracial hierarchies and raciolinguistic ideologies as\\nstructural conditions that govern the development\\nand deployment of technology,\\nimplies that\\ntechniques for measuring or mitigating “bias”\\nin NLP systems will necessarily be incomplete\\nunless they interrogate and dismantle these\\nstructural conditions, including the power relations\\nbetween technologists and racialized communities.\\nWe emphasize that engaging with the literature\\non AAE, racial hierarchies in the U.S., and\\nraciolinguistic ideologies can generate new lines of\\nengagement. These lines include work on the ways\\nthat the decisions made during the development\\nand deployment of NLP systems produce stigmati-\\nzation and disenfranchisement, and work on AAE\\nuse in practice, such as the ways that speakers\\nof AAE interact with NLP systems that were not\\ndesigned for them. This literature can also help re-\\nsearchers and practitioners address the allocational\\nharms that may be produced by NLP systems, and\\nensure that even well-intentioned NLP systems\\ndo not position racialized communities as needing\\nlinguistic intervention or accommodation to\\ndominant language practices. Finally, researchers\\nand practitioners wishing to design better systems\\ncan also draw on a growing body of work on\\nanti-racist language pedagogy that challenges the\\ndeﬁcit perspective of AAE and other racialized\\nlanguage practices (e.g. Flores and Chaparro, 2018;\\nBaker-Bell, 2019; Martínez and Mejía, 2019), as\\nwell as the work that we described in section 4.3\\non reimagining the power relations between tech-\\nnologists and communities affected by technology.\\n6\\nConclusion\\nBy surveying 146 papers analyzing “bias” in NLP\\nsystems, we found that (a) their motivations are\\noften vague, inconsistent, and lacking in norma-\\ntive reasoning; and (b) their proposed quantitative\\ntechniques for measuring or mitigating “bias” are\\npoorly matched to their motivations and do not en-\\ngage with the relevant literature outside of NLP.\\nTo help researchers and practitioners avoid these\\npitfalls, we proposed three recommendations that\\nshould guide work analyzing “bias” in NLP sys-\\ntems, and, for each, provided several concrete re-\\nsearch questions. These recommendations rest on\\na greater recognition of the relationships between\\nlanguage and social hierarchies—a step that we\\nsee as paramount to establishing a path forward.\\nAcknowledgments\\nThis paper is based upon work supported by the\\nNational Science Foundation Graduate Research\\nFellowship under Grant No. 1451512. Any opin-\\nion, ﬁndings, and conclusions or recommendations\\nexpressed in this material are those of the authors\\nand do not necessarily reﬂect the views of the Na-\\ntional Science Foundation. We thank the reviewers\\nfor their useful feedback, especially the sugges-\\ntion to include additional details about our method.\\n\\nReferences\\nArtem Abzaliev. 2019.\\nOn GAP coreference resolu-\\ntion shared task: insights from the 3rd place solution.\\nIn Proceedings of the Workshop on Gender Bias in\\nNatural Language Processing, pages 107–112, Flo-\\nrence, Italy.\\nADA. 2018.\\nGuidelines for Writing About Peo-\\nple With Disabilities.\\nADA National Network.\\nhttps://bit.ly/2KREbkB.\\nOshin Agarwal, Funda Durupinar, Norman I. Badler,\\nand Ani Nenkova. 2019. Word embeddings (also)\\nencode human personality stereotypes. In Proceed-\\nings of the Joint Conference on Lexical and Com-\\nputational Semantics, pages 205–211, Minneapolis,\\nMN.\\nH. Samy Alim. 2004. You Know My Steez: An Ethno-\\ngraphic and Sociolinguistic Study of Styleshifting in\\na Black American Speech Community. American Di-\\nalect Society.\\nH. Samy Alim, John R. Rickford, and Arnetha F. Ball,\\neditors. 2016.\\nRaciolinguistics:\\nHow Language\\nShapes Our Ideas About Race. Oxford University\\nPress.\\nSandeep Attree. 2019. Gendered ambiguous pronouns\\nshared task: Boosting model conﬁdence by evidence\\npooling. In Proceedings of the Workshop on Gen-\\nder Bias in Natural Language Processing, Florence,\\nItaly.\\nPinkesh Badjatiya,\\nManish Gupta,\\nand Vasudeva\\nVarma. 2019.\\nStereotypical bias removal for hate\\nspeech detection task using knowledge-based gen-\\neralizations.\\nIn Proceedings of the International\\nWorld Wide Web Conference, pages 49–59, San Fran-\\ncisco, CA.\\nEugene Bagdasaryan, Omid Poursaeed, and Vitaly\\nShmatikov. 2019.\\nDifferential Privacy Has Dis-\\nparate Impact on Model Accuracy. In Proceedings\\nof the Conference on Neural Information Processing\\nSystems, Vancouver, Canada.\\nApril Baker-Bell. 2019.\\nDismantling anti-black lin-\\nguistic racism in English language arts classrooms:\\nToward an anti-racist black language pedagogy. The-\\nory Into Practice.\\nDavid Bamman, Sejal Popat, and Sheng Shen. 2019.\\nAn annotated dataset of literary entities. In Proceed-\\nings of the North American Association for Com-\\nputational Linguistics (NAACL), pages 2138–2144,\\nMinneapolis, MN.\\nXingce Bao and Qianqian Qiao. 2019. Transfer Learn-\\ning from Pre-trained BERT for Pronoun Resolution.\\nIn Proceedings of the Workshop on Gender Bias\\nin Natural Language Processing, pages 82–88, Flo-\\nrence, Italy.\\nShaowen Bardzell and Jeffrey Bardzell. 2011. Towards\\na Feminist HCI Methodology: Social Science, Femi-\\nnism, and HCI. In Proceedings of the Conference on\\nHuman Factors in Computing Systems (CHI), pages\\n675–684, Vancouver, Canada.\\nSolon Barocas, Asia J. Biega, Benjamin Fish, J˛edrzej\\nNiklas, and Luke Stark. 2020.\\nWhen Not to De-\\nsign, Build, or Deploy. In Proceedings of the Confer-\\nence on Fairness, Accountability, and Transparency,\\nBarcelona, Spain.\\nSolon Barocas, Kate Crawford, Aaron Shapiro, and\\nHanna Wallach. 2017. The Problem With Bias: Al-\\nlocative Versus Representational Harms in Machine\\nLearning. In Proceedings of SIGCIS, Philadelphia,\\nPA.\\nChristine Basta, Marta R. Costa-jussà, and Noe Casas.\\n2019. Evaluating the underlying gender bias in con-\\ntextualized word embeddings.\\nIn Proceedings of\\nthe Workshop on Gender Bias for Natural Language\\nProcessing, pages 33–39, Florence, Italy.\\nJohn Baugh. 2018.\\nLinguistics in Pursuit of Justice.\\nCambridge University Press.\\nEmily M. Bender. 2019. A typology of ethical risks\\nin language technology with an eye towards where\\ntransparent documentation can help.\\nPresented at\\nThe Future of Artiﬁcial Intelligence:\\nLanguage,\\nEthics, Technology Workshop. https://bit.ly/\\n2P9t9M6.\\nRuha Benjamin. 2019. Race After Technology: Aboli-\\ntionist Tools for the New Jim Code. John Wiley &\\nSons.\\nRuha Benjamin. 2020. 2020 Vision: Reimagining the\\nDefault Settings of Technology & Society. Keynote\\nat ICLR.\\nCynthia L. Bennett and Os Keyes. 2019. What is the\\nPoint of Fairness?\\nDisability, AI, and The Com-\\nplexity of Justice.\\nIn Proceedings of the ASSETS\\nWorkshop on AI Fairness for People with Disabili-\\nties, Pittsburgh, PA.\\nCamiel J. Beukeboom and Christian Burgers. 2019.\\nHow Stereotypes Are Shared Through Language: A\\nReview and Introduction of the Social Categories\\nand Stereotypes Communication (SCSC) Frame-\\nwork. Review of Communication Research, 7:1–37.\\nShruti Bhargava and David Forsyth. 2019.\\nExpos-\\ning and Correcting the Gender Bias in Image\\nCaptioning Datasets and Models.\\narXiv preprint\\narXiv:1912.00578.\\nJayadev Bhaskaran and Isha Bhallamudi. 2019. Good\\nSecretaries, Bad Truck Drivers? Occupational Gen-\\nder Stereotypes in Sentiment Analysis. In Proceed-\\nings of the Workshop on Gender Bias in Natural Lan-\\nguage Processing, pages 62–68, Florence, Italy.\\n\\nSu Lin Blodgett, Lisa Green, and Brendan O’Connor.\\n2016.\\nDemographic Dialectal Variation in Social\\nMedia: A Case Study of African-American English.\\nIn Proceedings of Empirical Methods in Natural\\nLanguage Processing (EMNLP), pages 1119–1130,\\nAustin, TX.\\nSu Lin Blodgett and Brendan O’Connor. 2017. Racial\\nDisparity in Natural Language Processing: A Case\\nStudy of Social Media African-American English.\\nIn Proceedings of the Workshop on Fairness, Ac-\\ncountability, and Transparency in Machine Learning\\n(FAT/ML), Halifax, Canada.\\nSu Lin Blodgett, Johnny Wei, and Brendan O’Connor.\\n2018.\\nTwitter Universal Dependency Parsing for\\nAfrican-American and Mainstream American En-\\nglish. In Proceedings of the Association for Compu-\\ntational Linguistics (ACL), pages 1415–1425, Mel-\\nbourne, Australia.\\nTolga\\nBolukbasi,\\nKai-Wei\\nChang,\\nJames\\nZou,\\nVenkatesh Saligrama,\\nand Adam Kalai. 2016a.\\nMan is to Computer Programmer as Woman is to\\nHomemaker? Debiasing Word Embeddings. In Pro-\\nceedings of the Conference on Neural Information\\nProcessing Systems, pages 4349–4357, Barcelona,\\nSpain.\\nTolga\\nBolukbasi,\\nKai-Wei\\nChang,\\nJames\\nZou,\\nVenkatesh Saligrama, and Adam Kalai. 2016b.\\nQuantifying and reducing stereotypes in word\\nembeddings. In Proceedings of the ICML Workshop\\non #Data4Good: Machine Learning in Social Good\\nApplications, pages 41–45, New York, NY.\\nShikha Bordia and Samuel R. Bowman. 2019. Identify-\\ning and reducing gender bias in word-level language\\nmodels. In Proceedings of the NAACL Student Re-\\nsearch Workshop, pages 7–15, Minneapolis, MN.\\nMarc-Etienne Brunet, Colleen Alkalay-Houlihan, Ash-\\nton Anderson, and Richard Zemel. 2019.\\nUnder-\\nstanding the Origins of Bias in Word Embeddings.\\nIn Proceedings of the International Conference on\\nMachine Learning, pages 803–811, Long Beach,\\nCA.\\nMary Bucholtz, Dolores Inés Casillas, and Jin Sook\\nLee. 2016.\\nBeyond Empowerment: Accompani-\\nment and Sociolinguistic Justice in a Youth Research\\nProgram. In Robert Lawson and Dave Sayers, edi-\\ntors, Sociolinguistic Research: Application and Im-\\npact, pages 25–44. Routledge.\\nMary Bucholtz, Dolores Inés Casillas, and Jin Sook\\nLee. 2019.\\nCalifornia Latinx Youth as Agents of\\nSociolinguistic Justice. In Netta Avineri, Laura R.\\nGraham, Eric J. Johnson, Robin Conley Riner, and\\nJonathan Rosa, editors, Language and Social Justice\\nin Practice, pages 166–175. Routledge.\\nMary Bucholtz, Audrey Lopez, Allina Mojarro, Elena\\nSkapoulli, Chris VanderStouwe, and Shawn Warner-\\nGarcia. 2014. Sociolinguistic Justice in the Schools:\\nStudent Researchers as Linguistic Experts.\\nLan-\\nguage and Linguistics Compass, 8:144–157.\\nKaylee Burns, Lisa Anne Hendricks, Kate Saenko,\\nTrevor Darrell, and Anna Rohrbach. 2018. Women\\nalso Snowboard: Overcoming Bias in Captioning\\nModels. In Procedings of the European Conference\\non Computer Vision (ECCV), pages 793–811, Mu-\\nnich, Germany.\\nAylin\\nCaliskan,\\nJoanna\\nJ.\\nBryson,\\nand\\nArvind\\nNarayanan. 2017. Semantics derived automatically\\nfrom language corpora contain human-like biases.\\nScience, 356(6334).\\nKathryn Campbell-Kibler. 2009.\\nThe nature of so-\\nciolinguistic perception.\\nLanguage Variation and\\nChange, 21(1):135–156.\\nYang Trista Cao and Hal Daumé, III. 2019.\\nTo-\\nward gender-inclusive coreference resolution. arXiv\\npreprint arXiv:1910.13913.\\nRakesh Chada. 2019. Gendered pronoun resolution us-\\ning bert and an extractive question answering formu-\\nlation. In Proceedings of the Workshop on Gender\\nBias in Natural Language Processing, pages 126–\\n133, Florence, Italy.\\nKaytlin Chaloner and Alfredo Maldonado. 2019. Mea-\\nsuring Gender Bias in Word Embedding across Do-\\nmains and Discovering New Gender Bias Word Cat-\\negories. In Proceedings of the Workshop on Gender\\nBias in Natural Language Processing, pages 25–32,\\nFlorence, Italy.\\nAnne H. Charity Hudley. 2017. Language and Racial-\\nization. In Ofelia García, Nelson Flores, and Mas-\\nsimiliano Spotti, editors, The Oxford Handbook of\\nLanguage and Society. Oxford University Press.\\nWon Ik Cho, Ji Won Kim, Seok Min Kim, and\\nNam Soo Kim. 2019. On measuring gender bias in\\ntranslation of gender-neutral pronouns. In Proceed-\\nings of the Workshop on Gender Bias in Natural Lan-\\nguage Processing, pages 173–181, Florence, Italy.\\nShivang Chopra, Ramit Sawhney, Puneet Mathur, and\\nRajiv Ratn Shah. 2020. Hindi-English Hate Speech\\nDetection: Author Proﬁling, Debiasing, and Practi-\\ncal Perspectives. In Proceedings of the AAAI Con-\\nference on Artiﬁcial Intelligence (AAAI), New York,\\nNY.\\nMarika Cifor, Patricia Garcia, T.L. Cowan, Jasmine\\nRault, Tonia Sutherland, Anita Say Chan, Jennifer\\nRode, Anna Lauren Hoffmann, Niloufar Salehi, and\\nLisa Nakamura. 2019.\\nFeminist Data Manifest-\\nNo. Retrieved from https://www.manifestno.\\ncom/.\\nPatricia Hill Collins. 2000.\\nBlack Feminist Thought:\\nKnowledge, Consciousness, and the Politics of Em-\\npowerment. Routledge.\\n\\nJustin T. Craft, Kelly E. Wright, Rachel Elizabeth\\nWeissler, and Robin M. Queen. 2020.\\nLanguage\\nand Discrimination: Generating Meaning, Perceiv-\\ning Identities, and Discriminating Outcomes.\\nAn-\\nnual Review of Linguistics, 6(1).\\nKate Crawford. 2017. The Trouble with Bias. Keynote\\nat NeurIPS.\\nKimberle Crenshaw. 1989. Demarginalizing the Inter-\\nsection of Race and Sex: A Black Feminist Critique\\nof Antidiscrmination Doctrine, Feminist Theory and\\nAntiracist Politics. University of Chicago Legal Fo-\\nrum.\\nAmanda Cercas Curry and Verena Rieser. 2018.\\n#MeToo: How Conversational Systems Respond to\\nSexual Harassment. In Proceedings of the Workshop\\non Ethics in Natural Language Processing, pages 7–\\n14, New Orleans, LA.\\nKaran Dabas, Nishtha Madaan, Gautam Singh, Vi-\\njay Arya, Sameep Mehta, and Tanmoy Chakraborty.\\n2020. Fair Transfer of Multiple Style Attributes in\\nText. arXiv preprint arXiv:2001.06693.\\nThomas Davidson, Debasmita Bhattacharya, and Ing-\\nmar Weber. 2019. Racial bias in hate speech and\\nabusive language detection datasets. In Proceedings\\nof the Workshop on Abusive Language Online, pages\\n25–35, Florence, Italy.\\nMaria De-Arteaga, Alexey Romanov, Hanna Wal-\\nlach, Jennifer Chayes, Christian Borgs, Alexandra\\nChouldechova, Sahin Geyik, Krishnaram Kentha-\\npadi, and Adam Tauman Kalai. 2019. Bias in bios:\\nA case study of semantic representation bias in a\\nhigh-stakes setting. In Proceedings of the Confer-\\nence on Fairness, Accountability, and Transparency,\\npages 120–128, Atlanta, GA.\\nSunipa Dev, Tao Li, Jeff Phillips, and Vivek Sriku-\\nmar. 2019.\\nOn Measuring and Mitigating Biased\\nInferences of Word Embeddings.\\narXiv preprint\\narXiv:1908.09369.\\nSunipa Dev and Jeff Phillips. 2019. Attenuating Bias in\\nWord Vectors. In Proceedings of the International\\nConference on Artiﬁcial Intelligence and Statistics,\\npages 879–887, Naha, Japan.\\nMark Díaz, Isaac Johnson, Amanda Lazar, Anne Marie\\nPiper, and Darren Gergle. 2018.\\nAddressing age-\\nrelated bias in sentiment analysis. In Proceedings\\nof the Conference on Human Factors in Computing\\nSystems (CHI), Montréal, Canada.\\nEmily Dinan, Angela Fan, Adina Williams, Jack Ur-\\nbanek, Douwe Kiela, and Jason Weston. 2019.\\nQueens are Powerful too:\\nMitigating Gender\\nBias in Dialogue Generation.\\narXiv preprint\\narXiv:1911.03842.\\nCarl DiSalvo, Andrew Clement, and Volkmar Pipek.\\n2013. Communities: Participatory Design for, with\\nand by communities. In Jesper Simonsen and Toni\\nRobertson, editors, Routledge International Hand-\\nbook of Participatory Design, pages 182–209. Rout-\\nledge.\\nLucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain,\\nand Lucy Vasserman. 2018. Measuring and mitigat-\\ning unintended bias in text classiﬁcation.\\nIn Pro-\\nceedings of the Conference on Artiﬁcial Intelligence,\\nEthics, and Society (AIES), New Orleans, LA.\\nJacob Eisenstein. 2013.\\nWhat to do about bad lan-\\nguage on the Internet. In Proceedings of the North\\nAmerican Association for Computational Linguistics\\n(NAACL), pages 359–369.\\nKawin Ethayarajh. 2020. Is Your Classiﬁer Actually\\nBiased? Measuring Fairness under Uncertainty with\\nBernstein Bounds. In Proceedings of the Associa-\\ntion for Computational Linguistics (ACL).\\nKawin Ethayarajh, David Duvenaud, and Graeme Hirst.\\n2019. Understanding Undesirable Word Embedding\\nAssocations. In Proceedings of the Association for\\nComputational Linguistics (ACL), pages 1696–1705,\\nFlorence, Italy.\\nJoseph Fisher. 2019.\\nMeasuring social bias in\\nknowledge graph embeddings.\\narXiv preprint\\narXiv:1912.02761.\\nNelson Flores and Soﬁa Chaparro. 2018. What counts\\nas language education policy? Developing a materi-\\nalist Anti-racist approach to language activism. Lan-\\nguage Policy, 17(3):365–384.\\nOmar U. Florez. 2019. On the Unintended Social Bias\\nof Training Language Generation Models with Data\\nfrom Local Media. In Proceedings of the NeurIPS\\nWorkshop on Human-Centric Machine Learning,\\nVancouver, Canada.\\nJoel Escudé Font and Marta R. Costa-jussà. 2019.\\nEqualizing gender biases in neural machine trans-\\nlation with word embeddings techniques.\\nIn Pro-\\nceedings of the Workshop on Gender Bias for Natu-\\nral Language Processing, pages 147–154, Florence,\\nItaly.\\nBatya Friedman and David G. Hendry. 2019.\\nValue\\nSensitive Design: Shaping Technology with Moral\\nImagination. MIT Press.\\nBatya Friedman, Peter H. Kahn Jr., and Alan Borning.\\n2006. Value Sensitive Design and Information Sys-\\ntems. In Dennis Galletta and Ping Zhang, editors,\\nHuman-Computer Interaction in Management Infor-\\nmation Systems: Foundations, pages 348–372. M.E.\\nSharpe.\\nNikhil Garg, Londa Schiebinger, Dan Jurafsky, and\\nJames Zou. 2018. Word Embeddings Quantify 100\\nYears of Gender and Ethnic Stereotypes. Proceed-\\nings of the National Academy of Sciences, 115(16).\\n\\nSahaj Garg, Vincent Perot, Nicole Limtiaco, Ankur\\nTaly, Ed H. Chi, and Alex Beutel. 2019. Counter-\\nfactual fairness in text classiﬁcation through robust-\\nness. In Proceedings of the Conference on Artiﬁcial\\nIntelligence, Ethics, and Society (AIES), Honolulu,\\nHI.\\nAparna Garimella, Carmen Banea, Dirk Hovy, and\\nRada Mihalcea. 2019. Women’s syntactic resilience\\nand men’s grammatical luck: Gender bias in part-of-\\nspeech tagging and dependency parsing data. In Pro-\\nceedings of the Association for Computational Lin-\\nguistics (ACL), pages 3493–3498, Florence, Italy.\\nAndrew Gaut, Tony Sun, Shirlyn Tang, Yuxin Huang,\\nJing Qian,\\nMai ElSherief,\\nJieyu Zhao,\\nDiba\\nMirza, Elizabeth Belding, Kai-Wei Chang, and\\nWilliam Yang Wang. 2020.\\nTowards Understand-\\ning Gender Bias in Relation Extraction. In Proceed-\\nings of the Association for Computational Linguis-\\ntics (ACL).\\nR. Stuart Geiger, Kevin Yu, Yanlai Yang, Mindy Dai,\\nJie Qiu, Rebekah Tang, and Jenny Huang. 2020.\\nGarbage In, Garbage Out?\\nDo Machine Learn-\\ning Application Papers in Social Computing Report\\nWhere Human-Labeled Training Data Comes From?\\nIn Proceedings of the Conference on Fairness, Ac-\\ncountability, and Transparency, pages 325–336.\\nOguzhan Gencoglu. 2020.\\nCyberbullying Detec-\\ntion with Fairness Constraints.\\narXiv preprint\\narXiv:2005.06625.\\nAlexandra Reeve Givens and Meredith Ringel Morris.\\n2020. Centering Disability Perspecives in Algorith-\\nmic Fairness, Accountability, and Transparency. In\\nProceedings of the Conference on Fairness, Account-\\nability, and Transparency, Barcelona, Spain.\\nHila Gonen and Yoav Goldberg. 2019. Lipstick on a\\nPig: Debiasing Methods Cover up Systematic Gen-\\nder Biases in Word Embeddings But do not Remove\\nThem. In Proceedings of the North American As-\\nsociation for Computational Linguistics (NAACL),\\npages 609–614, Minneapolis, MN.\\nHila Gonen and Kellie Webster. 2020.\\nAuto-\\nmatically Identifying Gender Issues in Machine\\nTranslation using Perturbations.\\narXiv preprint\\narXiv:2004.14065.\\nBen Green. 2019. “Good” isn’t good enough. In Pro-\\nceedings of the AI for Social Good Workshop, Van-\\ncouver, Canada.\\nLisa J. Green. 2002. African American English: A Lin-\\nguistic Introduction. Cambridge University Press.\\nAnthony G. Greenwald, Debbie E. McGhee, and Jor-\\ndan L.K. Schwartz. 1998. Measuring individual dif-\\nferences in implicit cognition: The implicit associa-\\ntion test. Journal of Personality and Social Psychol-\\nogy, 74(6):1464–1480.\\nEnoch Opanin Gyamﬁ, Yunbo Rao, Miao Gou, and\\nYanhua Shao. 2020. deb2viz: Debiasing gender in\\nword embedding data using subspace visualization.\\nIn Proceedings of the International Conference on\\nGraphics and Image Processing.\\nFoad Hamidi, Morgan Klaus Scheuerman, and Stacy M.\\nBranham. 2018. Gender Recognition or Gender Re-\\nductionism? The Social Implications of Automatic\\nGender Recognition Systems. In Proceedings of the\\nConference on Human Factors in Computing Sys-\\ntems (CHI), Montréal, Canada.\\nAlex Hanna, Emily Denton, Andrew Smart, and Jamila\\nSmith-Loud. 2020. Towards a Critical Race Method-\\nology in Algorithmic Fairness. In Proceedings of the\\nConference on Fairness, Accountability, and Trans-\\nparency, pages 501–512, Barcelona, Spain.\\nMadeline E. Heilman, Aaaron S. Wallen, Daniella\\nFuchs, and Melinda M. Tamkins. 2004. Penalties\\nfor Success: Reactions to Women Who Succeed at\\nMale Gender-Typed Tasks. Journal of Applied Psy-\\nchology, 89(3):416–427.\\nJane H. Hill. 2008. The Everyday Language of White\\nRacism. Wiley-Blackwell.\\nDirk Hovy, Federico Bianchi, and Tommaso Fornaciari.\\n2020. Can You Translate that into Man? Commer-\\ncial Machine Translation Systems Include Stylistic\\nBiases. In Proceedings of the Association for Com-\\nputational Linguistics (ACL).\\nDirk Hovy and Anders Søgaard. 2015. Tagging Per-\\nformance Correlates with Author Age. In Proceed-\\nings of the Association for Computational Linguis-\\ntics and the International Joint Conference on Nat-\\nural Language Processing, pages 483–488, Beijing,\\nChina.\\nDirk Hovy and Shannon L. Spruit. 2016. The social\\nimpact of natural language processing. In Proceed-\\nings of the Association for Computational Linguis-\\ntics (ACL), pages 591–598, Berlin, Germany.\\nPo-Sen Huang, Huan Zhang, Ray Jiang, Robert\\nStanforth, Johannes Welbl, Jack W. Rae, Vishal\\nMaini, Dani Yogatama, and Pushmeet Kohli. 2019.\\nReducing Sentiment Bias in Language Models\\nvia Counterfactual Evaluation.\\narXiv preprint\\narXiv:1911.03064.\\nXiaolei Huang, Linzi Xing, Franck Dernoncourt, and\\nMichael J. Paul. 2020.\\nMultilingual Twitter Cor-\\npus and Baselines for Evaluating Demographic Bias\\nin Hate Speech Recognition.\\nIn Proceedings of\\nthe Language Resources and Evaluation Conference\\n(LREC), Marseille, France.\\nChristoph Hube, Maximilian Idahl, and Besnik Fetahu.\\n2020. Debiasing Word Embeddings from Sentiment\\nAssociations in Names. In Proceedings of the Inter-\\nnational Conference on Web Search and Data Min-\\ning, pages 259–267, Houston, TX.\\n\\nBen Hutchinson, Vinodkumar Prabhakaran, Emily\\nDenton, Kellie Webster, Yu Zhong, and Stephen De-\\nnuyl. 2020. Social Biases in NLP Models as Barriers\\nfor Persons with Disabilities. In Proceedings of the\\nAssociation for Computational Linguistics (ACL).\\nMatei Ionita, Yury Kashnitsky, Ken Krige, Vladimir\\nLarin, Dennis Logvinenko, and Atanas Atanasov.\\n2019.\\nResolving gendered ambiguous pronouns\\nwith BERT.\\nIn Proceedings of the Workshop on\\nGender Bias in Natural Language Processing, pages\\n113–119, Florence, Italy.\\nHailey James-Sorenson and David Alvarez-Melis.\\n2019. Probabilistic Bias Mitigation in Word Embed-\\ndings. In Proceedings of the Workshop on Human-\\nCentric Machine Learning, Vancouver, Canada.\\nShengyu Jia, Tao Meng, Jieyu Zhao, and Kai-Wei\\nChang. 2020. Mitigating Gender Bias Ampliﬁcation\\nin Distribution by Posterior Regularization. In Pro-\\nceedings of the Association for Computational Lin-\\nguistics (ACL).\\nTaylor Jones, Jessica Rose Kalbfeld, Ryan Hancock,\\nand Robin Clark. 2019. Testifying while black: An\\nexperimental study of court reporter accuracy in tran-\\nscription of African American English. Language,\\n95(2).\\nAnna Jørgensen, Dirk Hovy, and Anders Søgaard. 2015.\\nChallenges of studying and processing dialects in\\nsocial media. In Proceedings of the Workshop on\\nNoisy User-Generated Text, pages 9–18, Beijing,\\nChina.\\nAnna Jørgensen, Dirk Hovy, and Anders Søgaard. 2016.\\nLearning a POS tagger for AAVE-like language. In\\nProceedings of the North American Association for\\nComputational Linguistics (NAACL), pages 1115–\\n1120, San Diego, CA.\\nPratik Joshi, Sebastian Santy, Amar Budhiraja, Kalika\\nBali, and Monojit Choudhury. 2020. The State and\\nFate of Linguistic Diversity and Inclusion in the\\nNLP World. In Proceedings of the Association for\\nComputational Linguistics (ACL).\\nJaap Jumelet, Willem Zuidema, and Dieuwke Hupkes.\\n2019. Analysing Neural Language Models: Contex-\\ntual Decomposition Reveals Default Reasoning in\\nNumber and Gender Assignment. In Proceedings\\nof the Conference on Natural Language Learning,\\nHong Kong, China.\\nMarie-Odile Junker. 2018.\\nParticipatory action re-\\nsearch for Indigenous linguistics in the digital age.\\nIn Shannon T. Bischoff and Carmen Jany, editors,\\nInsights from Practices in Community-Based Re-\\nsearch, pages 164–175. De Gruyter Mouton.\\nDavid Jurgens, Yulia Tsvetkov, and Dan Jurafsky. 2017.\\nIncorporating Dialectal Variability for Socially Equi-\\ntable Language Identiﬁcation. In Proceedings of the\\nAssociation for Computational Linguistics (ACL),\\npages 51–57, Vancouver, Canada.\\nMasahiro Kaneko and Danushka Bollegala. 2019.\\nGender-preserving debiasing for pre-trained word\\nembeddings. In Proceedings of the Association for\\nComputational Linguistics (ACL), pages 1641–1650,\\nFlorence, Italy.\\nSaket Karve, Lyle Ungar, and João Sedoc. 2019. Con-\\nceptor debiasing of word representations evaluated\\non WEAT. In Proceedings of the Workshop on Gen-\\nder Bias in Natural Language Processing, pages 40–\\n48, Florence, Italy.\\nMichael Katell, Meg Young, Dharma Dailey, Bernease\\nHerman, Vivian Guetler, Aaron Tam, Corinne Bintz,\\nDanielle Raz, and P.M. Krafft. 2020.\\nToward sit-\\nuated interventions for algorithmic equity: lessons\\nfrom the ﬁeld. In Proceedings of the Conference on\\nFairness, Accountability, and Transparency, pages\\n45–55, Barcelona, Spain.\\nStephen Kemmis. 2006. Participatory action research\\nand the public sphere. Educational Action Research,\\n14(4):459–476.\\nOs Keyes. 2018.\\nThe Misgendering Machines:\\nTrans/HCI\\nImplications\\nof\\nAutomatic\\nGender\\nRecognition. Proceedings of the ACM on Human-\\nComputer Interaction, 2(CSCW).\\nOs Keyes, Josephine Hoy, and Margaret Drouhard.\\n2019. Human-Computer Insurrection: Notes on an\\nAnarchist HCI. In Proceedings of the Conference on\\nHuman Factors in Computing Systems (CHI), Glas-\\ngow, Scotland, UK.\\nJae Yeon Kim, Carlos Ortiz, Sarah Nam, Sarah Santi-\\nago, and Vivek Datta. 2020. Intersectional Bias in\\nHate Speech and Abusive Language Datasets.\\nIn\\nProceedings of the Association for Computational\\nLinguistics (ACL).\\nSvetlana Kiritchenko and Saif M. Mohammad. 2018.\\nExamining Gender and Race Bias in Two Hundred\\nSentiment Analysis Systems. In Proceedings of the\\nJoint Conference on Lexical and Computational Se-\\nmantics, pages 43–53, New Orleans, LA.\\nMoshe Koppel, Shlomo Argamon, and Anat Rachel\\nShimoni. 2002.\\nAutomatically Categorizing Writ-\\nten Texts by Author Gender. Literary and Linguistic\\nComputing, 17(4):401–412.\\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W.\\nBlack, and Yulia Tsvetkov. 2019. Measuring bias\\nin contextualized word representations. In Proceed-\\nings of the Workshop on Gender Bias for Natu-\\nral Language Processing, pages 166–172, Florence,\\nItaly.\\nSonja L. Lanehart and Ayesha M. Malik. 2018. Black\\nIs, Black Isn’t: Perceptions of Language and Black-\\nness. In Jeffrey Reaser, Eric Wilbanks, Karissa Woj-\\ncik, and Walt Wolfram, editors, Language Variety in\\nthe New South. University of North Carolina Press.\\n\\nBrian N. Larson. 2017. Gender as a variable in natural-\\nlanguage processing: Ethical considerations. In Pro-\\nceedings of the Workshop on Ethics in Natural Lan-\\nguage Processing, pages 30–40, Valencia, Spain.\\nAnne Lauscher and Goran Glavaš. 2019. Are We Con-\\nsistently Biased? Multidimensional Analysis of Bi-\\nases in Distributional Word Vectors. In Proceedings\\nof the Joint Conference on Lexical and Computa-\\ntional Semantics, pages 85–91, Minneapolis, MN.\\nAnne Lauscher, Goran Glavaš, Simone Paolo Ponzetto,\\nand Ivan Vuli´c. 2019. A General Framework for Im-\\nplicit and Explicit Debiasing of Distributional Word\\nVector Spaces. arXiv preprint arXiv:1909.06092.\\nChristopher A. Le Dantec, Erika Shehan Poole, and Su-\\nsan P. Wyche. 2009. Values as Lived Experience:\\nEvolving Value Sensitive Design in Support of Value\\nDiscovery. In Proceedings of the Conference on Hu-\\nman Factors in Computing Systems (CHI), Boston,\\nMA.\\nNayeon Lee, Andrea Madotto, and Pascale Fung. 2019.\\nExploring Social Bias in Chatbots using Stereotype\\nKnowledge.\\nIn Proceedings of the Workshop on\\nWidening NLP, pages 177–180, Florence, Italy.\\nWesley Y. Leonard. 2012. Reframing language recla-\\nmation programmes for everybody’s empowerment.\\nGender and Language, 6(2):339–367.\\nPaul Pu Liang, Irene Li, Emily Zheng, Yao Chong Lim,\\nRuslan Salakhutdinov, and Louis-Philippe Morency.\\n2019. Towards Debiasing Sentence Representations.\\nIn Proceedings of the NeurIPS Workshop on Human-\\nCentric Machine Learning, Vancouver, Canada.\\nRosina Lippi-Green. 2012.\\nEnglish with an Ac-\\ncent: Language, Ideology, and Discrimination in the\\nUnited States. Routledge.\\nBo Liu. 2019. Anonymized BERT: An Augmentation\\nApproach to the Gendered Pronoun Resolution Chal-\\nlenge. In Proceedings of the Workshop on Gender\\nBias in Natural Language Processing, pages 120–\\n125, Florence, Italy.\\nHaochen Liu, Jamell Dacon, Wenqi Fan, Hui Liu, Zi-\\ntao Liu, and Jiliang Tang. 2019. Does Gender Mat-\\nter? Towards Fairness in Dialogue Systems. arXiv\\npreprint arXiv:1910.10486.\\nFelipe Alfaro Lois, José A.R. Fonollosa, and Costa-jà.\\n2019. BERT Masked Language Modeling for Co-\\nreference Resolution. In Proceedings of the Work-\\nshop on Gender Bias in Natural Language Process-\\ning, pages 76–81, Florence, Italy.\\nBrandon C. Loudermilk. 2015. Implicit attitudes and\\nthe perception of sociolinguistic variation. In Alexei\\nPrikhodkine and Dennis R. Preston, editors, Re-\\nsponses to Language Varieties:\\nVariability, pro-\\ncesses and outcomes, pages 137–156.\\nAnastassia Loukina, Nitin Madnani, and Klaus Zech-\\nner. 2019. The many dimensions of algorithmic fair-\\nness in educational applications. In Proceedings of\\nthe Workshop on Innovative Use of NLP for Build-\\ning Educational Applications, pages 1–10, Florence,\\nItaly.\\nKaiji Lu, Peter Mardziel, Fangjing Wu, Preetam Aman-\\ncharla, and Anupam Datta. 2018.\\nGender bias in\\nneural natural language processing. arXiv preprint\\narXiv:1807.11714.\\nAnne Maass. 1999. Linguistic intergroup bias: Stereo-\\ntype perpetuation through language.\\nAdvances in\\nExperimental Social Psychology, 31:79–121.\\nNitin Madnani, Anastassia Loukina, Alina von Davier,\\nJill Burstein, and Aoife Cahill. 2017. Building Bet-\\nter Open-Source Tools to Support Fairness in Auto-\\nmated Scoring. In Proceedings of the Workshop on\\nEthics in Natural Language Processing, pages 41–\\n52, Valencia, Spain.\\nThomas Manzini, Yao Chong Lim, Yulia Tsvetkov, and\\nAlan W. Black. 2019. Black is to Criminal as Cau-\\ncasian is to Police: Detecting and Removing Multi-\\nclass Bias in Word Embeddings. In Proceedings of\\nthe North American Association for Computational\\nLinguistics (NAACL), pages 801–809, Minneapolis,\\nMN.\\nRamón Antonio Martínez and Alexander Feliciano\\nMejía. 2019.\\nLooking closely and listening care-\\nfully: A sociocultural approach to understanding\\nthe complexity of Latina/o/x students’ everyday lan-\\nguage. Theory Into Practice.\\nRowan Hall Maudslay, Hila Gonen, Ryan Cotterell,\\nand Simone Teufel. 2019. It’s All in the Name: Mit-\\nigating Gender Bias with Name-Based Counterfac-\\ntual Data Substitution. In Proceedings of Empirical\\nMethods in Natural Language Processing (EMNLP),\\npages 5270–5278, Hong Kong, China.\\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\\nBowman, and Rachel Rudinger. 2019. On Measur-\\ning Social Biases in Sentence Encoders. In Proceed-\\nings of the North American Association for Compu-\\ntational Linguistics (NAACL), pages 629–634, Min-\\nneapolis, MN.\\nElijah Mayﬁeld,\\nMichael Madaio,\\nShrimai Prab-\\nhumoye, David Gerritsen, Brittany McLaughlin,\\nEzekiel Dixon-Roman, and Alan W. Black. 2019.\\nEquity Beyond Bias in Language Technologies for\\nEducation. In Proceedings of the Workshop on Inno-\\nvative Use of NLP for Building Educational Appli-\\ncations, Florence, Italy.\\nKatherine McCurdy and O˘guz Serbetçi. 2017. Gram-\\nmatical gender associations outweigh topical gender\\nbias in crosslinguistic word embeddings.\\nIn Pro-\\nceedings of the Workshop for Women & Underrepre-\\nsented Minorities in Natural Language Processing,\\nVancouver, Canada.\\n\\nNinareh Mehrabi, Thamme Gowda, Fred Morstatter,\\nNanyun Peng, and Aram Galstyan. 2019. Man is to\\nPerson as Woman is to Location: Measuring Gender\\nBias in Named Entity Recognition. arXiv preprint\\narXiv:1910.10872.\\nMichela Menegatti and Monica Rubini. 2017. Gender\\nbias and sexism in language. In Oxford Research\\nEncyclopedia of Communication. Oxford University\\nPress.\\nInom Mirzaev, Anthony Schulte, Michael Conover, and\\nSam Shah. 2019. Considerations for the interpreta-\\ntion of bias measures of word embeddings. arXiv\\npreprint arXiv:1906.08379.\\nSalikoko S. Mufwene, Guy Bailey, and John R. Rick-\\nford, editors. 1998.\\nAfrican-American English:\\nStructure, History, and Use. Routledge.\\nMichael J. Muller. 2007.\\nParticipatory Design: The\\nThird Space in HCI. In The Human-Computer Inter-\\naction Handbook, pages 1087–1108. CRC Press.\\nMoin\\nNadeem,\\nAnna\\nBethke,\\nand\\nSiva\\nReddy.\\n2020.\\nStereoSet:\\nMeasuring stereotypical bias\\nin pretrained language models.\\narXiv preprint\\narXiv:2004.09456.\\nDong Nguyen, Rilana Gravel, Dolf Trieschnigg, and\\nTheo Meder. 2013.\\n“How Old Do You Think I\\nAm?”: A Study of Language and Age in Twitter. In\\nProceedings of the Conference on Web and Social\\nMedia (ICWSM), pages 439–448, Boston, MA.\\nMalvina Nissim, Rik van Noord, and Rob van der Goot.\\n2020. Fair is better than sensational: Man is to doc-\\ntor as woman is to doctor. Computational Linguis-\\ntics.\\nDebora Nozza, Claudia Volpetti, and Elisabetta Fersini.\\n2019. Unintended Bias in Misogyny Detection. In\\nProceedings of the Conference on Web Intelligence,\\npages 149–155.\\nAlexandra Olteanu, Carlos Castillo, Fernando Diaz,\\nand Emre Kıcıman. 2019.\\nSocial Data: Biases,\\nMethodological Pitfalls, and Ethical Boundaries.\\nFrontiers in Big Data, 2.\\nAlexandra Olteanu, Kartik Talamadupula, and Kush R.\\nVarshney. 2017. The Limits of Abstract Evaluation\\nMetrics: The Case of Hate Speech Detection.\\nIn\\nProceedings of the ACM Web Science Conference,\\nTroy, NY.\\nOrestis Papakyriakopoulos, Simon Hegelich, Juan Car-\\nlos Medina Serrano, and Fabienne Marco. 2020.\\nBias in word embeddings.\\nIn Proceedings of the\\nConference on Fairness, Accountability, and Trans-\\nparency, pages 446–457, Barcelona, Spain.\\nJi Ho Park, Jamin Shin, and Pascale Fung. 2018. Re-\\nducing Gender Bias in Abusive Language Detection.\\nIn Proceedings of Empirical Methods in Natural\\nLanguage Processing (EMNLP), pages 2799–2804,\\nBrussels, Belgium.\\nEllie Pavlick and Tom Kwiatkowski. 2019. Inherent\\nDisagreements in Human Textual Inferences. Trans-\\nactions of the Association for Computational Lin-\\nguistics, 7:677–694.\\nXiangyu Peng, Siyan Li, Spencer Frazier, and Mark\\nRiedl. 2020. Fine-Tuning a Transformer-Based Lan-\\nguage Model to Avoid Generating Non-Normative\\nText. arXiv preprint arXiv:2001.08764.\\nRadomir Popovi´c, Florian Lemmerich, and Markus\\nStrohmaier. 2020.\\nJoint Multiclass Debiasing of\\nWord Embeddings. In Proceedings of the Interna-\\ntional Symposium on Intelligent Systems, Graz, Aus-\\ntria.\\nVinodkumar Prabhakaran, Ben Hutchinson, and Mar-\\ngaret Mitchell. 2019. Perturbation Sensitivity Anal-\\nysis to Detect Unintended Model Biases.\\nIn Pro-\\nceedings of Empirical Methods in Natural Language\\nProcessing (EMNLP), pages 5744–5749,\\nHong\\nKong, China.\\nShrimai Prabhumoye, Elijah Mayﬁeld, and Alan W.\\nBlack. 2019. Principled Frameworks for Evaluating\\nEthics in NLP Systems. In Proceedings of the Work-\\nshop on Innovative Use of NLP for Building Educa-\\ntional Applications, Florence, Italy.\\nMarcelo Prates, Pedro Avelar, and Luis C. Lamb. 2019.\\nAssessing gender bias in machine translation: A\\ncase study with google translate. Neural Computing\\nand Applications.\\nRasmus Précenth. 2019. Word embeddings and gender\\nstereotypes in Swedish and English. Master’s thesis,\\nUppsala University.\\nDennis R. Preston. 2009.\\nAre you really smart (or\\nstupid, or cute, or ugly, or cool)? Or do you just talk\\nthat way? Language attitudes, standardization and\\nlanguage change. Oslo: Novus forlag, pages 105–\\n129.\\nFlavien Prost, Nithum Thain, and Tolga Bolukbasi.\\n2019. Debiasing Embeddings for Reduced Gender\\nBias in Text Classiﬁcation. In Proceedings of the\\nWorkshop on Gender Bias in Natural Language Pro-\\ncessing, pages 69–75, Florence, Italy.\\nReid Pryzant, Richard Diehl Martinez, Nathan Dass,\\nSadao Kurohashi, Dan Jurafsky, and Diyi Yang.\\n2020. Automatically Neutralizing Subjective Bias\\nin Text. In Proceedings of the AAAI Conference on\\nArtiﬁcial Intelligence (AAAI), New York, NY.\\nArun K. Pujari, Ansh Mittal, Anshuman Padhi, An-\\nshul Jain, Mukesh Jadon, and Vikas Kumar. 2019.\\nDebiasing Gender biased Hindi Words with Word-\\nembedding.\\nIn Proceedings of the International\\nConference on Algorithms, Computing and Artiﬁcial\\nIntelligence, pages 450–456.\\nYusu Qian, Urwa Muaz, Ben Zhang, and Jae Won\\nHyun. 2019.\\nReducing gender bias in word-level\\n\\nlanguage models with a gender-equalizing loss func-\\ntion. In Proceedings of the ACL Student Research\\nWorkshop, pages 223–228, Florence, Italy.\\nShauli Ravfogel, Yanai Elazar, Hila Gonen, Michael\\nTwiton, and Yoav Goldberg. 2020.\\nNull It Out:\\nGuarding Protected Attributes by Iterative Nullspace\\nProjection.\\nIn Proceedings of the Association for\\nComputational Linguistics (ACL).\\nJohn R. Rickford and Sharese King. 2016. Language\\nand linguistics on trial: Hearing Rachel Jeantel (and\\nother vernacular speakers) in the courtroom and be-\\nyond. Language, 92(4):948–988.\\nAnthony Rios. 2020. FuzzE: Fuzzy Fairness Evalua-\\ntion of Offensive Language Classiﬁers on African-\\nAmerican English. In Proceedings of the AAAI Con-\\nference on Artiﬁcial Intelligence (AAAI), New York,\\nNY.\\nGerald Roche. 2019.\\nArticulating language oppres-\\nsion: colonialism, coloniality and the erasure of Ti-\\nbetâ ˘A´Zs minority languages. Patterns of Prejudice.\\nAlexey Romanov, Maria De-Arteaga, Hanna Wal-\\nlach, Jennifer Chayes, Christian Borgs, Alexandra\\nChouldechova, Sahin Geyik, Krishnaram Kentha-\\npadi, Anna Rumshisky, and Adam Tauman Kalai.\\n2019. What’s in a Name? Reducing Bias in Bios\\nwithout Access to Protected Attributes. In Proceed-\\nings of the North American Association for Com-\\nputational Linguistics (NAACL), pages 4187–4195,\\nMinneapolis, MN.\\nJonathan Rosa. 2019. Contesting Representations of\\nMigrant “Illegality” through the Drop the I-Word\\nCampaign: Rethinking Language Change and So-\\ncial Change.\\nIn Netta Avineri, Laura R. Graham,\\nEric J. Johnson, Robin Conley Riner, and Jonathan\\nRosa, editors, Language and Social Justice in Prac-\\ntice. Routledge.\\nJonathan Rosa and Christa Burdick. 2017. Language\\nIdeologies.\\nIn Ofelia García, Nelson Flores, and\\nMassimiliano Spotti, editors, The Oxford Handbook\\nof Language and Society. Oxford University Press.\\nJonathan Rosa and Nelson Flores. 2017.\\nUnsettling\\nrace and language: Toward a raciolinguistic perspec-\\ntive. Language in Society, 46:621–647.\\nSara Rosenthal and Kathleen McKeown. 2011.\\nAge\\nPrediction in Blogs: A Study of Style, Content, and\\nOnline Behavior in Pre- and Post-Social Media Gen-\\nerations. In Proceedings of the North American As-\\nsociation for Computational Linguistics (NAACL),\\npages 763–772, Portland, OR.\\nCandace\\nRoss,\\nBoris\\nKatz,\\nand\\nAndrei\\nBarbu.\\n2020.\\nMeasuring Social Biases in Grounded Vi-\\nsion and Language Embeddings.\\narXiv preprint\\narXiv:2002.08911.\\nRichard Rothstein. 2017. The Color of Law: A For-\\ngotten History of How Our Government Segregated\\nAmerica. Liveright Publishing.\\nDavid Rozado. 2020. Wide range screening of algo-\\nrithmic bias in word embedding models using large\\nsentiment lexicons reveals underreported bias types.\\nPLOS One.\\nElayne Ruane, Abeba Birhane, and Anthony Ven-\\ntresque. 2019. Conversational AI: Social and Ethi-\\ncal Considerations. In Proceedings of the Irish Con-\\nference on Artiﬁcial Intelligence and Cognitive Sci-\\nence, Galway, Ireland.\\nRachel Rudinger,\\nChandler May,\\nand Benjamin\\nVan Durme. 2017. Social bias in elicited natural lan-\\nguage inferences. In Proceedings of the Workshop\\non Ethics in Natural Language Processing, pages\\n74–79, Valencia, Spain.\\nRachel Rudinger, Jason Naradowsky, Brian Leonard,\\nand Benjamin Van Durme. 2018.\\nGender Bias\\nin Coreference Resolution.\\nIn Proceedings of the\\nNorth American Association for Computational Lin-\\nguistics (NAACL), pages 8–14, New Orleans, LA.\\nElizabeth B.N. Sanders. 2002. From user-centered to\\nparticipatory design approaches. In Jorge Frascara,\\neditor, Design and the Social Sciences: Making Con-\\nnections, pages 18–25. CRC Press.\\nBrenda Salenave Santana, Vinicius Woloszyn, and Le-\\nandro Krug Wives. 2018. Is there gender bias and\\nstereotype in Portuguese word embeddings?\\nIn\\nProceedings of the International Conference on the\\nComputational Processing of Portuguese Student Re-\\nsearch Workshop, Canela, Brazil.\\nMaarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,\\nand Noah A. Smith. 2019. The risk of racial bias in\\nhate speech detection. In Proceedings of the Asso-\\nciation for Computational Linguistics (ACL), pages\\n1668–1678, Florence, Italy.\\nMaarten Sap, Saadia Gabriel, Lianhui Qin, Dan Juraf-\\nsky, Noah A. Smith, and Yejin Choi. 2020. Social\\nBias Frames: Reasoning about Social and Power Im-\\nplications of Language. In Proceedings of the Asso-\\nciation for Computational Linguistics (ACL).\\nHanna Sassaman, Jennifer Lee, Jenessa Irvine, and\\nShankar Narayan. 2020.\\nCreating Community-\\nBased Tech Policy: Case Studies, Lessons Learned,\\nand What Technologists and Communities Can Do\\nTogether. In Proceedings of the Conference on Fair-\\nness, Accountability, and Transparency, Barcelona,\\nSpain.\\nDanielle Saunders and Bill Byrne. 2020.\\nReducing\\nGender Bias in Neural Machine Translation as a Do-\\nmain Adaptation Problem. In Proceedings of the As-\\nsociation for Computational Linguistics (ACL).\\nTyler Schnoebelen. 2017.\\nGoal-Oriented Design for\\nEthical Machine Learning and NLP. In Proceedings\\nof the Workshop on Ethics in Natural Language Pro-\\ncessing, pages 88–93, Valencia, Spain.\\n\\nSabine Sczesny, Magda Formanowicz, and Franziska\\nMoser. 2016. Can gender-fair language reduce gen-\\nder stereotyping and discrimination?\\nFrontiers in\\nPsychology, 7.\\nJoão Sedoc and Lyle Ungar. 2019. The Role of Pro-\\ntected Class Word Lists in Bias Identiﬁcation of Con-\\ntextualized Word Representations. In Proceedings\\nof the Workshop on Gender Bias in Natural Lan-\\nguage Processing, pages 55–61, Florence, Italy.\\nProcheta Sen and Debasis Ganguly. 2020. Towards So-\\ncially Responsible AI: Cognitive Bias-Aware Multi-\\nObjective Learning.\\nIn Proceedings of the AAAI\\nConference on Artiﬁcial Intelligence (AAAI), New\\nYork, NY.\\nDeven Shah, H. Andrew Schwartz, and Dirk Hovy.\\n2020. Predictive Biases in Natural Language Pro-\\ncessing Models:\\nA Conceptual Framework and\\nOverview.\\nIn Proceedings of the Association for\\nComputational Linguistics (ACL).\\nJudy Hanwen Shen, Lauren Fratamico, Iyad Rahwan,\\nand Alexander M. Rush. 2018.\\nDarling or Baby-\\ngirl? Investigating Stylistic Bias in Sentiment Anal-\\nysis. In Proceedings of the Workshop on Fairness,\\nAccountability, and Transparency (FAT/ML), Stock-\\nholm, Sweden.\\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\\nand Nanyun Peng. 2019. The Woman Worked as\\na Babysitter: On Biases in Language Generation.\\nIn Proceedings of Empirical Methods in Natural\\nLanguage Processing (EMNLP), pages 3398–3403,\\nHong Kong, China.\\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\\nand Nanyun Peng. 2020.\\nTowards Controllable\\nBiases in Language Generation.\\narXiv preprint\\narXiv:2005.00268.\\nSeungjae Shin, Kyungwoo Song, JoonHo Jang, Hyemi\\nKim, Weonyoung Joo, and Il-Chul Moon. 2020.\\nNeutralizing Gender Bias in Word Embedding with\\nLatent Disentanglement and Counterfactual Genera-\\ntion. arXiv preprint arXiv:2004.03133.\\nJesper Simonsen and Toni Robertson, editors. 2013.\\nRoutledge International Handbook of Participatory\\nDesign. Routledge.\\nGabriel Stanovsky, Noah A. Smith, and Luke Zettle-\\nmoyer. 2019.\\nEvaluating gender bias in machine\\ntranslation. In Proceedings of the Association for\\nComputational Linguistics (ACL), pages 1679–1684,\\nFlorence, Italy.\\nYolande Strengers, Lizhe Qu, Qiongkai Xu, and Jarrod\\nKnibbe. 2020.\\nAdhering, Steering, and Queering:\\nTreatment of Gender in Natural Language Genera-\\ntion. In Proceedings of the Conference on Human\\nFactors in Computing Systems (CHI), Honolulu, HI.\\nTony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang,\\nMai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth\\nBelding, Kai-Wei Chang, and William Yang Wang.\\n2019.\\nMitigating Gender Bias in Natural Lan-\\nguage Processing: Literature Review. In Proceed-\\nings of the Association for Computational Linguis-\\ntics (ACL), pages 1630–1640, Florence, Italy.\\nAdam Sutton, Thomas Lansdall-Welfare, and Nello\\nCristianini. 2018.\\nBiased embeddings from wild\\ndata:\\nMeasuring, understanding and removing.\\nIn Proceedings of the International Symposium\\non Intelligent Data Analysis, pages 328–339, ’s-\\nHertogenbosch, Netherlands.\\nChris Sweeney and Maryam Najaﬁan. 2019. A Trans-\\nparent Framework for Evaluating Unintended De-\\nmographic Bias in Word Embeddings. In Proceed-\\nings of the Association for Computational Linguis-\\ntics (ACL), pages 1662–1667, Florence, Italy.\\nChris Sweeney and Maryam Najaﬁan. 2020. Reduc-\\ning sentiment polarity for demographic attributes\\nin word embeddings using adversarial learning. In\\nProceedings of the Conference on Fairness, Ac-\\ncountability, and Transparency, pages 359–368,\\nBarcelona, Spain.\\nNathaniel Swinger, Maria De-Arteaga, Neil Thomas\\nHeffernan, Mark D.M. Leiserson, and Adam Tau-\\nman Kalai. 2019. What are the biases in my word\\nembedding?\\nIn Proceedings of the Conference on\\nArtiﬁcial Intelligence, Ethics, and Society (AIES),\\nHonolulu, HI.\\nSamson Tan, Shaﬁq Joty, Min-Yen Kan, and Richard\\nSocher. 2020.\\nIt’s Morphin’ Time!\\nCombating\\nLinguistic Discrimination with Inﬂectional Perturba-\\ntions. In Proceedings of the Association for Compu-\\ntational Linguistics (ACL).\\nYi Chern Tan and L. Elisa Celis. 2019.\\nAssessing\\nSocial and Intersectional Biases in Contextualized\\nWord Representations. In Proceedings of the Con-\\nference on Neural Information Processing Systems,\\nVancouver, Canada.\\nJ. Michael Terry, Randall Hendrick, Evangelos Evan-\\ngelou, and Richard L. Smith. 2010.\\nVariable\\ndialect switching among African American chil-\\ndren: Inferences about working memory.\\nLingua,\\n120(10):2463–2475.\\nJoel Tetreault, Daniel Blanchard, and Aoife Cahill.\\n2013. A Report on the First Native Language Iden-\\ntiﬁcation Shared Task. In Proceedings of the Work-\\nshop on Innovative Use of NLP for Building Educa-\\ntional Applications, pages 48–57, Atlanta, GA.\\nMike Thelwall. 2018. Gender Bias in Sentiment Anal-\\nysis. Online Information Review, 42(1):45–57.\\nKristen Vaccaro, Karrie Karahalios, Deirdre K. Mul-\\nligan, Daniel Kluttz, and Tad Hirsch. 2019.\\nCon-\\ntestability in Algorithmic Systems. In Conference\\nCompanion Publication of the 2019 on Computer\\n\\nSupported Cooperative Work and Social Computing,\\npages 523–527, Austin, TX.\\nAmeya Vaidya, Feng Mai, and Yue Ning. 2019. Em-\\npirical Analysis of Multi-Task Learning for Reduc-\\ning Model Bias in Toxic Comment Detection. arXiv\\npreprint arXiv:1909.09758v2.\\nEva Vanmassenhove, Christian Hardmeier, and Andy\\nWay. 2018.\\nGetting Gender Right in Neural Ma-\\nchine Translation.\\nIn Proceedings of Empirical\\nMethods in Natural Language Processing (EMNLP),\\npages 3003–3008, Brussels, Belgium.\\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\\nSharon Qian, Daniel Nevo, Yaron Singer, and Stu-\\nart Shieber. 2020.\\nCausal Mediation Analysis for\\nInterpreting Neural NLP: The Case of Gender Bias.\\narXiv preprint arXiv:2004.12265.\\nTianlu Wang, Xi Victoria Lin, Nazneen Fatema Ra-\\njani, Bryan McCann, Vicente Ordonez, and Caim-\\ning Xiong. 2020.\\nDouble-Hard Debias: Tailoring\\nWord Embeddings for Gender Bias Mitigation. In\\nProceedings of the Association for Computational\\nLinguistics (ACL).\\nZili Wang. 2019. MSnet: A BERT-based Network for\\nGendered Pronoun Resolution.\\nIn Proceedings of\\nthe Workshop on Gender Bias in Natural Language\\nProcessing, pages 89–95, Florence, Italy.\\nKellie Webster, Marta R. Costa-jussà, Christian Hard-\\nmeier, and Will Radford. 2019. Gendered Ambigu-\\nous Pronoun (GAP) Shared Task at the Gender Bias\\nin NLP Workshop 2019. In Proceedings of the Work-\\nshop on Gender Bias in Natural Language Process-\\ning, pages 1–7, Florence, Italy.\\nKellie Webster, Marta Recasens, Vera Axelrod, and Ja-\\nson Baldridge. 2018. Mind the GAP: A balanced\\ncorpus of gendered ambiguous pronouns. Transac-\\ntions of the Association for Computational Linguis-\\ntics, 6:605–618.\\nWalt Wolfram and Natalie Schilling. 2015. American\\nEnglish: Dialects and Variation, 3 edition. Wiley\\nBlackwell.\\nAustin P. Wright, Omar Shaikh, Haekyu Park, Will Ep-\\nperson, Muhammed Ahmed, Stephane Pinel, Diyi\\nYang, and Duen Horng (Polo) Chau. 2020.\\nRE-\\nCAST: Interactive Auditing of Automatic Toxicity\\nDetection Models.\\nIn Proceedings of the Con-\\nference on Human Factors in Computing Systems\\n(CHI), Honolulu, HI.\\nYinchuan Xu and Junlin Yang. 2019. Look again at\\nthe syntax: Relational graph convolutional network\\nfor gendered ambiguous pronoun resolution. In Pro-\\nceedings of the Workshop on Gender Bias in Natu-\\nral Language Processing, pages 96–101, Florence,\\nItaly.\\nKai-Chou Yang, Timothy Niven, Tzu-Hsuan Chou, and\\nHung-Yu Kao. 2019.\\nFill the GAP: Exploiting\\nBERT for Pronoun Resolution. In Proceedings of\\nthe Workshop on Gender Bias in Natural Language\\nProcessing, pages 102–106, Florence, Italy.\\nZekun Yang and Juan Feng. 2020. A Causal Inference\\nMethod for Reducing Gender Bias in Word Embed-\\nding Relations.\\nIn Proceedings of the AAAI Con-\\nference on Artiﬁcial Intelligence (AAAI), New York,\\nNY.\\nDaisy Yoo, Anya Ernest, Soﬁa Serholt, Eva Eriksson,\\nand Peter Dalsgaard. 2019. Service Design in HCI\\nResearch: The Extended Value Co-creation Model.\\nIn Proceedings of the Halfway to the Future Sympo-\\nsium, Nottingham, United Kingdom.\\nBrian Hu Zhang,\\nBlake Lemoine,\\nand Margaret\\nMitchell. 2018. Mitigating unwanted biases with ad-\\nversarial learning. In Proceedings of the Conference\\non Artiﬁcial Intelligence, Ethics, and Society (AIES),\\nNew Orleans, LA.\\nGuanhua Zhang, Bing Bai, Junqi Zhang, Kun Bai, Con-\\nghui Zhu, and Tiejun Zhao. 2020a. Demographics\\nShould Not Be the Reason of Toxicity: Mitigating\\nDiscrimination in Text Classiﬁcations with Instance\\nWeighting.\\nIn Proceedings of the Association for\\nComputational Linguistics (ACL).\\nHaoran Zhang,\\nAmy X. Lu,\\nMohamed Abdalla,\\nMatthew\\nMcDermott,\\nand\\nMarzyeh\\nGhassemi.\\n2020b. Hurtful Words: Quantifying Biases in Clin-\\nical Contextual Word Embeddings. In Proceedings\\nof the ACM Conference on Health, Inference, and\\nLearning.\\nJieyu Zhao, Subhabrata Mukherjee, Saghar Hosseini,\\nKai-Wei Chang, and Ahmed Hassan Awadallah.\\n2020. Gender Bias in Multilingual Embeddings and\\nCross-Lingual Transfer. In Proceedings of the Asso-\\nciation for Computational Linguistics (ACL).\\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cot-\\nterell, Vicente Ordonez, and Kai-Wei Chang. 2019.\\nGender Bias in Contextualized Word Embeddings.\\nIn Proceedings of the North American Association\\nfor Computational Linguistics (NAACL), pages 629–\\n634, Minneapolis, MN.\\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\\ndonez, and Kai-Wei Chang. 2017.\\nMen also like\\nshopping: Reducing gender bias ampliﬁcation us-\\ning corpus-level constraints.\\nIn Proceedings of\\nEmpirical Methods in Natural Language Process-\\ning (EMNLP), pages 2979–2989, Copenhagen, Den-\\nmark.\\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\\ndonez, and Kai-Wei Chang. 2018a.\\nGender Bias\\nin Coreference Resolution: Evaluation and Debias-\\ning Methods. In Proceedings of the North American\\nAssociation for Computational Linguistics (NAACL),\\npages 15–20, New Orleans, LA.\\n\\nJieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai-\\nWei Chang. 2018b. Learning Gender-Neutral Word\\nEmbeddings. In Proceedings of Empirical Methods\\nin Natural Language Processing (EMNLP), pages\\n4847–4853, Brussels, Belgium.\\nAlina Zhiltsova, Simon Caton, and Catherine Mulwa.\\n2019. Mitigation of Unintended Biases against Non-\\nNative English Texts in Sentiment Analysis. In Pro-\\nceedings of the Irish Conference on Artiﬁcial Intelli-\\ngence and Cognitive Science, Galway, Ireland.\\nPei Zhou, Weijia Shi, Jieyu Zhao, Kuan-Hao Huang,\\nMuhao Chen, and Kai-Wei Chang. 2019. Examin-\\ning gender bias in languages with grammatical gen-\\nders. In Proceedings of Empirical Methods in Nat-\\nural Language Processing (EMNLP), pages 5279–\\n5287, Hong Kong, China.\\nRan Zmigrod, S. J. Mielke, Hanna Wallach, and Ryan\\nCotterell. 2019. Counterfactual data augmentation\\nfor mitigating gender stereotypes in languages with\\nrich morphology. In Proceedings of the Association\\nfor Computational Linguistics (ACL), pages 1651–\\n1661, Florence, Italy.\\nA\\nAppendix\\nIn Table 3, we provide examples of the papers’ mo-\\ntivations and techniques across several NLP tasks.\\nA.1\\nCategorization details\\nIn this section, we provide some additional details\\nabout our method—speciﬁcally, our categorization.\\nWhat counts as being covered by an NLP task?\\nWe considered a paper to cover a given NLP task if\\nit analyzed “bias” with respect to that task, but not\\nif it only evaluated overall performance on that task.\\nFor example, a paper examining the impact of miti-\\ngating “bias” in word embeddings on “bias” in sen-\\ntiment analysis would be counted as covering both\\nNLP tasks. In contrast, a paper assessing whether\\nperformance on sentiment analysis degraded after\\nmitigating “bias” in word embeddings would be\\ncounted only as focusing on embeddings.\\nWhat counts as a motivation?\\nWe considered a\\nmotivation to include any description of the prob-\\nlem that motivated the paper or proposed quantita-\\ntive technique, including any normative reasoning.\\nWe excluded from the “Vague/unstated” cate-\\ngory of motivations the papers that participated in\\nthe Gendered Ambiguous Pronoun (GAP) Shared\\nTask at the First ACL Workshop on Gender Bias in\\nNLP. In an ideal world, shared task papers would\\nengage with “bias” more critically, but given the\\nnature of shared tasks it is understandable that they\\ndo not. As a result, we excluded them from our\\ncounts for techniques as well. We cite the papers\\nhere; most propose techniques we would have cate-\\ngorized as “Questionable correlations,” with a few\\nas “Other representational harms” (Abzaliev, 2019;\\nAttree, 2019; Bao and Qiao, 2019; Chada, 2019;\\nIonita et al., 2019; Liu, 2019; Lois et al., 2019;\\nWang, 2019; Xu and Yang, 2019; Yang et al., 2019).\\nWe excluded Dabas et al. (2020) from our survey\\nbecause we could not determine what this paper’s\\nuser study on fairness was actually measuring.\\nFinally, we actually categorized the motivation\\nfor Liu et al. (2019) (i.e., the last row in Table 3) as\\n“Questionable correlations” due to a sentence else-\\nwhere in the paper; had the paragraph we quoted\\nbeen presented without more detail, we would have\\ncategorized the motivation as “Vague/unstated.”\\nA.2\\nFull categorization: Motivations\\nAllocational harms\\nHovy and Spruit (2016);\\nCaliskan et al. (2017); Madnani et al. (2017);\\nDixon et al. (2018); Kiritchenko and Mohammad\\n(2018); Shen et al. (2018); Zhao et al. (2018b);\\nBhaskaran and Bhallamudi (2019); Bordia and\\nBowman (2019); Brunet et al. (2019); Chaloner\\nand Maldonado (2019); De-Arteaga et al. (2019);\\nDev and Phillips (2019); Font and Costa-jussà\\n(2019); James-Sorenson and Alvarez-Melis (2019);\\nKurita et al. (2019); Mayﬁeld et al. (2019); Pu-\\njari et al. (2019); Romanov et al. (2019); Ruane\\net al. (2019); Sedoc and Ungar (2019); Sun et al.\\n(2019); Zmigrod et al. (2019); Hutchinson et al.\\n(2020); Papakyriakopoulos et al. (2020); Ravfo-\\ngel et al. (2020); Strengers et al. (2020); Sweeney\\nand Najaﬁan (2020); Tan et al. (2020); Zhang et al.\\n(2020b).\\nStereotyping\\nBolukbasi\\net\\nal.\\n(2016a,b);\\nCaliskan et al. (2017); McCurdy and Serbetçi\\n(2017); Rudinger et al. (2017); Zhao et al. (2017);\\nCurry and Rieser (2018); Díaz et al. (2018);\\nSantana et al. (2018); Sutton et al. (2018); Zhao\\net al. (2018a,b); Agarwal et al. (2019); Basta et al.\\n(2019); Bhaskaran and Bhallamudi (2019); Bordia\\nand Bowman (2019); Brunet et al. (2019); Cao\\nand Daumé (2019); Chaloner and Maldonado\\n(2019); Cho et al. (2019); Dev and Phillips (2019);\\nFont and Costa-jussà (2019); Gonen and Goldberg\\n(2019); James-Sorenson and Alvarez-Melis (2019);\\nKaneko and Bollegala (2019); Karve et al. (2019);\\nKurita et al. (2019); Lauscher and Glavaš (2019);\\nLee et al. (2019); Manzini et al. (2019); Mayﬁeld\\n\\nCategories\\nNLP task\\nStated motivation\\nMotivations\\nTechniques\\nLanguage\\nmodeling\\n(Bordia and\\nBowman,\\n2019)\\n“Existing biases in data can be ampliﬁed by models and the\\nresulting output consumed by the public can inﬂuence them, en-\\ncourage and reinforce harmful stereotypes, or distort the truth.\\nAutomated systems that depend on these models can take prob-\\nlematic actions based on biased proﬁling of individuals.”\\nAllocational\\nharms,\\nstereotyping\\nQuestionable\\ncorrelations\\nSentiment\\nanalysis\\n(Kiritchenko\\nand\\nMohammad,\\n2018)\\n“Other biases can be inappropriate and result in negative ex-\\nperiences for some groups of people. Examples include, loan\\neligibility and crime recidivism prediction systems...and resumé\\nsorting systems that believe that men are more qualiﬁed to be\\nprogrammers than women (Bolukbasi et al., 2016). Similarly,\\nsentiment and emotion analysis systems can also perpetuate and\\naccentuate inappropriate human biases, e.g., systems that consider\\nutterances from one race or gender to be less positive simply be-\\ncause of their race or gender, or customer support systems that\\nprioritize a call from an angry male over a call from the equally\\nangry female.”\\nAllocational\\nharms, other\\nrepresentational\\nharms (system\\nperformance\\ndifferences w.r.t.\\ntext written by\\ndifferent social\\ngroups)\\nQuestionable\\ncorrelations\\n(differences in\\nsentiment\\nintensity scores\\nw.r.t. text about\\ndifferent social\\ngroups)\\nMachine\\ntranslation\\n(Cho et al.,\\n2019)\\n“[MT training] may incur an association of gender-speciﬁed pro-\\nnouns (in the target) and gender-neutral ones (in the source) for\\nlexicon pairs that frequently collocate in the corpora. We claim\\nthat this kind of phenomenon seriously threatens the fairness of a\\ntranslation system, in the sense that it lacks generality and inserts\\nsocial bias to the inference. Moreover, the input is not fully cor-\\nrect (considering gender-neutrality) and might offend the users\\nwho expect fairer representations.”\\nQuestionable\\ncorrelations,\\nother\\nrepresentational\\nharms\\nQuestionable\\ncorrelations\\nMachine\\ntranslation\\n(Stanovsky\\net al., 2019)\\n“Learned models exhibit social bias when their training data\\nencode stereotypes not relevant for the task, but the correlations\\nare picked up anyway.”\\nStereotyping,\\nquestionable\\ncorrelations\\nStereotyping,\\nother\\nrepresentational\\nharms (system\\nperformance\\ndifferences),\\nquestionable\\ncorrelations\\nType-level\\nembeddings\\n(Zhao et al.,\\n2018b)\\n“However, embeddings trained on human-generated corpora have\\nbeen demonstrated to inherit strong gender stereotypes that re-\\nﬂect social constructs....Such a bias substantially affects down-\\nstream applications....This concerns the practitioners who use\\nthe embedding model to build gender-sensitive applications such\\nas a resume ﬁltering system or a job recommendation system as\\nthe automated system may discriminate candidates based on their\\ngender, as reﬂected by their name. Besides, biased embeddings\\nmay implicitly affect downstream applications used in our daily\\nlives. For example, when searching for ‘computer scientist’ using\\na search engine...a search algorithm using an embedding model in\\nthe backbone tends to rank male scientists higher than females’\\n[sic], hindering women from being recognized and further exac-\\nerbating the gender inequality in the community.”\\nAllocational\\nharms,\\nstereotyping,\\nother\\nrepresentational\\nharms\\nStereotyping\\nType-level\\nand contextu-\\nalized\\nembeddings\\n(May et al.,\\n2019)\\n“[P]rominent word embeddings such as word2vec (Mikolov et\\nal., 2013) and GloVe (Pennington et al., 2014) encode systematic\\nbiases against women and black people (Bolukbasi et al., 2016;\\nGarg et al., 2018), implicating many NLP systems in scaling up\\nsocial injustice.”\\nVague\\nStereotyping\\nDialogue\\ngeneration\\n(Liu et al.,\\n2019)\\n“Since the goal of dialogue systems is to talk with users...if the\\nsystems show discriminatory behaviors in the interactions, the\\nuser experience will be adversely affected. Moreover, public com-\\nmercial chatbots can get resisted for their improper speech.”\\nVague/unstated\\nStereotyping,\\nother\\nrepresentational\\nharms,\\nquestionable\\ncorrelations\\nTable 3: Examples of the categories into which the papers’ motivations and proposed quantitative techniques for\\nmeasuring or mitigating “bias” fall. Bold text in the quotes denotes the content that yields our categorizations.\\n\\net al. (2019); Précenth (2019); Pujari et al. (2019);\\nRuane et al. (2019); Stanovsky et al. (2019);\\nSun et al. (2019); Tan and Celis (2019); Webster\\net al. (2019); Zmigrod et al. (2019); Gyamﬁet al.\\n(2020); Hube et al. (2020); Hutchinson et al.\\n(2020); Kim et al. (2020); Nadeem et al. (2020);\\nPapakyriakopoulos et al. (2020); Ravfogel et al.\\n(2020); Rozado (2020); Sen and Ganguly (2020);\\nShin et al. (2020); Strengers et al. (2020).\\nOther representational harms\\nHovy and Sø-\\ngaard (2015); Blodgett et al. (2016); Bolukbasi\\net al. (2016b); Hovy and Spruit (2016); Blodgett\\nand O’Connor (2017); Larson (2017); Schnoebelen\\n(2017); Blodgett et al. (2018); Curry and Rieser\\n(2018); Díaz et al. (2018); Dixon et al. (2018); Kir-\\nitchenko and Mohammad (2018); Park et al. (2018);\\nShen et al. (2018); Thelwall (2018); Zhao et al.\\n(2018b); Badjatiya et al. (2019); Bagdasaryan et al.\\n(2019); Bamman et al. (2019); Cao and Daumé\\n(2019); Chaloner and Maldonado (2019); Cho et al.\\n(2019); Davidson et al. (2019); De-Arteaga et al.\\n(2019); Fisher (2019); Font and Costa-jussà (2019);\\nGarimella et al. (2019); Loukina et al. (2019); May-\\nﬁeld et al. (2019); Mehrabi et al. (2019); Nozza\\net al. (2019); Prabhakaran et al. (2019); Romanov\\net al. (2019); Ruane et al. (2019); Sap et al. (2019);\\nSheng et al. (2019); Sun et al. (2019); Sweeney\\nand Najaﬁan (2019); Vaidya et al. (2019); Gaut\\net al. (2020); Gencoglu (2020); Hovy et al. (2020);\\nHutchinson et al. (2020); Kim et al. (2020); Peng\\net al. (2020); Rios (2020); Sap et al. (2020); Shah\\net al. (2020); Sheng et al. (2020); Tan et al. (2020);\\nZhang et al. (2020a,b).\\nQuestionable\\ncorrelations\\nJørgensen\\net\\nal.\\n(2015); Hovy and Spruit (2016); Madnani et al.\\n(2017); Rudinger et al. (2017); Zhao et al. (2017);\\nBurns et al. (2018); Dixon et al. (2018); Kir-\\nitchenko and Mohammad (2018); Lu et al. (2018);\\nPark et al. (2018); Shen et al. (2018); Zhang\\net al. (2018); Badjatiya et al. (2019); Bhargava\\nand Forsyth (2019); Cao and Daumé (2019); Cho\\net al. (2019); Davidson et al. (2019); Dev et al.\\n(2019); Garimella et al. (2019); Garg et al. (2019);\\nHuang et al. (2019); James-Sorenson and Alvarez-\\nMelis (2019); Kaneko and Bollegala (2019); Liu\\net al. (2019); Karve et al. (2019); Nozza et al.\\n(2019); Prabhakaran et al. (2019); Romanov et al.\\n(2019); Sap et al. (2019); Sedoc and Ungar (2019);\\nStanovsky et al. (2019); Sweeney and Najaﬁan\\n(2019); Vaidya et al. (2019); Zhiltsova et al. (2019);\\nChopra et al. (2020); Gonen and Webster (2020);\\nGyamﬁet al. (2020); Hube et al. (2020); Ravfogel\\net al. (2020); Rios (2020); Ross et al. (2020); Saun-\\nders and Byrne (2020); Sen and Ganguly (2020);\\nShah et al. (2020); Sweeney and Najaﬁan (2020);\\nYang and Feng (2020); Zhang et al. (2020a).\\nVague/unstated\\nRudinger et al. (2018); Webster\\net al. (2018); Dinan et al. (2019); Florez (2019);\\nJumelet et al. (2019); Lauscher et al. (2019); Liang\\net al. (2019); Maudslay et al. (2019); May et al.\\n(2019); Prates et al. (2019); Prost et al. (2019);\\nQian et al. (2019); Swinger et al. (2019); Zhao\\net al. (2019); Zhou et al. (2019); Ethayarajh (2020);\\nHuang et al. (2020); Jia et al. (2020); Popovi´c et al.\\n(2020); Pryzant et al. (2020); Vig et al. (2020);\\nWang et al. (2020); Zhao et al. (2020).\\nSurveys,\\nframeworks,\\nand\\nmeta-analyses\\nHovy and Spruit (2016); Larson (2017); McCurdy\\nand Serbetçi (2017); Schnoebelen (2017); Basta\\net al. (2019); Ethayarajh et al. (2019); Gonen and\\nGoldberg (2019); Lauscher and Glavaš (2019);\\nLoukina et al. (2019); Mayﬁeld et al. (2019);\\nMirzaev et al. (2019); Prabhumoye et al. (2019);\\nRuane et al. (2019); Sedoc and Ungar (2019); Sun\\net al. (2019); Nissim et al. (2020); Rozado (2020);\\nShah et al. (2020); Strengers et al. (2020); Wright\\net al. (2020).\\nB\\nFull categorization: Techniques\\nAllocational harms\\nDe-Arteaga et al. (2019);\\nProst et al. (2019); Romanov et al. (2019); Zhao\\net al. (2020).\\nStereotyping\\nBolukbasi\\net\\nal.\\n(2016a,b);\\nCaliskan et al. (2017); McCurdy and Serbetçi\\n(2017); Díaz et al. (2018); Santana et al. (2018);\\nSutton et al. (2018); Zhang et al. (2018); Zhao\\net al. (2018a,b); Agarwal et al. (2019); Basta et al.\\n(2019); Bhaskaran and Bhallamudi (2019); Brunet\\net al. (2019); Cao and Daumé (2019); Chaloner\\nand Maldonado (2019); Dev and Phillips (2019);\\nEthayarajh et al. (2019); Gonen and Goldberg\\n(2019); James-Sorenson and Alvarez-Melis (2019);\\nJumelet et al. (2019); Kaneko and Bollegala\\n(2019); Karve et al. (2019); Kurita et al. (2019);\\nLauscher and Glavaš (2019); Lauscher et al.\\n(2019); Lee et al. (2019); Liang et al. (2019); Liu\\net al. (2019); Manzini et al. (2019); Maudslay et al.\\n(2019); May et al. (2019); Mirzaev et al. (2019);\\nPrates et al. (2019); Précenth (2019); Prost et al.\\n(2019); Pujari et al. (2019); Qian et al. (2019);\\n\\nSedoc and Ungar (2019); Stanovsky et al. (2019);\\nTan and Celis (2019); Zhao et al. (2019); Zhou\\net al. (2019); Chopra et al. (2020); Gyamﬁet al.\\n(2020); Nadeem et al. (2020); Nissim et al. (2020);\\nPapakyriakopoulos et al. (2020); Popovi´c et al.\\n(2020); Ravfogel et al. (2020); Ross et al. (2020);\\nRozado (2020); Saunders and Byrne (2020); Shin\\net al. (2020); Vig et al. (2020); Wang et al. (2020);\\nYang and Feng (2020); Zhao et al. (2020).\\nOther representational harms\\nJørgensen et al.\\n(2015); Hovy and Søgaard (2015); Blodgett et al.\\n(2016); Blodgett and O’Connor (2017); Blodgett\\net al. (2018); Curry and Rieser (2018); Dixon et al.\\n(2018); Park et al. (2018); Thelwall (2018); Web-\\nster et al. (2018); Badjatiya et al. (2019); Bag-\\ndasaryan et al. (2019); Bamman et al. (2019); Bhar-\\ngava and Forsyth (2019); Cao and Daumé (2019);\\nFont and Costa-jussà (2019); Garg et al. (2019);\\nGarimella et al. (2019); Liu et al. (2019); Louk-\\nina et al. (2019); Mehrabi et al. (2019); Nozza\\net al. (2019); Sap et al. (2019); Sheng et al. (2019);\\nStanovsky et al. (2019); Vaidya et al. (2019);\\nWebster et al. (2019); Ethayarajh (2020); Gaut\\net al. (2020); Gencoglu (2020); Hovy et al. (2020);\\nHuang et al. (2020); Kim et al. (2020); Peng et al.\\n(2020); Ravfogel et al. (2020); Rios (2020); Sap\\net al. (2020); Saunders and Byrne (2020); Sheng\\net al. (2020); Sweeney and Najaﬁan (2020); Tan\\net al. (2020); Zhang et al. (2020a,b).\\nQuestionable correlations\\nJurgens et al. (2017);\\nMadnani et al. (2017); Rudinger et al. (2017);\\nZhao et al. (2017); Burns et al. (2018); Díaz\\net al. (2018); Kiritchenko and Mohammad (2018);\\nLu et al. (2018); Rudinger et al. (2018); Shen\\net al. (2018); Bordia and Bowman (2019); Cao\\nand Daumé (2019); Cho et al. (2019); David-\\nson et al. (2019); Dev et al. (2019); Dinan et al.\\n(2019); Fisher (2019); Florez (2019); Font and\\nCosta-jussà (2019); Garg et al. (2019); Huang et al.\\n(2019); Liu et al. (2019); Nozza et al. (2019);\\nPrabhakaran et al. (2019); Qian et al. (2019); Sap\\net al. (2019); Stanovsky et al. (2019); Sweeney and\\nNajaﬁan (2019); Swinger et al. (2019); Zhiltsova\\net al. (2019); Zmigrod et al. (2019); Hube et al.\\n(2020); Hutchinson et al. (2020); Jia et al. (2020);\\nPapakyriakopoulos et al. (2020); Popovi´c et al.\\n(2020); Pryzant et al. (2020); Saunders and Byrne\\n(2020); Sen and Ganguly (2020); Shah et al. (2020);\\nSweeney and Najaﬁan (2020); Zhang et al. (2020b).\\nVague/unstated\\nNone.\\nSurveys,\\nframeworks,\\nand\\nmeta-analyses\\nHovy and Spruit (2016); Larson (2017); McCurdy\\nand Serbetçi (2017); Schnoebelen (2017); Basta\\net al. (2019); Ethayarajh et al. (2019); Gonen and\\nGoldberg (2019); Lauscher and Glavaš (2019);\\nLoukina et al. (2019); Mayﬁeld et al. (2019);\\nMirzaev et al. (2019); Prabhumoye et al. (2019);\\nRuane et al. (2019); Sedoc and Ungar (2019); Sun\\net al. (2019); Nissim et al. (2020); Rozado (2020);\\nShah et al. (2020); Strengers et al. (2020); Wright\\net al. (2020).\\n'}, {'source': '/Users/sir/Downloads/Data/PDF/Thoracic organ transplantation.pdf', 'text': 'American Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\nBlackwell Munksgaard\\nBlackwell Munksgaard 2004\\nThoracic organ transplantation\\nRichard N. Pierson IIIa,∗, Mark L. Barrb, Keith P.\\nMcCulloughc, Thomas Egand, Edward Garritye,\\nMariell Jessupf and Susan Murrayg\\naUniversity of Maryland Medical Center and VA,\\nBaltimore, MD;\\nbUniversity of Southern California, Los Angeles, CA;\\ncSRTR/URREA, Ann Arbor, MI;\\ndUniversity of North Carolina at Chapel Hill, Chapel Hill,\\nNC;\\neLoyola University Chicago, Chicago, IL;\\nfUniversity of Pennsylvania, Philadelphia, PA;\\ngSRTR/University of Michigan, Ann Arbor, MI\\n∗Corresponding author: Richard N. Pierson,\\nrpierson@smail.umaryland.edu\\nThis article presents an overview of factors associ-\\nated with thoracic transplantation outcomes over the\\npast decade and provides valuable information regard-\\ning the heart, lung, and heart-lung waiting lists and\\nthoracic organ transplant recipients. Waiting list and\\npost-transplant information is used to assess the im-\\nportance of patient demographics, risk factors, and pri-\\nmary cardiopulmonary disease on outcomes.\\nThe time that the typical listed patient has been\\nwaiting for a heart, lung, or heart-lung transplant\\nhas markedly increased over the past decade, while\\nthe number of transplants performed has declined\\nslightly and survival after transplant has plateaued.\\nWaiting list mortality, however, appears to be declin-\\ning for each organ and for most diseases and high-\\nseverity subgroups, perhaps in response to recent\\nchanges in organ allocation algorithms. Based on per-\\nceived inequity in organ access and in response to a\\nmandate from Health Resources and Services Ad-\\nministration,\\nthe\\nlung\\ntransplant\\ncommunity\\nis\\nNotes on Sources:\\nThe articles in this report are based on the\\nreference tables in the 2003 OPTN/SRTR Annual Report, which\\nare not included in this publication. Many relevant data appear in\\nﬁgures and tables included here; other tables from the Annual\\nReport that serve as the basis for this article include the follow-\\ning: Tables 1.8, 1.12a, 1.14, 11.1–11.5, 11.7, 11.9, 11.11, 11.12,\\n12.1–12.4b, 12.8a, 12.9a, 12.10a, 12.10b, 12.11b, 13.1–13.4, 13.7,\\n13.9a, 13.11, and 13.13. All of these tables are also available online\\nat http://www.ustransplant.org.\\nFunding:\\nThe Scientiﬁc Registry of Transplant Recipients (SRTR)\\nis funded by contract #231-00-0116 from the Health Resources\\nand Services Administration (HRSA). The views expressed herein\\nare those of the authors and not necessarily those of the US Gov-\\nernment. This is a US Government-sponsored work. There are no\\nrestrictions on its use.\\ndeveloping a lung allocation system designed to min-\\nimize deaths on the waiting list while maximizing the\\nbeneﬁt of transplant by incorporating post-transplant\\nsurvival and quality of life into the algorithm. Areas\\nwhere improved data collection could inform evolving\\norgan allocation and candidate selection policies are\\nemphasized.\\nKey words:\\nAllocation policy, deceased donors, graft\\nsurvival, heart transplantation, heart-lung transplanta-\\ntion, living donors, lung transplantation, organ dona-\\ntion, patient survival, SRTR, waiting list\\nIntroduction\\nThe intent of this analysis is to present an annual perspec-\\ntive on the evolution of thoracic transplantation in the USA\\nand to offer insights that may lead to more efﬁcacious al-\\nlocation of donor organs, to be measured as improved net\\noutcomes for the entire population of patients who might\\nbeneﬁt from a thoracic organ transplant procedure. In addi-\\ntion to the survival beneﬁt, quality of life and equity should\\nbe considered integral to any net outcome analysis, and,\\nwhere relevant data are available, perspectives are pre-\\nsented on what this information reveals.\\nData compiled over the past decade for thoracic organ\\ntransplant patients were analyzed to assess the impor-\\ntance of patient demographics, risk factors, and primary\\ncardiopulmonary disease on trends in waiting list time and\\nmortality. Analysis also sought to identify the character-\\nistics of thoracic transplant recipients and their associ-\\nated post-transplant outcomes. The focus in this report is\\non emerging trends either evident from or arguably ob-\\nscured by the available data. Areas where improved data\\ncollection and analysis could favorably inﬂuence heart and\\nlung transplant outcomes through policy are particularly\\nemphasized.\\nUnless otherwise noted, the statistics in this article come\\nfrom reference tables in the 2003 OPTN/SRTR Annual\\nReport. Two companion articles in this report, ‘Trans-\\nplant data: sources, collection, and caveats’ and ‘Ana-\\nlytical approaches for transplant research’, explain the\\nmethods of data collection, organization, and analysis that\\nserve as the basis for this article (1,2). Additional detail\\non the methods of analysis may be found in the ref-\\nerence tables themselves or in the Technical Notes of\\nthe OPTN/SRTR Annual Report, both available online at\\nhttp://www.ustransplant.org.\\n93\\n\\nRichard N. Pierson III et al.\\nHeart\\nHeart waiting list characteristics\\nConﬁrming a trend that became evident at the recent turn\\nof the century, the number of total registrants on the heart\\ntransplant waiting list decreased again in 2002, from 3934\\nin 2001 to 3803. This number still represents a large in-\\ncrease over the 2798 candidates who were listed at the end\\nof 1993. Nevertheless, the waiting list growth observed in\\nthe early 1990s seems to have peaked in the USA, as in the\\nUK (3). The decline in total numbers of registrants has been\\nassociated with a gradual increase in average candidate\\nage. While the percentage of registrants aged 1–34 years\\nhas remained fairly constant over the past 10 years, the\\nproportion of potential recipients aged 65 years or older\\nhas increased from 5% in 1993 to 12% in 2002, at the\\nrelative expense of younger adults (Figure 1).\\nMost waiting list characteristics have not substantially\\nchanged since the last report and reﬂect the racial charac-\\nteristics of the US population as a whole (Figure 2). Notably,\\nthe percentage of Hispanic/Latino registrants has grown\\nSource: 2003 OPTN/SRTR Annual Report, Table 11.1. \\n0\\n20\\n40\\n60\\n80\\n100\\n1993 1994 1995 1996 1997 1998 1999 2000 2001 2002\\nYear\\nPatients (%)\\n1–10\\n11–34\\n35–49\\n50–64\\n65+\\nFigure 1. Age distribution of heart waiting list registrants at year-\\nend, 1993–2002.\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\nWhite\\nAfrican\\nAmerican\\nAsian\\nOther/\\nMulti-race\\nRace\\n Patients (%)\\nWaiting List\\nUS Population*\\nSource: 2003 OPTN/SRTR Annual Report, Table 11.1. \\n*Data from 2000 U.S. Census.\\nFigure 2. Heart waiting list registrants, by race, 2002.\\nSource: 2003 OPTN/SRTR Annual Report, Table 11.1. \\n0\\n20\\n40\\n60\\n80\\n100\\n1993 1994 1995 1996 1997 1998 1999 2000 2001 2002\\nYear\\nPatients (%)\\n<1 year\\n1–2 years\\n>2 years\\nFigure 3. Heart waiting list registrants, by median time waiting,\\n1993–2002.\\nfrom 4% at the end of 1993 to 8% in 2002. Although\\nthey are now our most populous minority, relative under-\\nrepresentation of this ethnic group probably reﬂects their\\ncomparative youth, although socioeconomic factors may\\nalso contribute. The actual number of female candidates\\nhas stabilized over the past few years, but their total per-\\ncentage has increased as the number of male candidates\\nhas decreased. Adult females with heart failure have less\\ncoronary disease and tend to be older than their male co-\\nhorts, a factor that may contribute to the aging of the over-\\nall waiting list (4,5). Alternatively, the older average age of\\nthe waiting list may reﬂect a willingness to list carefully\\nselected patients over the age of 60 years, because their\\noutcome has not substantially differed from younger can-\\ndidates after transplant.\\nIn the past 10 years, there has been a marked increase\\nin the time that the typical patient listed at year-end has\\nwaited for a transplant, as depicted in Figure 3. At the end\\nof 2002, 48% of candidates had spent more than 2 years\\non the waiting list, compared with 17% in 1993. In addi-\\ntion, 1742 registrants were listed as temporarily inactive at\\nyear-end (about 46% of the entire cohort), up from 37% in\\n1999. There are a variety of reasons why a candidate may\\nbecome inactive, and it is difﬁcult to determine why this\\npercentage has risen over the years. Whether listing prac-\\ntice, improvement in medical management, or other fac-\\ntors are responsible is a question worthy of further inquiry.\\nIt is also clear that some registrants are removed from the\\nlist for reasons other than death or transplant. In 2002, for\\nexample, there were 571 candidates removed from the list\\nfor reasons other than death, transplantation, or transfer,\\n48% of whom were noted to have improved signiﬁcantly\\nand no longer required heart transplantation. Delineation\\nof reasons for inactive listing and removal from the waiting\\nlist could improve our understanding of this phenomenon\\nand guide appropriate policy development.\\nSince 1999, the highest priority candidates have been\\nsorted into Status 1A and 1B, with the latter listing\\n94\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n\\nThoracic organ transplantation\\ncategory consistently about three times larger than the for-\\nmer. Interestingly, the median time to transplant for both\\nStatus 1A and 1B candidates has steadily dropped since\\nthe 1A/1B system was implemented in 1999; the median\\ntime for candidates originally listed at Status 1A decreased\\nfrom 145 days in 1999 to 94 days in 2002. Many factors that\\nare not apparent in this data set—interregional variation in\\nstatus at listing or transplant, how this changes over time,\\nand characteristics that distinguish 1A from 1B patients—\\nwould provide valuable information to assess efﬁcacy and\\nequity of the evolving allocation paradigm.\\nOverall, there has been a declining trend to the median\\ntime to transplant for patients over 10 years old (Figure 4).\\nAs has been true for decades, registrants with blood type O\\ncontinue to wait much longer than those with other blood\\ntypes. Whereas race/ethnicity does not seem to correlate\\nwith differences in waiting time, women had a shorter\\nmedian time to transplant than did men (179 vs. 247 days\\nin 2002). This pattern has persisted over several years and\\nmay reﬂect relatively greater access of women, who have\\nlower average weight and height than men, to a larger pro-\\nportion of donors of either sex.\\nOver the 10 years of this report, waiting list mortality rates\\nhave declined steadily (Figure 5). There were 143.8 deaths\\nper 1000 years at risk in 2002, the lowest rate in the last\\n10 years. This trend in survival improvement was seen in\\nmost age and racial subgroups. Improved medical manage-\\nment of heart failure (such as increasing use of implanted\\ncardiac deﬁbrillators) to prevent sudden death in wait-listed\\npatients, more optimal timing of listing for transplant, and\\nthe altered organ allocation policy implemented in 1999 all\\nprobably contributed to this favorable trend. It is also pos-\\nsible, however, that transplant teams removed candidates\\nfrom the active list when death was imminent, artiﬁcially\\nreducing program-speciﬁc and overall waiting list mortal-\\nity. Although the total number of candidates initially listed\\nas Status 1A has steadily increased from 1999 to the end\\nof 2002, the death rate for this critically ill group has also\\ndeclined dramatically, from 1399 per 1000 years at risk in\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n1993\\n1994\\n1995\\n1996\\n1997\\n1998\\n1999\\n2000\\n2001\\n2002\\nAge\\nMedian time to transplant (days)\\n1–10\\n11–34\\n35–49\\n50–64\\n65+\\nSource: 2003 OPTN/SRTR Annual Report, Table 11.2. \\nFigure 4. Median time to heart transplant by recipient age, 1993–\\n2002.\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\n800\\n900\\n1993\\n1994\\n1995\\n1996\\n1997\\n1998\\n1999\\n2000\\n2001\\n2002\\nYear\\nAnnual Death Rate \\n(per 1000 patient years at risk)\\n1–10\\n11–34\\n35–49\\n50–64\\n65 +\\nSource: 2003 OPTN/SRTR Annual Report, Table 11.3.\\nFigure 5. Annual death rates for heart transplant registrants by\\nage, 1993–2002.\\n1999 to 785 in 2002. Candidates with blood type AB appear\\nto have beneﬁted the least, in that waiting list mortality for\\nthis relatively small group (17 of 558 deaths among 6990\\nlisted patients in 2002) has not fallen.\\nIn summary, despite great advances in the detection, pre-\\nvention, and management of heart failure, there continue\\nto be more candidates for heart transplant than there are\\navailable donor organs deemed suitable for use (6–8). Af-\\nter being placed on the waiting list, patients face one of\\nthree competing outcomes: transplantation, death on the\\nwaiting list, or removal from the waiting list. The relative\\nrate of heart transplant has remained level and death on\\nthe waiting list has apparently decreased, therefore one is\\nforced to conclude that the rate of removal from the wait-\\ning list has increased correspondingly. The fate of these\\npatients is not currently accessible, thus the signiﬁcance\\nof this observation is uncertain. This limitation to continued\\ncandidate listing appears to have forestalled the impending\\ncrisis predicted for cardiac transplantation in 1994 (9). If we\\nwere to learn that the decrease in waiting list mortality rate\\nis an artifact of program policies rather than more timely\\naccess to life-saving organs for a shrinking population in\\nneed, the implications for organ allocation policy would be\\nfundamentally altered.\\nHeart transplant recipient characteristics\\nAfter rising steadily during the 1980s and early 1990s,\\nthe total number of heart transplants performed has de-\\nclined by about 8% over the last 5 years, from a mid-1990s\\nplateau of about 2350 to about 2150 per year over the past\\n4 years (Figure 6). Although patients between the ages of\\n35 and 64 years continue to receive the majority of donated\\nhearts (67%), the proportion of all recipients in this group\\nhas declined by about 9% since 1993, as the number of\\nrecipients over 65 has increased from 5% to 10% of those\\ntransplanted. Women now constitute 28% of recipients,\\nup from 22% a decade ago. The relative rate of transplan-\\ntation per million population has declined for men to 2.7\\nfrom 3.8 in 1993 (Figure 7). These data may reﬂect the in-\\ncreasing incidence of end-stage heart failure in women and\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n95\\n\\nRichard N. Pierson III et al.\\nSource: 2003 OPTN/SRTR Annual Report, Table 11.4.\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\n1993 1994 1995 1996 1997 1998 1999 2000 2001 2002\\nYear\\nNumber of Patients\\n<11\\n11–17\\n18–34\\n35–64\\n65+\\nFigure 6. Heart transplant distribution by age, 1993–2002.\\n0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\n1993 1994 1995 1996 1997 1998 1999 2000 2001 2002\\nYear\\nRelative rate of transplant\\nSource: 2003 OPTN/SRTR Annual Report, Table 11.5. \\nFigure 7. Incidence rate of heart transplant male relative to fe-\\nmale, 1993–2002.\\nthe persistent inﬂuence of a younger average age at onset\\nin men. Racial and ethnic breakdowns are similar over the\\npast 5 years, except that the recent trend towards an in-\\ncreasing proportion of Hispanic/Latino recipients appears\\nto have stabilized at about 8% of all heart recipients, in\\nconcert with their relative representation on the waiting\\nlist.\\nFrom 1999 to 2002, the distribution of patient status at\\ntransplant remained stable; about 38% of patients trans-\\nplanted were classiﬁed at Status 1A, 36% at Status 1B,\\nand 26% at Status 2. Sixty-ﬁve percent of heart recipients\\nwere on life support (principally inotrope infusion or ven-\\ntricular assist devices, or VAD) at the time of transplant in\\n2002, unchanged for the most part since 1995. The per-\\ncentage of recipients who were hospitalized at the time\\nof transplant has declined from a high of 68% in 1997 to\\n53% in 2002. Similarly, the percentage of those in the in-\\ntensive care unit at the time of transplant has dropped from\\n59% in 1997 to 34% in 2002. These trends are an intended\\nconsequence of the policy change implemented in 1999,\\nwhen patient location (in or out of hospital) was removed\\nfrom the allocation algorithm as a dominant factor. No ad-\\nverse impact on waiting list or post-transplant mortality is\\nevident in the available data, although the uncertain fate of\\npatients removed from the waiting list (discussed above)\\nlends some uncertainty to this apparently favorable trend.\\nThe consequences of this policy change and associated\\ntrends on costs, quality of life, physician practices, and pa-\\ntient safety while waiting would be fruitful areas for study,\\nto identify opportunities for further policy improvement.\\nCoronary artery disease and cardiomyopathy continue to\\nbe the primary diagnoses associated with heart transplant,\\nas they are with listing, and are represented in similar pro-\\nportions (approximately 45% each) over the past 10 years.\\nCongenital heart disease accounts for about 8%, and re-\\ntransplantation, most often for primary graft failure or car-\\ndiac allograft vasculopathy, remains uncommon, account-\\ning for about 3% of both listings and transplants in recent\\nyears.\\nUnadjusted heart recipient survival at 1 year has risen\\nslowly, from 81% for patients transplanted in 1992 to\\n86% in 2001, and at 5 years exceeds 70% for the co-\\nhort of patients transplanted in 1996–1997. Graft survival\\nis only slightly lower, as expected for an organ where near-\\nterm survival is critically dependent on initial function of a\\nscarce graft and where mechanical circulatory support in\\nthe event of initial graft failure is highly morbid. Survival\\ntrends evaluated by race, ethnicity, gender, blood type,\\nlife support requirement, and location (in or out of hospi-\\ntal) did not change appreciably relative to the 2002 report.\\nAmong all demographic parameters, a history of prior heart\\ntransplant and African-American race portended the worst\\n5-year graft survival when compared with primary trans-\\nplant and other racial categories, respectively. Efforts to\\nprevent graft failure, attenuate the physiologic burden of\\nchronic immunosuppression, and understand how racial\\ndifferences interact with survival may lead to improved out-\\ncomes for these important patient subgroups and, thus, to\\nincremental improvement in overall outcomes.\\nOne-year graft survival by primary diagnosis remains simi-\\nlar for cardiomyopathy (88%) and coronary artery disease\\n(85%); 5-year graft survival rates were highest for valvular\\nheart disease (76%) and cardiomyopathy (72%). After re-\\nmaining very constant for 6 years, the overall death rate\\nwithin the ﬁrst year after transplant fell in 2002, from 161–\\n179 to 121 per 1000 patient years at risk. While incomplete\\nfollow-up for patients transplanted in the most recent year\\nprobably overestimates the actual increase in patient sur-\\nvival, the death rate within the ﬁrst year for patients trans-\\nplanted in each category of urgency (1A, 1B, 2) at the time\\nof transplant appears to be declining. Survival at 1 year is\\nsimilar for Status 1B and Status 2 (88%), and is lower for\\nStatus 1A (81%), with the difference appearing within the\\nﬁrst 3 months. Early mortality is thus probably explained\\nby known risk factors that qualify patients for the 1A cat-\\negory, including pathophysiologic events complicating the\\ninitial transplant episode in patients who are unstable be-\\nfore transplant, such as those with infected VADs or requir-\\n96\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n\\nThoracic organ transplantation\\ning mechanical ventilation. Logically, better approaches to\\nmanage patients on life support could reduce this early\\nattrition, while identiﬁcation of low-risk windows of oppor-\\ntunity within which to perform the transplant after invasive\\nor intensive support measures could justify policy changes\\ndesigned to take advantage of this information.\\nSurvival data are not currently collected separately for tech-\\nnical variations (bicaval right atrial or total atrioventricular\\nvs. conventional biatrial cuff) on the prevalent orthotopic\\nsurgical approach. While heterotopic (‘piggy-back’) heart\\ntransplantation is rare (4–21 per year in the USA since\\n1993), patient survival for this procedure is decreased at\\n1 year (82%) and 5 years (35%) compared with orthotopic\\ntransplantation (86% and 72%, respectively). Early mortal-\\nity is surprisingly low despite selection of high-risk patients\\n(typically those with ﬁxed elevation of pulmonary vascu-\\nlar resistance) to undergo this uncommon and technically\\ndemanding procedure. If causes of adverse late outcome\\nwere known and could be addressed, it is conceivable that\\nthis technique could allow safer use of expanded criteria\\ndonor hearts without adversely inﬂuencing recipient out-\\ncome.\\nDonor age remains a signiﬁcant risk factor for adverse\\nintermediate-term outcome. Heart transplant numbers and\\nrates are declining despite a signiﬁcant increase in av-\\nerage organ donor age and in heart donor age over the\\npast decade, while demographics suggest that demand\\nfor hearts should increasingly outstrip supply. Considered\\ntogether, these facts suggest that heart transplant teams\\nmay be reluctant to use older organs. Efforts to under-\\nstand the causes of increased risk with increasing heart\\ndonor age will be important to identify solutions and thus\\njustify increased use of these organs. In particular, better\\nprediction of who is likely to beneﬁt from receipt of an\\nolder donor heart would be valuable. These imperatives\\nwill be tempered if results using long-term mechanical\\ncirculatory support devices improve to equal those with\\ntransplantation.\\nWhen analyzed by recipient age, intermediate term sur-\\nvival data show that the very young (those less than 1\\nyear old, about 70 per year) and patients between 50 and\\n64 years of age (about 1100 per year) have relatively low\\nmortality after the ﬁrst year (about 14% additional attri-\\ntion by year 5), compared with approximately 18% inter-\\nmediate term mortality for patients aged 1–50 years and\\nthose over 65 years (estimated based on data in Table\\n11.11, OPTN/SRTR 2003 Annual Report). This observation\\nindirectly supports the hypothesis that either immature or\\nsenescent immunity facilitates relatively good long-term\\noutcome. Higher intermediate-term mortality among those\\npatients over 65 years of age who are deemed suitable can-\\ndidates probably reﬂects the expected inﬂuence of rising\\nall-cause mortality with increasing age in this age range.\\nData from the Registry of the International Society for\\nHeart and Lung Transplantation (ISHLT) provides longitudi-\\nnal information on more than 59 000 cardiac transplants\\nworld-wide (10,11). Actuarial survival over the past two\\ndecades shows a patient half-life of 9.3 years with a condi-\\ntional half-life of 12 years among those surviving to hos-\\npital discharge. Risk factors for both early (1 year) and\\nintermediate-term (5 year) mortality in adult cardiac trans-\\nplantation includes preoperative ventilator dependence,\\nprior cardiac transplantation, congenital heart disease as\\nthe indication, increasing recipient and donor age, and in-\\ncreasing donor ischemia time. In the cohort transplanted\\nsince the beginning of 1999, the need for dialysis, increas-\\ning recipient age (over 50 years), residence in an ICU at\\nthe time of transplant, and low center volume appear to be\\nincreasingly important risk factors for 1-year mortality, rel-\\native to patients transplanted in 1995–1998. At 1-year\\nfollow-up, the majority of deaths are attributed to infection\\nand acute rejection. By 5 years, cardiac allograft vascu-\\nlopathy (chronic rejection), malignancy, and graft failure of\\nunspeciﬁed or unknown etiology are the principle causes\\nof mortality.\\nOur clinical impression is that the current data set may\\nunderestimate a major demographic shift in the patient\\npopulation coming to heart transplant. Young patients with\\nidiopathic dilated cardiomyopathy come to transplant less\\nfrequently, perhaps as a consequence of better medical\\nmanagement. Increasingly, elective referrals are of older\\ndiabetic patients with coronary artery disease. The typical\\npatient requiring a VAD has changed from the slowly dete-\\nriorating listed patient with idiopathic disease to the acute\\nmyocardial infarction patient with hemodynamic instability\\npresenting for emergent support, and whose evaluation\\nof necessity can only occur subsequent to device implant\\nand stabilization. We expect this clinical impression will be-\\ncome more evident in future analyses.\\nLung\\nLung waiting list characteristics\\nThe lung waiting list has continued to expand during the\\npast year, reaching a new record high of 3756 registrants\\nas of December 31, 2002. This growth reﬂects a small in-\\ncrease over 2001 and an increase of over 300% since 1992.\\nOver the past 5 years (1998–2002), however, the number\\nof active patients at year-end has stabilized between about\\n2300 and 2500, while the percentage of active registrants\\nhas continued to decline. The number of new registrations\\nalso dropped slightly to 1892, the lowest number since\\n1996.\\nA trend towards increased numbers of patients with in-\\nactive status on a waiting list snapshot, as seen in Fig-\\nure 8, partially accounts for the observed increase in the\\ntotal number of registrants on the lung waiting list, with\\na 15% increase among inactive patients since 2001 and a\\nmore than threefold increase since 1995. Although a re-\\nversible deterioration in patient status may, on occasion,\\ntransiently prohibit transplantation, it is likely that this trend\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n97\\n\\nRichard N. Pierson III et al.\\nreﬂects an increasingly common practice of early place-\\nment on the waiting list. End-stage lung patients are ac-\\ntively listed relatively early in the course of their disease\\nin order to accrue waiting time in the likely eventuality of\\nsubsequent deterioration. Under current allocation guide-\\nlines, remaining inactive on the waiting list (rather than be-\\ning removed from the list) allows previously accrued active\\ntime to be retained. This approach gives the individual pa-\\ntient with relatively stable and predictable disease a bet-\\nter chance of getting an organ, because waiting list time\\nis the most inﬂuential single determinant of organ prior-\\nity (assuming geographical proximity and blood type com-\\npatibility). As such patients begin to receive organ offers,\\nthey are inactivated until the transplant team judges that\\nthey are ill enough to beneﬁt from acceptance of an or-\\ngan. This practice of early placement on the waiting list, al-\\nthough advantageous on a patient-by-patient basis, is not\\nnecessarily in the best interest of the larger community\\nof wait-listed lung patients, because, frequently, patients\\nin need of transplantation do not have the opportunity to\\nbe wait-listed early in the course of their disease. Future\\nallocation plans will probably (and appropriately) involve\\nmore parameters that reﬂect medical urgency and, per-\\nhaps, probable utility, while de-emphasizing time waited\\nafter listing, as discussed in the ﬁnal section of this\\narticle.\\nCompared with 10 years ago, a higher percentage of lung\\nwaiting list registrants are older than 50 years, increasing\\nfrom 38% in 1993 to 50% in 2002 (Figure 9). The percent-\\nage of African-American and Hispanic/Latino registrants on\\nthe lung waiting list increased from 6% and 2%, respec-\\ntively, in 1993 to 11% and 5%, respectively, in 2002. Wait-\\ning list registrants were most commonly female (58%),\\nolder than 50 years of age (50%), white (87%), blood type\\nO (49%), US residents (99%), and awaiting their ﬁrst trans-\\nplant (97%). Approximately 65% of registrants have been\\nwaiting more than a year for an available organ, and 42%\\nof all listed patients waited for more than 2 years (these\\nSource: 2003 OPTN/SRTR Annual Report, Table 12.1. \\n0\\n1000\\n2000\\n3000\\n4000\\n1995\\n1996\\n1997\\n1998\\n1999\\n2000\\n2001\\n2002\\nYear\\nNumber of Patients\\nActive\\nInactive\\nFigure 8. Active vs. inactive lung waiting list registrants at year-\\nend, 1995–2002.\\n0\\n1000\\n2000\\n3000\\n4000\\n1993 1994 1995 1996 1997 1998 1999 2000 2001 2002\\nYear\\nNumber of Patients\\n10 and under\\n11–17\\n18–34\\n35–49\\n50–64\\n65+\\nSource: 2003 OPTN/SRTR Annual Report,Table 12.1. \\nFigure 9. Age distribution of lung waiting list at year-end, 1993–\\n2002.\\nwaiting times include periods of inactive waiting list sta-\\ntus). Even with the current allocation of 90 days of bonus\\nwaiting time for idiopathic pulmonary ﬁbrosis (IPF) patients,\\nthese wait time statistics are daunting in the context of this\\nunpredictable and often rapidly progressive disease.\\nAlthough observed quartiles of time to transplant showed\\na tendency to increase between 1993 and 1999, more re-\\ncent data suggest a reversal of this trend. As shown in\\nFigure 10, 25% of recipients in 1999 were transplanted\\nwithin 451 days of listing, whereas in 2002, this same per-\\ncentage of recipients was transplanted within 251 days of\\nlisting, a 41% reduction to a level also seen in 1993 and\\n1996. Counterbalancing the general trend towards longer\\naverage times to transplant were decreasing annual death\\nrates on the waiting list (Figure 11), which decreased from\\n236 deaths per 1000 patient years at risk in 1993 to a\\n10-year low of 131 in 2002. This trend is probably a re-\\nsult, at least in part, of improving care for end-stage lung\\npatients over time. However, it may also reﬂect the, on av-\\nerage, healthier patient years at risk contributed by patients\\nwho have elected to register on the waiting list early in\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n350\\n400\\n450\\n500\\n1993 1994 1995 1996 1997 1998 1999 2000 2001 2002\\nYear\\nDays\\nSource: 2003 OPTN/SRTR Annual Report, Table 12.2. \\nFigure 10. Twenty-ﬁfth percentile time to transplant of new lung\\nwaiting list registrants, 1993–2002.\\n98\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n\\nThoracic organ transplantation\\n0\\n50\\n100\\n150\\n200\\n250\\n1993 1994 1995 1996 1997 1998 1999 2000 2001 2002\\nYear\\nAnnual Death Rate \\n(per 1000 patient years at risk)\\nSource: 2003 OPTN/SRTR Annual Report, Table 12.3. \\nFigure 11. Annual death rates per 1000 patient years at risk on\\nthe lung waiting list, 1993–2002.\\ncalculating the death rate. Hence, the change in average\\nwaiting list mortality over time should be interpreted cau-\\ntiously. That is, these trends simultaneously reﬂect the\\noverlapping inﬂuences of the wider acceptance of trans-\\nplant for end-stage lung disease, the concomitant recogni-\\ntion of the need for accumulated wait time to enable ac-\\ncess to this life-saving therapy for individual patients, and\\nthe resulting change in the typical wait-listed patient on the\\nwaiting list. As individuals attempt to adjust to the current\\nallocation system in anticipation of their own needs, it is\\nlikely that some of the patients most in need of transplant\\nare being increasingly disadvantaged by the current alloca-\\ntion rules.\\nFor patients listed in 2002, relatively favorable times\\nto transplant were observed for candidates older than\\n50 years, with 25% of recipients aged 50–64 years trans-\\nplanted within 222 days and 25% of recipients over\\n65 years of age being transplanted within 100 days in 2002.\\nIn contrast, over 75% of patients aged 11–17 years, 18–\\n34 years, and 35–49 years waited for more than 421 days,\\n399 days, and 336 days, respectively, to be transplanted.\\nGreater ﬂexibility in organ acceptance criteria may be con-\\ntributing to the shorter times to transplant in older waiting\\nlist patients. Annual death rates per 1000 patient years on\\nthe lung waiting list in 2002 were relatively low for patients\\naged 11–17 years (148), 18–34 years (138), 35–49 years\\n(111), and 50–64 years (132), respectively. The 1–5 year\\nage group had the highest annual death rate while waiting\\n(238 deaths per 1000 patient years).\\nPerhaps because there have been consistently between\\n10% and 20% more women than men on the waiting list\\nover the past decade, the observed time to transplant has\\ntended to be longer for women than for men. Interest-\\ningly, despite the longer average wait time for women, in\\nmost years annual death rates per 1000 patient years at\\nrisk on the waiting list were slightly higher for men than\\nfor women. For example, in 2002 men experienced 147\\ndeaths per 1000 patient years at risk vs. a death rate of\\n119 among women. The explanation for these apparently\\ndiscordant statistics is obscure, but, speculatively, may re-\\nﬂect earlier onset of pulmonary symptoms but relatively re-\\nduced physiologic consequences in patients with smaller\\nbody size or a different hormonal environment.\\nPotential approaches for increasing the average years of\\nlife saved per organ via risk-based waiting list prioritizations\\nare growing in popularity (see below, Current Proposal for\\nDeceased Donor Lung Allocation Policy in the USA). Re-\\nvised listing and allocation criteria may eventually reduce\\nthe perceived imperative to place candidates on the wait-\\ning list at early disease stages. The OPTN/UNOS Thoracic\\nCommittee is currently investigating allocation algorithms\\nfor this purpose, with the objective of creating priority on\\nthe waiting list by balancing risk of death on the waiting list\\nvs. post-transplant outcome (12).\\nLung transplant recipient characteristics\\nOver the last 10 years, the total number of lung transplants\\nhas slowly increased from 667 transplants performed in\\n1993 to 1054 in 2001. In 2002, the total number decreased,\\nto 1041; this is the third time in the past 10 years that there\\nhas been a slight decline in volume in comparison with the\\nprevious year. This plateau in the number of recipients is\\nmost likely because of both the relatively small increase\\nin the total number of lung donors combined and the in-\\ncreasing number of double (vs. single) lung transplants\\nperformed (discussed below). Patients in their fourth, ﬁfth,\\nand sixth decades of life account for the majority of trans-\\nplant recipients, with the largest cohort, those 50–64 years\\nold, representing nearly 56%. Racial breakdowns have re-\\nmained unchanged, with the great majority (>90%) of re-\\ncipients characterized as white. Gender distribution over\\nthe years has varied slightly, with an approximately equal\\ndistribution between male and female recipients. Given the\\nhigher proportion of women on the waiting list, it is unclear\\nwhy such a discrepancy exists. A possible explanation is\\nsmaller recipient size, with fewer small donors and a reluc-\\ntance to oversize. Other factors may relate to differences\\nin gender distribution within each of the pretransplant di-\\nagnoses combined with the differences in the waiting time\\nfor those diagnoses and improvements in medical care.\\nThe major primary diagnoses and percentages for the 2002\\ncohort were as follows: emphysema (39%), IPF (19%),\\ncystic ﬁbrosis (16%), alpha-1-antitrypsin deﬁciency (8%),\\nand primary pulmonary hypertension (PPH, 5%). The per-\\ncentage of transplants for emphysema decreased, while\\nthe other diagnostic groups had commensurate small in-\\ncreases since 2001 (Figure 12). Furthermore, 98% of lung\\nrecipients had not undergone any previous solid organ\\ntransplant. The majority of recipients were not hospital-\\nized at the time of transplantation, and less than 6% of\\npatients were on life support when transplanted. Recipi-\\nents in the intensive care unit immediately prior to trans-\\nplant had signiﬁcantly lower graft survival rates at all time\\npoints compared with the hospitalized or nonhospitalized\\ncohorts. In comparison with 1993 and 1994, when more\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n99\\n\\nRichard N. Pierson III et al.\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n1993\\n1994\\n1995\\n1996\\n1997\\n1998\\n1999\\n2000\\n2001\\n2002\\nYear\\n Patients (%)\\nCOPD\\nCF\\nIPF\\nA1A\\nPPH\\nSource: 2003 OPTN/SRTR Annual Report, Table 12.4a \\nFigure 12. Deceased donor lung transplant recipients, by diagno-\\nsis, 1993–2002.\\n0\\n20\\n40\\n60\\n80\\n100\\n1993 1994 1995 1996 1997 1998 1999 2000 2001 2002\\nYear\\n Patients (%)\\nDouble Lung\\nSingle Lung\\nSource: 2003 OPTN/SRTR Annual Report, Table 12.4a \\nFigure 13. Deceased donor lung transplant recipients, by proce-\\ndure, 1993–2002.\\nthan 60% of lung transplant procedures involved single\\nlung transplantation, the period from 1995 through 2001\\nshows an increasing use of double lung transplantation,\\nwith the data from 2002 showing, for the ﬁrst time, dou-\\nble lung transplantation exceeding single lung procedures\\n(Figure 13). Whether this trend reﬂects an increasing ac-\\nceptance of double lung transplantation as a preferen-\\ntial procedure for recipients with diagnoses that have\\npreviously been treated with single lung transplantation\\nremains to be seen. While there have been retrospec-\\ntive analyses showing an improved intermediate and\\nlong-term survival for double lung recipients, these com-\\nparisons have not been adjusted for potentially signiﬁ-\\ncant confounding variables, such as age and underlying\\ndiagnosis.\\nOne-year adjusted graft survival was 77% (for the 2001\\ncohort) and has been essentially unchanged over the past\\n9 years (75% graft survival in 1993) (Figure 14). With the\\nvery low incidence of retransplantation (2% of all recip-\\nients in 2002), patient survival was only slightly higher\\n0\\n20\\n40\\n60\\n80\\n100\\n1992 1993 1994 1995 1996 1997 1998 1999 2000 2001\\nYear of Transplant\\n Graft Survival (%)\\nSource: 2003 OPTN/SRTR Annual Report, Table 1.12a.\\nFigure 14. One-year adjusted lung graft survival, 1992–2001.\\nthan graft survival. One- and 5-year patient survivals were\\n78% and 45%, respectively. Adjusted graft survival was\\n77% at 1 year (2000–2001 cohort) and 44% at 5 years\\n(1996–1997 cohort). The 11–17 year old group had the low-\\nest 3-month graft survival at 82%, and this same group\\n(along with the 6–10 year olds) had the worst 5-year graft\\nsurvival at only 23%. The best 5-year graft survival, al-\\nbeit still only 50%, was seen among those aged 35–49\\nyears. By race, 5-year graft survival was lowest in African-\\nAmericans at 30%, whereas whites had a higher 5-year\\nsurvival rate of 45%. Ethnicity, gender, and blood type did\\nnot have a notable independent effect on short- or long-\\nterm graft survival. Of all demographics, a history of prior\\nlung transplant portended the worst 1-year (53%), 3-year\\n(30%), and 5-year (35%) graft survival. This is in keep-\\ning with other published reports, such as the 2003 ISHLT\\nregistry report (13), which shows a signiﬁcantly increased\\nrisk of 1-year mortality in this group, with an odds ratio\\nof 2.03. Interestingly, those data reveal that the increased\\nmortality is only signiﬁcant for the ﬁrst year, and not at\\n5 years. Graft survival in relationship to the primary di-\\nagnosis leading to transplant was highest at all periods\\nduring the ﬁrst 3 years for emphysema/chronic obstruc-\\ntive pulmonary disease, followed by cystic ﬁbrosis. The\\ndiagnosis with the highest 5-year graft survival rate was\\nalpha-1-antitrypsin deﬁciency (46%)—aside from the 212\\npatients with a diagnosis of ‘other’ (50%). These were fol-\\nlowed by emphysema/chronic obstructive pulmonary dis-\\nease (45%). Retransplantation, congenital (heart) disease,\\nand PPH had the lowest 5-year graft survival rates at 35%,\\n32%, and 28%, respectively (Figure 15).\\nWhile the number of recipients of living donor lungs is in-\\nsufﬁcient for statistical comparisons with recipients of de-\\nceased donor lungs, there are some interesting differences\\nbetween the two groups. The vast majority of the living\\ndonor lung recipients are in the 11–17 year and 18–34 year\\nage ranges, consistent with the high percentage of these\\nrecipients having cystic ﬁbrosis. The next largest diagnos-\\ntic indication is retransplantation. The majority of this group\\nwas hospitalized prior to the transplant: in 2002, 39% were\\n100\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n\\nThoracic organ transplantation\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\n3 months\\n1 year\\n3 years\\n5 years\\nTime after Transplant\\nPatient survival (%)\\nEmphysema/COPD\\nCF\\nIPF\\nA1A\\nPPH\\nSource: 2003 OPTN/SRTR Annual Report,Table 12.8a. Cohorts \\nare for transplants performed during 2000-2001 for 3 month & 1 year;\\n1998–1999 for 3 year; and 1996-1997 for 5 year survival.\\nFigure 15. Adjusted graft survival among deceased donor lung\\ntransplant recipients by diagnosis.\\nin the hospital and an additional 15% were in the intensive\\ncare unit prior to the transplant. Interestingly, adjusted pa-\\ntient survival was 73% (vs. 78% for the deceased donor\\ngroup) at 1 year and 50% (vs. 45%) at 5 years The low-\\nest unadjusted 5-year patient survival rates were seen in\\nthose recipients in the intensive care unit preoperatively\\n(25%) and in those recipients on preoperative life support\\n(17%).\\nData from the ISHLT registry provides information on more\\nthan 12 000 adult lung transplants world-wide (13). Actuar-\\nial survival over the past decade shows a patient half-life of\\n4.1 years with a conditional half-life of 6.6 years. Major risk\\nfactors for 1-year mortality include pretransplantation diag-\\nnosis (PPH > sarcoidosis > idiopathic pulmonary ﬁbrosis >\\nall other diagnoses), preoperative ventilator dependence,\\npreoperative intravenous inotropes, and prior lung trans-\\nplantation. Increasing recipient and donor age, increasing\\norgan ischemia time, and decreased center volume (cases\\nper year) were also signiﬁcant risk factors. Continuous vari-\\nables that signiﬁcantly affected 5-year mortality included\\nincreasing recipient age beyond 50 years and increasing\\ndonor age beyond 30 years.\\nLung transplantation is now widely accepted as a viable\\ntreatment for a heterogeneous group of end-stage lung\\ndiseases, with an associated expansion in the number of\\npotential transplant candidates. Although recent interna-\\ntional guidelines have been developed for determining can-\\ndidacy, the listing decisions are still highly inﬂuenced by\\nindividual patient considerations. Listing policy, however,\\nis difﬁcult to codify because of varied pathogenesis and\\noften unpredictable natural histories. As mentioned previ-\\nously, the current system exerts pressure to place patients\\non the waiting list at earlier stages of lung disease in re-\\nsponse to longer average times to organ availability, as has\\nbeen commented on recently in the context of cystic ﬁ-\\nbrosis, pulmonary ﬁbrosis, and sarcoidosis (14–18). These\\ntrends toward earlier diagnosis and more broadly deﬁned\\ncriteria for adding patients to the waiting list are not with-\\nout potential consequences. In describing 5-year survival\\nrates for wait-listed cystic ﬁbrosis patients, Liou et al. re-\\ncently argued that an increase in the number of patients\\nwith long survival rates on the waiting list has a deleteri-\\nous effect on survival for patients with poorer short-term\\nprognosis at diagnosis, who ﬁnd themselves at a competi-\\ntive disadvantage (19). The increasing numbers of patients\\nwith apparently better prognoses appearing on the waiting\\nlist, not to mention improvements in patient care, may have\\noverwhelmed this effect and resulted in the observed de-\\ncrease in the average waiting list mortality rate. The critical\\nshortage of donor lungs remains painfully evident to pa-\\ntients and clinicians. With respect to policy, the key ques-\\ntion may be whether utilization of a scarce resource (lungs)\\nis being optimized for efﬁcacy (net years of life saved for\\nthe end-stage lung failure population, improved net qual-\\nity of life for those transplanted) or equity (fair access, and\\nimproved access for patients at greatest risk of death while\\nwaiting).\\nHeart-Lung\\nHeart-lung waiting list characteristics\\nThe total number of registrants awaiting heart-lung trans-\\nplant fell below 200 in 2002, and in that year only 88 new\\npatients were listed. Among listed patients, 54% were on\\nthe active waiting list. Median time to transplant cannot\\nbe calculated for heart-lung candidates listed since 1993,\\nbecause more than 50% of the patients listed each year\\nhave yet to be transplanted. Although the 25th percentile\\nfor time to transplant declined from a high of over 700 days\\nfor patients listed in 1997 to just under 400 days for patients\\nlisted in 1999, for patients listed in the past 3 years it con-\\ntinues to hover between 1 and 2 years. The average age\\nand proportion of minority registrants increased over the\\npast 10 years. The waiting list death rate declined slightly,\\nto 186 per 1000 years at risk, still among the highest for any\\ngroup of transplant patients. Although the absolute num-\\nber of deaths is relatively small (38 in 2002), it continues to\\nexceed the annual number of transplants performed. Thus,\\nonly a minority of heart-lung registrants actually received a\\ntransplant and most candidates waited more than 2 years.\\nFurther changes in the allocation of heart/lung are being\\nconsidered by the OPTN/UNOS Thoracic Committee.\\nHeart-lung transplant recipient characteristics\\nOnly 32 heart-lung transplants were reported in 2002, up\\nslightly from 27 in the previous year. Common indications\\nare congenital heart disease (31%, primarily Eisenmenger\\nSyndrome), and PPH with irreversible heart failure (38%).\\nPotential heart-lung candidates often do not receive organs\\nuntil they are listed as Status 1A on the heart waiting list\\n(47% hospitalized, 50% on life support).\\nThe annual death rate following heart-lung transplanta-\\ntion remains high compared with other organs, but it has\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n101\\n\\nRichard N. Pierson III et al.\\ndropped consistently since peaking in 1997, and in 2002\\nwas only 171 per 1000 patient years at risk, down sharply\\nfrom 432 in 2001. Actuarial graft and patient survival at\\n3 months and 1 year lags behind that for double lung\\ntransplantation. This early survival disadvantage is prob-\\nably related to the technical challenges of controlling the\\nbleeding associated with scarring from prior surgery or in-\\nﬂammation, complex anatomy, proliﬁc vascular collaterals\\ncommon in these patients, and liver congestion from right\\nheart failure.\\nLong-term outcome after heart-lung transplantation is sim-\\nilar to that for double lung transplantation, with adjusted\\n5-year survival at about 40%. Only four centers performed\\nmore than two heart-lung transplants in 2002; only 18 pro-\\ngrams have performed 10 or more since 1993, and only\\nStanford, with 85 over the past decade, has averaged more\\nthan eight per year. Organ allocation policy should continue\\nto consider the needs of this small group of young, chal-\\nlenging patients who rarely have other good options.\\nCurrent Proposal for Deceased Donor Lung\\nAllocation Policy in the USA\\nThe current algorithm for lung distribution in the USA was\\nintroduced in June 1990 and was modeled after the dis-\\ntribution system for hearts. Lungs are offered ﬁrst to re-\\ncipients within the OPO where the donor is hospitalized\\nbased on active waiting time, and, if not allocated locally,\\nthe lungs are then offered to appropriate ABO identical\\nor compatible recipients listed at transplant centers within\\nconcentric 500 nautical mile circles. In March 1995, the al-\\ngorithm was modiﬁed to assign 90 days of waiting time at\\nlisting to patients with IPF in response to the perception\\nthat these patients were deteriorating more rapidly than\\nother patients and dying before organs were being made\\navailable to them.\\nThe 1999 Final Rule for operation of the OPTN included\\na requirement to examine organ distribution algorithms to\\nminimize the impact of geography on prospects for trans-\\nplantation. The mandate for the OPTN is to achieve the best\\nuse of organs by directing organs to those most in need,\\nwhile at the same time maximizing utility of organs by not\\nwasting them on futile transplantation of individuals likely\\nto be too sick to survive the operation or derive an impor-\\ntant quality of life beneﬁt (20). In addition, the Final Rule\\nrequired that the OPTN Board of Directors develop poli-\\ncies for organ allocation that are based on sound medical\\njudgment and that seek to achieve the best use of organs.\\nAlthough the initial debate focused on liver distribution, the\\nFinal Rule required the OPTN to examine all organ distribu-\\ntion algorithms and either demonstrate that they satisﬁed\\nthe principles espoused or alter the algorithms to address\\nthe new philosophy.\\nThe OPTN/UNOS Thoracic Organ Committee established\\na subcommittee to study the lung distribution algorithm\\nand make recommendations to comply with the Final Rule.\\nMembers of the Lung Allocation Subcommittee concluded\\nthat the current system was ﬂawed. One effect of alloca-\\ntion based on waiting time was the growing practice of\\nlisting patients before they truly needed to be transplanted\\n(21). A second consequence was the observation that pro-\\nportionately fewer patients with chronic obstructive pul-\\nmonary disease (COPD) were dying on the list compared\\nwith those with IPF and cystic ﬁbrosis. Presumably, those\\nwho could survive the longest on the waiting list had a bet-\\nter chance of being offered a lung or lungs for transplant,\\neven though there appears to be no survival beneﬁt of lung\\ntransplant for the large number of patients with COPD un-\\ndergoing the procedure (22).\\nThe subcommittee believed that an ideal allocation system\\nwould minimize deaths on the waiting list, while at the\\nsame time maximize the beneﬁt of transplant by incorpo-\\nrating post-transplant survival into the algorithm. Several\\nanalyses were performed to determine the feasibility of\\ndesigning such an algorithm. First, patients added to the\\nlung transplant waiting list between January 1, 1997 and\\nDecember 31, 1998 with the four most common diagnoses\\nwere analyzed to determine if data submitted at the time\\nof listing could predict death on the waiting list. Over 3100\\npatients with COPD or alpha-1-antitrypsin deﬁciency em-\\nphysema (n = 1461), cystic ﬁbrosis (n = 708), IPF (n =\\n608), or PPH (n = 327) were included in the analysis. A\\nlogistic regression model was ﬁtted for each of the four\\ndiagnoses using death on the waiting list as the outcome.\\nOver 30 clinical and demographic variables collected at the\\ntime of listing were included in the models. Patients were\\ncensored at the time of transplant. A number of factors\\nwere identiﬁed for each diagnosis that were associated\\nwith a signiﬁcantly increased risk of death on the waiting\\nlist (23,24).\\nTo establish if any of these risk factors could predict death\\nafter lung transplant, another analysis was performed on\\npatients with the same four diagnoses. Patients undergo-\\ning lung transplant between January 1, 1996 and June 30,\\n1999 (n = 2484) were analyzed to establish if data col-\\nlected at the time of listing could predict survival proba-\\nbility 1 year after transplant. There was a signiﬁcant effect\\nof diagnosis on 1-year survival: COPD/emphysema 79.7%,\\ncystic ﬁbrosis 80.2%, IPF 66%, and PPH 64% (p < 0.0001,\\nlog rank). For each diagnosis, additional factors were identi-\\nﬁed that were associated with a signiﬁcantly increased risk\\nof death following lung transplant. One-year survival was\\nchosen for this analysis based on the premise that the pre-\\ntransplant factors that played a role in post-transplant sur-\\nvival would have a diminishing effect as time went on after\\ntransplant.\\nAlthough these analyses were useful, they were limited to\\nadult patients with the four most common diagnoses and\\nthus represented approximately 80% of patients listed or\\ntransplanted. The impact of diagnosis was so strong that\\n102\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n\\nThoracic organ transplantation\\nthe subcommittee chose to establish whether all patients\\nlisted could be assigned to a diagnostic group for purposes\\nof identifying risk factors that might be useful to construct\\na distribution algorithm.\\nAnalysis of distribution of diagnoses, waiting list survival\\nprobabilities, and post-transplant survival by age for pa-\\ntients under the age of 18 years, suggested that there\\nwas a break point at age 12. Adolescent and teenage\\nlung transplant recipients aged 12 and older had similar\\ndiagnoses and survival (waiting list and post-transplant) as\\nyoung adults, while children younger than 12 years had dif-\\nferent diagnoses and survival probabilities. Thus, the sub-\\ncommittee decided to place all potential recipients under\\nthe age of 12 in the pediatric group and all patients 12 and\\nolder in the adult group. Analysis of waiting list and post-\\ntransplant 1-year survival for other end-stage lung disease\\ndiagnoses, along with consideration of pathophysiology,\\nled to the creation of four diagnostic groups for patients\\naged 12 years and older (25).\\nPatients were grouped as follows for additional analyses\\n(Table 1). Group A is composed primarily of patients with\\nobstructive lung diseases and includes sarcoidosis patients\\nwith mean PA pressure <30 mmHg. Group B is composed\\nof patients with pulmonary vascular diseases. Group C is\\ndominated by patients with cystic ﬁbrosis, and Group D\\nis composed primarily of patients with restrictive lung dis-\\neases. The prognosis for patients with sarcoidosis corre-\\nlated with PA (pulmonary artery) pressure: those patients\\nwith a mean PA pressure <30 mmHg had survival simi-\\nlar to patients with COPD, whereas sarcoidosis patients\\nwith mean PA pressure >30 mmHg had survival similar\\nto patients with IPF, and were thus considered in those\\nTable 1. Survival-based diagnosis groupings for lung allocation\\nmodeling\\nGroup A (predominantly obstructive)\\nCOPD, emphysema, chronic bronchitis\\nAlpha-one antitrypsin deﬁciency emphysema\\nBronchiectasis\\nLymphangioleiomyomatosis (LAM)\\nSarcoidosis with mean PA pressure ≤30 mmHg\\nGroup B (predominantly pulmonary vascular disease)\\nPulmonary hypertension, primary and secondary (includes\\nEisenmenger’s syndrome)\\nPulmonary veno-occlusive disease\\nGroup C\\nCystic ﬁbrosis, immunoglobulin deﬁciency, ﬁbrocavitary\\nlung disease\\nGroup D (predominantly restrictive)\\nPulmonary ﬁbrosis, including IPF, occupational lung disease\\nCollagen vascular diseases\\nBronchoalveolar carcinoma\\nSarcoidosis with mean PA pressure > 30 mmHg\\nAlveolar proteinosis\\nEosinophilic granulomatosis\\nGroup E\\nAll patients < age 12, regardless of diagnosis\\ngroups. Group E consists of all patients under the age of 12\\nyears, irrespective of the diagnosis of their end-stage lung\\ndisease.\\nAnalyses were repeated for all patients listed between\\nJanuary 1, 1997 and December 31, 1998. Because of\\nthe relatively small number of patients with pulmonary\\nvascular disease, data were collected for Group B pa-\\ntients from January 1, 1995 to December 31, 1998. Group-\\nspeciﬁc Cox regression models were used to predict\\ndeath on the waiting list. Cox models, rather than lo-\\ngistic regression, were selected to explore survival rates\\nat more than one time point. Patients were censored\\nat the time of transplant. Statistically signiﬁcant factors\\nwere identiﬁed for Groups A–D that were similar to the\\nearlier analysis, and hazard ratios were calculated for\\nthese factors (26). The number of patients in Group E\\n(n = 131) and the small number of waiting list deaths\\n(n = 43) made interpretation of the data for this group\\nunreliable.\\nA subsequent analysis of all patients transplanted in the\\nsame timeframe was performed to establish whether data\\nat the time of listing or transplant could identify factors as-\\nsociated with survival following transplant. Group-speciﬁc\\nCox regression models were ﬁtted with post-transplant\\ndeath as the outcome to identify these factors and associ-\\nated hazard ratios (27).\\nA number of factors identiﬁed as statistically signiﬁcant\\nwere judged by the Lung Allocation Subcommittee to be\\ninappropriate to put into an organ distribution algorithm be-\\ncause they were too subjective to be applied consistently.\\nAn example is being on 5 mg of more of prednisone daily for\\nGroup A patients, which was associated with an increased\\nrisk of death on the waiting list. These factors were elim-\\ninated from the models but had little effect on the other\\nvariables that were judged appropriate for inclusion in the\\nalgorithm.\\nBased on individual patient risk factors and associated haz-\\nard ratios, the subcommittee then considered options for\\nsummarizing a patient’s risk of death on the waiting list\\nover the subsequent year as opposed to the patient’s risk\\nof death during this same period if transplanted. One-year\\nsurvival estimates provided by the Cox models were con-\\nsidered, as well as estimates of the length of time each\\npatient would live during the next year with or without\\ntransplant. This latter type of summary measure, often re-\\nferred to as a 1-year expected lifetime, is calculated for\\neach individual at the time a donor organ is being consid-\\nered by summing up the area under the patient’s 1-year\\nCox model estimated survival curve. The subcommittee\\nselected these 1-year expected lifetime summary mea-\\nsures to describe each patient’s anticipated waiting list and\\npost-transplant prognosis after noting that these measures\\ncaptured more information about the patient survival pro-\\nﬁles over time than a 1-year survival rate. Each patient’s\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n103\\n\\nRichard N. Pierson III et al.\\n1-year survival beneﬁt with transplant (measured in days\\nof life saved with transplant) was then calculated by look-\\ning at the difference between the days of life a patient\\nwould be expected to live over the next year if trans-\\nplanted minus the days of life a patient would be expected\\nto live if maintained on the waiting list over the coming\\nyear (28).\\nAllocation scores based on the days of life saved within\\nthe ﬁrst year if transplanted (beneﬁt), days of life expected\\nduring the subsequent year if maintained on the waiting list\\n(urgency), and combinations of transplant beneﬁt/urgency\\nwere presented to members of the pulmonary medicine\\nand transplant communities at large at a Lung Allocation\\nConsensus Conference held in March 2003. Feedback was\\nobtained for further consideration by the Lung Allocation\\nSubcommittee.\\nIt is anticipated that serial clinical data will be useful to iden-\\ntify new factors that should be incorporated into the dis-\\ntribution algorithm and that serially collected patient data\\nmay affect the import of factors identiﬁed as signiﬁcant in\\nthe analyses. Indeed, it is the recommendation of the Lung\\nAllocation Subcommittee that analyses be undertaken to\\nidentify factors and modify their hazard ratios in the algo-\\nrithm at least every 6 months. Thus, as patients are trans-\\nplanted and removed from the list and new patients are\\nadded, risk is assessed using the most recent cohort of\\npatients.\\nOne shortcoming of the planned change to the algorithm is\\nthat hazard ratios were identiﬁed for a cohort of historical\\npatients, and data were collected at only one or at most\\ntwo points in time (post-transplant survival factors were\\navailable at listing or at transplant). To address this issue,\\nthe OPTN is undertaking an analysis of a cohort of recently\\nlisted patients at a large number of centers to determine\\nif the risk factors and their calculated hazard ratios are ap-\\npropriate and can justify altering the existing algorithm. It\\nis anticipated that this approach to lung distribution will\\nreduce deaths on the waiting list and improve survival fol-\\nlowing lung transplant.\\nSummary\\nImportant trends over the past decade are documented for\\nheart, lung, and heart-lung waiting lists and for correspond-\\ning organ transplant recipients. Wait-listed candidates and\\nthoracic organ recipients include increasing percentages of\\nolder age groups. In general, median time to transplant is\\ndeclining and post-transplant survival rates have gradually\\nimproved over the last decade for all thoracic organs. It is\\nimportant to note that the large decrease in mortality rate,\\napparent for all thoracic organ transplant recipients during\\n2002 (39% for heart, 66% for heart-lung, 14% for lung re-\\ncipients), appears to be an artifact of the data collection\\nprocess. Similar ﬁndings were reported last year for the\\n2001 results but are not conﬁrmed in this year’s report for\\nthe same 2001 calendar year interval. We conclude that\\nthe most recent year’s data will not be a valuable source\\nfor policy-making unless collection procedures can be im-\\nproved. The most likely source of this apparent problem is\\ndelayed submission of patient follow-up data to the OPTN.\\nNonetheless, gradual improvement in patient access and in\\nshort- and long-term survival appear to be sustained for all\\nthoracic organs and in most patient demographic groups,\\nsuggesting favorable inﬂuences of recently implemented\\npolicies and continued improvement in patient selection\\nand management. Improved information will permit op-\\ntimal use of these precious organs according to criteria\\nthat are generally agreed upon (and, increasingly, objec-\\ntive) among the pool of patients, providers, and payers\\nwhose interests are intimately involved in this miraculous\\nprocess.\\nReferences\\n1.\\nDickinson DM, Bryant PC, Williams MC et al. Transplant data:\\nsources, collection, and caveats. Am J Transplant 2004; 4 (suppl.\\n9): 13–26.\\n2.\\nWolfe RA, Schaubel DE, Webb RL et al. Analytical approaches\\nfor transplant research. Am J Transplant 2004; 4 (suppl. 9): 106–\\n113.\\n3.\\nAnyanwu AC, Rogers CA, Murday AJ, Steering Group. Intratho-\\nracic organ transplantation in the United Kingdom 1995–99: re-\\nsults from the UK cardiothoracic transplant audit. Heart 2002; 87:\\n449–454.\\n4.\\nLevy D, Kenchaiah S, Larson MG et al. Long-term trends in the\\nincidence of and survival with heart failure. N Engl J Med 2002;\\n347: 1397–1402.\\n5.\\nPetrie MC, Dawson NF, Murdoch DR, Davie AP, McMurray JJV.\\nFailure of women’s hearts. Circulation 1999; 99: 2334–2341.\\n6.\\nCleland JG, Alamgir F, Nikitin NP, Clark AL, Norell M. What is\\nthe optimal medical management of ischemic heart failure? Prog\\nCardiovasc Dis 2001; 43: 433–455.\\n7.\\nHunt SA, Baker DW, Chin MH et al. ACC/AHA guidelines for\\nthe evaluation and management of chronic heart failure in the\\nadult: executive summary. J Heart Lung Transplant 2002; 21: 189–\\n203.\\n8.\\nKonstam MA, Mann DL. Contemporary medical options for treat-\\ning patients with heart failure. Circulation 2002; 105: 2244–2246.\\n9.\\nStevenson LW, Warner SL, Steimle AE et al. The impending crisis\\nawaiting cardiac transplantation. Modeling a solution based on\\nselection. Circulation 1994; 89: 450–457.\\n10.\\nHosenpud JD, Bennett LE, Keck BM, Boucek MM, Novick RJ.The\\nRegistry of the International Society for Heart and Lung Transplan-\\ntation: Eighteenth Ofﬁcial Report – 2001. J Heart Lung Transplant\\n2001; 20: 805–815.\\n11.\\nBoucek MM, Edwards LB, Keck BM et al. The Registry of the\\nInternational Society for Heart and Lung Transplantation: Fifth Of-\\nﬁcial Pediatric Report – 2001 to 2002. J Heart Lung Transplant\\n2002; 21: 827–840.\\n12.\\nOrgan Procurement and Transplantation Network Thoracic Organ\\nTransplantation Committee Chaired by Frederick Grover. Report\\nto the OPTN Board of Directors, June 28–29, 2001. San Diego,\\nCA.\\n13.\\nTrulock EP, Edwards LB, Taylor DO et al. The Registry of the In-\\nternational Society for Heart and Lung Transplantation: Twentieth\\n104\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n\\nThoracic organ transplantation\\nOfﬁcial Adult Lung and Heart-Lung Transplant Report – 2003. J\\nHeart Lung Transplant 2003; 22: 625–635.\\n14.\\nAugarten A, Hannah A, Aviram M et al. Prediction of mortality\\nand timing of referral for lung transplantation in cystic ﬁbrosis\\npatients. Pediatr Transplant 2001; 5: 339–342.\\n15.\\nStern M, Reynaud-Gaubert M, Haloun A, Bertocchi M, Grenet D.\\nLung transplantation in cystic ﬁbrosis. Patient selection criteria.\\nRevue des Maladies Respiratoires 2000; 17: 779–784.\\n16.\\nFlaherty KR, White ES, Gay SE, Martinez FJ, Lynch JP. Timing\\nof lung transplantation for patients with ﬁbrotic lung diseases.\\nSemin Respir Crit Care Med 2001; 22: 517–531.\\n17.\\nMogulkoc N, Brutsche MH, Bishop PW, Greaves SM, Horrocks\\nAW, Egan JJ. Pulmonary function in idiopathic pulmonary ﬁbrosis\\nand referral for lung transplantation. Am J Respir Crit Care Med\\n2001; 164: 103–108.\\n18.\\nArcasoy SM, Christie JD, Pochettino A et al. Characteristics and\\noutcomes of patients with sarcoidosis listed for lung transplanta-\\ntion. Chest 2001; 120: 873–880.\\n19.\\nLiou TG, Adler FR, Cahill BC et al. Survival effect of lung trans-\\nplantation among patients with cystic ﬁbrosis. JAMA 2001; 286:\\n2683–2689.\\n20.\\nDepartment of Health and Human Services. Organ Procurement\\nand Transplantation Network; Final Rule. In: 42 CFR – Part 121:\\nFederal Register, October 20, 1999: 56 649–56 661.\\n21.\\nInternational guidelines for the selection of lung transplant candi-\\ndates. Am J Respir Crit Care Med 1998; 158: 335–339.\\n22.\\nHosenpud JD, Bennett LE, Keck BM, Edwards EB, Novick RJ.\\nEffect of diagnosis on survival beneﬁt of lung transplantation for\\nend-stage lung disease. Lancet 1998; 351: 24–27.\\n23.\\nEgan TM, Bennett LE, Garrity ER et al. Predictors of death on\\nthe UNOS lung transplant waiting list: results of a multivari-\\nate analysis (abstract). J Heart Lung Transplant 2001; 20 (2):\\n242.\\n24.\\nEgan TM, Bennett LE, Garrity ER et al. Are there predictors of\\ndeath at the time of listing for lung transplant (abstract)? J Heart\\nLung Transplant 2002; 21 (1): 154.\\n25.\\nMurray S, Merion R, McCullough K et al. Diagnosis-based models\\nof lung transplant waiting list mortality (abstract). Am J Transplant\\n2002; 2 (Suppl. 3): 270.\\n26.\\nEgan T, McCullough K, Bustami R et al. Predictors of death on\\nthe UNOS lung transplant waiting list (abstract). J Heart Lung\\nTransplant 2003; 22: S147.\\n27.\\nEgan T, McCullough K, Murray S et al. Risk factors for death\\nafter lung transplant in the U.S. (abstract). J Heart Lung Transplant\\n2003; 22: S146–S147.\\n28.\\nMurray S, Yu J, Wolfe RA et al. Risk/beneﬁt-based lung alloca-\\ntion algorithms (abstract). J Heart Lung Transplant 2003; 22 (15):\\nS128–S129.\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n105\\n'}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/Thoracic organ transplantation.pdf', 'text': 'American Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\nBlackwell Munksgaard\\nBlackwell Munksgaard 2004\\nThoracic organ transplantation\\nRichard N. Pierson IIIa,∗, Mark L. Barrb, Keith P.\\nMcCulloughc, Thomas Egand, Edward Garritye,\\nMariell Jessupf and Susan Murrayg\\naUniversity of Maryland Medical Center and VA,\\nBaltimore, MD;\\nbUniversity of Southern California, Los Angeles, CA;\\ncSRTR/URREA, Ann Arbor, MI;\\ndUniversity of North Carolina at Chapel Hill, Chapel Hill,\\nNC;\\neLoyola University Chicago, Chicago, IL;\\nfUniversity of Pennsylvania, Philadelphia, PA;\\ngSRTR/University of Michigan, Ann Arbor, MI\\n∗Corresponding author: Richard N. Pierson,\\nrpierson@smail.umaryland.edu\\nThis article presents an overview of factors associ-\\nated with thoracic transplantation outcomes over the\\npast decade and provides valuable information regard-\\ning the heart, lung, and heart-lung waiting lists and\\nthoracic organ transplant recipients. Waiting list and\\npost-transplant information is used to assess the im-\\nportance of patient demographics, risk factors, and pri-\\nmary cardiopulmonary disease on outcomes.\\nThe time that the typical listed patient has been\\nwaiting for a heart, lung, or heart-lung transplant\\nhas markedly increased over the past decade, while\\nthe number of transplants performed has declined\\nslightly and survival after transplant has plateaued.\\nWaiting list mortality, however, appears to be declin-\\ning for each organ and for most diseases and high-\\nseverity subgroups, perhaps in response to recent\\nchanges in organ allocation algorithms. Based on per-\\nceived inequity in organ access and in response to a\\nmandate from Health Resources and Services Ad-\\nministration,\\nthe\\nlung\\ntransplant\\ncommunity\\nis\\nNotes on Sources:\\nThe articles in this report are based on the\\nreference tables in the 2003 OPTN/SRTR Annual Report, which\\nare not included in this publication. Many relevant data appear in\\nﬁgures and tables included here; other tables from the Annual\\nReport that serve as the basis for this article include the follow-\\ning: Tables 1.8, 1.12a, 1.14, 11.1–11.5, 11.7, 11.9, 11.11, 11.12,\\n12.1–12.4b, 12.8a, 12.9a, 12.10a, 12.10b, 12.11b, 13.1–13.4, 13.7,\\n13.9a, 13.11, and 13.13. All of these tables are also available online\\nat http://www.ustransplant.org.\\nFunding:\\nThe Scientiﬁc Registry of Transplant Recipients (SRTR)\\nis funded by contract #231-00-0116 from the Health Resources\\nand Services Administration (HRSA). The views expressed herein\\nare those of the authors and not necessarily those of the US Gov-\\nernment. This is a US Government-sponsored work. There are no\\nrestrictions on its use.\\ndeveloping a lung allocation system designed to min-\\nimize deaths on the waiting list while maximizing the\\nbeneﬁt of transplant by incorporating post-transplant\\nsurvival and quality of life into the algorithm. Areas\\nwhere improved data collection could inform evolving\\norgan allocation and candidate selection policies are\\nemphasized.\\nKey words:\\nAllocation policy, deceased donors, graft\\nsurvival, heart transplantation, heart-lung transplanta-\\ntion, living donors, lung transplantation, organ dona-\\ntion, patient survival, SRTR, waiting list\\nIntroduction\\nThe intent of this analysis is to present an annual perspec-\\ntive on the evolution of thoracic transplantation in the USA\\nand to offer insights that may lead to more efﬁcacious al-\\nlocation of donor organs, to be measured as improved net\\noutcomes for the entire population of patients who might\\nbeneﬁt from a thoracic organ transplant procedure. In addi-\\ntion to the survival beneﬁt, quality of life and equity should\\nbe considered integral to any net outcome analysis, and,\\nwhere relevant data are available, perspectives are pre-\\nsented on what this information reveals.\\nData compiled over the past decade for thoracic organ\\ntransplant patients were analyzed to assess the impor-\\ntance of patient demographics, risk factors, and primary\\ncardiopulmonary disease on trends in waiting list time and\\nmortality. Analysis also sought to identify the character-\\nistics of thoracic transplant recipients and their associ-\\nated post-transplant outcomes. The focus in this report is\\non emerging trends either evident from or arguably ob-\\nscured by the available data. Areas where improved data\\ncollection and analysis could favorably inﬂuence heart and\\nlung transplant outcomes through policy are particularly\\nemphasized.\\nUnless otherwise noted, the statistics in this article come\\nfrom reference tables in the 2003 OPTN/SRTR Annual\\nReport. Two companion articles in this report, ‘Trans-\\nplant data: sources, collection, and caveats’ and ‘Ana-\\nlytical approaches for transplant research’, explain the\\nmethods of data collection, organization, and analysis that\\nserve as the basis for this article (1,2). Additional detail\\non the methods of analysis may be found in the ref-\\nerence tables themselves or in the Technical Notes of\\nthe OPTN/SRTR Annual Report, both available online at\\nhttp://www.ustransplant.org.\\n93\\n\\nRichard N. Pierson III et al.\\nHeart\\nHeart waiting list characteristics\\nConﬁrming a trend that became evident at the recent turn\\nof the century, the number of total registrants on the heart\\ntransplant waiting list decreased again in 2002, from 3934\\nin 2001 to 3803. This number still represents a large in-\\ncrease over the 2798 candidates who were listed at the end\\nof 1993. Nevertheless, the waiting list growth observed in\\nthe early 1990s seems to have peaked in the USA, as in the\\nUK (3). The decline in total numbers of registrants has been\\nassociated with a gradual increase in average candidate\\nage. While the percentage of registrants aged 1–34 years\\nhas remained fairly constant over the past 10 years, the\\nproportion of potential recipients aged 65 years or older\\nhas increased from 5% in 1993 to 12% in 2002, at the\\nrelative expense of younger adults (Figure 1).\\nMost waiting list characteristics have not substantially\\nchanged since the last report and reﬂect the racial charac-\\nteristics of the US population as a whole (Figure 2). Notably,\\nthe percentage of Hispanic/Latino registrants has grown\\nSource: 2003 OPTN/SRTR Annual Report, Table 11.1. \\n0\\n20\\n40\\n60\\n80\\n100\\n1993 1994 1995 1996 1997 1998 1999 2000 2001 2002\\nYear\\nPatients (%)\\n1–10\\n11–34\\n35–49\\n50–64\\n65+\\nFigure 1. Age distribution of heart waiting list registrants at year-\\nend, 1993–2002.\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\nWhite\\nAfrican\\nAmerican\\nAsian\\nOther/\\nMulti-race\\nRace\\n Patients (%)\\nWaiting List\\nUS Population*\\nSource: 2003 OPTN/SRTR Annual Report, Table 11.1. \\n*Data from 2000 U.S. Census.\\nFigure 2. Heart waiting list registrants, by race, 2002.\\nSource: 2003 OPTN/SRTR Annual Report, Table 11.1. \\n0\\n20\\n40\\n60\\n80\\n100\\n1993 1994 1995 1996 1997 1998 1999 2000 2001 2002\\nYear\\nPatients (%)\\n<1 year\\n1–2 years\\n>2 years\\nFigure 3. Heart waiting list registrants, by median time waiting,\\n1993–2002.\\nfrom 4% at the end of 1993 to 8% in 2002. Although\\nthey are now our most populous minority, relative under-\\nrepresentation of this ethnic group probably reﬂects their\\ncomparative youth, although socioeconomic factors may\\nalso contribute. The actual number of female candidates\\nhas stabilized over the past few years, but their total per-\\ncentage has increased as the number of male candidates\\nhas decreased. Adult females with heart failure have less\\ncoronary disease and tend to be older than their male co-\\nhorts, a factor that may contribute to the aging of the over-\\nall waiting list (4,5). Alternatively, the older average age of\\nthe waiting list may reﬂect a willingness to list carefully\\nselected patients over the age of 60 years, because their\\noutcome has not substantially differed from younger can-\\ndidates after transplant.\\nIn the past 10 years, there has been a marked increase\\nin the time that the typical patient listed at year-end has\\nwaited for a transplant, as depicted in Figure 3. At the end\\nof 2002, 48% of candidates had spent more than 2 years\\non the waiting list, compared with 17% in 1993. In addi-\\ntion, 1742 registrants were listed as temporarily inactive at\\nyear-end (about 46% of the entire cohort), up from 37% in\\n1999. There are a variety of reasons why a candidate may\\nbecome inactive, and it is difﬁcult to determine why this\\npercentage has risen over the years. Whether listing prac-\\ntice, improvement in medical management, or other fac-\\ntors are responsible is a question worthy of further inquiry.\\nIt is also clear that some registrants are removed from the\\nlist for reasons other than death or transplant. In 2002, for\\nexample, there were 571 candidates removed from the list\\nfor reasons other than death, transplantation, or transfer,\\n48% of whom were noted to have improved signiﬁcantly\\nand no longer required heart transplantation. Delineation\\nof reasons for inactive listing and removal from the waiting\\nlist could improve our understanding of this phenomenon\\nand guide appropriate policy development.\\nSince 1999, the highest priority candidates have been\\nsorted into Status 1A and 1B, with the latter listing\\n94\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n\\nThoracic organ transplantation\\ncategory consistently about three times larger than the for-\\nmer. Interestingly, the median time to transplant for both\\nStatus 1A and 1B candidates has steadily dropped since\\nthe 1A/1B system was implemented in 1999; the median\\ntime for candidates originally listed at Status 1A decreased\\nfrom 145 days in 1999 to 94 days in 2002. Many factors that\\nare not apparent in this data set—interregional variation in\\nstatus at listing or transplant, how this changes over time,\\nand characteristics that distinguish 1A from 1B patients—\\nwould provide valuable information to assess efﬁcacy and\\nequity of the evolving allocation paradigm.\\nOverall, there has been a declining trend to the median\\ntime to transplant for patients over 10 years old (Figure 4).\\nAs has been true for decades, registrants with blood type O\\ncontinue to wait much longer than those with other blood\\ntypes. Whereas race/ethnicity does not seem to correlate\\nwith differences in waiting time, women had a shorter\\nmedian time to transplant than did men (179 vs. 247 days\\nin 2002). This pattern has persisted over several years and\\nmay reﬂect relatively greater access of women, who have\\nlower average weight and height than men, to a larger pro-\\nportion of donors of either sex.\\nOver the 10 years of this report, waiting list mortality rates\\nhave declined steadily (Figure 5). There were 143.8 deaths\\nper 1000 years at risk in 2002, the lowest rate in the last\\n10 years. This trend in survival improvement was seen in\\nmost age and racial subgroups. Improved medical manage-\\nment of heart failure (such as increasing use of implanted\\ncardiac deﬁbrillators) to prevent sudden death in wait-listed\\npatients, more optimal timing of listing for transplant, and\\nthe altered organ allocation policy implemented in 1999 all\\nprobably contributed to this favorable trend. It is also pos-\\nsible, however, that transplant teams removed candidates\\nfrom the active list when death was imminent, artiﬁcially\\nreducing program-speciﬁc and overall waiting list mortal-\\nity. Although the total number of candidates initially listed\\nas Status 1A has steadily increased from 1999 to the end\\nof 2002, the death rate for this critically ill group has also\\ndeclined dramatically, from 1399 per 1000 years at risk in\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n1993\\n1994\\n1995\\n1996\\n1997\\n1998\\n1999\\n2000\\n2001\\n2002\\nAge\\nMedian time to transplant (days)\\n1–10\\n11–34\\n35–49\\n50–64\\n65+\\nSource: 2003 OPTN/SRTR Annual Report, Table 11.2. \\nFigure 4. Median time to heart transplant by recipient age, 1993–\\n2002.\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\n800\\n900\\n1993\\n1994\\n1995\\n1996\\n1997\\n1998\\n1999\\n2000\\n2001\\n2002\\nYear\\nAnnual Death Rate \\n(per 1000 patient years at risk)\\n1–10\\n11–34\\n35–49\\n50–64\\n65 +\\nSource: 2003 OPTN/SRTR Annual Report, Table 11.3.\\nFigure 5. Annual death rates for heart transplant registrants by\\nage, 1993–2002.\\n1999 to 785 in 2002. Candidates with blood type AB appear\\nto have beneﬁted the least, in that waiting list mortality for\\nthis relatively small group (17 of 558 deaths among 6990\\nlisted patients in 2002) has not fallen.\\nIn summary, despite great advances in the detection, pre-\\nvention, and management of heart failure, there continue\\nto be more candidates for heart transplant than there are\\navailable donor organs deemed suitable for use (6–8). Af-\\nter being placed on the waiting list, patients face one of\\nthree competing outcomes: transplantation, death on the\\nwaiting list, or removal from the waiting list. The relative\\nrate of heart transplant has remained level and death on\\nthe waiting list has apparently decreased, therefore one is\\nforced to conclude that the rate of removal from the wait-\\ning list has increased correspondingly. The fate of these\\npatients is not currently accessible, thus the signiﬁcance\\nof this observation is uncertain. This limitation to continued\\ncandidate listing appears to have forestalled the impending\\ncrisis predicted for cardiac transplantation in 1994 (9). If we\\nwere to learn that the decrease in waiting list mortality rate\\nis an artifact of program policies rather than more timely\\naccess to life-saving organs for a shrinking population in\\nneed, the implications for organ allocation policy would be\\nfundamentally altered.\\nHeart transplant recipient characteristics\\nAfter rising steadily during the 1980s and early 1990s,\\nthe total number of heart transplants performed has de-\\nclined by about 8% over the last 5 years, from a mid-1990s\\nplateau of about 2350 to about 2150 per year over the past\\n4 years (Figure 6). Although patients between the ages of\\n35 and 64 years continue to receive the majority of donated\\nhearts (67%), the proportion of all recipients in this group\\nhas declined by about 9% since 1993, as the number of\\nrecipients over 65 has increased from 5% to 10% of those\\ntransplanted. Women now constitute 28% of recipients,\\nup from 22% a decade ago. The relative rate of transplan-\\ntation per million population has declined for men to 2.7\\nfrom 3.8 in 1993 (Figure 7). These data may reﬂect the in-\\ncreasing incidence of end-stage heart failure in women and\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n95\\n\\nRichard N. Pierson III et al.\\nSource: 2003 OPTN/SRTR Annual Report, Table 11.4.\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\n1993 1994 1995 1996 1997 1998 1999 2000 2001 2002\\nYear\\nNumber of Patients\\n<11\\n11–17\\n18–34\\n35–64\\n65+\\nFigure 6. Heart transplant distribution by age, 1993–2002.\\n0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\n1993 1994 1995 1996 1997 1998 1999 2000 2001 2002\\nYear\\nRelative rate of transplant\\nSource: 2003 OPTN/SRTR Annual Report, Table 11.5. \\nFigure 7. Incidence rate of heart transplant male relative to fe-\\nmale, 1993–2002.\\nthe persistent inﬂuence of a younger average age at onset\\nin men. Racial and ethnic breakdowns are similar over the\\npast 5 years, except that the recent trend towards an in-\\ncreasing proportion of Hispanic/Latino recipients appears\\nto have stabilized at about 8% of all heart recipients, in\\nconcert with their relative representation on the waiting\\nlist.\\nFrom 1999 to 2002, the distribution of patient status at\\ntransplant remained stable; about 38% of patients trans-\\nplanted were classiﬁed at Status 1A, 36% at Status 1B,\\nand 26% at Status 2. Sixty-ﬁve percent of heart recipients\\nwere on life support (principally inotrope infusion or ven-\\ntricular assist devices, or VAD) at the time of transplant in\\n2002, unchanged for the most part since 1995. The per-\\ncentage of recipients who were hospitalized at the time\\nof transplant has declined from a high of 68% in 1997 to\\n53% in 2002. Similarly, the percentage of those in the in-\\ntensive care unit at the time of transplant has dropped from\\n59% in 1997 to 34% in 2002. These trends are an intended\\nconsequence of the policy change implemented in 1999,\\nwhen patient location (in or out of hospital) was removed\\nfrom the allocation algorithm as a dominant factor. No ad-\\nverse impact on waiting list or post-transplant mortality is\\nevident in the available data, although the uncertain fate of\\npatients removed from the waiting list (discussed above)\\nlends some uncertainty to this apparently favorable trend.\\nThe consequences of this policy change and associated\\ntrends on costs, quality of life, physician practices, and pa-\\ntient safety while waiting would be fruitful areas for study,\\nto identify opportunities for further policy improvement.\\nCoronary artery disease and cardiomyopathy continue to\\nbe the primary diagnoses associated with heart transplant,\\nas they are with listing, and are represented in similar pro-\\nportions (approximately 45% each) over the past 10 years.\\nCongenital heart disease accounts for about 8%, and re-\\ntransplantation, most often for primary graft failure or car-\\ndiac allograft vasculopathy, remains uncommon, account-\\ning for about 3% of both listings and transplants in recent\\nyears.\\nUnadjusted heart recipient survival at 1 year has risen\\nslowly, from 81% for patients transplanted in 1992 to\\n86% in 2001, and at 5 years exceeds 70% for the co-\\nhort of patients transplanted in 1996–1997. Graft survival\\nis only slightly lower, as expected for an organ where near-\\nterm survival is critically dependent on initial function of a\\nscarce graft and where mechanical circulatory support in\\nthe event of initial graft failure is highly morbid. Survival\\ntrends evaluated by race, ethnicity, gender, blood type,\\nlife support requirement, and location (in or out of hospi-\\ntal) did not change appreciably relative to the 2002 report.\\nAmong all demographic parameters, a history of prior heart\\ntransplant and African-American race portended the worst\\n5-year graft survival when compared with primary trans-\\nplant and other racial categories, respectively. Efforts to\\nprevent graft failure, attenuate the physiologic burden of\\nchronic immunosuppression, and understand how racial\\ndifferences interact with survival may lead to improved out-\\ncomes for these important patient subgroups and, thus, to\\nincremental improvement in overall outcomes.\\nOne-year graft survival by primary diagnosis remains simi-\\nlar for cardiomyopathy (88%) and coronary artery disease\\n(85%); 5-year graft survival rates were highest for valvular\\nheart disease (76%) and cardiomyopathy (72%). After re-\\nmaining very constant for 6 years, the overall death rate\\nwithin the ﬁrst year after transplant fell in 2002, from 161–\\n179 to 121 per 1000 patient years at risk. While incomplete\\nfollow-up for patients transplanted in the most recent year\\nprobably overestimates the actual increase in patient sur-\\nvival, the death rate within the ﬁrst year for patients trans-\\nplanted in each category of urgency (1A, 1B, 2) at the time\\nof transplant appears to be declining. Survival at 1 year is\\nsimilar for Status 1B and Status 2 (88%), and is lower for\\nStatus 1A (81%), with the difference appearing within the\\nﬁrst 3 months. Early mortality is thus probably explained\\nby known risk factors that qualify patients for the 1A cat-\\negory, including pathophysiologic events complicating the\\ninitial transplant episode in patients who are unstable be-\\nfore transplant, such as those with infected VADs or requir-\\n96\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n\\nThoracic organ transplantation\\ning mechanical ventilation. Logically, better approaches to\\nmanage patients on life support could reduce this early\\nattrition, while identiﬁcation of low-risk windows of oppor-\\ntunity within which to perform the transplant after invasive\\nor intensive support measures could justify policy changes\\ndesigned to take advantage of this information.\\nSurvival data are not currently collected separately for tech-\\nnical variations (bicaval right atrial or total atrioventricular\\nvs. conventional biatrial cuff) on the prevalent orthotopic\\nsurgical approach. While heterotopic (‘piggy-back’) heart\\ntransplantation is rare (4–21 per year in the USA since\\n1993), patient survival for this procedure is decreased at\\n1 year (82%) and 5 years (35%) compared with orthotopic\\ntransplantation (86% and 72%, respectively). Early mortal-\\nity is surprisingly low despite selection of high-risk patients\\n(typically those with ﬁxed elevation of pulmonary vascu-\\nlar resistance) to undergo this uncommon and technically\\ndemanding procedure. If causes of adverse late outcome\\nwere known and could be addressed, it is conceivable that\\nthis technique could allow safer use of expanded criteria\\ndonor hearts without adversely inﬂuencing recipient out-\\ncome.\\nDonor age remains a signiﬁcant risk factor for adverse\\nintermediate-term outcome. Heart transplant numbers and\\nrates are declining despite a signiﬁcant increase in av-\\nerage organ donor age and in heart donor age over the\\npast decade, while demographics suggest that demand\\nfor hearts should increasingly outstrip supply. Considered\\ntogether, these facts suggest that heart transplant teams\\nmay be reluctant to use older organs. Efforts to under-\\nstand the causes of increased risk with increasing heart\\ndonor age will be important to identify solutions and thus\\njustify increased use of these organs. In particular, better\\nprediction of who is likely to beneﬁt from receipt of an\\nolder donor heart would be valuable. These imperatives\\nwill be tempered if results using long-term mechanical\\ncirculatory support devices improve to equal those with\\ntransplantation.\\nWhen analyzed by recipient age, intermediate term sur-\\nvival data show that the very young (those less than 1\\nyear old, about 70 per year) and patients between 50 and\\n64 years of age (about 1100 per year) have relatively low\\nmortality after the ﬁrst year (about 14% additional attri-\\ntion by year 5), compared with approximately 18% inter-\\nmediate term mortality for patients aged 1–50 years and\\nthose over 65 years (estimated based on data in Table\\n11.11, OPTN/SRTR 2003 Annual Report). This observation\\nindirectly supports the hypothesis that either immature or\\nsenescent immunity facilitates relatively good long-term\\noutcome. Higher intermediate-term mortality among those\\npatients over 65 years of age who are deemed suitable can-\\ndidates probably reﬂects the expected inﬂuence of rising\\nall-cause mortality with increasing age in this age range.\\nData from the Registry of the International Society for\\nHeart and Lung Transplantation (ISHLT) provides longitudi-\\nnal information on more than 59 000 cardiac transplants\\nworld-wide (10,11). Actuarial survival over the past two\\ndecades shows a patient half-life of 9.3 years with a condi-\\ntional half-life of 12 years among those surviving to hos-\\npital discharge. Risk factors for both early (1 year) and\\nintermediate-term (5 year) mortality in adult cardiac trans-\\nplantation includes preoperative ventilator dependence,\\nprior cardiac transplantation, congenital heart disease as\\nthe indication, increasing recipient and donor age, and in-\\ncreasing donor ischemia time. In the cohort transplanted\\nsince the beginning of 1999, the need for dialysis, increas-\\ning recipient age (over 50 years), residence in an ICU at\\nthe time of transplant, and low center volume appear to be\\nincreasingly important risk factors for 1-year mortality, rel-\\native to patients transplanted in 1995–1998. At 1-year\\nfollow-up, the majority of deaths are attributed to infection\\nand acute rejection. By 5 years, cardiac allograft vascu-\\nlopathy (chronic rejection), malignancy, and graft failure of\\nunspeciﬁed or unknown etiology are the principle causes\\nof mortality.\\nOur clinical impression is that the current data set may\\nunderestimate a major demographic shift in the patient\\npopulation coming to heart transplant. Young patients with\\nidiopathic dilated cardiomyopathy come to transplant less\\nfrequently, perhaps as a consequence of better medical\\nmanagement. Increasingly, elective referrals are of older\\ndiabetic patients with coronary artery disease. The typical\\npatient requiring a VAD has changed from the slowly dete-\\nriorating listed patient with idiopathic disease to the acute\\nmyocardial infarction patient with hemodynamic instability\\npresenting for emergent support, and whose evaluation\\nof necessity can only occur subsequent to device implant\\nand stabilization. We expect this clinical impression will be-\\ncome more evident in future analyses.\\nLung\\nLung waiting list characteristics\\nThe lung waiting list has continued to expand during the\\npast year, reaching a new record high of 3756 registrants\\nas of December 31, 2002. This growth reﬂects a small in-\\ncrease over 2001 and an increase of over 300% since 1992.\\nOver the past 5 years (1998–2002), however, the number\\nof active patients at year-end has stabilized between about\\n2300 and 2500, while the percentage of active registrants\\nhas continued to decline. The number of new registrations\\nalso dropped slightly to 1892, the lowest number since\\n1996.\\nA trend towards increased numbers of patients with in-\\nactive status on a waiting list snapshot, as seen in Fig-\\nure 8, partially accounts for the observed increase in the\\ntotal number of registrants on the lung waiting list, with\\na 15% increase among inactive patients since 2001 and a\\nmore than threefold increase since 1995. Although a re-\\nversible deterioration in patient status may, on occasion,\\ntransiently prohibit transplantation, it is likely that this trend\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n97\\n\\nRichard N. Pierson III et al.\\nreﬂects an increasingly common practice of early place-\\nment on the waiting list. End-stage lung patients are ac-\\ntively listed relatively early in the course of their disease\\nin order to accrue waiting time in the likely eventuality of\\nsubsequent deterioration. Under current allocation guide-\\nlines, remaining inactive on the waiting list (rather than be-\\ning removed from the list) allows previously accrued active\\ntime to be retained. This approach gives the individual pa-\\ntient with relatively stable and predictable disease a bet-\\nter chance of getting an organ, because waiting list time\\nis the most inﬂuential single determinant of organ prior-\\nity (assuming geographical proximity and blood type com-\\npatibility). As such patients begin to receive organ offers,\\nthey are inactivated until the transplant team judges that\\nthey are ill enough to beneﬁt from acceptance of an or-\\ngan. This practice of early placement on the waiting list, al-\\nthough advantageous on a patient-by-patient basis, is not\\nnecessarily in the best interest of the larger community\\nof wait-listed lung patients, because, frequently, patients\\nin need of transplantation do not have the opportunity to\\nbe wait-listed early in the course of their disease. Future\\nallocation plans will probably (and appropriately) involve\\nmore parameters that reﬂect medical urgency and, per-\\nhaps, probable utility, while de-emphasizing time waited\\nafter listing, as discussed in the ﬁnal section of this\\narticle.\\nCompared with 10 years ago, a higher percentage of lung\\nwaiting list registrants are older than 50 years, increasing\\nfrom 38% in 1993 to 50% in 2002 (Figure 9). The percent-\\nage of African-American and Hispanic/Latino registrants on\\nthe lung waiting list increased from 6% and 2%, respec-\\ntively, in 1993 to 11% and 5%, respectively, in 2002. Wait-\\ning list registrants were most commonly female (58%),\\nolder than 50 years of age (50%), white (87%), blood type\\nO (49%), US residents (99%), and awaiting their ﬁrst trans-\\nplant (97%). Approximately 65% of registrants have been\\nwaiting more than a year for an available organ, and 42%\\nof all listed patients waited for more than 2 years (these\\nSource: 2003 OPTN/SRTR Annual Report, Table 12.1. \\n0\\n1000\\n2000\\n3000\\n4000\\n1995\\n1996\\n1997\\n1998\\n1999\\n2000\\n2001\\n2002\\nYear\\nNumber of Patients\\nActive\\nInactive\\nFigure 8. Active vs. inactive lung waiting list registrants at year-\\nend, 1995–2002.\\n0\\n1000\\n2000\\n3000\\n4000\\n1993 1994 1995 1996 1997 1998 1999 2000 2001 2002\\nYear\\nNumber of Patients\\n10 and under\\n11–17\\n18–34\\n35–49\\n50–64\\n65+\\nSource: 2003 OPTN/SRTR Annual Report,Table 12.1. \\nFigure 9. Age distribution of lung waiting list at year-end, 1993–\\n2002.\\nwaiting times include periods of inactive waiting list sta-\\ntus). Even with the current allocation of 90 days of bonus\\nwaiting time for idiopathic pulmonary ﬁbrosis (IPF) patients,\\nthese wait time statistics are daunting in the context of this\\nunpredictable and often rapidly progressive disease.\\nAlthough observed quartiles of time to transplant showed\\na tendency to increase between 1993 and 1999, more re-\\ncent data suggest a reversal of this trend. As shown in\\nFigure 10, 25% of recipients in 1999 were transplanted\\nwithin 451 days of listing, whereas in 2002, this same per-\\ncentage of recipients was transplanted within 251 days of\\nlisting, a 41% reduction to a level also seen in 1993 and\\n1996. Counterbalancing the general trend towards longer\\naverage times to transplant were decreasing annual death\\nrates on the waiting list (Figure 11), which decreased from\\n236 deaths per 1000 patient years at risk in 1993 to a\\n10-year low of 131 in 2002. This trend is probably a re-\\nsult, at least in part, of improving care for end-stage lung\\npatients over time. However, it may also reﬂect the, on av-\\nerage, healthier patient years at risk contributed by patients\\nwho have elected to register on the waiting list early in\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n350\\n400\\n450\\n500\\n1993 1994 1995 1996 1997 1998 1999 2000 2001 2002\\nYear\\nDays\\nSource: 2003 OPTN/SRTR Annual Report, Table 12.2. \\nFigure 10. Twenty-ﬁfth percentile time to transplant of new lung\\nwaiting list registrants, 1993–2002.\\n98\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n\\nThoracic organ transplantation\\n0\\n50\\n100\\n150\\n200\\n250\\n1993 1994 1995 1996 1997 1998 1999 2000 2001 2002\\nYear\\nAnnual Death Rate \\n(per 1000 patient years at risk)\\nSource: 2003 OPTN/SRTR Annual Report, Table 12.3. \\nFigure 11. Annual death rates per 1000 patient years at risk on\\nthe lung waiting list, 1993–2002.\\ncalculating the death rate. Hence, the change in average\\nwaiting list mortality over time should be interpreted cau-\\ntiously. That is, these trends simultaneously reﬂect the\\noverlapping inﬂuences of the wider acceptance of trans-\\nplant for end-stage lung disease, the concomitant recogni-\\ntion of the need for accumulated wait time to enable ac-\\ncess to this life-saving therapy for individual patients, and\\nthe resulting change in the typical wait-listed patient on the\\nwaiting list. As individuals attempt to adjust to the current\\nallocation system in anticipation of their own needs, it is\\nlikely that some of the patients most in need of transplant\\nare being increasingly disadvantaged by the current alloca-\\ntion rules.\\nFor patients listed in 2002, relatively favorable times\\nto transplant were observed for candidates older than\\n50 years, with 25% of recipients aged 50–64 years trans-\\nplanted within 222 days and 25% of recipients over\\n65 years of age being transplanted within 100 days in 2002.\\nIn contrast, over 75% of patients aged 11–17 years, 18–\\n34 years, and 35–49 years waited for more than 421 days,\\n399 days, and 336 days, respectively, to be transplanted.\\nGreater ﬂexibility in organ acceptance criteria may be con-\\ntributing to the shorter times to transplant in older waiting\\nlist patients. Annual death rates per 1000 patient years on\\nthe lung waiting list in 2002 were relatively low for patients\\naged 11–17 years (148), 18–34 years (138), 35–49 years\\n(111), and 50–64 years (132), respectively. The 1–5 year\\nage group had the highest annual death rate while waiting\\n(238 deaths per 1000 patient years).\\nPerhaps because there have been consistently between\\n10% and 20% more women than men on the waiting list\\nover the past decade, the observed time to transplant has\\ntended to be longer for women than for men. Interest-\\ningly, despite the longer average wait time for women, in\\nmost years annual death rates per 1000 patient years at\\nrisk on the waiting list were slightly higher for men than\\nfor women. For example, in 2002 men experienced 147\\ndeaths per 1000 patient years at risk vs. a death rate of\\n119 among women. The explanation for these apparently\\ndiscordant statistics is obscure, but, speculatively, may re-\\nﬂect earlier onset of pulmonary symptoms but relatively re-\\nduced physiologic consequences in patients with smaller\\nbody size or a different hormonal environment.\\nPotential approaches for increasing the average years of\\nlife saved per organ via risk-based waiting list prioritizations\\nare growing in popularity (see below, Current Proposal for\\nDeceased Donor Lung Allocation Policy in the USA). Re-\\nvised listing and allocation criteria may eventually reduce\\nthe perceived imperative to place candidates on the wait-\\ning list at early disease stages. The OPTN/UNOS Thoracic\\nCommittee is currently investigating allocation algorithms\\nfor this purpose, with the objective of creating priority on\\nthe waiting list by balancing risk of death on the waiting list\\nvs. post-transplant outcome (12).\\nLung transplant recipient characteristics\\nOver the last 10 years, the total number of lung transplants\\nhas slowly increased from 667 transplants performed in\\n1993 to 1054 in 2001. In 2002, the total number decreased,\\nto 1041; this is the third time in the past 10 years that there\\nhas been a slight decline in volume in comparison with the\\nprevious year. This plateau in the number of recipients is\\nmost likely because of both the relatively small increase\\nin the total number of lung donors combined and the in-\\ncreasing number of double (vs. single) lung transplants\\nperformed (discussed below). Patients in their fourth, ﬁfth,\\nand sixth decades of life account for the majority of trans-\\nplant recipients, with the largest cohort, those 50–64 years\\nold, representing nearly 56%. Racial breakdowns have re-\\nmained unchanged, with the great majority (>90%) of re-\\ncipients characterized as white. Gender distribution over\\nthe years has varied slightly, with an approximately equal\\ndistribution between male and female recipients. Given the\\nhigher proportion of women on the waiting list, it is unclear\\nwhy such a discrepancy exists. A possible explanation is\\nsmaller recipient size, with fewer small donors and a reluc-\\ntance to oversize. Other factors may relate to differences\\nin gender distribution within each of the pretransplant di-\\nagnoses combined with the differences in the waiting time\\nfor those diagnoses and improvements in medical care.\\nThe major primary diagnoses and percentages for the 2002\\ncohort were as follows: emphysema (39%), IPF (19%),\\ncystic ﬁbrosis (16%), alpha-1-antitrypsin deﬁciency (8%),\\nand primary pulmonary hypertension (PPH, 5%). The per-\\ncentage of transplants for emphysema decreased, while\\nthe other diagnostic groups had commensurate small in-\\ncreases since 2001 (Figure 12). Furthermore, 98% of lung\\nrecipients had not undergone any previous solid organ\\ntransplant. The majority of recipients were not hospital-\\nized at the time of transplantation, and less than 6% of\\npatients were on life support when transplanted. Recipi-\\nents in the intensive care unit immediately prior to trans-\\nplant had signiﬁcantly lower graft survival rates at all time\\npoints compared with the hospitalized or nonhospitalized\\ncohorts. In comparison with 1993 and 1994, when more\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n99\\n\\nRichard N. Pierson III et al.\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n1993\\n1994\\n1995\\n1996\\n1997\\n1998\\n1999\\n2000\\n2001\\n2002\\nYear\\n Patients (%)\\nCOPD\\nCF\\nIPF\\nA1A\\nPPH\\nSource: 2003 OPTN/SRTR Annual Report, Table 12.4a \\nFigure 12. Deceased donor lung transplant recipients, by diagno-\\nsis, 1993–2002.\\n0\\n20\\n40\\n60\\n80\\n100\\n1993 1994 1995 1996 1997 1998 1999 2000 2001 2002\\nYear\\n Patients (%)\\nDouble Lung\\nSingle Lung\\nSource: 2003 OPTN/SRTR Annual Report, Table 12.4a \\nFigure 13. Deceased donor lung transplant recipients, by proce-\\ndure, 1993–2002.\\nthan 60% of lung transplant procedures involved single\\nlung transplantation, the period from 1995 through 2001\\nshows an increasing use of double lung transplantation,\\nwith the data from 2002 showing, for the ﬁrst time, dou-\\nble lung transplantation exceeding single lung procedures\\n(Figure 13). Whether this trend reﬂects an increasing ac-\\nceptance of double lung transplantation as a preferen-\\ntial procedure for recipients with diagnoses that have\\npreviously been treated with single lung transplantation\\nremains to be seen. While there have been retrospec-\\ntive analyses showing an improved intermediate and\\nlong-term survival for double lung recipients, these com-\\nparisons have not been adjusted for potentially signiﬁ-\\ncant confounding variables, such as age and underlying\\ndiagnosis.\\nOne-year adjusted graft survival was 77% (for the 2001\\ncohort) and has been essentially unchanged over the past\\n9 years (75% graft survival in 1993) (Figure 14). With the\\nvery low incidence of retransplantation (2% of all recip-\\nients in 2002), patient survival was only slightly higher\\n0\\n20\\n40\\n60\\n80\\n100\\n1992 1993 1994 1995 1996 1997 1998 1999 2000 2001\\nYear of Transplant\\n Graft Survival (%)\\nSource: 2003 OPTN/SRTR Annual Report, Table 1.12a.\\nFigure 14. One-year adjusted lung graft survival, 1992–2001.\\nthan graft survival. One- and 5-year patient survivals were\\n78% and 45%, respectively. Adjusted graft survival was\\n77% at 1 year (2000–2001 cohort) and 44% at 5 years\\n(1996–1997 cohort). The 11–17 year old group had the low-\\nest 3-month graft survival at 82%, and this same group\\n(along with the 6–10 year olds) had the worst 5-year graft\\nsurvival at only 23%. The best 5-year graft survival, al-\\nbeit still only 50%, was seen among those aged 35–49\\nyears. By race, 5-year graft survival was lowest in African-\\nAmericans at 30%, whereas whites had a higher 5-year\\nsurvival rate of 45%. Ethnicity, gender, and blood type did\\nnot have a notable independent effect on short- or long-\\nterm graft survival. Of all demographics, a history of prior\\nlung transplant portended the worst 1-year (53%), 3-year\\n(30%), and 5-year (35%) graft survival. This is in keep-\\ning with other published reports, such as the 2003 ISHLT\\nregistry report (13), which shows a signiﬁcantly increased\\nrisk of 1-year mortality in this group, with an odds ratio\\nof 2.03. Interestingly, those data reveal that the increased\\nmortality is only signiﬁcant for the ﬁrst year, and not at\\n5 years. Graft survival in relationship to the primary di-\\nagnosis leading to transplant was highest at all periods\\nduring the ﬁrst 3 years for emphysema/chronic obstruc-\\ntive pulmonary disease, followed by cystic ﬁbrosis. The\\ndiagnosis with the highest 5-year graft survival rate was\\nalpha-1-antitrypsin deﬁciency (46%)—aside from the 212\\npatients with a diagnosis of ‘other’ (50%). These were fol-\\nlowed by emphysema/chronic obstructive pulmonary dis-\\nease (45%). Retransplantation, congenital (heart) disease,\\nand PPH had the lowest 5-year graft survival rates at 35%,\\n32%, and 28%, respectively (Figure 15).\\nWhile the number of recipients of living donor lungs is in-\\nsufﬁcient for statistical comparisons with recipients of de-\\nceased donor lungs, there are some interesting differences\\nbetween the two groups. The vast majority of the living\\ndonor lung recipients are in the 11–17 year and 18–34 year\\nage ranges, consistent with the high percentage of these\\nrecipients having cystic ﬁbrosis. The next largest diagnos-\\ntic indication is retransplantation. The majority of this group\\nwas hospitalized prior to the transplant: in 2002, 39% were\\n100\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n\\nThoracic organ transplantation\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\n3 months\\n1 year\\n3 years\\n5 years\\nTime after Transplant\\nPatient survival (%)\\nEmphysema/COPD\\nCF\\nIPF\\nA1A\\nPPH\\nSource: 2003 OPTN/SRTR Annual Report,Table 12.8a. Cohorts \\nare for transplants performed during 2000-2001 for 3 month & 1 year;\\n1998–1999 for 3 year; and 1996-1997 for 5 year survival.\\nFigure 15. Adjusted graft survival among deceased donor lung\\ntransplant recipients by diagnosis.\\nin the hospital and an additional 15% were in the intensive\\ncare unit prior to the transplant. Interestingly, adjusted pa-\\ntient survival was 73% (vs. 78% for the deceased donor\\ngroup) at 1 year and 50% (vs. 45%) at 5 years The low-\\nest unadjusted 5-year patient survival rates were seen in\\nthose recipients in the intensive care unit preoperatively\\n(25%) and in those recipients on preoperative life support\\n(17%).\\nData from the ISHLT registry provides information on more\\nthan 12 000 adult lung transplants world-wide (13). Actuar-\\nial survival over the past decade shows a patient half-life of\\n4.1 years with a conditional half-life of 6.6 years. Major risk\\nfactors for 1-year mortality include pretransplantation diag-\\nnosis (PPH > sarcoidosis > idiopathic pulmonary ﬁbrosis >\\nall other diagnoses), preoperative ventilator dependence,\\npreoperative intravenous inotropes, and prior lung trans-\\nplantation. Increasing recipient and donor age, increasing\\norgan ischemia time, and decreased center volume (cases\\nper year) were also signiﬁcant risk factors. Continuous vari-\\nables that signiﬁcantly affected 5-year mortality included\\nincreasing recipient age beyond 50 years and increasing\\ndonor age beyond 30 years.\\nLung transplantation is now widely accepted as a viable\\ntreatment for a heterogeneous group of end-stage lung\\ndiseases, with an associated expansion in the number of\\npotential transplant candidates. Although recent interna-\\ntional guidelines have been developed for determining can-\\ndidacy, the listing decisions are still highly inﬂuenced by\\nindividual patient considerations. Listing policy, however,\\nis difﬁcult to codify because of varied pathogenesis and\\noften unpredictable natural histories. As mentioned previ-\\nously, the current system exerts pressure to place patients\\non the waiting list at earlier stages of lung disease in re-\\nsponse to longer average times to organ availability, as has\\nbeen commented on recently in the context of cystic ﬁ-\\nbrosis, pulmonary ﬁbrosis, and sarcoidosis (14–18). These\\ntrends toward earlier diagnosis and more broadly deﬁned\\ncriteria for adding patients to the waiting list are not with-\\nout potential consequences. In describing 5-year survival\\nrates for wait-listed cystic ﬁbrosis patients, Liou et al. re-\\ncently argued that an increase in the number of patients\\nwith long survival rates on the waiting list has a deleteri-\\nous effect on survival for patients with poorer short-term\\nprognosis at diagnosis, who ﬁnd themselves at a competi-\\ntive disadvantage (19). The increasing numbers of patients\\nwith apparently better prognoses appearing on the waiting\\nlist, not to mention improvements in patient care, may have\\noverwhelmed this effect and resulted in the observed de-\\ncrease in the average waiting list mortality rate. The critical\\nshortage of donor lungs remains painfully evident to pa-\\ntients and clinicians. With respect to policy, the key ques-\\ntion may be whether utilization of a scarce resource (lungs)\\nis being optimized for efﬁcacy (net years of life saved for\\nthe end-stage lung failure population, improved net qual-\\nity of life for those transplanted) or equity (fair access, and\\nimproved access for patients at greatest risk of death while\\nwaiting).\\nHeart-Lung\\nHeart-lung waiting list characteristics\\nThe total number of registrants awaiting heart-lung trans-\\nplant fell below 200 in 2002, and in that year only 88 new\\npatients were listed. Among listed patients, 54% were on\\nthe active waiting list. Median time to transplant cannot\\nbe calculated for heart-lung candidates listed since 1993,\\nbecause more than 50% of the patients listed each year\\nhave yet to be transplanted. Although the 25th percentile\\nfor time to transplant declined from a high of over 700 days\\nfor patients listed in 1997 to just under 400 days for patients\\nlisted in 1999, for patients listed in the past 3 years it con-\\ntinues to hover between 1 and 2 years. The average age\\nand proportion of minority registrants increased over the\\npast 10 years. The waiting list death rate declined slightly,\\nto 186 per 1000 years at risk, still among the highest for any\\ngroup of transplant patients. Although the absolute num-\\nber of deaths is relatively small (38 in 2002), it continues to\\nexceed the annual number of transplants performed. Thus,\\nonly a minority of heart-lung registrants actually received a\\ntransplant and most candidates waited more than 2 years.\\nFurther changes in the allocation of heart/lung are being\\nconsidered by the OPTN/UNOS Thoracic Committee.\\nHeart-lung transplant recipient characteristics\\nOnly 32 heart-lung transplants were reported in 2002, up\\nslightly from 27 in the previous year. Common indications\\nare congenital heart disease (31%, primarily Eisenmenger\\nSyndrome), and PPH with irreversible heart failure (38%).\\nPotential heart-lung candidates often do not receive organs\\nuntil they are listed as Status 1A on the heart waiting list\\n(47% hospitalized, 50% on life support).\\nThe annual death rate following heart-lung transplanta-\\ntion remains high compared with other organs, but it has\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n101\\n\\nRichard N. Pierson III et al.\\ndropped consistently since peaking in 1997, and in 2002\\nwas only 171 per 1000 patient years at risk, down sharply\\nfrom 432 in 2001. Actuarial graft and patient survival at\\n3 months and 1 year lags behind that for double lung\\ntransplantation. This early survival disadvantage is prob-\\nably related to the technical challenges of controlling the\\nbleeding associated with scarring from prior surgery or in-\\nﬂammation, complex anatomy, proliﬁc vascular collaterals\\ncommon in these patients, and liver congestion from right\\nheart failure.\\nLong-term outcome after heart-lung transplantation is sim-\\nilar to that for double lung transplantation, with adjusted\\n5-year survival at about 40%. Only four centers performed\\nmore than two heart-lung transplants in 2002; only 18 pro-\\ngrams have performed 10 or more since 1993, and only\\nStanford, with 85 over the past decade, has averaged more\\nthan eight per year. Organ allocation policy should continue\\nto consider the needs of this small group of young, chal-\\nlenging patients who rarely have other good options.\\nCurrent Proposal for Deceased Donor Lung\\nAllocation Policy in the USA\\nThe current algorithm for lung distribution in the USA was\\nintroduced in June 1990 and was modeled after the dis-\\ntribution system for hearts. Lungs are offered ﬁrst to re-\\ncipients within the OPO where the donor is hospitalized\\nbased on active waiting time, and, if not allocated locally,\\nthe lungs are then offered to appropriate ABO identical\\nor compatible recipients listed at transplant centers within\\nconcentric 500 nautical mile circles. In March 1995, the al-\\ngorithm was modiﬁed to assign 90 days of waiting time at\\nlisting to patients with IPF in response to the perception\\nthat these patients were deteriorating more rapidly than\\nother patients and dying before organs were being made\\navailable to them.\\nThe 1999 Final Rule for operation of the OPTN included\\na requirement to examine organ distribution algorithms to\\nminimize the impact of geography on prospects for trans-\\nplantation. The mandate for the OPTN is to achieve the best\\nuse of organs by directing organs to those most in need,\\nwhile at the same time maximizing utility of organs by not\\nwasting them on futile transplantation of individuals likely\\nto be too sick to survive the operation or derive an impor-\\ntant quality of life beneﬁt (20). In addition, the Final Rule\\nrequired that the OPTN Board of Directors develop poli-\\ncies for organ allocation that are based on sound medical\\njudgment and that seek to achieve the best use of organs.\\nAlthough the initial debate focused on liver distribution, the\\nFinal Rule required the OPTN to examine all organ distribu-\\ntion algorithms and either demonstrate that they satisﬁed\\nthe principles espoused or alter the algorithms to address\\nthe new philosophy.\\nThe OPTN/UNOS Thoracic Organ Committee established\\na subcommittee to study the lung distribution algorithm\\nand make recommendations to comply with the Final Rule.\\nMembers of the Lung Allocation Subcommittee concluded\\nthat the current system was ﬂawed. One effect of alloca-\\ntion based on waiting time was the growing practice of\\nlisting patients before they truly needed to be transplanted\\n(21). A second consequence was the observation that pro-\\nportionately fewer patients with chronic obstructive pul-\\nmonary disease (COPD) were dying on the list compared\\nwith those with IPF and cystic ﬁbrosis. Presumably, those\\nwho could survive the longest on the waiting list had a bet-\\nter chance of being offered a lung or lungs for transplant,\\neven though there appears to be no survival beneﬁt of lung\\ntransplant for the large number of patients with COPD un-\\ndergoing the procedure (22).\\nThe subcommittee believed that an ideal allocation system\\nwould minimize deaths on the waiting list, while at the\\nsame time maximize the beneﬁt of transplant by incorpo-\\nrating post-transplant survival into the algorithm. Several\\nanalyses were performed to determine the feasibility of\\ndesigning such an algorithm. First, patients added to the\\nlung transplant waiting list between January 1, 1997 and\\nDecember 31, 1998 with the four most common diagnoses\\nwere analyzed to determine if data submitted at the time\\nof listing could predict death on the waiting list. Over 3100\\npatients with COPD or alpha-1-antitrypsin deﬁciency em-\\nphysema (n = 1461), cystic ﬁbrosis (n = 708), IPF (n =\\n608), or PPH (n = 327) were included in the analysis. A\\nlogistic regression model was ﬁtted for each of the four\\ndiagnoses using death on the waiting list as the outcome.\\nOver 30 clinical and demographic variables collected at the\\ntime of listing were included in the models. Patients were\\ncensored at the time of transplant. A number of factors\\nwere identiﬁed for each diagnosis that were associated\\nwith a signiﬁcantly increased risk of death on the waiting\\nlist (23,24).\\nTo establish if any of these risk factors could predict death\\nafter lung transplant, another analysis was performed on\\npatients with the same four diagnoses. Patients undergo-\\ning lung transplant between January 1, 1996 and June 30,\\n1999 (n = 2484) were analyzed to establish if data col-\\nlected at the time of listing could predict survival proba-\\nbility 1 year after transplant. There was a signiﬁcant effect\\nof diagnosis on 1-year survival: COPD/emphysema 79.7%,\\ncystic ﬁbrosis 80.2%, IPF 66%, and PPH 64% (p < 0.0001,\\nlog rank). For each diagnosis, additional factors were identi-\\nﬁed that were associated with a signiﬁcantly increased risk\\nof death following lung transplant. One-year survival was\\nchosen for this analysis based on the premise that the pre-\\ntransplant factors that played a role in post-transplant sur-\\nvival would have a diminishing effect as time went on after\\ntransplant.\\nAlthough these analyses were useful, they were limited to\\nadult patients with the four most common diagnoses and\\nthus represented approximately 80% of patients listed or\\ntransplanted. The impact of diagnosis was so strong that\\n102\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n\\nThoracic organ transplantation\\nthe subcommittee chose to establish whether all patients\\nlisted could be assigned to a diagnostic group for purposes\\nof identifying risk factors that might be useful to construct\\na distribution algorithm.\\nAnalysis of distribution of diagnoses, waiting list survival\\nprobabilities, and post-transplant survival by age for pa-\\ntients under the age of 18 years, suggested that there\\nwas a break point at age 12. Adolescent and teenage\\nlung transplant recipients aged 12 and older had similar\\ndiagnoses and survival (waiting list and post-transplant) as\\nyoung adults, while children younger than 12 years had dif-\\nferent diagnoses and survival probabilities. Thus, the sub-\\ncommittee decided to place all potential recipients under\\nthe age of 12 in the pediatric group and all patients 12 and\\nolder in the adult group. Analysis of waiting list and post-\\ntransplant 1-year survival for other end-stage lung disease\\ndiagnoses, along with consideration of pathophysiology,\\nled to the creation of four diagnostic groups for patients\\naged 12 years and older (25).\\nPatients were grouped as follows for additional analyses\\n(Table 1). Group A is composed primarily of patients with\\nobstructive lung diseases and includes sarcoidosis patients\\nwith mean PA pressure <30 mmHg. Group B is composed\\nof patients with pulmonary vascular diseases. Group C is\\ndominated by patients with cystic ﬁbrosis, and Group D\\nis composed primarily of patients with restrictive lung dis-\\neases. The prognosis for patients with sarcoidosis corre-\\nlated with PA (pulmonary artery) pressure: those patients\\nwith a mean PA pressure <30 mmHg had survival simi-\\nlar to patients with COPD, whereas sarcoidosis patients\\nwith mean PA pressure >30 mmHg had survival similar\\nto patients with IPF, and were thus considered in those\\nTable 1. Survival-based diagnosis groupings for lung allocation\\nmodeling\\nGroup A (predominantly obstructive)\\nCOPD, emphysema, chronic bronchitis\\nAlpha-one antitrypsin deﬁciency emphysema\\nBronchiectasis\\nLymphangioleiomyomatosis (LAM)\\nSarcoidosis with mean PA pressure ≤30 mmHg\\nGroup B (predominantly pulmonary vascular disease)\\nPulmonary hypertension, primary and secondary (includes\\nEisenmenger’s syndrome)\\nPulmonary veno-occlusive disease\\nGroup C\\nCystic ﬁbrosis, immunoglobulin deﬁciency, ﬁbrocavitary\\nlung disease\\nGroup D (predominantly restrictive)\\nPulmonary ﬁbrosis, including IPF, occupational lung disease\\nCollagen vascular diseases\\nBronchoalveolar carcinoma\\nSarcoidosis with mean PA pressure > 30 mmHg\\nAlveolar proteinosis\\nEosinophilic granulomatosis\\nGroup E\\nAll patients < age 12, regardless of diagnosis\\ngroups. Group E consists of all patients under the age of 12\\nyears, irrespective of the diagnosis of their end-stage lung\\ndisease.\\nAnalyses were repeated for all patients listed between\\nJanuary 1, 1997 and December 31, 1998. Because of\\nthe relatively small number of patients with pulmonary\\nvascular disease, data were collected for Group B pa-\\ntients from January 1, 1995 to December 31, 1998. Group-\\nspeciﬁc Cox regression models were used to predict\\ndeath on the waiting list. Cox models, rather than lo-\\ngistic regression, were selected to explore survival rates\\nat more than one time point. Patients were censored\\nat the time of transplant. Statistically signiﬁcant factors\\nwere identiﬁed for Groups A–D that were similar to the\\nearlier analysis, and hazard ratios were calculated for\\nthese factors (26). The number of patients in Group E\\n(n = 131) and the small number of waiting list deaths\\n(n = 43) made interpretation of the data for this group\\nunreliable.\\nA subsequent analysis of all patients transplanted in the\\nsame timeframe was performed to establish whether data\\nat the time of listing or transplant could identify factors as-\\nsociated with survival following transplant. Group-speciﬁc\\nCox regression models were ﬁtted with post-transplant\\ndeath as the outcome to identify these factors and associ-\\nated hazard ratios (27).\\nA number of factors identiﬁed as statistically signiﬁcant\\nwere judged by the Lung Allocation Subcommittee to be\\ninappropriate to put into an organ distribution algorithm be-\\ncause they were too subjective to be applied consistently.\\nAn example is being on 5 mg of more of prednisone daily for\\nGroup A patients, which was associated with an increased\\nrisk of death on the waiting list. These factors were elim-\\ninated from the models but had little effect on the other\\nvariables that were judged appropriate for inclusion in the\\nalgorithm.\\nBased on individual patient risk factors and associated haz-\\nard ratios, the subcommittee then considered options for\\nsummarizing a patient’s risk of death on the waiting list\\nover the subsequent year as opposed to the patient’s risk\\nof death during this same period if transplanted. One-year\\nsurvival estimates provided by the Cox models were con-\\nsidered, as well as estimates of the length of time each\\npatient would live during the next year with or without\\ntransplant. This latter type of summary measure, often re-\\nferred to as a 1-year expected lifetime, is calculated for\\neach individual at the time a donor organ is being consid-\\nered by summing up the area under the patient’s 1-year\\nCox model estimated survival curve. The subcommittee\\nselected these 1-year expected lifetime summary mea-\\nsures to describe each patient’s anticipated waiting list and\\npost-transplant prognosis after noting that these measures\\ncaptured more information about the patient survival pro-\\nﬁles over time than a 1-year survival rate. Each patient’s\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n103\\n\\nRichard N. Pierson III et al.\\n1-year survival beneﬁt with transplant (measured in days\\nof life saved with transplant) was then calculated by look-\\ning at the difference between the days of life a patient\\nwould be expected to live over the next year if trans-\\nplanted minus the days of life a patient would be expected\\nto live if maintained on the waiting list over the coming\\nyear (28).\\nAllocation scores based on the days of life saved within\\nthe ﬁrst year if transplanted (beneﬁt), days of life expected\\nduring the subsequent year if maintained on the waiting list\\n(urgency), and combinations of transplant beneﬁt/urgency\\nwere presented to members of the pulmonary medicine\\nand transplant communities at large at a Lung Allocation\\nConsensus Conference held in March 2003. Feedback was\\nobtained for further consideration by the Lung Allocation\\nSubcommittee.\\nIt is anticipated that serial clinical data will be useful to iden-\\ntify new factors that should be incorporated into the dis-\\ntribution algorithm and that serially collected patient data\\nmay affect the import of factors identiﬁed as signiﬁcant in\\nthe analyses. Indeed, it is the recommendation of the Lung\\nAllocation Subcommittee that analyses be undertaken to\\nidentify factors and modify their hazard ratios in the algo-\\nrithm at least every 6 months. Thus, as patients are trans-\\nplanted and removed from the list and new patients are\\nadded, risk is assessed using the most recent cohort of\\npatients.\\nOne shortcoming of the planned change to the algorithm is\\nthat hazard ratios were identiﬁed for a cohort of historical\\npatients, and data were collected at only one or at most\\ntwo points in time (post-transplant survival factors were\\navailable at listing or at transplant). To address this issue,\\nthe OPTN is undertaking an analysis of a cohort of recently\\nlisted patients at a large number of centers to determine\\nif the risk factors and their calculated hazard ratios are ap-\\npropriate and can justify altering the existing algorithm. It\\nis anticipated that this approach to lung distribution will\\nreduce deaths on the waiting list and improve survival fol-\\nlowing lung transplant.\\nSummary\\nImportant trends over the past decade are documented for\\nheart, lung, and heart-lung waiting lists and for correspond-\\ning organ transplant recipients. Wait-listed candidates and\\nthoracic organ recipients include increasing percentages of\\nolder age groups. In general, median time to transplant is\\ndeclining and post-transplant survival rates have gradually\\nimproved over the last decade for all thoracic organs. It is\\nimportant to note that the large decrease in mortality rate,\\napparent for all thoracic organ transplant recipients during\\n2002 (39% for heart, 66% for heart-lung, 14% for lung re-\\ncipients), appears to be an artifact of the data collection\\nprocess. Similar ﬁndings were reported last year for the\\n2001 results but are not conﬁrmed in this year’s report for\\nthe same 2001 calendar year interval. We conclude that\\nthe most recent year’s data will not be a valuable source\\nfor policy-making unless collection procedures can be im-\\nproved. The most likely source of this apparent problem is\\ndelayed submission of patient follow-up data to the OPTN.\\nNonetheless, gradual improvement in patient access and in\\nshort- and long-term survival appear to be sustained for all\\nthoracic organs and in most patient demographic groups,\\nsuggesting favorable inﬂuences of recently implemented\\npolicies and continued improvement in patient selection\\nand management. Improved information will permit op-\\ntimal use of these precious organs according to criteria\\nthat are generally agreed upon (and, increasingly, objec-\\ntive) among the pool of patients, providers, and payers\\nwhose interests are intimately involved in this miraculous\\nprocess.\\nReferences\\n1.\\nDickinson DM, Bryant PC, Williams MC et al. Transplant data:\\nsources, collection, and caveats. Am J Transplant 2004; 4 (suppl.\\n9): 13–26.\\n2.\\nWolfe RA, Schaubel DE, Webb RL et al. Analytical approaches\\nfor transplant research. Am J Transplant 2004; 4 (suppl. 9): 106–\\n113.\\n3.\\nAnyanwu AC, Rogers CA, Murday AJ, Steering Group. Intratho-\\nracic organ transplantation in the United Kingdom 1995–99: re-\\nsults from the UK cardiothoracic transplant audit. Heart 2002; 87:\\n449–454.\\n4.\\nLevy D, Kenchaiah S, Larson MG et al. Long-term trends in the\\nincidence of and survival with heart failure. N Engl J Med 2002;\\n347: 1397–1402.\\n5.\\nPetrie MC, Dawson NF, Murdoch DR, Davie AP, McMurray JJV.\\nFailure of women’s hearts. Circulation 1999; 99: 2334–2341.\\n6.\\nCleland JG, Alamgir F, Nikitin NP, Clark AL, Norell M. What is\\nthe optimal medical management of ischemic heart failure? Prog\\nCardiovasc Dis 2001; 43: 433–455.\\n7.\\nHunt SA, Baker DW, Chin MH et al. ACC/AHA guidelines for\\nthe evaluation and management of chronic heart failure in the\\nadult: executive summary. J Heart Lung Transplant 2002; 21: 189–\\n203.\\n8.\\nKonstam MA, Mann DL. Contemporary medical options for treat-\\ning patients with heart failure. Circulation 2002; 105: 2244–2246.\\n9.\\nStevenson LW, Warner SL, Steimle AE et al. The impending crisis\\nawaiting cardiac transplantation. Modeling a solution based on\\nselection. Circulation 1994; 89: 450–457.\\n10.\\nHosenpud JD, Bennett LE, Keck BM, Boucek MM, Novick RJ.The\\nRegistry of the International Society for Heart and Lung Transplan-\\ntation: Eighteenth Ofﬁcial Report – 2001. J Heart Lung Transplant\\n2001; 20: 805–815.\\n11.\\nBoucek MM, Edwards LB, Keck BM et al. The Registry of the\\nInternational Society for Heart and Lung Transplantation: Fifth Of-\\nﬁcial Pediatric Report – 2001 to 2002. J Heart Lung Transplant\\n2002; 21: 827–840.\\n12.\\nOrgan Procurement and Transplantation Network Thoracic Organ\\nTransplantation Committee Chaired by Frederick Grover. Report\\nto the OPTN Board of Directors, June 28–29, 2001. San Diego,\\nCA.\\n13.\\nTrulock EP, Edwards LB, Taylor DO et al. The Registry of the In-\\nternational Society for Heart and Lung Transplantation: Twentieth\\n104\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n\\nThoracic organ transplantation\\nOfﬁcial Adult Lung and Heart-Lung Transplant Report – 2003. J\\nHeart Lung Transplant 2003; 22: 625–635.\\n14.\\nAugarten A, Hannah A, Aviram M et al. Prediction of mortality\\nand timing of referral for lung transplantation in cystic ﬁbrosis\\npatients. Pediatr Transplant 2001; 5: 339–342.\\n15.\\nStern M, Reynaud-Gaubert M, Haloun A, Bertocchi M, Grenet D.\\nLung transplantation in cystic ﬁbrosis. Patient selection criteria.\\nRevue des Maladies Respiratoires 2000; 17: 779–784.\\n16.\\nFlaherty KR, White ES, Gay SE, Martinez FJ, Lynch JP. Timing\\nof lung transplantation for patients with ﬁbrotic lung diseases.\\nSemin Respir Crit Care Med 2001; 22: 517–531.\\n17.\\nMogulkoc N, Brutsche MH, Bishop PW, Greaves SM, Horrocks\\nAW, Egan JJ. Pulmonary function in idiopathic pulmonary ﬁbrosis\\nand referral for lung transplantation. Am J Respir Crit Care Med\\n2001; 164: 103–108.\\n18.\\nArcasoy SM, Christie JD, Pochettino A et al. Characteristics and\\noutcomes of patients with sarcoidosis listed for lung transplanta-\\ntion. Chest 2001; 120: 873–880.\\n19.\\nLiou TG, Adler FR, Cahill BC et al. Survival effect of lung trans-\\nplantation among patients with cystic ﬁbrosis. JAMA 2001; 286:\\n2683–2689.\\n20.\\nDepartment of Health and Human Services. Organ Procurement\\nand Transplantation Network; Final Rule. In: 42 CFR – Part 121:\\nFederal Register, October 20, 1999: 56 649–56 661.\\n21.\\nInternational guidelines for the selection of lung transplant candi-\\ndates. Am J Respir Crit Care Med 1998; 158: 335–339.\\n22.\\nHosenpud JD, Bennett LE, Keck BM, Edwards EB, Novick RJ.\\nEffect of diagnosis on survival beneﬁt of lung transplantation for\\nend-stage lung disease. Lancet 1998; 351: 24–27.\\n23.\\nEgan TM, Bennett LE, Garrity ER et al. Predictors of death on\\nthe UNOS lung transplant waiting list: results of a multivari-\\nate analysis (abstract). J Heart Lung Transplant 2001; 20 (2):\\n242.\\n24.\\nEgan TM, Bennett LE, Garrity ER et al. Are there predictors of\\ndeath at the time of listing for lung transplant (abstract)? J Heart\\nLung Transplant 2002; 21 (1): 154.\\n25.\\nMurray S, Merion R, McCullough K et al. Diagnosis-based models\\nof lung transplant waiting list mortality (abstract). Am J Transplant\\n2002; 2 (Suppl. 3): 270.\\n26.\\nEgan T, McCullough K, Bustami R et al. Predictors of death on\\nthe UNOS lung transplant waiting list (abstract). J Heart Lung\\nTransplant 2003; 22: S147.\\n27.\\nEgan T, McCullough K, Murray S et al. Risk factors for death\\nafter lung transplant in the U.S. (abstract). J Heart Lung Transplant\\n2003; 22: S146–S147.\\n28.\\nMurray S, Yu J, Wolfe RA et al. Risk/beneﬁt-based lung alloca-\\ntion algorithms (abstract). J Heart Lung Transplant 2003; 22 (15):\\nS128–S129.\\nAmerican Journal of Transplantation 2004; 4 (Suppl. 9): 93–105\\n105\\n'}, {'source': '/Users/sir/Downloads/Data/PDF/Temporal shift and predictive performance of machine learning for heart transplant outcomes - ScienceDirect.pdf', 'text': 'The Journal of Heart and Lung Transplantation\\nVolume 41, Issue 7, July 2022, Pages 928-936\\nTemporal shift and predictive performance of machine learning\\nfor heart transplant outcomes\\nRobert J.H. Miller MD  \\n, František Sabovčik MSc  , Nicholas Cauwenberghs PhD , Celine Vens PhD , Kiran K. Khush MD, MAS ,\\nPaul A. Heidenreich MD , Francois Haddad MD , Tatiana Kuznetsova MD, PhD \\nShow more\\nhttps://doi.org/10.1016/j.healun.2022.03.019\\nGet rights and content\\nBackground\\nOutcome prediction following heart transplant is critical to explaining risks and benefits to patients and decision-making when\\nconsidering potential organ offers. Given the large number of potential variables to be considered, this task may be most efficiently\\nperformed using machine learning (ML). We trained and tested ML and statistical algorithms to predict outcomes following cardiac\\ntransplant using the United Network of Organ Sharing (UNOS) database.\\nMethods\\nWe included 59,590 adult and 8,349 pediatric patients enrolled in the UNOS database between January 1994 and December 2016\\nwho underwent cardiac transplantation. We evaluated 3 classification and 3 survival methods. Algorithms were evaluated using\\nshuffled 10-fold cross-validation (CV) and rolling CV. Predictive performance for 1 year and 90 days all-cause mortality was\\ncharacterized using the area under the receiver-operating characteristic curve (AUC) with 95% confidence interval.\\nResults\\nIn total, 8,394 (12.4%) patients died within 1 year of transplant. For predicting 1-year survival, using the shuffled 10-fold CV, Random\\nForest achieved the highest AUC (0.893; 0.889-0.897) followed by XGBoost and logistic regression. In the rolling CV, prediction\\nperformance was more modest and comparable among the models with XGBoost and Logistic regression achieving the highest AUC\\n0.657 (0.647-0.667) and 0.641(0.631-0.651), respectively. There was a trend toward higher prediction performance in pediatric\\npatients.\\nConclusions\\nOur study suggests that ML and statistical models can be used to predict mortality post-transplant, but based on the results from\\nrolling CV, the overall prediction performance will be limited by temporal shifts inpatient and donor selection.\\nSection snippets\\nPatient population\\nWe included adult and pediatric patients enrolled in the UNOS database between January 1994 and December 2016 who underwent\\ncardiac transplantation (n = 73,678). We excluded patients undergoing combined heart and lung transplant (n = 842), patients with\\na #\\nb #\\nb\\nc\\nd\\nd\\nd\\nb\\nShare\\nCite\\n\\nless than 365 days follow-up for death (n = 3,301), and patients with fewer than 80 variables available (n\\xa0=\\xa01,596 see\\nSupplemental\\xa0Figure 1) since missing variables may influence the accuracy of ML predictions. In total, 67,939 patients were…\\nPatient population\\nIn total, 67,939 patients were included with median age 54 (IQR 39-61) and 49,351 (72.6%) male. In the overall population, there\\nwere 59,590 adult and 8,349 pediatric patients. During the first year, 8,394 (12.4%) patients died including 7,435 (12.4%) adult\\npatients and 959 (11.5%) pediatric patients. As expected, there were significant differences between pediatric and adult populations\\nas shown in Table 1. Population characteristics in patients who survived 1 year compared to patients who…\\nDiscussion\\nIn this study, we applied popular ML and statistical methods to predict death within the\\xa0first-year\\xa0post-cardiac transplant. In the\\nshuffled 10-fold CV, we demonstrated substantial improvements in predictive performance with tree-based models. However, there\\nwas less of a difference when predictive performance was assessed using the rolling CV procedure, which likely provides a better\\nestimate of prospective model performance. The results of our study suggest that ML and statistical models…\\nConclusions\\nOur study suggests that ML and statistical models could be used to improve prediction of mortality in the first-year post-transplant.\\nHowever, the prospective prediction performance of risk prediction models will be impacted by temporal shifts in patient and donor\\nselection. Future studies are needed to investigate methods to account for temporal shift, clarify different aspects of clinical\\nimplementation, and evaluate the net clinical impact of integrating risk predictions on post-transplant…\\nAuthor contributions\\nRobert Miller contributed to study design, data analysis, manuscript preparation, and manuscript revisions. František Sabovčik\\ncontributed to study design, data analysis, manuscript preparation, and manuscript revisions. Nicholas Cauwenberghs contributed to\\ndata analysis and manuscript revisions Celine Vens contributed to study design and manuscript revisions. Kiran Khush contributed\\nto study design and manuscript revisions. Paul Heidenreich contributed to study design and manuscript revisions. …\\nDisclosure statement\\nThe authors have no relevant conflicts of interest to disclose.…\\nAcknowledgments\\nThis work was supported in part by Health Resources and Services Administration contract 234-2005-370011C. The content is the\\nresponsibility of the authors alone and does not necessarily reflect the views or policies of the Department of Health and Human\\nServices, nor does mention of trade names, commercial products, or organizations imply endorsement by the U.S. Government.\\nResearch Unit Hypertension and Cardiovascular Epidemiology, University of Leuven, Belgium is supported by the Research…\\nRecommended articles\\nReferences (46)\\nDC Chambers et al.\\nThe international thoracic organ transplant registry of the international society for heart and lung transplantation:\\n37th adult lung transplantation report; 2020; focus on deceased donor characteristics\\nJ Heart Lung Transplant (2020)\\nM Colvin et al.\\n\\nOPTN/SRTR 2018 annual data report: heart\\nAm J Transplant (2020)\\nRM Reed et al.\\nCardiac size and sex-matching in heart transplantation: size matters in matters of sex and the heart\\nJACC Heart Fail (2014)\\nEP Kransdorf et al.\\nPredicted heart mass is the optimal metric for size match in heart transplantation\\nJ Heart Lung Transplant (2019)\\nRJH Miller et al.\\nOutcomes in patients undergoing cardiac retransplantation: a propensity matched cohort analysis of the UNOS\\nRegistry\\nJ Heart Lung Transplant (2019)\\nKK Khush et al.\\nGreat variability in donor heart acceptance practices across the United States\\nAm J Transplant (2020)\\nDL Joyce et al.\\nPredicting 1-year cardiac transplantation survival using a donor-recipient risk-assessment tool\\nJ Thorac Cardiovasc Surg (2018)\\nBD Ershoff et al.\\nTraining and validation of deep neural networks for the prediction of 90-day post-liver transplant mortality using\\nunos registry data\\nTransplant Proc (2020)\\nS Senanayake et al.\\nMachine learning in predicting graft failure following kidney transplantation: a systematic review of published\\npredictive models\\nInt J Med Informatics (2019)\\nDH Wolpert\\nStacked generalization\\nNeural Netw (1992)\\nView more references\\nCited by (9)\\nArtificial Intelligence in Nuclear Cardiology\\n2023, Cardiology Clinics\\nA machine learning model for prediction of 30-day primary graft failure after heart transplantation\\n2023, Heliyon\\nShow abstract\\nDeveloping machine learning models to predict primary graft dysfunction after lung transplantation\\n2023, American Journal of Transplantation\\nShow abstract\\n\\nHeart Transplantation for Cardiac Amyloidosis: The Need for High-Quality Data to Improve Patient Selection\\n2022, Canadian Journal of Cardiology\\nPrediction of Outcomes After Heart Transplantation in Pediatric Patients Using National Registry Data: Evaluation of\\nMachine Learning Approaches\\n2023, JMIR Cardio\\nArtificial intelligence guidance of advanced heart failure therapies: A systematic scoping review\\n2023, Frontiers in Cardiovascular Medicine\\nView all citing articles on Scopus\\nBoth authors contributed equally to this work.\\nView full text\\n© 2022 International Society for Heart and Lung Transplantation. All rights reserved.\\nAll content on this site: Copyright © 2023 Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the\\nCreative Commons licensing terms apply.\\n#\\n'}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/Temporal shift and predictive performance of machine learning for heart transplant outcomes - ScienceDirect.pdf', 'text': 'The Journal of Heart and Lung Transplantation\\nVolume 41, Issue 7, July 2022, Pages 928-936\\nTemporal shift and predictive performance of machine learning\\nfor heart transplant outcomes\\nRobert J.H. Miller MD  \\n, František Sabovčik MSc  , Nicholas Cauwenberghs PhD , Celine Vens PhD , Kiran K. Khush MD, MAS ,\\nPaul A. Heidenreich MD , Francois Haddad MD , Tatiana Kuznetsova MD, PhD \\nShow more\\nhttps://doi.org/10.1016/j.healun.2022.03.019\\nGet rights and content\\nBackground\\nOutcome prediction following heart transplant is critical to explaining risks and benefits to patients and decision-making when\\nconsidering potential organ offers. Given the large number of potential variables to be considered, this task may be most efficiently\\nperformed using machine learning (ML). We trained and tested ML and statistical algorithms to predict outcomes following cardiac\\ntransplant using the United Network of Organ Sharing (UNOS) database.\\nMethods\\nWe included 59,590 adult and 8,349 pediatric patients enrolled in the UNOS database between January 1994 and December 2016\\nwho underwent cardiac transplantation. We evaluated 3 classification and 3 survival methods. Algorithms were evaluated using\\nshuffled 10-fold cross-validation (CV) and rolling CV. Predictive performance for 1 year and 90 days all-cause mortality was\\ncharacterized using the area under the receiver-operating characteristic curve (AUC) with 95% confidence interval.\\nResults\\nIn total, 8,394 (12.4%) patients died within 1 year of transplant. For predicting 1-year survival, using the shuffled 10-fold CV, Random\\nForest achieved the highest AUC (0.893; 0.889-0.897) followed by XGBoost and logistic regression. In the rolling CV, prediction\\nperformance was more modest and comparable among the models with XGBoost and Logistic regression achieving the highest AUC\\n0.657 (0.647-0.667) and 0.641(0.631-0.651), respectively. There was a trend toward higher prediction performance in pediatric\\npatients.\\nConclusions\\nOur study suggests that ML and statistical models can be used to predict mortality post-transplant, but based on the results from\\nrolling CV, the overall prediction performance will be limited by temporal shifts inpatient and donor selection.\\nSection snippets\\nPatient population\\nWe included adult and pediatric patients enrolled in the UNOS database between January 1994 and December 2016 who underwent\\ncardiac transplantation (n = 73,678). We excluded patients undergoing combined heart and lung transplant (n = 842), patients with\\na #\\nb #\\nb\\nc\\nd\\nd\\nd\\nb\\nShare\\nCite\\n\\nless than 365 days follow-up for death (n = 3,301), and patients with fewer than 80 variables available (n\\xa0=\\xa01,596 see\\nSupplemental\\xa0Figure 1) since missing variables may influence the accuracy of ML predictions. In total, 67,939 patients were…\\nPatient population\\nIn total, 67,939 patients were included with median age 54 (IQR 39-61) and 49,351 (72.6%) male. In the overall population, there\\nwere 59,590 adult and 8,349 pediatric patients. During the first year, 8,394 (12.4%) patients died including 7,435 (12.4%) adult\\npatients and 959 (11.5%) pediatric patients. As expected, there were significant differences between pediatric and adult populations\\nas shown in Table 1. Population characteristics in patients who survived 1 year compared to patients who…\\nDiscussion\\nIn this study, we applied popular ML and statistical methods to predict death within the\\xa0first-year\\xa0post-cardiac transplant. In the\\nshuffled 10-fold CV, we demonstrated substantial improvements in predictive performance with tree-based models. However, there\\nwas less of a difference when predictive performance was assessed using the rolling CV procedure, which likely provides a better\\nestimate of prospective model performance. The results of our study suggest that ML and statistical models…\\nConclusions\\nOur study suggests that ML and statistical models could be used to improve prediction of mortality in the first-year post-transplant.\\nHowever, the prospective prediction performance of risk prediction models will be impacted by temporal shifts in patient and donor\\nselection. Future studies are needed to investigate methods to account for temporal shift, clarify different aspects of clinical\\nimplementation, and evaluate the net clinical impact of integrating risk predictions on post-transplant…\\nAuthor contributions\\nRobert Miller contributed to study design, data analysis, manuscript preparation, and manuscript revisions. František Sabovčik\\ncontributed to study design, data analysis, manuscript preparation, and manuscript revisions. Nicholas Cauwenberghs contributed to\\ndata analysis and manuscript revisions Celine Vens contributed to study design and manuscript revisions. Kiran Khush contributed\\nto study design and manuscript revisions. Paul Heidenreich contributed to study design and manuscript revisions. …\\nDisclosure statement\\nThe authors have no relevant conflicts of interest to disclose.…\\nAcknowledgments\\nThis work was supported in part by Health Resources and Services Administration contract 234-2005-370011C. The content is the\\nresponsibility of the authors alone and does not necessarily reflect the views or policies of the Department of Health and Human\\nServices, nor does mention of trade names, commercial products, or organizations imply endorsement by the U.S. Government.\\nResearch Unit Hypertension and Cardiovascular Epidemiology, University of Leuven, Belgium is supported by the Research…\\nRecommended articles\\nReferences (46)\\nDC Chambers et al.\\nThe international thoracic organ transplant registry of the international society for heart and lung transplantation:\\n37th adult lung transplantation report; 2020; focus on deceased donor characteristics\\nJ Heart Lung Transplant (2020)\\nM Colvin et al.\\n\\nOPTN/SRTR 2018 annual data report: heart\\nAm J Transplant (2020)\\nRM Reed et al.\\nCardiac size and sex-matching in heart transplantation: size matters in matters of sex and the heart\\nJACC Heart Fail (2014)\\nEP Kransdorf et al.\\nPredicted heart mass is the optimal metric for size match in heart transplantation\\nJ Heart Lung Transplant (2019)\\nRJH Miller et al.\\nOutcomes in patients undergoing cardiac retransplantation: a propensity matched cohort analysis of the UNOS\\nRegistry\\nJ Heart Lung Transplant (2019)\\nKK Khush et al.\\nGreat variability in donor heart acceptance practices across the United States\\nAm J Transplant (2020)\\nDL Joyce et al.\\nPredicting 1-year cardiac transplantation survival using a donor-recipient risk-assessment tool\\nJ Thorac Cardiovasc Surg (2018)\\nBD Ershoff et al.\\nTraining and validation of deep neural networks for the prediction of 90-day post-liver transplant mortality using\\nunos registry data\\nTransplant Proc (2020)\\nS Senanayake et al.\\nMachine learning in predicting graft failure following kidney transplantation: a systematic review of published\\npredictive models\\nInt J Med Informatics (2019)\\nDH Wolpert\\nStacked generalization\\nNeural Netw (1992)\\nView more references\\nCited by (9)\\nArtificial Intelligence in Nuclear Cardiology\\n2023, Cardiology Clinics\\nA machine learning model for prediction of 30-day primary graft failure after heart transplantation\\n2023, Heliyon\\nShow abstract\\nDeveloping machine learning models to predict primary graft dysfunction after lung transplantation\\n2023, American Journal of Transplantation\\nShow abstract\\n\\nHeart Transplantation for Cardiac Amyloidosis: The Need for High-Quality Data to Improve Patient Selection\\n2022, Canadian Journal of Cardiology\\nPrediction of Outcomes After Heart Transplantation in Pediatric Patients Using National Registry Data: Evaluation of\\nMachine Learning Approaches\\n2023, JMIR Cardio\\nArtificial intelligence guidance of advanced heart failure therapies: A systematic scoping review\\n2023, Frontiers in Cardiovascular Medicine\\nView all citing articles on Scopus\\nBoth authors contributed equally to this work.\\nView full text\\n© 2022 International Society for Heart and Lung Transplantation. All rights reserved.\\nAll content on this site: Copyright © 2023 Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the\\nCreative Commons licensing terms apply.\\n#\\n'}, {'source': '/Users/sir/Downloads/Data/PDF/A-machine-learning-based-approach-to-prognostic-an_2010_Artificial-Intellige.pdf', 'text': 'A machine learning-based approach to prognostic analysis of thoracic\\ntransplantations\\nDursun Delen a,*, Asil Oztekin b,c, Zhenyu (James) Kong b\\na Spears School of Business, Oklahoma State University, T-NCB 378, 700 North Greenwood Avenue, Tulsa, OK, 74106, USA\\nb School of Industrial Engineering and Management, Oklahoma State University, 322 Engineering North, Stillwater, OK 74078, USA\\nc Department of Industrial Engineering, Gediz University, 35230 Cankaya-Izmir, Turkey\\n1. Introduction\\n1.1. Motivation\\nThoracic (heart and lung) transplantation has been accepted as a\\nviable treatment for end-stage cardiac and pulmonary failure. The\\nArtiﬁcial Intelligence in Medicine 49 (2010) 33–42\\nA R T I C L E\\nI N F O\\nArticle history:\\nReceived 25 February 2009\\nReceived in revised form 15 December 2009\\nAccepted 10 January 2010\\nKeywords:\\nData mining\\nMachine learning\\nUNOS\\nThoracic Transplantation\\nSurvival analysis\\nPrognostic index\\nA B S T R A C T\\nObjective: The prediction of survival time after organ transplantations and prognosis analysis of different\\nrisk groups of transplant patients are not only clinically important but also technically challenging. The\\ncurrent studies, which are mostly linear modeling-based statistical analyses, have focused on small sets\\nof disparate predictive factors where many potentially important variables are neglected in their\\nanalyses. Data mining methods, such as machine learning-based approaches, are capable of providing an\\neffective way of overcoming these limitations by utilizing sufﬁciently large data sets with many\\npredictive factors to identify not only linear associations but also highly complex, non-linear\\nrelationships. Therefore, this study is aimed at exploring risk groups of thoracic recipients through\\nmachine learning-based methods.\\nMethods and material: A large, feature-rich, nation-wide thoracic transplantation dataset (obtained from\\nthe United Network for Organ Sharing—UNOS) is used to develop predictive models for the survival time\\nestimation. The predictive factors that are most relevant to the survival time identiﬁed via, (1) conducting\\nsensitivity analysis on models developed by the machine learning methods, (2) extraction of variables\\nfrom the published literature, and (3) eliciting variables from the medical experts and other domain\\nspeciﬁc knowledge bases. A uniﬁed set of predictors is then used to develop a Cox regression model and\\nthe related prognosis indices. A comparison of clustering algorithm-based and conventional risk\\ngrouping techniques is conducted based on the outcome of the Cox regression model in order to identify\\noptimal number of risk groups of thoracic recipients. Finally, the Kaplan–Meier survival analysis is\\nperformed to validate the discrimination among the identiﬁed various risk groups.\\nResults: The machine learning models performed very effectively in predicting the survival time: the\\nsupport vector machine model with a radial basis Kernel function produced the best ﬁt with an R2 value of\\n0.879, the artiﬁcial neural network (multilayer perceptron-MLP-model) came the second with an R2\\nvalue of 0.847, and the M5 algorithm-based regression tree model came last with an R2 value of 0.785.\\nFollowing the proposed method, a consolidated set of predictive variables are determined and used to\\nbuild the Cox survival model. Using the prognosis indices revealed by the Cox survival model along with a\\nk-means clustering algorithm, an optimal number of ‘‘three’’ risk groups is identiﬁed. The signiﬁcance of\\ndifferences among these risk groups are also validated using the Kaplan–Meier survival analysis.\\nConclusions: This study demonstrated that the integrated machine learning method to select the predictor\\nvariables is more effective in developing the Cox survival models than the traditional methods commonly\\nfound in the literature. The signiﬁcant distinction among the risk groups of thoracic patients also validates\\nthe effectiveness of the methodology proposed herein. We anticipate that this study (and other AI based\\nanalytic studies like this one) will lead to more effective analyses of thoracic transplant procedures to better\\nunderstand the prognosis of thoracic organ recipients. It would potentially lead to new medical and\\nbiological advances and more effective allocation policies in the ﬁeld of organ transplantation.\\n\\x02 2010 Elsevier B.V. All rights reserved.\\n* Corresponding author. Tel.: +1 918 594 8283; fax: +1 918 594 8283.\\nE-mail address: dursun.delen@okstate.edu (D. Delen).\\nContents lists available at ScienceDirect\\nArtificial Intelligence in Medicine\\njournal homepage: www.elsevier.com/locate/aiim\\n0933-3657/$ – see front matter \\x02 2010 Elsevier B.V. All rights reserved.\\ndoi:10.1016/j.artmed.2010.01.002\\n\\nincreased experience in cardiac and pulmonary transplantation,\\nimprovements in patient selection, organ preservation, and preop-\\nerativesupporthavesigniﬁcantlyreducedtheearlythreatstopatient\\nsurvival [1]. Over the past decade, the thoracic transplant waiting\\ntime for a listed patient has markedly increased, but the number of\\ntransplants performed has declined. In addition, the research also\\nfound thatthere is a perceived inequity inaccesstoorgans. Theorgan\\nallocation system needs to be improved since it may become a major\\nfactor negatively inﬂuencing the survivability of thoracic transplant\\n[2].\\nThe survivability prediction is becoming increasingly more\\nimportant in medicine. When a resource is scarce, the need for\\naccurate prediction becomes acute [3]. Especially prediction of\\nsurvival time and prognosis prediction of medical treatments are\\nclinically important and challenging problems [4]. Scarceness of\\norgans necessitates the development of effective and efﬁcient\\nprocedures to select the most optimal organ receiver since demand\\nfor organs of all patients might not be satisﬁed. To achieve this, one\\ncritical step is to reveal the knowledge underlying huge amount of\\ndata collected and stored from organ transplantation procedures\\nperformedinthepast.Theobjectivesare(1)tomaximizethepatients’\\nsurvival time after the organ transplantation surgery, and (2) to\\noptimize the prognosis for the organ recipients. These can be\\npotentially achieved by discovering the knowledge that may be\\ncontained in large dataset consisting of more than hundreds of\\ndeterminative variables regarding the donors, the potential recipi-\\nents, and transplantation procedures. Therefore, in this study a data\\nmining method is proposed to process large amount of transplanta-\\ntion data obtained from UNOS to identify the important factors as\\nwell as their relationships to the survival of the graft and the patient.\\nThereafter, a prognostic index [5,6] is developed to classify the\\npatients into different risk groups for better understanding of the\\ntransplantation phenomenon. In short, this study will address the\\nfollowing questions: (1) what are the most important variables to be\\nincluded in an effective prognostic index related to thoracic organ\\ntransplantations?(2)whatarethemostcoherentriskgroupsthatcan\\nbe formed based on the prognostic index? Predicting the thoracic\\nsurvivability and classifying the patients (potential thoracic organ\\nreceivers)intodifferentclassesofriskswouldhelpdecisionmakersin\\ndeterminingpatients’priorityfortransplantationsourceassignment.\\n1.2. Literature review\\n1.2.1. Related research in survival analysis for organ transplantation\\nIn the recent past, a number of studies were conducted using\\ndata-driven analytics on various organ transplantation datasets.\\nClosely related to the study reported herein, Hariharan et al. [7]\\nfocused on the analysis of improved graft survival rate using\\ncyclosporine after renal transplantation in both short-term (less\\nthan 1 year) and long-term (more than 1 year). A regression analysis\\nwas used to predict the probability of the graft failure after kidney\\ntransplantation in both short-term and long-term period in the light\\nof demographic characteristics, transplant-related variables, and\\npost-transplantation variables. The study performed by Herrero\\net al. [8] included 116 patients who received a liver transplant\\nbetween the years 1994 and 2000. Statistical tests are used to\\ncompare the demographic and characteristic variables, pretrans-\\nplant, andintra-operativevariables betweenthetwo groups, namely\\nyounger and older than 60. The results indicate that there is a clear\\ntrend showing that older patients have lower survival after liver\\ntransplantation. Hong et al. [9] presented a survival analysis of liver\\ntransplant patients in Canada by considering some factors such as\\nage, blood type, donor type (cadaveric or alive), race, and gender of\\nrecipient and donors. However, having limited the variables with\\nthis scope, they also admitted that the clinical information lacks of\\nmany potential details.\\nTaking a data mining approach, Kusiak et al. [10] compared two\\nrule-based data mining techniques, i.e. decision trees and rough\\nsets, to predict survival time of kidney dialysis patients. This study\\nachieved satisfactorily high prediction accuracy. The main limita-\\ntion of the study was the utilization of a small dataset with only\\n188 patients in total and also many patient-related parameters\\nwere neglected in the problem formulation. Using more traditional\\nmethods, and speciﬁcally having focused on thoracic transplanta-\\ntion, Jenkins et al. [11] and Fernandez-Yanez et al. [12] had a rich\\npool of independent variables for survivability prediction. Their\\nstudies used popular statistical techniques such as Kaplan–Meier\\nmethod of survival analysis with Mantel–Haenszel log-rank test.\\nHowever, both of these techniques have been criticized with two\\nmajor limitations: (1) linear relationships are assumed, which\\nhence cannot capture the nonlinearity among the variables, and (2)\\nthe independent variables were selected solely based on the\\nexperiences and intuitions of the analysts who conducted these\\nstudies. Thus, many potentially signiﬁcant variables might be left\\noutside the scope of this study. Tjang et al. [13] added more\\nexplanatory variables to determine the survivability in heart\\ntransplantation, such as body mass index, waiting time on the list,\\nand previous cardiac surgery, their study also ignored the non-\\nlinear relationships among the pool of survivability-related\\nvariables. Similar limitations exist in some other studies focused\\ndirectly or indirectly on thoracic transplantation [14–16].\\nThe existing studies implicitly assume that the relationships\\namong the predictive variables and output variable are linear and\\nthe predictor variables are independent of each other, which may\\nnot be valid in reality. Moreover, the abovementioned studies focus\\non\\nsmall\\ndatasets\\nwith\\nlimited\\nnumber\\nof\\npredictors\\nfor\\nsurvivability of patients after transplantation. This limitation\\nmay cause incomprehensive modeling due to the insufﬁcient\\ninformation contents (i.e., omission of a number of potentially\\nimportant predictor variables).\\n1.2.2. Related research in devising a prognostic index\\nPrognostic index (PI) provides compact prognosis information\\nregarding a speciﬁc patient based on the results of a Cox\\nproportional hazards model [5]. Cox proportional hazards model\\nhelps identify variables of prognostic importance and hence\\nprognostic index can be used to deﬁne groups of individuals at\\ndifferent risk categories. Even though prognostic index is a\\nconvenient tool to measure how well the patients are doing after\\nthe transplantation, its use in the organ transplantation area has\\nbeen limited mostly due to the lack of follow-up data. Some\\nexisting studies related to devising a PI in transplant area are\\nsummarized as follows.\\nIn the study conducted by Christensen et al. [17], it is\\nmentioned that primary biliary cirrhosis requires a liver trans-\\nplantation operation at the end stage. Based on the prognosis\\nanalysis with as well as without transplantation, it is decided\\nwhether or not the transplantation is required, if so when. To\\nachieve this goal, corresponding PIs and probabilities of surviving\\nare computed for transplantation and non-transplantation cases.\\nYoo et al. [18] developed a similar index and revealed that\\nsocioeconomic status does not inﬂuence patient or graft survival\\nthat undergoes liver transplantation at the institute where they\\nperformed their study. Deng et al. [19] conducted a study with a\\nnational dataset in Germany, which discovers the effect of\\nreceiving a heart transplant for the patients in a waiting list.\\nThe results indicate that cardiac transplant is associated with\\nsurvival beneﬁt only for patients with a predicted high risk of dying\\non the waiting list. Ghobrial et al. [20] performed a study to\\ndetermine prognostic factors for overall survival in 107 adult\\npatients with post-transplantation lymphoproliferative disorders\\n(PTLDs). It is validated that in discriminating the low and high\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n34\\n\\nscored patients the proposed prognostic scoring signiﬁcantly\\nperforms better than the International Prognostic Index for the\\nsubset of the patients (56 out of 107) with lactate dehydrogenase.\\nThe common limitation in all of these studies is similar to the\\nlimitations of the studies summarized in Section 1.2.1. Namely,\\nthey directly devise a prognostic index without determining if the\\nvariables used in prognostic index devising phase are necessary\\nand sufﬁcient. This motivates a machine learning-based initial step\\nof variable selection procedure. Because, if the critical predictive\\nfactors are not captured effectively due to the intuition- and\\nexperience-based\\nselection,\\nthe\\nresulting\\nprognostic\\nindices\\ndeveloped based on the selected variables would be inaccurate\\nand, in turn, related risk groups of patients would be deviated from\\nthe real classes. This may cause mistakes for decision maker in\\nmaking organ transplantation policies.\\n2. Proposed method\\nSection 1.2 shows that the most of the existing studies for organ\\ntransplantation\\nprocedures\\nutilize\\nconventional\\nstatistical\\napproaches such as Kaplan–Meier function and log-rank test\\nalong with expert-selected variables to predict the survivability.\\nHowever, organ transplantation procedures consist of a large\\nnumber of variables (several hundred) that may have nontrivial\\nimpact on modeling the prognosis of the grafts/patients. Using a\\nsomewhat comprehensive variable list may help discriminate\\npatients from each other by placing them into proper risk groups.\\nUnintentional omission of the important variables may lead to\\ninaccurate classiﬁcation of patient risk groups, which may, in turn,\\nlead to suboptimal organ allocation policies and ineffective\\ntreatments.\\nThis study is aimed at overcoming the abovementioned\\nshortcomings by employing both machine learning techniques\\nas well as statistical methods to identify the most critical factors\\naffecting the survivability of thoracic transplant patients. To\\nachieve this goal, this study proposes adopting a 5-step approach\\nillustrated in Fig. 1. Step 1 involves data understanding and\\npreparation, which is arguably the most time demanding step in\\nthe process. Step 2 employs various predictive modeling techni-\\nques such as support vector machines, artiﬁcial neural networks,\\nand regression trees to develop survival time prediction models\\nand to extract the most important variables by means of sensitivity\\nanalysis through the best performing model. Step 3 determines the\\nconsolidated candidate set of critical predictor variables. Step 4\\ndevelops a Cox regression model using the consolidated set of\\npredictor variables and also devises a prognostic index. The last\\nstep, Step 5, classiﬁes the patients into various risk categories by\\ncomparing and contrasting the clustering performance of algo-\\nrithm-based and manually calculated groups. Then the resulting\\nrisk categories are validated by using the Kaplan–Meier survival\\ncurves. These steps will be further explained in details in Sections\\n2.1–2.5, respectively.\\n2.1. Step one: data source and data preparation\\nIn this study, the data source that was used to validate the\\nproposed method was thoracic organ transplant dataset provided by\\nUNOS, which is a tax-exempt, medical, scientiﬁc, and educational\\norganization that operates the national Organ Procurement and\\nTransplantation Network underthe contract to the Division of Organ\\nTransplantation of the Department of Health and Human Services\\n[21]. The data ﬁles were obtained from UNOS using a formal data\\nrequisition procedure (which includes submission of speciﬁc data\\nneeds, purpose of the study, and a data use agreement). These data\\nﬁles are named as UNOS Standard Transplant Analysis and Research\\n(STAR)\\nﬁles\\nfor\\nheart,\\nlung,\\nand\\nsimultaneous\\nheart–lung\\ntransplants, namely thoracic transplants. Each transplant STAR ﬁle\\nconsists of information on all thoracic transplants that had been\\nperformed in the US and reported to UNOS since October 1, 1987. It\\nincludes both deceased- and living-donor transplants. None of the\\nﬁles include any speciﬁc patient or transplant hospital identiﬁers\\ndue to the privacy and security issues. However, there is a patient\\nidentiﬁcation number, unique to each patient, which allows linking\\nmultiple ﬁles and tracking the patient. Considering these features,\\nUNOS’s data ﬁles are recognized as the most comprehensive source\\nof information available in any single ﬁeld of medicine and for organ\\ntransplantation in US [22].\\nThere are two datasets involved in our study, which are regular\\ndataset and follow-up dataset. The regular dataset contains all\\ninformation of donors and recipients before transplantation\\noccurred, and the follow-up dataset provides all information of\\ndonors and recipients after the transplantation. The TRR_ID\\nvariable (transplant identiﬁer) is the common variable between\\nthese two datasets and the one which is proposed by UNOS to\\nmerge and integrate these two datasets. Therefore, these two\\ndatasets were combined in a relational database environment\\nusing the link (a.k.a. primary key) of TRR_ID.\\nOverall, the complete dataset consists of 310,773 records and\\n565 variables. These variables include the socio-demographic and\\nFig. 1. A ﬂowchart representation of the proposed method.\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n35\\n\\nhealth-related factors with regard to both the donor and the\\nrecipients. There are also procedure-related factors among the\\ndataset. To assign as an output (dependent variable), there are four\\npossible variables which are called pstatus, ptime, gstatus, and\\ngtime. These variables have the following meanings: whether or\\nnot the patient died after transplantation occurred (referring to\\npstatus, with dead = 1 and alive = 0). A very similar variable was\\ngstatus, referring to whether or not graft has failed (1 denoting\\n‘‘failed’’ and 0 denoting ‘‘succeeded’’). The variable ptime denoted\\npatient follow-up time (in days) from transplant to death/last\\nfollow up time. Similarly, gtime is explained as graft lifespan from\\ntransplant to death/last follow up time. Since the goal of this study\\nis to develop models to predict the survivability solely based on\\nthoracic transplant, the dependent variable was assigned as gtime.\\nThis assignment was done to discriminate the patients who died\\nsolely due to the thoracic graft incompatibility from the ones who\\ndied from any other reasons. Therefore, the rest of the potential\\ndependent variables (pstatus and ptime) were eliminated from the\\ndataset. Besides, gstatus was kept inactive up to the stage where\\nCox regression model was implemented (Step 4 in Fig. 1).\\nConsidering the gtime as the continuous dependent variable,\\nthe records for the patients whose gtime information were missing\\nwere removed from the dataset. The data set also includes some\\nidentiﬁcation variables (e.g., Donor ID) which help track the\\nrecipient patient anonymously, track the thoracic transplant\\nprocedure, or link records from multiple data ﬁles to each other.\\nSince these types of identiﬁcation variables do not have any\\ninformation content to enhance the prediction capability of the\\nmodels, after linking and integrating the ﬁles they were also\\nexcluded from the analysis dataset. Moreover, the name of\\ntransplantation type was recorded in the dataset as a variable\\nnamed Dataset which had one value (TH referring to thoracic) and\\nthe date of data processing is recorded as a variable named Date of\\nRun which are useful for data integration purposes but has no\\ninformation for contributing to the prediction of survivability and\\nhence are also excluded from the analysis dataset. Similarly, other\\nvariables having only one possible value for all records in the\\ndataset, which have no discriminating information, are also\\neliminated from the predictive modeling.\\nThis dataset had excessive number of missing values which\\nrender most of the records and variables seemingly insigniﬁcant.\\nHowever, in data mining studies one should be very reluctant to\\nremove the candidate predictor variables while at the same time\\ntrying to avoid artiﬁcial data imputation procedures. There is an\\nobvious trade-off here. As a rule of thumb, for column (variable)\\ndeletion, we were cautious to remove any variable from the\\nanalysis and assumed that if a variable has more than 95% missing\\nvalues, only then it should be regarded as not having signiﬁcant\\ninformation content and hence should be deleted. Next step was to\\nhandle the missing values by following the general convention: for\\nthe categorical variables we ﬁlled the missing values with some\\nheuristic values such as E (referring to empty) or NR (referring to\\nnot reported), and for the continuous variables we imputed the\\nmissing values with the average of the existing records. After\\nadopting these data preparation strategies, the ﬁnal dataset was\\nreduced to 372 cleansed independent variables and one dependent\\nvariable (gtime) with the total record count of 106,398.\\n2.2. Step two: predictive modeling\\nSince the dependent variable herein was a continuous variable\\n(graft survival time, which is the number of days from transplant to\\ndeath or last follow-up), the problem refers to a prediction (or\\nregression) problem (as opposed to a classiﬁcation problem). Since\\nthe\\nrelationships between the\\ndependent\\nvariable\\nand\\nthe\\nindependent variables were not known in advance, this step\\nwas to develop various predictive models for graft survival time\\nusing all of the available independent variables. It is also required\\nto check whether the models have passed the pre-speciﬁed\\nthreshold values of performance measures, speciﬁcally the R2 and\\nmean square error (MSE), to determine the best model that\\nexplains these unknown relationships between dependent and\\nindependent variables by ranking them according to these\\nmeasures. The model which is deemed to be the most successful\\none would be kept for further modeling steps to determine the\\nimportance of the independent variables.\\nSupport vector machines (SVMs) are supervised learning\\nmethods that generate input–output mapping functions from a\\nset of training data. They belong to a family of generalized linear\\nmodels which achieve a classiﬁcation or regression decision based\\non the value of the linear combination of features. They are also\\nsaid to belong to the kernel methods [23]. The mapping function in\\nSVMs can be either a classiﬁcation function (used to categorize the\\ndata) or a regression function (used to estimate the numerical\\nvalue of the desired output, as is the case in this study). Nonlinear\\nkernel functions are often used to transform the input data\\n(inherently representing highly complex nonlinear relationships)\\nto a high dimensional feature space in which the input data\\nbecome more separable (i.e. linearly separable) compared to the\\noriginal input space. Then, maximum-margin hyperplanes are\\nconstructed to optimally separate the classes in the training data.\\nTwo parallel hyperplanes are constructed on each side of the\\nhyperplane that separates the data by maximizing the distance\\nbetween the two parallel hyperplanes. An assumption is made that\\nthe\\nlarger\\nthe\\nmargin\\nor\\ndistance\\nbetween\\nthese\\nparallel\\nhyperplanes, the better the generalization error of the prediction\\nwould be.\\nArtiﬁcial neural networks (ANNs) have been utilized to model\\ncomplex relationships (such as nonlinear functions and multi-\\ncollinearity) among the predictor variables and the dependent\\nvariable [24]. ANNs are highly sophisticated analytic techniques\\ncapable of predicting new observations (on speciﬁc variables)\\nfrom other observations (on the same or other variables) after\\nexecuting a process of so-called ‘‘learning’’ from existing data\\n[25]. ANNs have been one of the most popular artiﬁcial\\nintelligence-based data modeling algorithms used in recent\\nmedical informatics studies due to their satisfactory predictive\\nperformance [26]. On the other hand, compared to other\\nmachine learning methods (such as ANNs), decision trees have\\nthe advantage of not being a black box model, namely having the\\ncapability to explain the inner structure of the model in the form\\nof a graphically represented inverse tree or a collection of\\ncondition-action rules. This advantage has made them a viable\\nand desirable alternative method in medical informatics [27]. If\\nthe dependent variable is continuous (as in the case in this\\nstudy) the resulting decision tree is called a regression tree.\\nRegression trees are known to be among the highly adaptable,\\nrelatively ﬂexible, yet computationally intensive data mining\\ntechniques [28]. Popular regression tree algorithms are CART (or\\nC&RT) [29], CHAID [30], and M5 [31] which can be used for both\\nclassiﬁcation and regression type prediction problems. ID3 and\\nits successors, C4.5 and C5 are also among the popular decision\\ntree algorithms, but they can only work for classiﬁcation type\\nprediction problems.\\n2.2.1. Performance criteria\\nTo compare the abovementioned prediction models, two\\nperformance criteria are considered: mean squared error (MSE)\\nof the model on testing dataset and R2 value between the actual\\nobservation for the target variable (Yt) and the predicted value by\\nthe model (Ft). MSE which is given by Eq. (1) does not have a rule-\\nof-thumb threshold cut-off value for acceptable models. It is a\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n36\\n\\nrelative criterion to select the best model, namely the smaller the\\nvalue the better the model has performed [32].\\nMSE ¼ 1\\nn\\nX\\nn\\nt¼1\\nðYt \\x02 FtÞ2\\n(1)\\nOn the other hand, R2 (R2\\nFt;Yt or shortly R2) which is given by\\nEq. (2) can be considered as both an absolute measure and a\\nrelative measure to determine and rank the satisfactory models\\n[33]. Unlike the MSE, the higher the R2, the better the performance\\nfor the compared models.\\nR2 ¼ 1 \\x02\\nPn\\nt¼1 ðFt \\x02 YtÞ2\\nPn\\nt¼1 ðYt \\x02 ¯YtÞ\\n2\\n(2)\\n2.2.2. k-Fold cross-validation\\nIn order to minimize the bias associated with the random\\nsampling of the training and holdout data samples in comparing\\nthe predictive accuracy of two or more methods, researchers tend\\nto use k-fold cross-validation [34]. In k-fold cross-validation, also\\ncalled rotation estimation, the complete dataset (D) is randomly\\nsplit into k mutually exclusive subsets (the folds: D1, D2, . . ., Dk) of\\napproximately equal size. The prediction model is trained and\\ntested k times. Each time (t 2 {1, 2, . . ., k}), it is trained on all but one\\nfold (Dt) and tested on the remaining single fold (Dt). The cross-\\nvalidation estimate of the overall performance criteria is calculated\\nas simply the average of the k individual performance measures as\\nin Eq. (3),\\nCV ¼ 1\\nk\\nX\\nk\\ni¼1\\nPMi\\n(3)\\nwhere CV stands for cross-validation, k is the number of folds used,\\nand PM is the performance measure for each fold [35].\\nIn this study, to estimate the performance of the prediction\\nmodels a 10-fold cross-validation approach was used. Empirical\\nstudies showed that 10 seems to be an optimal number of folds\\n(that optimizes the time it takes to complete the test while\\nminimizing the bias and variance associated with the validation\\nprocess) [34]. In 10-fold cross-validation the entire dataset is\\ndivided into 10 mutually exclusive subsets (or folds). Each fold is\\nused once to test the performance of the prediction model that is\\ngenerated from the combined data of the remaining nine folds,\\nleading to 10 independent performance estimates.\\n2.2.3. Sensitivity analysis\\nAfter selecting the best prediction model based on the\\nperformance criteria as explained in Section 2.2.1, it is required\\nto determine the importance of the independent variables. In\\nmachine learning algorithms, sensitivity analysis is a method for\\nextracting the cause and effect relationship between the inputs\\nand outputs of a trained model [36]. In the process of performing\\nsensitivity analysis, after the model is trained the learning is\\ndisabled so that the network weights are not affected. The\\nfundamental idea is that the sensitivity analysis measures the\\npredictor variables based on the change in modeling performance\\nthat occurs if a predictor variable is not included in the model.\\nHence, the measure of sensitivity of a speciﬁc predictor variable is\\nthe ratio of the error of the trained model without the predictor\\nvariable to the error of the model that includes this predictor\\nvariable [37]. The more sensitive the network is to a particular\\nvariable, the greater the performance decrease would be in the\\nabsence of that variable, and therefore the greater the ratio of\\nimportance. This method is followed in support vector machines\\nand artiﬁcial neural networks to rank the variables in terms of their\\nimportance according to the sensitivity measure deﬁned in Eq. (4)\\n[38].\\nSi ¼\\nV\\nCðFtÞ ¼ VðEðFtjXiÞÞ\\nVðFtÞ\\n(4)\\nwhere V(Ft) is the unconditional output variance. In the numerator,\\nthe expectation operator E calls for an integral over X\\x02i; that is, over\\nall input variables but Xi, then the variance operator V implies a\\nfurther integral over Xi. Variable importance is then computed as\\nthe normalized sensitivity. Saltelli et al. [39] show that Eq. (4) is the\\nproper measure of sensitivity to rank the predictors in order of\\nimportance for any combination of interaction and non-orthogo-\\nnality among predictors. As for the decision trees, variable\\nimportance measures were used to judge the relative importance\\nof each predictor variable. Variable importance ranking uses\\nsurrogate splitting to produce a scale which is a relative\\nimportance measure for each predictor variable included in the\\nanalysis. Further details on this procedure can be seen in Breiman\\net al. [29].\\n2.3. Step three: determining the candidate sets of predictor variables\\nStep 3 is to determine which predictor variables to be used in\\ndevising a prognostic index in Step 4. This step helps eliminate\\nthe insigniﬁcant variables and improves the accuracy of the\\nmodel by optimizing the predictor variables list. The potential\\ninput variables to this step consist of three candidate sets of\\npredictor variables. The ﬁrst set is composed of variables\\nselected\\nby\\nthe\\npredictive\\nmodels.\\nThe\\npredictive\\nmodels\\nexplained in Section 2.2 rank the predictor variables based on\\ntheir importance level in predicting the graft survival time. The\\npredictive variables selected by the sensitivity analysis of the\\nbest-performing model (ranked in terms of R2 and MSE) are\\nchosen as the ﬁrst set of predictive variables. The second set of\\npredictive variables is obtained by considering the expert\\ndomain\\nknowledge.\\nThis\\nset\\nincludes\\nvariables\\nwhich\\nare\\nlogically related to heart and lung transplantation such as\\ndonor’s history of cigarette usage. The third set of predictive\\nvariables is selected from the related literature. This set consists\\nof the variables which have been commonly and repeatedly used\\nin previous studies in the organ transplantation area. The second\\nand third sets of predictive variables provide more comprehen-\\nsive information for the next step, the Cox regression model, by\\nincluding the variables that might have importance in the\\nsurvival analysis but were determined to be insigniﬁcant by the\\npredictive models in Step 2.\\n2.4. Step four: survival analysis and prognostic index devising\\nStep 4 takes all the three sets of predictive variables identiﬁed\\nin Step 3, and then applies Cox regression to model the graft\\nsurvivability and ﬁlter out the candidate predictive variables\\nwhich do not have signiﬁcant survival effect. Hence, in Step 4 the\\nﬁnal critical predictive variables are determined by the Cox\\nregression model. Cox regression model also enables devising a\\nprognostic index to categorize the patients into various groups\\nwith different levels of risks.\\nCox regression model is a semi-parametric model extensively\\nused in survival analysis [6]. The survival time of each patient is\\nassumed to follow the hazard function (hi) given by Eq. (5) as\\nfollows:\\nhi ¼ h0 expðxibÞ\\n(5)\\nwhere h0 is the baseline hazard function and xi is the vector of\\npredictor variables for the ith patient. b is the vector of regression\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n37\\n\\ncoefﬁcients for the predictor variables and is assumed to be the\\nsame for all patients [40,41].\\nOne important application of Cox regression model is to identify\\nvariables which may be of prognostic importance [5]. Once\\nidentiﬁed, knowledge from these variables will be combined and\\nused to deﬁne a prognostic index, which in turn deﬁnes groups of\\norgan recipients with different levels of risk. To use the prognostic\\nindex, key patient characteristics are recorded, from which a score\\nis derived. This score gives an indication of whether a particular\\npatient has high, intermediate, or low levels of prognosis for the\\ndisease [5,42]. Recalling Eq. (5), the prognostic index (PI) for each\\npatient can be calculated by Eq. (6):\\nPI ¼ x1b1 þ x2b2 þ . . . þ xnbn\\n(6)\\nwhere x1 to xn are the patient’s values for the variables in the Cox\\nmodel, and b1 to bn are the corresponding regression coefﬁcients\\ndetermined by Cox regression model [42].\\nNote that PI in Eq. (6) represents the exponent portion in Eq. (5).\\nTherefore, the smaller the PI, the smaller the hazard function value,\\nand hence the smaller the risk associated with a particular\\nrecipient.\\n2.5. Step ﬁve: determining risk groups of thoracic recipients\\nAnimportantquestionfollowingStep4is‘‘Howmanyriskgroups\\nshould the patients be classiﬁed into?’’ In Step 5, k-means clustering\\nalgorithm, two-step cluster analysis, and conventional heuristics-\\nbased approaches are used to answer to this question. As a statistical\\nand/or pictorial veriﬁcation mechanism for the number of groups\\ndetermined by the best performing abovementioned clustering\\napproaches, ﬁnally the Kaplan–Meier survival analysis [43] is\\nadopted and corresponding survival curves are generated.\\nk-Means method is an extensively used, arguably the most\\npopular clustering algorithm that searches for a nearly optimal\\npartition with ﬁxed number of clusters represented by the\\nparameter k [44]. It proceeds by assigning k initial centroids to\\nthe multidimensional datasets. Each record in the dataset is\\nallocated to the centroid which is nearest and hence forming a\\ncluster. Each cluster centroid is then updated to be the center of its\\nmembers, followed by a new assignment of records to the nearest\\ncentroids to re-construct the clusters. The algorithm converges\\nwhen there is no further change in allocation of members to clusters\\nor some predeﬁned time-based stopping criteria is satisﬁed [45].\\nAnother popular clustering algorithm istwo-stepcluster analysis\\n(TSCA) [46,47]. It has two steps: (1) to pre-cluster the cases (or\\nrecords) into many small sub-clusters, and (2) to cluster the sub-\\nclusters resulting from pre-cluster step into the desired number of\\nclusters. The pre-cluster step uses a sequential clustering approach. It\\nscans the data records one by one and decides if the current record\\nshould be merged with the previously formed clusters or starts a\\nnew cluster based on the distance criterion. Then the cluster step\\ntakes sub-clusters resulting from the pre-cluster step as input, and\\ngroups them into the desired number of clusters. Since the number\\nof sub-clusters is much less than the number of original records, the\\ntraditional clustering methods can be used effectively. This step uses\\nthe agglomerative hierarchical clustering method [46,47]. Although\\nthere are several other clustering algorithms (e.g. Kohonen net-\\nworks) they do not allow the modeler to specify a desired number of\\nclusters at the beginning of the clustering algorithm. k-Means and\\nTSCA algorithms overcome this issue. The modeler can predeﬁne a\\nspeciﬁcnumber of clusters to group the variables and compare them\\naccording to their clustering performances. Since this is the main\\nfocus of our study, we utilized k-means and TSCA algorithms for\\nclustering the PIs and thus identfying the risk groups of thoracic\\npatients.\\nThe Kaplan–Meier analysis is a non-parametric technique used\\nto test the statistical signiﬁcance of differences between the\\nsurvival curves associated with two different circumstances [43].\\nThe analysis expresses the distribution of patient survival times in\\nterms of the proportion of patients still alive up to a given time. On\\nthe other hand, the Kaplan–Meier survival curves plot the\\nproportion of patients surviving against time which has a\\ncharacteristic decline. In biostatistics, a typical application of\\nKaplan–Meier survival curves involves grouping patients into risk\\ngroups such as low, medium, and high risks.\\n3. The case study and discussion\\nIn order to demonstrate and validate the proposed methodolo-\\ngy in Section 2, two most popular data mining toolkit are used,\\nnamely SPSS PASW Modeler1 [48] and SAS 9.1.31 [49] statistical\\nsoftware package. Using the UNOS data set, Sections 3.1–3.5\\ndiscuss the results obtained by following the above mentioned\\nmodeling procedures presented in Section 2. The prediction\\nperformance results reported herein are all based on the test (or\\nholdout) dataset.\\n3.1. Predictive model results\\nTo reveal the initially unknown relationship between the\\nthoracic input/independent variables and the continuous output/\\ndependent variable (gtime), due to the high computational time\\nrequired for 10-fold cross-validation of each model we only used\\ntwo most popular models from each family of machine learning\\ntechniques. Radial basis function (RBF) and polynomial functions\\nas Kernel methods in support vector machine were deployed. We\\nused multilayer perceptron (MLP) and RBF type of network\\nstructures for ANNs. The most recent algorithms C&RT and M5\\nwere utilized for prediction with the decision trees. The 10-fold\\naveraged prediction results in terms of MSE and R2 for each model\\nare tabulated in Table 1. The acceptance of predictive models is\\nﬁrst evaluated based on their coefﬁcient of determination (R2)\\nvalues. It is widely accepted that if R2 is higher than 0.6, the\\npredictive model has performed fairly well [50,51]. Therefore, we\\nset this as a threshold value for the model sufﬁciency. Since all the\\nmodels have passed this threshold, we kept the one with the\\nhighest R2 and the smallest MSE for further analyses, which came\\nout to be the support vector machine model with radial basis\\nKernel function in this case study.\\n3.2. Determination of the candidate covariates for Cox regression\\nmodel\\nStep 3 in the proposed method provides three different sets of\\ncandidate covariates to be used in the Cox model. Since the best\\nperforming model to explain the relationships of independent and\\nTable 1\\nComparison of machine learning prediction model results.\\nPerformance measures\\nPrediction models\\nMSE\\nR2\\nSupport vector machine\\nRBF\\n0.023\\n0.879\\nPolynomial\\n0.793\\n0.643\\nArtiﬁcial neural network\\nMLP\\n0.031\\n0.847\\nRBF\\n0.146\\n0.835\\nDecision trees\\nM5\\n0.324\\n0.785\\nC&RT\\n0.578\\n0.766\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n38\\n\\ndependent variables was found to be RBF-SVM, the sensitivity\\nanalysis as explained in Section 2.2.3 by Eq. (4) was conducted on\\nthe predictor variables to rank them in terms of their importance in\\npredicting the gtime output variable. This ﬁrst set consists of the\\npredictor variables which are presented in Table 2.\\nThe secondsetofpredictorvariableswereselectedbythe authors\\nthrough brainstorming sessions with medical professionals. The\\nsecond set of candidate covariates are tabulated in Table 3.\\nThe third set of candidate covariates was determined through\\nthe recent literature [52]. This set includes the variables commonly\\nused in the\\npreviously published studies related to organ\\ntransplantation. The third set of candidate covariates are shown\\nin Table 4.\\nThe second and third set of candidate covariates can be\\nperceived as the expert component of the method. If the predictive\\nmodels in Step 3 do not reveal some very critical predictor\\nvariables (such as the age of the recipient in our case study), the\\nmethod proposes to force the Cox model once more to review the\\nsigniﬁcance of this kind of predictor variables.\\n3.3. Deployment of Cox regression model and devising the prognostic\\nindices\\nAll the candidate covariates as determined in Section 3.2 were\\nassigned to Cox regression model at this step. The stepwise\\nvariable selection procedure was applied with 0.05 for entry and\\nTable 3\\nThe 2nd set of candidate covariates.\\nVariables\\nExplanation\\nAntiarry\\nHeart medical factors: antiarrythmics\\nat registration\\nContin_Alcohol_Old_Don\\nDeceased donor-history of alcohol\\ndependency + recent 6 months use\\nContin_Cig_Don\\nDeceased donor-history of cigarettes in past\\nand >20 pack years + recent 6 months use\\nContin_IV_Drug_Old_Don\\nDeceased donor-history of iv drug use\\n+ recent 6 months use\\nContin_Oth_Drug_Don\\nDeceased donor-history of other drugs in\\npast + recent 6 months use\\nEint\\nEthnicity interaction between recipient and\\ndonor (in the same ethnic group, y/n)\\nGint\\nGender interaction between recipient and\\ndonor (having the same sex, y/n)\\nHist_Alcohol_Old_Don\\nDeceased donor-history of alcohol dependency\\nHist_Cancer_Don\\nDeceased donor-history of cancer (y/n)\\nHist_Cig_Don\\nDeceased donor-history of cigarettes in past\\nand >20pack yrs\\nHist_Cocaine_Don\\nDeceased donor-history of cocaine use in past\\nHist_Diabetes_Don\\nDeceased donor-history of diabetes, incl.\\nDuration of disease\\nHist_Hypertens_Don\\nDeceased donor-history of hypertension\\nLOS\\nRecipient length of stay post-transplant\\nOth_Tobacco\\nOther tobacco use\\nPack_Yrs\\nIf history of cigarette use, number of\\npack years\\nTable 2\\nThe 1st set of candidate covariates generated from RBF-SVM.\\nVariables\\nExplanation\\nCitizenship\\nRecipient citizenship @ registration\\nContin_alcohol_old_don\\nDeceased donor-history of alcohol\\ndependency + recent 6 months use\\nContin_iv_drug_old_don\\nDeceased donor-history of iv drug use\\n+ recent 6 months use\\nCreat2_old\\nMost recent creatinine >2.0 mg/dl y/n\\nDa2\\nDonor a2 antigen\\nDantiarr_old\\nDeceased donor given antiarrythmics 24 h\\nprior to cross-clamp\\nDayswait_chron\\nActive days on waiting list\\nDobut_don_old\\nDeceased donor-dobutamine w/in 24 h\\npre-cross-clamp\\nEducation\\nRecipient highest educational level @\\nregistration\\nEthcat_don\\nDonor ethnicity category\\nFluvaccine\\nAnti-viral treatment—ﬂuvaccine\\nFunc_stat_tcr\\nRecipient functional status @ registration\\nFunc_stat_trr\\nRecipient functional status @transplant\\nGender\\nRecipient gender\\nHbsab_don\\nDeceased donor hbsab test result\\nHemo_pa_dia_tcr\\nMost recent hemodynamics pa (dia) mm/hg\\n@ registration\\nHemo_pa_mn_tcr\\nMost recent hemodynamics pa (mean) mm/hg\\n@ registration\\nHeparin_don\\nDeceased donor management—heparin\\nHgt_cm_tcr\\nRecipient height @ registration\\nHist_alcohol_old_don\\nDeceased donor-history of alcohol\\ndependency\\nHtlv2_old_don\\nDeceased donor-antibody to htlv ii result\\nImpl_deﬁbril_after_list\\nImplantable deﬁbrillator inserted between\\nlisting and transplant\\nInotrop_agents\\nDeceased donor—three or more inotropic\\nagents at time of incision\\nInotrop_support_don\\nDeceased donor inotropic medication at\\nprocurement (y/n)\\nIschtime\\nIschemic time in hours\\nMed_cond_tcr\\nRecipient medical condition @ registration\\nMed_cond_trr\\nRecipient medical condition pretransplant\\n@ transplant\\nPhysical_capacity_tcr\\nPhysical capacity at listing\\nPretreat_med_don_old\\nDeceased donor medication(s) from brain\\ndeath to 24 h prior to procurement\\nPrior_lung_surg_tcr\\nRecipient prior lung surgery (non-transplant)\\nat listing\\nPst_airway\\nEvents prior to discharge: airway dehiscence\\nPst_cardiac\\nEvents prior to discharge: cardiac re-operation\\nPst_dial\\nEvents prior to discharge: dialysis\\nPst_drug_trt_infect\\nEvents prior to discharge: any drug\\ntreated infection\\nPst_surgical\\nEvents prior to discharge: other surgical\\nprocedures\\nPt_t4_don\\nDeceased donor-thyroxine-t4 b/n brain\\ndeath w/in 24 h of procurement\\nSternotomy_tcr\\nEvents occurring prior to listing: sternotomy\\nSternotomy_trr\\nEvents occurring between listing and\\ntransplant: sternotomy\\nSteroid\\nChronic steroid use y/n/u @ transplant\\nTrtrej1y\\nTreated for rejection within 1 year\\nTrt_pulm_sepsis\\nIV treated pulmonary sepsis y/n/u @\\nregistration\\nVad_tah_tcr\\nRecipient on life support—ventilator @\\nregistration (1 = yes, 0 = no)\\nVad_tah_trr\\nRecipient on life support—ventilator @\\ntransplant\\nTable 4\\nThe 3rd set of candidate covariates.\\nVariables\\nExplanation\\nABO\\nRecipient blood group at registration\\nABO_Don\\nDonor blood type\\nABO_Mat\\nDonor-recipient ABO match level\\nAge\\nRecipient age (years)\\nAge_Don\\nDonor age (years)\\nDayswait_Chron\\nActive days on waiting list\\nDon_TY\\nDonor type—deceased/living\\nEthcat\\nRecipient ethnicity category\\nEthcat_Don\\nDonor ethnicity category\\nGender\\nRecipient gender\\nGender_Don\\nDonor gender\\nHbsab_Don\\nDeceased donor hbsab test result\\nIschtime\\nIschemic time in hours\\nMed_Cond_Tcr\\nRecipient medical condition at registration\\nMed_Cond_Trr\\nRecipient medical condition pretransplant\\nat transplant\\nWgt_kg_Don\\nDonor weight (kg)\\nWgt_kg_Tcr\\nRecipient weight (kg) at registration\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n39\\n\\n0.1 for removal as signiﬁcance threshold criteria. The predictor\\nvariables determined to be signiﬁcant by Cox regression model are\\nlisted along with their corresponding statistics in Table 5. The rest\\nof the variables (which were in Tables 2, 3, or 4 but not in Table 5)\\nwere eliminated since they were found to be insigniﬁcant by Cox\\nregression model.\\nAs listed in Table 5, 14 of the variables had prognostic value\\nwhich are determined by the Cox model as signiﬁcant and kept in\\nthe Cox equation. Therefore, they were used to calculate the PIs by\\nmeans of Eq. (6). The PI values received here were ranging between\\n0 and 3.\\n3.4. Clustering the prognostic indices\\nOnce the prognostic indices (PIs) for each recipient calculated,\\nthe next step was to cluster the recipients through these PIs.\\nHowever, the problem of deﬁning these clusters and deciding\\nwhich value to cut off and categorize the recipients should be\\nsolved\\nﬁrst.\\nTwo\\ncommonly\\nused\\nclustering\\nalgorithms\\nas\\ndescribed in Section 2.5, namely k-means and TSCA were used\\nto determine these clusters. We also compared these algorithm-\\nbased clusters to conventional PI devising methods in medicine.\\nTwo potential ways to do the clustering are constructing equal-\\nwidth PIs and equal-percentile PIs in this research domain. In the\\nformer one, the PIs are separated in groups so that the increments\\nof PI in each group are equal whereas the latter method focuses on\\nallocating the patients equally to each group. The algorithms k-\\nmeans and TSCA were run by changing the value for k (number of\\nclusters to be formed). The value of k with 2, 3, 4, and 5 were tried\\nbecause it was considered that having clusters more than 5 would\\nnot provide logical risk groups to categorize and would probably\\nnot be easy to name and interpret medically afterwards. The\\nresults for each run are represented in Table 6.\\nThe performances of these entire four approaches with different\\nnumber of clusters (k = 2–5) were compared using intraclass inertia\\nas the performance measure to decide which one to adopt. It is a\\nmeasure which shows how compact each cluster is. Intraclass\\ninertia is the average of the distances between the means and the\\nobservations in each cluster. Eq. (7) indicates this value for given k\\nnumber of clusters [53].\\nFðkÞ ¼ 1\\nn\\nX\\nk\\nX\\ni 2 Ck\\nX\\nm\\nP¼1\\nðXiP \\x02 mkPÞ2\\n(7)\\nwhere n is the number of total observations, CK is the set of kth\\ncluster, XiP is the value of the attribute P for observation i and mkP is\\nthe mean of the attribute P in the kth cluster. Note that in our case\\nthere is only one attribute which is PI, and hence m = 1.\\nThe intraclass inertia values for each possible cluster are also\\nsummarized in Table 6. Prognostic indices were clustered best with\\nk = 3 with k-means clustering algorithm in our case as seen in\\nTable 6 considering its low intraclass inertia value. As seen in\\nTable 6, this classiﬁcation not only gives the lowest intraclass\\ninertia value but also provides an even distribution of the thoracic\\npatients for our nation-wide dataset (38%, 16%, and 46% for low,\\nmedium, and high risk groups of patients, respectively). Although 5\\nclusters with k-means algorithm and 3 clusters in two-step cluster\\nanalysis perform very close to k-means algorithm with 3 clusters,\\nneither of them provides such an even distribution of patients.\\nNote that in addition to considerably higher inertia scores,\\nheuristic calculation with equal-width PIs distribute the nation-\\nwide patients highly skewed to lower tails of risk groups for all ﬁve\\npotential cluster formations. Therefore, we conclude that the k-\\nmeans algorithm based clustering performs better than the other\\npotential groupings in terms of both objective and subjective\\naspects.\\n3.5. Validation of risk groups by Kaplan–Meier survival analysis\\nTo validate the established prognostic indices with 3 clusters\\nand hence the various risk groups in Section 3.4, Kaplan–Meier\\nsurvival analysis [43] was conducted. The corresponding PI\\nclusters were matched with the patients and their predictor\\nvariables from Table 5. In Kaplan–Meier survival analysis the\\npredictor variables were used as explanatory variables and the PI-\\nbased clusters were used as the strata variable to label the patients\\nwith different risks. The main objective here was to compare\\nsurvivor functions for different risk groups of thoracic recipients.\\nIf the survivor function for one risk group is always higher than the\\nsurvivor function for another risk group, than the ﬁrst group\\nclearly lives longer than the second one. The less the survivor\\nfunctions cross, the better the discrimination of the patients\\nwould be. Fig. 2 shows this clear distinction for k-means\\nalgorithm-based PIs.\\nIn order to show that there is a statistically signiﬁcant\\ndifference among these three risk groups, the test of equality\\nover strata was also conducted. Test of equality over strata\\ncontains rank and likelihood-based statistics for testing homoge-\\nneity of survivor functions across strata. The rank tests with the\\nlog-rank test and Wilcoxon test indicate a signiﬁcant difference\\nbetween the risk groups. These results are also supported by\\nlikelihood-based\\nstatistics.\\nThese\\nstatistical\\ntest\\nresults\\nare\\nsummarized in Table 7.\\nTable 5\\nThe variables kept in the Cox regression model.\\nVariable\\nSE\\nChi_square test\\nDF\\nSigniﬁcance\\nexp(b)\\n95% CI for exp(b)\\nLower\\nUpper\\nLOS\\n0.0002\\n385.8701\\n1\\n<.0001\\n1.004\\n1.004\\n1.005\\nEint\\n0.0178\\n56.9447\\n1\\n<.0001\\n0.844\\n0.844\\n0.905\\nGint\\n0.0183\\n11.8644\\n1\\n0.0006\\n0.906\\n0.906\\n0.973\\nAge_Don\\n0.0006\\n247.3162\\n1\\n<.0001\\n1.009\\n1.009\\n1.011\\nWgt_kg_Tcr\\n0.0004\\n5.5091\\n1\\n0.0189\\n0.998\\n0.998\\n1.000\\nWgt_kg_Don\\n0.0005\\n21.3483\\n1\\n<.0001\\n0.997\\n0.997\\n0.999\\nAcyclovir\\n0.0300\\n14.6651\\n1\\n0.0001\\n0.840\\n0.840\\n0.945\\nCitizenship\\n0.0554\\n5.5538\\n1\\n0.0184\\n0.787\\n0.787\\n0.978\\nDayswait_Chron\\n0.0002\\n7.5318\\n1\\n0.0061\\n1.000\\n1.000\\n1.000\\nFluvaccine\\n0.0189\\n15.9915\\n1\\n<.0001\\n1.039\\n1.039\\n1.119\\nIschtime\\n0.0058\\n239.5080\\n1\\n<.0001\\n1.081\\n1.081\\n1.105\\nMed_Cond_Tcr\\n0.0109\\n75.6231\\n1\\n<.0001\\n1.076\\n1.076\\n1.123\\nVad_Tah_Trr\\n0.0002\\n5.7861\\n1\\n0.0162\\n1.000\\n1.000\\n1.001\\nVad_Tah_Tcr\\n0.0077\\n48.9955\\n1\\n<.0001\\n1.040\\n1.040\\n1.072\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n40\\n\\n4. Conclusions and future research directions\\nThis study demonstrates that machine learning-based meth-\\nodology for selecting predictor variables in survivability and\\nprognostic modeling of thoracic organ transplantation is superior\\nto the approaches adopting only expert-selected variables. The\\nstudy showed that of the comprehensive list of predictors, some\\nhave been included in the previous studies (such as gender and age\\nof the recipient, his/her medical condition at registration) while\\nsome others (which are found to be critical) have been absent from\\nthe related literature. These variables (e.g. such as recipient length\\nof stay post-transplant and the interaction of gender and ethnicity\\nbetween the recipient and the donor) should be combined with the\\nfactors identiﬁed in previous studies to better understand and\\nimprove the organ transplantation process.\\nThe study revealed that based on k-means clustering algorithm\\nthe thoracic organ recipients should be allocated into an optimal\\nnumber of ‘‘three’’ risk groups, namely low, medium, and high. This\\nﬁnding conﬁrms the conventional medical discrimination com-\\nmonly used in this ﬁeld of study. However, it also proves that this\\ngrouping should be better performed through a data mining\\nperspective rather than a heuristics-based approach because the\\nlatter one gives more skewed distribution of patients for our US\\nnation-wide\\ndataset.\\nThis\\nis\\nthe\\npoint\\nwhere\\nthe\\nmedical\\nprofessionals should be advised to handle the problem in the\\nfuture.\\nSome of the research extensions to the study reported in this\\narticle includes analysis of other organ types as well as the analysis\\nFig. 2. Kaplan–Meier survival curves for three PIs.\\nTable 7\\nTests of equality over risk groups for k-means based three PI cluster.\\nTest\\nChi-square\\nDF\\nPr > Chi-square\\nLog-rank\\n1002.6135\\n2\\n<.0001\\nWilcoxon\\n939.7492\\n2\\n<.0001\\n\\x022 log(LR)\\n1013.3153\\n2\\n<.0001\\nTable 6\\nThe comparative results for clustering and heuristic-based algorithms.\\nBy clustering algorithms\\nk-Means algorithm\\nTwo-step cluster analysis\\nNumber of clusters\\nRisk group\\nPrognostic index\\nNumber of patients\\nIntraclass inertia\\nPrognostic index\\nNumber of patients\\nIntraclass inertia\\nCluster 1\\nLow\\n0–0.69\\n21163 (58%)\\n12.4 \\x03 10\\x028\\n0–1.09\\n34199 (94%)\\n866.30 \\x03 10\\x028\\nCluster 2\\nHigh\\n0.70–3\\n15262 (41%)\\n1.1–3\\n2226 (6%)\\nCluster 1\\nLow\\n0–0.56\\n13766 (38%)\\n1.68 \\x03 10\\x028\\n0–1.04\\n33529 (92%)\\n2.20 \\x03 10\\x028\\nCluster 2\\nMedium\\n0.57–0.91\\n5834 (16%)\\n1.05–1.83\\n2807 (7.7%)\\nCluster 3\\nHigh\\n0.92–3\\n16825 (46%)\\n1.84–3\\n89(0.3%)\\nCluster 1\\nLow\\n0–0.49\\n15227 (42%)\\n11.2 \\x03 10\\x028\\n0–0.41\\n6410 (17%)\\n445.39 \\x03 10\\x028\\nCluster 2\\nLow–medium\\n0.50–0.77\\n1764 (5%)\\n0.42–0.70\\n15163 (42%)\\nCluster 3\\nMedium–high\\n0.78–1.12\\n9542 (26%)\\n0.71–1.04\\n11892(33%)\\nCluster 4\\nHigh\\n1.13–3\\n9892 (27%)\\n1.05–3\\n2960 (8%)\\nCluster 1\\nVery low\\n0–0.44\\n13266 (36%)\\n3.02 \\x03 10\\x028\\n0–0.36\\n2960 (8%)\\n720.71 \\x03 10\\x028\\nCluster 2\\nLow\\n0.45–0.69\\n451(1%)\\n0.37–0.53\\n10475 (29%)\\nCluster 3\\nMedium\\n0.70–0.95\\n4449 (12%)\\n0.54–0.73\\n10815 (29%)\\nCluster 4\\nHigh\\n0.96–1.39\\n7814 (22%)\\n0.74–1.04\\n4674(13%)\\nCluster 5\\nVery high\\n1.40–3\\n10445 (29%)\\n1.05–3\\n7501 (21%)\\nBy heuristics-based calculation\\nWith equal PI widths\\nWith equal percentiles\\nNumber of clusters\\nRisk group\\nPrognostic index\\nNumber of patients\\nIntraclass inertia\\nPrognostic index\\nNumber of patients\\nIntraclas s inertia\\nCluster 1\\nLow\\n0–1.5\\n36154(99%)\\n713.68 \\x03 10\\x028\\n0–0.64\\n18212(50%)\\n7.02 \\x03 10\\x026\\nCluster 2\\nHigh\\n1.6–3\\n271 (1%)\\n0.65–3\\n18213 (50%)\\nCluster 1\\nLow\\n0–0.9\\n32571 (89%)\\n1678.65 \\x03 10\\x028\\n0–0.53\\n12142 (33.5%;\\n2.01 \\x03 10\\x026\\nCluster 2\\nMedium\\n1–1.9\\n3794 (10%)\\n0.54–0.76\\n12141 (33%)\\nCluster 3\\nHigh\\n2–3.0\\n60 (1%)\\n0.77–3\\n12142 (33.5%;\\nCluster 1\\nLow\\n0–0.7\\n26087 (72%)\\n12961.43 \\x03 10\\x028\\n0–0.47\\n9106 (25%)\\n2755.48 \\x03 10\\x026\\nCluster 2\\nLow–medium\\n0.8–1.5\\n10153 (28%)\\n0.48–0.64\\n9106(25%)\\nCluster 3\\nMedium–high\\n1.6–2.3\\n162(0.4%)\\n0.65–0.82\\n9106 (25%)\\nCluster 4\\nHigh\\n2.4–3\\n23 (0.06%)\\n0.83–3\\n9107 (25%)\\nCluster 1\\nVery low\\n0–0.5\\n15605 (43%)\\n457.67 \\x03 10\\x028\\n0–0.43\\n7285 (20%)\\n3.16 \\x03 10\\x026\\nCluster 2\\nLow\\n0.6–1.1\\n19608 (54%)\\n0.44–0.58\\n7285 (20%)\\nCluster 3\\nMedium\\n1.2–1.7\\n1109(3%)\\n0.59–0.71\\n7285 (20%)\\nCluster 4\\nHigh\\n1.8–2.3\\n80 (0.2%)\\n0.72–0.87\\n7285 (20%)\\nCluster 5\\nVery high\\n2.4–3\\n23 (0.06%)\\n0.88–3\\n7285 (20%)\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n41\\n\\nof multiorgan scenarios where the correlations among the organs\\ncoming from the same donor are also included in the formulation\\nof the problem. Another potential further research direction of this\\nstudy is to validate the patterns obtained from the data mining\\nmodels with a comprehensive simulation model of the organ\\ntransplantation process. Using actual cases, a comprehensive\\ndiscrete-event simulation model can be developed and used as a\\ntest-bed where the potential beneﬁts and limitations of these\\nnovel patterns are tested and validated for a sufﬁciently long\\nperiod of time in the computer simulation environment.\\nReferences\\n[1] Trigt PV, Davis D, Shaeffer GS, Gaynor JW, Landolfo KP, Higginbotham MB, et al.\\nSurvival beneﬁts of heart and lung transplantation. Annals of Surgery\\n1996;223:576–84.\\n[2] Pierson RN, Barr ML, McCullough KP, Egan T, Garrity E, Jessup M, et al. Thoracic\\norgan transplantation. American Journal of Transplantation 2004;4:93–105.\\n[3] Sheppard D, McPhee D, Darke C, Shretha B, Moore R, Jurewitz A, et al. Predicting\\ncytomegalovirus disease after renal transplantation: an artiﬁcial neural network\\napproach. International Journal of Medical Informatics 1999;54:\\n55–76.\\n[4] Lin RS, Horn SD, Hurdle JF, Goldfarb-Rumyantzev S. Single and multiple time-\\npoint prediction models in kidney transplant outcomes. Journal of Biomedical\\nInformatics 2008;41:944–52.\\n[5] Parmar MKB, Machin D. Survival analysis: a practical approach. Cambridge,\\nUK: John Wiley & Sons; 1996.\\n[6] Cox DR. Analysis of survival data. London: Chapman&Hall; 1984.\\n[7] Hariharan S, Johnson CP, Bresnahan BA, Taranto SE, McIntosh MJ, Stablein D.\\nImproved graft survival after renal transplantation in the United States, 1988\\nto 1996. The New England Journal of Medicine 2000;342:605–12.\\n[8] Herrero JI, Lucena JF, Quiroga J, Sangro B, Pardo F, Rotellar F, et al. Liver transplant\\nrecipients older than 60 years have lower survival and higher incidence of\\nmalignancy. American Journal of Transplantation 2003;3:1407–12.\\n[9] Hong Z, Wu J, Smart G, Kaita K, Wen SW, Paton S, et al. Survival analysis of liver\\ntransplant patients in Canada. Transplantation Proceedings 2006;38:2951–6.\\n[10] Kusiak A, Dixon B, Shah S. Predicting survival time for kidney dialysis patients:\\na data mining approach. Computers in Biology and Medicine 2005;35:311–27.\\n[11] Jenkins PC, Flanagan MF, Jenkins KJ, Sargent JD, Canter CE, Chinnock RE, et al.\\nSurvival analysis and risk factors for mortality in transplantation and staged\\nsurgery for hypoplastic left heart syndrome. Journal of the American College of\\nCardiology 2000;36:1178–85.\\n[12] Fernandez-Yanez J, Palomo J, Torrecilla EG, Pascual D, Garrido G, de Diego JJG,\\net al. Prognosis of heart transplant candidates stabilized on medical therapy.\\nRevista Espanola de Cardiologia 2005;58:1162–70.\\n[13] Tjang YS, Heijdan GJMG, Tenderich G, Grobbee D, Korfer R. Survival analysis in\\nheart transplantation: results from an analysis of 1290 Cases in a single center.\\nEuropean Journal of Cardio-Thoracic Surgery 2008;33:856–61.\\n[14] Lin HM, Kaufmann HM, McBride MA, Davies DB, Rosendale JD, Smith CM, et al.\\nCenter-speciﬁc graf and patient survival rates: 1997 UNOS report. JAMA\\n1998;280:1153–60.\\n[15] Cope JT, Kaza AK, Reade CC, Shockey KS, Kern JA, Tribble CG, et al. A cost\\ncomparison of heart transplantation versus alternative operations for cardio-\\nmyopathy. Annual thoracic Surgery 2001;72:1298–305.\\n[16] Aguero J, Almenar L, Martinez-Dolz L, Moro J, Izquierdo MT, Cano O, et al.\\nDifferences in clinical proﬁle and survival after heart transplantation accord-\\ning to prior heart disease. Transplantation Proceedings 2007;39:2350–2.\\n[17] Christensen E, Gunson B, Neuberger J. Optimal timing of liver transplantation\\nfor patients with primary biliary cirrhosis: use of prognostic modeling. Journal\\nof Hepatology 1999;30:285–92.\\n[18] Yoo HY, Galabova V, Edwin D, Thuluvath PJ. Socioeconomicstatus does not affect\\nthe outcome of liver transplantation. Liver Transplantation 2002;8:1133–7.\\n[19] Deng MC, DeMeester MJ, Smiths JMA, Heinecke J, Scheld HH. Effect of receiving\\na heart transplant: analysis of a national cohort entered on to waiting list,\\nstratiﬁed by heart failure severity. British Medical Journal 2000;321:540–5.\\n[20] Ghobrial IM, Habermann TM, Maurer MJ, Geyer SM, Ristow KM, Larson TS,\\net al. Prognostic analysis for survival in adult solid organ transplant recipients\\nwith posy-transplantation lymphoproliferative disorders. Journal of Clinical\\nOncology 2005;23:7574–82.\\n[21] Harper AM, Taranto SE, Edwards EB, Daily OP. An update on a successful\\nsimulation project: the UNOS liver allocation model. In: Joines JA, Barton RR,\\nKang K, Fishwick PA, editors. Proceedings of the winter simulation conference.\\nNew York, NY: ACM Publications; 2000. p. 1955–62.\\n[22] Cupples SA, Ohler L. Transplantation nursing secrets. St. Louis, MO: Hanley &\\nBelfus Publication; 2002.\\n[23] Cristianini N, Shawe-Taylor J. An introduction to support vector machines and\\nother Kernel-based learning methods. London: Cambridge University Press;\\n2000.\\n[24] Mitchell T. Machine learning. New York, NY: McGraw-Hill; 1997.\\n[25] Haykin S. Neural networks: a comprehensive foundation. Upper Saddle River,\\nNJ: Prentice Hall; 1998.\\n[26] Bellazzi R, Zupan B. Predictive data mining in clinical medicine: current issues\\nand guidelines. International Journal of Medical Informatics 2008;77:81–97.\\n[27] Dreiseitl S, Ohno-Machado L. Logistic regression and artiﬁcial neural network\\nclassiﬁcation models: a methodology review. Journal of Biomedical Informat-\\nics 2002;35:352–9.\\n[28] Efron B, Tibshirani R. Statistical data analysis in the computer age. Science\\n1991;253:390–5.\\n[29] Breiman L, Friedman JH, Olshen RA, Stone CJ. Classiﬁcation and regression\\ntrees. Monterey, CA: Wadsworth & Brooks/Cole Advanced Books & Software;\\n1984.\\n[30] Kass GV. An exploratory technique for investigating large quantities of cate-\\ngorical data. Applied Statistics 1980;29:119–27.\\n[31] Quinlan JR. Learning with continuous classes.\\nIn: Adams, Sterling, editors.\\nProceedings of 5th Australian joint conference on artiﬁcial intelligence. Sin-\\ngapore: World Scientiﬁc; 1992. p. 343–8.\\n[32] Makridakis S, Wheelwright SC, Hyndman RJ. Forecasting: methods and appli-\\ncations. New York, NY: John Wiley and Sons; 1998.\\n[33] Everitt BS. Cambridge dictionary of statistics. Cambridge, UK: Cambridge\\nUniversity Press; 2002.\\n[34] Kohavi R. A study of cross-validation and bootstrap for accuracy estimation\\nand model selection. In: Boutilier C, editor. Proceedings of the 14th interna-\\ntional conference on AI (IJCAI). San Mateo, CA: Morgan Kaufmann; 1995. p.\\n1137–45.\\n[35] Olson DL, Delen D. Advanced data mining techniques. New York, NY: Springer;\\n2008.\\n[36] Davis G. Sensitivity analysis in neural net solutions. IEEE Transactions on\\nSystems Man and Cybernetics 1989;19:1078–82.\\n[37] Principe JC, Euliano NR, Lefebvre WC. Neural and adaptive systems. New York,\\nNY: John Wiley and Sons; 2001.\\n[38] Saltelli A. Making best use of model evaluations to compute sensitivity indices.\\nComputer Physics Communications 2002;145:280–97.\\n[39] Saltelli A, Tarantola S, Campolongo F, Ratto M. Sensitivity analysis in practice—\\na guide to assessing scientiﬁc models. New York, NY: John Wiley and Sons;\\n2004.\\n[40] Ohno-Machado L. Modeling medical prognosis: survival analysis techniques.\\nJournal of Biomedical Informatics 2001;34:428–39.\\n[41] Grambsch P, Therneau T. Proportional hazards rates and diagnostics based on\\nweighted residuals. Biometrika 1994;81:515–26.\\n[42] Christensen E. Multivariate survival analysis using Cox’s regression model.\\nHepatology 1987;7:1346–58.\\n[43] Kaplan E, Meier P. Nonparametric estimation from incomplete observations.\\nJournal of the American Statistical Association 1958;53:187–220.\\n[44] MacQueen JB. Some methods for classiﬁcation and analysis of multivariate\\nobservations.\\nIn: Le Cam LM, Neyman J, editors. Proceedings of the ﬁfth\\nsymposium on math, statistics, and probability. Berkeley, CA, USA: University\\nof California Press; 1967. p. 281–97.\\n[45] Krishna K, Murty MN. Genetic k-means algorithm. IEEE Transactions on\\nSystems Man and Cybernetics-Part B Cybernetics 1999;29:433–9.\\n[46] Chiu T, Fang D, Chen J, Wang Y, Jeris C. A robust and scalable clustering\\nalgorithm for mixed type attributes in large database environment. In: Lee D,\\neditor. Proceedings of the seventh ACM SIGKDD international conference on\\nknowledge discovery and data mining. New York, NY: ACM Publications; 2001.\\np. 263.\\n[47] Li ZH, Luo P. Statistical analysis lectures of SPSS for windows. Beijing, China:\\nBeijing Publishing House of Electronics Industry; 2004.\\n[48] SPSS\\nInc.\\nPASW\\nModeler\\nData\\nMining\\nToolkit,\\nVersion\\n13.0,\\nhttp://\\nwww.spss.com/software/modeling/modeler/ 2009 (accessed: June 5, 2009).\\n[49] SAS Institute Inc. Statistical Analysis Systems, Version 9.1.3, http://www.sas.\\ncom/technologies/analytics/statistics/stat/ 2008 (accessed: May 11, 2009).\\n[50] Hair JF, Anderson RE, Tatham RL, Black W. Multivariate data analysis. Upper\\nSaddle River, NJ: Prentice Hall; 1998.\\n[51] Johnson DE. Applied multivariate methods for data analysts. Paciﬁc Grove, CA:\\nDuxbury Press; 1998.\\n[52] Oztekin A, Delen D, Kong ZJ. Predicting the graft survival for heart–lung\\ntransplantation patients: An integrated data mining methodology. Interna-\\ntional Journal of Medical Informatics 2009;78:84–96.\\n[53] Michaud P. Clustering techniques. Future Generation Computer Systems\\n1997;13:135–47.\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n42\\n'}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/A-machine-learning-based-approach-to-prognostic-an_2010_Artificial-Intellige.pdf', 'text': 'A machine learning-based approach to prognostic analysis of thoracic\\ntransplantations\\nDursun Delen a,*, Asil Oztekin b,c, Zhenyu (James) Kong b\\na Spears School of Business, Oklahoma State University, T-NCB 378, 700 North Greenwood Avenue, Tulsa, OK, 74106, USA\\nb School of Industrial Engineering and Management, Oklahoma State University, 322 Engineering North, Stillwater, OK 74078, USA\\nc Department of Industrial Engineering, Gediz University, 35230 Cankaya-Izmir, Turkey\\n1. Introduction\\n1.1. Motivation\\nThoracic (heart and lung) transplantation has been accepted as a\\nviable treatment for end-stage cardiac and pulmonary failure. The\\nArtiﬁcial Intelligence in Medicine 49 (2010) 33–42\\nA R T I C L E\\nI N F O\\nArticle history:\\nReceived 25 February 2009\\nReceived in revised form 15 December 2009\\nAccepted 10 January 2010\\nKeywords:\\nData mining\\nMachine learning\\nUNOS\\nThoracic Transplantation\\nSurvival analysis\\nPrognostic index\\nA B S T R A C T\\nObjective: The prediction of survival time after organ transplantations and prognosis analysis of different\\nrisk groups of transplant patients are not only clinically important but also technically challenging. The\\ncurrent studies, which are mostly linear modeling-based statistical analyses, have focused on small sets\\nof disparate predictive factors where many potentially important variables are neglected in their\\nanalyses. Data mining methods, such as machine learning-based approaches, are capable of providing an\\neffective way of overcoming these limitations by utilizing sufﬁciently large data sets with many\\npredictive factors to identify not only linear associations but also highly complex, non-linear\\nrelationships. Therefore, this study is aimed at exploring risk groups of thoracic recipients through\\nmachine learning-based methods.\\nMethods and material: A large, feature-rich, nation-wide thoracic transplantation dataset (obtained from\\nthe United Network for Organ Sharing—UNOS) is used to develop predictive models for the survival time\\nestimation. The predictive factors that are most relevant to the survival time identiﬁed via, (1) conducting\\nsensitivity analysis on models developed by the machine learning methods, (2) extraction of variables\\nfrom the published literature, and (3) eliciting variables from the medical experts and other domain\\nspeciﬁc knowledge bases. A uniﬁed set of predictors is then used to develop a Cox regression model and\\nthe related prognosis indices. A comparison of clustering algorithm-based and conventional risk\\ngrouping techniques is conducted based on the outcome of the Cox regression model in order to identify\\noptimal number of risk groups of thoracic recipients. Finally, the Kaplan–Meier survival analysis is\\nperformed to validate the discrimination among the identiﬁed various risk groups.\\nResults: The machine learning models performed very effectively in predicting the survival time: the\\nsupport vector machine model with a radial basis Kernel function produced the best ﬁt with an R2 value of\\n0.879, the artiﬁcial neural network (multilayer perceptron-MLP-model) came the second with an R2\\nvalue of 0.847, and the M5 algorithm-based regression tree model came last with an R2 value of 0.785.\\nFollowing the proposed method, a consolidated set of predictive variables are determined and used to\\nbuild the Cox survival model. Using the prognosis indices revealed by the Cox survival model along with a\\nk-means clustering algorithm, an optimal number of ‘‘three’’ risk groups is identiﬁed. The signiﬁcance of\\ndifferences among these risk groups are also validated using the Kaplan–Meier survival analysis.\\nConclusions: This study demonstrated that the integrated machine learning method to select the predictor\\nvariables is more effective in developing the Cox survival models than the traditional methods commonly\\nfound in the literature. The signiﬁcant distinction among the risk groups of thoracic patients also validates\\nthe effectiveness of the methodology proposed herein. We anticipate that this study (and other AI based\\nanalytic studies like this one) will lead to more effective analyses of thoracic transplant procedures to better\\nunderstand the prognosis of thoracic organ recipients. It would potentially lead to new medical and\\nbiological advances and more effective allocation policies in the ﬁeld of organ transplantation.\\n\\x02 2010 Elsevier B.V. All rights reserved.\\n* Corresponding author. Tel.: +1 918 594 8283; fax: +1 918 594 8283.\\nE-mail address: dursun.delen@okstate.edu (D. Delen).\\nContents lists available at ScienceDirect\\nArtificial Intelligence in Medicine\\njournal homepage: www.elsevier.com/locate/aiim\\n0933-3657/$ – see front matter \\x02 2010 Elsevier B.V. All rights reserved.\\ndoi:10.1016/j.artmed.2010.01.002\\n\\nincreased experience in cardiac and pulmonary transplantation,\\nimprovements in patient selection, organ preservation, and preop-\\nerativesupporthavesigniﬁcantlyreducedtheearlythreatstopatient\\nsurvival [1]. Over the past decade, the thoracic transplant waiting\\ntime for a listed patient has markedly increased, but the number of\\ntransplants performed has declined. In addition, the research also\\nfound thatthere is a perceived inequity inaccesstoorgans. Theorgan\\nallocation system needs to be improved since it may become a major\\nfactor negatively inﬂuencing the survivability of thoracic transplant\\n[2].\\nThe survivability prediction is becoming increasingly more\\nimportant in medicine. When a resource is scarce, the need for\\naccurate prediction becomes acute [3]. Especially prediction of\\nsurvival time and prognosis prediction of medical treatments are\\nclinically important and challenging problems [4]. Scarceness of\\norgans necessitates the development of effective and efﬁcient\\nprocedures to select the most optimal organ receiver since demand\\nfor organs of all patients might not be satisﬁed. To achieve this, one\\ncritical step is to reveal the knowledge underlying huge amount of\\ndata collected and stored from organ transplantation procedures\\nperformedinthepast.Theobjectivesare(1)tomaximizethepatients’\\nsurvival time after the organ transplantation surgery, and (2) to\\noptimize the prognosis for the organ recipients. These can be\\npotentially achieved by discovering the knowledge that may be\\ncontained in large dataset consisting of more than hundreds of\\ndeterminative variables regarding the donors, the potential recipi-\\nents, and transplantation procedures. Therefore, in this study a data\\nmining method is proposed to process large amount of transplanta-\\ntion data obtained from UNOS to identify the important factors as\\nwell as their relationships to the survival of the graft and the patient.\\nThereafter, a prognostic index [5,6] is developed to classify the\\npatients into different risk groups for better understanding of the\\ntransplantation phenomenon. In short, this study will address the\\nfollowing questions: (1) what are the most important variables to be\\nincluded in an effective prognostic index related to thoracic organ\\ntransplantations?(2)whatarethemostcoherentriskgroupsthatcan\\nbe formed based on the prognostic index? Predicting the thoracic\\nsurvivability and classifying the patients (potential thoracic organ\\nreceivers)intodifferentclassesofriskswouldhelpdecisionmakersin\\ndeterminingpatients’priorityfortransplantationsourceassignment.\\n1.2. Literature review\\n1.2.1. Related research in survival analysis for organ transplantation\\nIn the recent past, a number of studies were conducted using\\ndata-driven analytics on various organ transplantation datasets.\\nClosely related to the study reported herein, Hariharan et al. [7]\\nfocused on the analysis of improved graft survival rate using\\ncyclosporine after renal transplantation in both short-term (less\\nthan 1 year) and long-term (more than 1 year). A regression analysis\\nwas used to predict the probability of the graft failure after kidney\\ntransplantation in both short-term and long-term period in the light\\nof demographic characteristics, transplant-related variables, and\\npost-transplantation variables. The study performed by Herrero\\net al. [8] included 116 patients who received a liver transplant\\nbetween the years 1994 and 2000. Statistical tests are used to\\ncompare the demographic and characteristic variables, pretrans-\\nplant, andintra-operativevariables betweenthetwo groups, namely\\nyounger and older than 60. The results indicate that there is a clear\\ntrend showing that older patients have lower survival after liver\\ntransplantation. Hong et al. [9] presented a survival analysis of liver\\ntransplant patients in Canada by considering some factors such as\\nage, blood type, donor type (cadaveric or alive), race, and gender of\\nrecipient and donors. However, having limited the variables with\\nthis scope, they also admitted that the clinical information lacks of\\nmany potential details.\\nTaking a data mining approach, Kusiak et al. [10] compared two\\nrule-based data mining techniques, i.e. decision trees and rough\\nsets, to predict survival time of kidney dialysis patients. This study\\nachieved satisfactorily high prediction accuracy. The main limita-\\ntion of the study was the utilization of a small dataset with only\\n188 patients in total and also many patient-related parameters\\nwere neglected in the problem formulation. Using more traditional\\nmethods, and speciﬁcally having focused on thoracic transplanta-\\ntion, Jenkins et al. [11] and Fernandez-Yanez et al. [12] had a rich\\npool of independent variables for survivability prediction. Their\\nstudies used popular statistical techniques such as Kaplan–Meier\\nmethod of survival analysis with Mantel–Haenszel log-rank test.\\nHowever, both of these techniques have been criticized with two\\nmajor limitations: (1) linear relationships are assumed, which\\nhence cannot capture the nonlinearity among the variables, and (2)\\nthe independent variables were selected solely based on the\\nexperiences and intuitions of the analysts who conducted these\\nstudies. Thus, many potentially signiﬁcant variables might be left\\noutside the scope of this study. Tjang et al. [13] added more\\nexplanatory variables to determine the survivability in heart\\ntransplantation, such as body mass index, waiting time on the list,\\nand previous cardiac surgery, their study also ignored the non-\\nlinear relationships among the pool of survivability-related\\nvariables. Similar limitations exist in some other studies focused\\ndirectly or indirectly on thoracic transplantation [14–16].\\nThe existing studies implicitly assume that the relationships\\namong the predictive variables and output variable are linear and\\nthe predictor variables are independent of each other, which may\\nnot be valid in reality. Moreover, the abovementioned studies focus\\non\\nsmall\\ndatasets\\nwith\\nlimited\\nnumber\\nof\\npredictors\\nfor\\nsurvivability of patients after transplantation. This limitation\\nmay cause incomprehensive modeling due to the insufﬁcient\\ninformation contents (i.e., omission of a number of potentially\\nimportant predictor variables).\\n1.2.2. Related research in devising a prognostic index\\nPrognostic index (PI) provides compact prognosis information\\nregarding a speciﬁc patient based on the results of a Cox\\nproportional hazards model [5]. Cox proportional hazards model\\nhelps identify variables of prognostic importance and hence\\nprognostic index can be used to deﬁne groups of individuals at\\ndifferent risk categories. Even though prognostic index is a\\nconvenient tool to measure how well the patients are doing after\\nthe transplantation, its use in the organ transplantation area has\\nbeen limited mostly due to the lack of follow-up data. Some\\nexisting studies related to devising a PI in transplant area are\\nsummarized as follows.\\nIn the study conducted by Christensen et al. [17], it is\\nmentioned that primary biliary cirrhosis requires a liver trans-\\nplantation operation at the end stage. Based on the prognosis\\nanalysis with as well as without transplantation, it is decided\\nwhether or not the transplantation is required, if so when. To\\nachieve this goal, corresponding PIs and probabilities of surviving\\nare computed for transplantation and non-transplantation cases.\\nYoo et al. [18] developed a similar index and revealed that\\nsocioeconomic status does not inﬂuence patient or graft survival\\nthat undergoes liver transplantation at the institute where they\\nperformed their study. Deng et al. [19] conducted a study with a\\nnational dataset in Germany, which discovers the effect of\\nreceiving a heart transplant for the patients in a waiting list.\\nThe results indicate that cardiac transplant is associated with\\nsurvival beneﬁt only for patients with a predicted high risk of dying\\non the waiting list. Ghobrial et al. [20] performed a study to\\ndetermine prognostic factors for overall survival in 107 adult\\npatients with post-transplantation lymphoproliferative disorders\\n(PTLDs). It is validated that in discriminating the low and high\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n34\\n\\nscored patients the proposed prognostic scoring signiﬁcantly\\nperforms better than the International Prognostic Index for the\\nsubset of the patients (56 out of 107) with lactate dehydrogenase.\\nThe common limitation in all of these studies is similar to the\\nlimitations of the studies summarized in Section 1.2.1. Namely,\\nthey directly devise a prognostic index without determining if the\\nvariables used in prognostic index devising phase are necessary\\nand sufﬁcient. This motivates a machine learning-based initial step\\nof variable selection procedure. Because, if the critical predictive\\nfactors are not captured effectively due to the intuition- and\\nexperience-based\\nselection,\\nthe\\nresulting\\nprognostic\\nindices\\ndeveloped based on the selected variables would be inaccurate\\nand, in turn, related risk groups of patients would be deviated from\\nthe real classes. This may cause mistakes for decision maker in\\nmaking organ transplantation policies.\\n2. Proposed method\\nSection 1.2 shows that the most of the existing studies for organ\\ntransplantation\\nprocedures\\nutilize\\nconventional\\nstatistical\\napproaches such as Kaplan–Meier function and log-rank test\\nalong with expert-selected variables to predict the survivability.\\nHowever, organ transplantation procedures consist of a large\\nnumber of variables (several hundred) that may have nontrivial\\nimpact on modeling the prognosis of the grafts/patients. Using a\\nsomewhat comprehensive variable list may help discriminate\\npatients from each other by placing them into proper risk groups.\\nUnintentional omission of the important variables may lead to\\ninaccurate classiﬁcation of patient risk groups, which may, in turn,\\nlead to suboptimal organ allocation policies and ineffective\\ntreatments.\\nThis study is aimed at overcoming the abovementioned\\nshortcomings by employing both machine learning techniques\\nas well as statistical methods to identify the most critical factors\\naffecting the survivability of thoracic transplant patients. To\\nachieve this goal, this study proposes adopting a 5-step approach\\nillustrated in Fig. 1. Step 1 involves data understanding and\\npreparation, which is arguably the most time demanding step in\\nthe process. Step 2 employs various predictive modeling techni-\\nques such as support vector machines, artiﬁcial neural networks,\\nand regression trees to develop survival time prediction models\\nand to extract the most important variables by means of sensitivity\\nanalysis through the best performing model. Step 3 determines the\\nconsolidated candidate set of critical predictor variables. Step 4\\ndevelops a Cox regression model using the consolidated set of\\npredictor variables and also devises a prognostic index. The last\\nstep, Step 5, classiﬁes the patients into various risk categories by\\ncomparing and contrasting the clustering performance of algo-\\nrithm-based and manually calculated groups. Then the resulting\\nrisk categories are validated by using the Kaplan–Meier survival\\ncurves. These steps will be further explained in details in Sections\\n2.1–2.5, respectively.\\n2.1. Step one: data source and data preparation\\nIn this study, the data source that was used to validate the\\nproposed method was thoracic organ transplant dataset provided by\\nUNOS, which is a tax-exempt, medical, scientiﬁc, and educational\\norganization that operates the national Organ Procurement and\\nTransplantation Network underthe contract to the Division of Organ\\nTransplantation of the Department of Health and Human Services\\n[21]. The data ﬁles were obtained from UNOS using a formal data\\nrequisition procedure (which includes submission of speciﬁc data\\nneeds, purpose of the study, and a data use agreement). These data\\nﬁles are named as UNOS Standard Transplant Analysis and Research\\n(STAR)\\nﬁles\\nfor\\nheart,\\nlung,\\nand\\nsimultaneous\\nheart–lung\\ntransplants, namely thoracic transplants. Each transplant STAR ﬁle\\nconsists of information on all thoracic transplants that had been\\nperformed in the US and reported to UNOS since October 1, 1987. It\\nincludes both deceased- and living-donor transplants. None of the\\nﬁles include any speciﬁc patient or transplant hospital identiﬁers\\ndue to the privacy and security issues. However, there is a patient\\nidentiﬁcation number, unique to each patient, which allows linking\\nmultiple ﬁles and tracking the patient. Considering these features,\\nUNOS’s data ﬁles are recognized as the most comprehensive source\\nof information available in any single ﬁeld of medicine and for organ\\ntransplantation in US [22].\\nThere are two datasets involved in our study, which are regular\\ndataset and follow-up dataset. The regular dataset contains all\\ninformation of donors and recipients before transplantation\\noccurred, and the follow-up dataset provides all information of\\ndonors and recipients after the transplantation. The TRR_ID\\nvariable (transplant identiﬁer) is the common variable between\\nthese two datasets and the one which is proposed by UNOS to\\nmerge and integrate these two datasets. Therefore, these two\\ndatasets were combined in a relational database environment\\nusing the link (a.k.a. primary key) of TRR_ID.\\nOverall, the complete dataset consists of 310,773 records and\\n565 variables. These variables include the socio-demographic and\\nFig. 1. A ﬂowchart representation of the proposed method.\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n35\\n\\nhealth-related factors with regard to both the donor and the\\nrecipients. There are also procedure-related factors among the\\ndataset. To assign as an output (dependent variable), there are four\\npossible variables which are called pstatus, ptime, gstatus, and\\ngtime. These variables have the following meanings: whether or\\nnot the patient died after transplantation occurred (referring to\\npstatus, with dead = 1 and alive = 0). A very similar variable was\\ngstatus, referring to whether or not graft has failed (1 denoting\\n‘‘failed’’ and 0 denoting ‘‘succeeded’’). The variable ptime denoted\\npatient follow-up time (in days) from transplant to death/last\\nfollow up time. Similarly, gtime is explained as graft lifespan from\\ntransplant to death/last follow up time. Since the goal of this study\\nis to develop models to predict the survivability solely based on\\nthoracic transplant, the dependent variable was assigned as gtime.\\nThis assignment was done to discriminate the patients who died\\nsolely due to the thoracic graft incompatibility from the ones who\\ndied from any other reasons. Therefore, the rest of the potential\\ndependent variables (pstatus and ptime) were eliminated from the\\ndataset. Besides, gstatus was kept inactive up to the stage where\\nCox regression model was implemented (Step 4 in Fig. 1).\\nConsidering the gtime as the continuous dependent variable,\\nthe records for the patients whose gtime information were missing\\nwere removed from the dataset. The data set also includes some\\nidentiﬁcation variables (e.g., Donor ID) which help track the\\nrecipient patient anonymously, track the thoracic transplant\\nprocedure, or link records from multiple data ﬁles to each other.\\nSince these types of identiﬁcation variables do not have any\\ninformation content to enhance the prediction capability of the\\nmodels, after linking and integrating the ﬁles they were also\\nexcluded from the analysis dataset. Moreover, the name of\\ntransplantation type was recorded in the dataset as a variable\\nnamed Dataset which had one value (TH referring to thoracic) and\\nthe date of data processing is recorded as a variable named Date of\\nRun which are useful for data integration purposes but has no\\ninformation for contributing to the prediction of survivability and\\nhence are also excluded from the analysis dataset. Similarly, other\\nvariables having only one possible value for all records in the\\ndataset, which have no discriminating information, are also\\neliminated from the predictive modeling.\\nThis dataset had excessive number of missing values which\\nrender most of the records and variables seemingly insigniﬁcant.\\nHowever, in data mining studies one should be very reluctant to\\nremove the candidate predictor variables while at the same time\\ntrying to avoid artiﬁcial data imputation procedures. There is an\\nobvious trade-off here. As a rule of thumb, for column (variable)\\ndeletion, we were cautious to remove any variable from the\\nanalysis and assumed that if a variable has more than 95% missing\\nvalues, only then it should be regarded as not having signiﬁcant\\ninformation content and hence should be deleted. Next step was to\\nhandle the missing values by following the general convention: for\\nthe categorical variables we ﬁlled the missing values with some\\nheuristic values such as E (referring to empty) or NR (referring to\\nnot reported), and for the continuous variables we imputed the\\nmissing values with the average of the existing records. After\\nadopting these data preparation strategies, the ﬁnal dataset was\\nreduced to 372 cleansed independent variables and one dependent\\nvariable (gtime) with the total record count of 106,398.\\n2.2. Step two: predictive modeling\\nSince the dependent variable herein was a continuous variable\\n(graft survival time, which is the number of days from transplant to\\ndeath or last follow-up), the problem refers to a prediction (or\\nregression) problem (as opposed to a classiﬁcation problem). Since\\nthe\\nrelationships between the\\ndependent\\nvariable\\nand\\nthe\\nindependent variables were not known in advance, this step\\nwas to develop various predictive models for graft survival time\\nusing all of the available independent variables. It is also required\\nto check whether the models have passed the pre-speciﬁed\\nthreshold values of performance measures, speciﬁcally the R2 and\\nmean square error (MSE), to determine the best model that\\nexplains these unknown relationships between dependent and\\nindependent variables by ranking them according to these\\nmeasures. The model which is deemed to be the most successful\\none would be kept for further modeling steps to determine the\\nimportance of the independent variables.\\nSupport vector machines (SVMs) are supervised learning\\nmethods that generate input–output mapping functions from a\\nset of training data. They belong to a family of generalized linear\\nmodels which achieve a classiﬁcation or regression decision based\\non the value of the linear combination of features. They are also\\nsaid to belong to the kernel methods [23]. The mapping function in\\nSVMs can be either a classiﬁcation function (used to categorize the\\ndata) or a regression function (used to estimate the numerical\\nvalue of the desired output, as is the case in this study). Nonlinear\\nkernel functions are often used to transform the input data\\n(inherently representing highly complex nonlinear relationships)\\nto a high dimensional feature space in which the input data\\nbecome more separable (i.e. linearly separable) compared to the\\noriginal input space. Then, maximum-margin hyperplanes are\\nconstructed to optimally separate the classes in the training data.\\nTwo parallel hyperplanes are constructed on each side of the\\nhyperplane that separates the data by maximizing the distance\\nbetween the two parallel hyperplanes. An assumption is made that\\nthe\\nlarger\\nthe\\nmargin\\nor\\ndistance\\nbetween\\nthese\\nparallel\\nhyperplanes, the better the generalization error of the prediction\\nwould be.\\nArtiﬁcial neural networks (ANNs) have been utilized to model\\ncomplex relationships (such as nonlinear functions and multi-\\ncollinearity) among the predictor variables and the dependent\\nvariable [24]. ANNs are highly sophisticated analytic techniques\\ncapable of predicting new observations (on speciﬁc variables)\\nfrom other observations (on the same or other variables) after\\nexecuting a process of so-called ‘‘learning’’ from existing data\\n[25]. ANNs have been one of the most popular artiﬁcial\\nintelligence-based data modeling algorithms used in recent\\nmedical informatics studies due to their satisfactory predictive\\nperformance [26]. On the other hand, compared to other\\nmachine learning methods (such as ANNs), decision trees have\\nthe advantage of not being a black box model, namely having the\\ncapability to explain the inner structure of the model in the form\\nof a graphically represented inverse tree or a collection of\\ncondition-action rules. This advantage has made them a viable\\nand desirable alternative method in medical informatics [27]. If\\nthe dependent variable is continuous (as in the case in this\\nstudy) the resulting decision tree is called a regression tree.\\nRegression trees are known to be among the highly adaptable,\\nrelatively ﬂexible, yet computationally intensive data mining\\ntechniques [28]. Popular regression tree algorithms are CART (or\\nC&RT) [29], CHAID [30], and M5 [31] which can be used for both\\nclassiﬁcation and regression type prediction problems. ID3 and\\nits successors, C4.5 and C5 are also among the popular decision\\ntree algorithms, but they can only work for classiﬁcation type\\nprediction problems.\\n2.2.1. Performance criteria\\nTo compare the abovementioned prediction models, two\\nperformance criteria are considered: mean squared error (MSE)\\nof the model on testing dataset and R2 value between the actual\\nobservation for the target variable (Yt) and the predicted value by\\nthe model (Ft). MSE which is given by Eq. (1) does not have a rule-\\nof-thumb threshold cut-off value for acceptable models. It is a\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n36\\n\\nrelative criterion to select the best model, namely the smaller the\\nvalue the better the model has performed [32].\\nMSE ¼ 1\\nn\\nX\\nn\\nt¼1\\nðYt \\x02 FtÞ2\\n(1)\\nOn the other hand, R2 (R2\\nFt;Yt or shortly R2) which is given by\\nEq. (2) can be considered as both an absolute measure and a\\nrelative measure to determine and rank the satisfactory models\\n[33]. Unlike the MSE, the higher the R2, the better the performance\\nfor the compared models.\\nR2 ¼ 1 \\x02\\nPn\\nt¼1 ðFt \\x02 YtÞ2\\nPn\\nt¼1 ðYt \\x02 ¯YtÞ\\n2\\n(2)\\n2.2.2. k-Fold cross-validation\\nIn order to minimize the bias associated with the random\\nsampling of the training and holdout data samples in comparing\\nthe predictive accuracy of two or more methods, researchers tend\\nto use k-fold cross-validation [34]. In k-fold cross-validation, also\\ncalled rotation estimation, the complete dataset (D) is randomly\\nsplit into k mutually exclusive subsets (the folds: D1, D2, . . ., Dk) of\\napproximately equal size. The prediction model is trained and\\ntested k times. Each time (t 2 {1, 2, . . ., k}), it is trained on all but one\\nfold (Dt) and tested on the remaining single fold (Dt). The cross-\\nvalidation estimate of the overall performance criteria is calculated\\nas simply the average of the k individual performance measures as\\nin Eq. (3),\\nCV ¼ 1\\nk\\nX\\nk\\ni¼1\\nPMi\\n(3)\\nwhere CV stands for cross-validation, k is the number of folds used,\\nand PM is the performance measure for each fold [35].\\nIn this study, to estimate the performance of the prediction\\nmodels a 10-fold cross-validation approach was used. Empirical\\nstudies showed that 10 seems to be an optimal number of folds\\n(that optimizes the time it takes to complete the test while\\nminimizing the bias and variance associated with the validation\\nprocess) [34]. In 10-fold cross-validation the entire dataset is\\ndivided into 10 mutually exclusive subsets (or folds). Each fold is\\nused once to test the performance of the prediction model that is\\ngenerated from the combined data of the remaining nine folds,\\nleading to 10 independent performance estimates.\\n2.2.3. Sensitivity analysis\\nAfter selecting the best prediction model based on the\\nperformance criteria as explained in Section 2.2.1, it is required\\nto determine the importance of the independent variables. In\\nmachine learning algorithms, sensitivity analysis is a method for\\nextracting the cause and effect relationship between the inputs\\nand outputs of a trained model [36]. In the process of performing\\nsensitivity analysis, after the model is trained the learning is\\ndisabled so that the network weights are not affected. The\\nfundamental idea is that the sensitivity analysis measures the\\npredictor variables based on the change in modeling performance\\nthat occurs if a predictor variable is not included in the model.\\nHence, the measure of sensitivity of a speciﬁc predictor variable is\\nthe ratio of the error of the trained model without the predictor\\nvariable to the error of the model that includes this predictor\\nvariable [37]. The more sensitive the network is to a particular\\nvariable, the greater the performance decrease would be in the\\nabsence of that variable, and therefore the greater the ratio of\\nimportance. This method is followed in support vector machines\\nand artiﬁcial neural networks to rank the variables in terms of their\\nimportance according to the sensitivity measure deﬁned in Eq. (4)\\n[38].\\nSi ¼\\nV\\nCðFtÞ ¼ VðEðFtjXiÞÞ\\nVðFtÞ\\n(4)\\nwhere V(Ft) is the unconditional output variance. In the numerator,\\nthe expectation operator E calls for an integral over X\\x02i; that is, over\\nall input variables but Xi, then the variance operator V implies a\\nfurther integral over Xi. Variable importance is then computed as\\nthe normalized sensitivity. Saltelli et al. [39] show that Eq. (4) is the\\nproper measure of sensitivity to rank the predictors in order of\\nimportance for any combination of interaction and non-orthogo-\\nnality among predictors. As for the decision trees, variable\\nimportance measures were used to judge the relative importance\\nof each predictor variable. Variable importance ranking uses\\nsurrogate splitting to produce a scale which is a relative\\nimportance measure for each predictor variable included in the\\nanalysis. Further details on this procedure can be seen in Breiman\\net al. [29].\\n2.3. Step three: determining the candidate sets of predictor variables\\nStep 3 is to determine which predictor variables to be used in\\ndevising a prognostic index in Step 4. This step helps eliminate\\nthe insigniﬁcant variables and improves the accuracy of the\\nmodel by optimizing the predictor variables list. The potential\\ninput variables to this step consist of three candidate sets of\\npredictor variables. The ﬁrst set is composed of variables\\nselected\\nby\\nthe\\npredictive\\nmodels.\\nThe\\npredictive\\nmodels\\nexplained in Section 2.2 rank the predictor variables based on\\ntheir importance level in predicting the graft survival time. The\\npredictive variables selected by the sensitivity analysis of the\\nbest-performing model (ranked in terms of R2 and MSE) are\\nchosen as the ﬁrst set of predictive variables. The second set of\\npredictive variables is obtained by considering the expert\\ndomain\\nknowledge.\\nThis\\nset\\nincludes\\nvariables\\nwhich\\nare\\nlogically related to heart and lung transplantation such as\\ndonor’s history of cigarette usage. The third set of predictive\\nvariables is selected from the related literature. This set consists\\nof the variables which have been commonly and repeatedly used\\nin previous studies in the organ transplantation area. The second\\nand third sets of predictive variables provide more comprehen-\\nsive information for the next step, the Cox regression model, by\\nincluding the variables that might have importance in the\\nsurvival analysis but were determined to be insigniﬁcant by the\\npredictive models in Step 2.\\n2.4. Step four: survival analysis and prognostic index devising\\nStep 4 takes all the three sets of predictive variables identiﬁed\\nin Step 3, and then applies Cox regression to model the graft\\nsurvivability and ﬁlter out the candidate predictive variables\\nwhich do not have signiﬁcant survival effect. Hence, in Step 4 the\\nﬁnal critical predictive variables are determined by the Cox\\nregression model. Cox regression model also enables devising a\\nprognostic index to categorize the patients into various groups\\nwith different levels of risks.\\nCox regression model is a semi-parametric model extensively\\nused in survival analysis [6]. The survival time of each patient is\\nassumed to follow the hazard function (hi) given by Eq. (5) as\\nfollows:\\nhi ¼ h0 expðxibÞ\\n(5)\\nwhere h0 is the baseline hazard function and xi is the vector of\\npredictor variables for the ith patient. b is the vector of regression\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n37\\n\\ncoefﬁcients for the predictor variables and is assumed to be the\\nsame for all patients [40,41].\\nOne important application of Cox regression model is to identify\\nvariables which may be of prognostic importance [5]. Once\\nidentiﬁed, knowledge from these variables will be combined and\\nused to deﬁne a prognostic index, which in turn deﬁnes groups of\\norgan recipients with different levels of risk. To use the prognostic\\nindex, key patient characteristics are recorded, from which a score\\nis derived. This score gives an indication of whether a particular\\npatient has high, intermediate, or low levels of prognosis for the\\ndisease [5,42]. Recalling Eq. (5), the prognostic index (PI) for each\\npatient can be calculated by Eq. (6):\\nPI ¼ x1b1 þ x2b2 þ . . . þ xnbn\\n(6)\\nwhere x1 to xn are the patient’s values for the variables in the Cox\\nmodel, and b1 to bn are the corresponding regression coefﬁcients\\ndetermined by Cox regression model [42].\\nNote that PI in Eq. (6) represents the exponent portion in Eq. (5).\\nTherefore, the smaller the PI, the smaller the hazard function value,\\nand hence the smaller the risk associated with a particular\\nrecipient.\\n2.5. Step ﬁve: determining risk groups of thoracic recipients\\nAnimportantquestionfollowingStep4is‘‘Howmanyriskgroups\\nshould the patients be classiﬁed into?’’ In Step 5, k-means clustering\\nalgorithm, two-step cluster analysis, and conventional heuristics-\\nbased approaches are used to answer to this question. As a statistical\\nand/or pictorial veriﬁcation mechanism for the number of groups\\ndetermined by the best performing abovementioned clustering\\napproaches, ﬁnally the Kaplan–Meier survival analysis [43] is\\nadopted and corresponding survival curves are generated.\\nk-Means method is an extensively used, arguably the most\\npopular clustering algorithm that searches for a nearly optimal\\npartition with ﬁxed number of clusters represented by the\\nparameter k [44]. It proceeds by assigning k initial centroids to\\nthe multidimensional datasets. Each record in the dataset is\\nallocated to the centroid which is nearest and hence forming a\\ncluster. Each cluster centroid is then updated to be the center of its\\nmembers, followed by a new assignment of records to the nearest\\ncentroids to re-construct the clusters. The algorithm converges\\nwhen there is no further change in allocation of members to clusters\\nor some predeﬁned time-based stopping criteria is satisﬁed [45].\\nAnother popular clustering algorithm istwo-stepcluster analysis\\n(TSCA) [46,47]. It has two steps: (1) to pre-cluster the cases (or\\nrecords) into many small sub-clusters, and (2) to cluster the sub-\\nclusters resulting from pre-cluster step into the desired number of\\nclusters. The pre-cluster step uses a sequential clustering approach. It\\nscans the data records one by one and decides if the current record\\nshould be merged with the previously formed clusters or starts a\\nnew cluster based on the distance criterion. Then the cluster step\\ntakes sub-clusters resulting from the pre-cluster step as input, and\\ngroups them into the desired number of clusters. Since the number\\nof sub-clusters is much less than the number of original records, the\\ntraditional clustering methods can be used effectively. This step uses\\nthe agglomerative hierarchical clustering method [46,47]. Although\\nthere are several other clustering algorithms (e.g. Kohonen net-\\nworks) they do not allow the modeler to specify a desired number of\\nclusters at the beginning of the clustering algorithm. k-Means and\\nTSCA algorithms overcome this issue. The modeler can predeﬁne a\\nspeciﬁcnumber of clusters to group the variables and compare them\\naccording to their clustering performances. Since this is the main\\nfocus of our study, we utilized k-means and TSCA algorithms for\\nclustering the PIs and thus identfying the risk groups of thoracic\\npatients.\\nThe Kaplan–Meier analysis is a non-parametric technique used\\nto test the statistical signiﬁcance of differences between the\\nsurvival curves associated with two different circumstances [43].\\nThe analysis expresses the distribution of patient survival times in\\nterms of the proportion of patients still alive up to a given time. On\\nthe other hand, the Kaplan–Meier survival curves plot the\\nproportion of patients surviving against time which has a\\ncharacteristic decline. In biostatistics, a typical application of\\nKaplan–Meier survival curves involves grouping patients into risk\\ngroups such as low, medium, and high risks.\\n3. The case study and discussion\\nIn order to demonstrate and validate the proposed methodolo-\\ngy in Section 2, two most popular data mining toolkit are used,\\nnamely SPSS PASW Modeler1 [48] and SAS 9.1.31 [49] statistical\\nsoftware package. Using the UNOS data set, Sections 3.1–3.5\\ndiscuss the results obtained by following the above mentioned\\nmodeling procedures presented in Section 2. The prediction\\nperformance results reported herein are all based on the test (or\\nholdout) dataset.\\n3.1. Predictive model results\\nTo reveal the initially unknown relationship between the\\nthoracic input/independent variables and the continuous output/\\ndependent variable (gtime), due to the high computational time\\nrequired for 10-fold cross-validation of each model we only used\\ntwo most popular models from each family of machine learning\\ntechniques. Radial basis function (RBF) and polynomial functions\\nas Kernel methods in support vector machine were deployed. We\\nused multilayer perceptron (MLP) and RBF type of network\\nstructures for ANNs. The most recent algorithms C&RT and M5\\nwere utilized for prediction with the decision trees. The 10-fold\\naveraged prediction results in terms of MSE and R2 for each model\\nare tabulated in Table 1. The acceptance of predictive models is\\nﬁrst evaluated based on their coefﬁcient of determination (R2)\\nvalues. It is widely accepted that if R2 is higher than 0.6, the\\npredictive model has performed fairly well [50,51]. Therefore, we\\nset this as a threshold value for the model sufﬁciency. Since all the\\nmodels have passed this threshold, we kept the one with the\\nhighest R2 and the smallest MSE for further analyses, which came\\nout to be the support vector machine model with radial basis\\nKernel function in this case study.\\n3.2. Determination of the candidate covariates for Cox regression\\nmodel\\nStep 3 in the proposed method provides three different sets of\\ncandidate covariates to be used in the Cox model. Since the best\\nperforming model to explain the relationships of independent and\\nTable 1\\nComparison of machine learning prediction model results.\\nPerformance measures\\nPrediction models\\nMSE\\nR2\\nSupport vector machine\\nRBF\\n0.023\\n0.879\\nPolynomial\\n0.793\\n0.643\\nArtiﬁcial neural network\\nMLP\\n0.031\\n0.847\\nRBF\\n0.146\\n0.835\\nDecision trees\\nM5\\n0.324\\n0.785\\nC&RT\\n0.578\\n0.766\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n38\\n\\ndependent variables was found to be RBF-SVM, the sensitivity\\nanalysis as explained in Section 2.2.3 by Eq. (4) was conducted on\\nthe predictor variables to rank them in terms of their importance in\\npredicting the gtime output variable. This ﬁrst set consists of the\\npredictor variables which are presented in Table 2.\\nThe secondsetofpredictorvariableswereselectedbythe authors\\nthrough brainstorming sessions with medical professionals. The\\nsecond set of candidate covariates are tabulated in Table 3.\\nThe third set of candidate covariates was determined through\\nthe recent literature [52]. This set includes the variables commonly\\nused in the\\npreviously published studies related to organ\\ntransplantation. The third set of candidate covariates are shown\\nin Table 4.\\nThe second and third set of candidate covariates can be\\nperceived as the expert component of the method. If the predictive\\nmodels in Step 3 do not reveal some very critical predictor\\nvariables (such as the age of the recipient in our case study), the\\nmethod proposes to force the Cox model once more to review the\\nsigniﬁcance of this kind of predictor variables.\\n3.3. Deployment of Cox regression model and devising the prognostic\\nindices\\nAll the candidate covariates as determined in Section 3.2 were\\nassigned to Cox regression model at this step. The stepwise\\nvariable selection procedure was applied with 0.05 for entry and\\nTable 3\\nThe 2nd set of candidate covariates.\\nVariables\\nExplanation\\nAntiarry\\nHeart medical factors: antiarrythmics\\nat registration\\nContin_Alcohol_Old_Don\\nDeceased donor-history of alcohol\\ndependency + recent 6 months use\\nContin_Cig_Don\\nDeceased donor-history of cigarettes in past\\nand >20 pack years + recent 6 months use\\nContin_IV_Drug_Old_Don\\nDeceased donor-history of iv drug use\\n+ recent 6 months use\\nContin_Oth_Drug_Don\\nDeceased donor-history of other drugs in\\npast + recent 6 months use\\nEint\\nEthnicity interaction between recipient and\\ndonor (in the same ethnic group, y/n)\\nGint\\nGender interaction between recipient and\\ndonor (having the same sex, y/n)\\nHist_Alcohol_Old_Don\\nDeceased donor-history of alcohol dependency\\nHist_Cancer_Don\\nDeceased donor-history of cancer (y/n)\\nHist_Cig_Don\\nDeceased donor-history of cigarettes in past\\nand >20pack yrs\\nHist_Cocaine_Don\\nDeceased donor-history of cocaine use in past\\nHist_Diabetes_Don\\nDeceased donor-history of diabetes, incl.\\nDuration of disease\\nHist_Hypertens_Don\\nDeceased donor-history of hypertension\\nLOS\\nRecipient length of stay post-transplant\\nOth_Tobacco\\nOther tobacco use\\nPack_Yrs\\nIf history of cigarette use, number of\\npack years\\nTable 2\\nThe 1st set of candidate covariates generated from RBF-SVM.\\nVariables\\nExplanation\\nCitizenship\\nRecipient citizenship @ registration\\nContin_alcohol_old_don\\nDeceased donor-history of alcohol\\ndependency + recent 6 months use\\nContin_iv_drug_old_don\\nDeceased donor-history of iv drug use\\n+ recent 6 months use\\nCreat2_old\\nMost recent creatinine >2.0 mg/dl y/n\\nDa2\\nDonor a2 antigen\\nDantiarr_old\\nDeceased donor given antiarrythmics 24 h\\nprior to cross-clamp\\nDayswait_chron\\nActive days on waiting list\\nDobut_don_old\\nDeceased donor-dobutamine w/in 24 h\\npre-cross-clamp\\nEducation\\nRecipient highest educational level @\\nregistration\\nEthcat_don\\nDonor ethnicity category\\nFluvaccine\\nAnti-viral treatment—ﬂuvaccine\\nFunc_stat_tcr\\nRecipient functional status @ registration\\nFunc_stat_trr\\nRecipient functional status @transplant\\nGender\\nRecipient gender\\nHbsab_don\\nDeceased donor hbsab test result\\nHemo_pa_dia_tcr\\nMost recent hemodynamics pa (dia) mm/hg\\n@ registration\\nHemo_pa_mn_tcr\\nMost recent hemodynamics pa (mean) mm/hg\\n@ registration\\nHeparin_don\\nDeceased donor management—heparin\\nHgt_cm_tcr\\nRecipient height @ registration\\nHist_alcohol_old_don\\nDeceased donor-history of alcohol\\ndependency\\nHtlv2_old_don\\nDeceased donor-antibody to htlv ii result\\nImpl_deﬁbril_after_list\\nImplantable deﬁbrillator inserted between\\nlisting and transplant\\nInotrop_agents\\nDeceased donor—three or more inotropic\\nagents at time of incision\\nInotrop_support_don\\nDeceased donor inotropic medication at\\nprocurement (y/n)\\nIschtime\\nIschemic time in hours\\nMed_cond_tcr\\nRecipient medical condition @ registration\\nMed_cond_trr\\nRecipient medical condition pretransplant\\n@ transplant\\nPhysical_capacity_tcr\\nPhysical capacity at listing\\nPretreat_med_don_old\\nDeceased donor medication(s) from brain\\ndeath to 24 h prior to procurement\\nPrior_lung_surg_tcr\\nRecipient prior lung surgery (non-transplant)\\nat listing\\nPst_airway\\nEvents prior to discharge: airway dehiscence\\nPst_cardiac\\nEvents prior to discharge: cardiac re-operation\\nPst_dial\\nEvents prior to discharge: dialysis\\nPst_drug_trt_infect\\nEvents prior to discharge: any drug\\ntreated infection\\nPst_surgical\\nEvents prior to discharge: other surgical\\nprocedures\\nPt_t4_don\\nDeceased donor-thyroxine-t4 b/n brain\\ndeath w/in 24 h of procurement\\nSternotomy_tcr\\nEvents occurring prior to listing: sternotomy\\nSternotomy_trr\\nEvents occurring between listing and\\ntransplant: sternotomy\\nSteroid\\nChronic steroid use y/n/u @ transplant\\nTrtrej1y\\nTreated for rejection within 1 year\\nTrt_pulm_sepsis\\nIV treated pulmonary sepsis y/n/u @\\nregistration\\nVad_tah_tcr\\nRecipient on life support—ventilator @\\nregistration (1 = yes, 0 = no)\\nVad_tah_trr\\nRecipient on life support—ventilator @\\ntransplant\\nTable 4\\nThe 3rd set of candidate covariates.\\nVariables\\nExplanation\\nABO\\nRecipient blood group at registration\\nABO_Don\\nDonor blood type\\nABO_Mat\\nDonor-recipient ABO match level\\nAge\\nRecipient age (years)\\nAge_Don\\nDonor age (years)\\nDayswait_Chron\\nActive days on waiting list\\nDon_TY\\nDonor type—deceased/living\\nEthcat\\nRecipient ethnicity category\\nEthcat_Don\\nDonor ethnicity category\\nGender\\nRecipient gender\\nGender_Don\\nDonor gender\\nHbsab_Don\\nDeceased donor hbsab test result\\nIschtime\\nIschemic time in hours\\nMed_Cond_Tcr\\nRecipient medical condition at registration\\nMed_Cond_Trr\\nRecipient medical condition pretransplant\\nat transplant\\nWgt_kg_Don\\nDonor weight (kg)\\nWgt_kg_Tcr\\nRecipient weight (kg) at registration\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n39\\n\\n0.1 for removal as signiﬁcance threshold criteria. The predictor\\nvariables determined to be signiﬁcant by Cox regression model are\\nlisted along with their corresponding statistics in Table 5. The rest\\nof the variables (which were in Tables 2, 3, or 4 but not in Table 5)\\nwere eliminated since they were found to be insigniﬁcant by Cox\\nregression model.\\nAs listed in Table 5, 14 of the variables had prognostic value\\nwhich are determined by the Cox model as signiﬁcant and kept in\\nthe Cox equation. Therefore, they were used to calculate the PIs by\\nmeans of Eq. (6). The PI values received here were ranging between\\n0 and 3.\\n3.4. Clustering the prognostic indices\\nOnce the prognostic indices (PIs) for each recipient calculated,\\nthe next step was to cluster the recipients through these PIs.\\nHowever, the problem of deﬁning these clusters and deciding\\nwhich value to cut off and categorize the recipients should be\\nsolved\\nﬁrst.\\nTwo\\ncommonly\\nused\\nclustering\\nalgorithms\\nas\\ndescribed in Section 2.5, namely k-means and TSCA were used\\nto determine these clusters. We also compared these algorithm-\\nbased clusters to conventional PI devising methods in medicine.\\nTwo potential ways to do the clustering are constructing equal-\\nwidth PIs and equal-percentile PIs in this research domain. In the\\nformer one, the PIs are separated in groups so that the increments\\nof PI in each group are equal whereas the latter method focuses on\\nallocating the patients equally to each group. The algorithms k-\\nmeans and TSCA were run by changing the value for k (number of\\nclusters to be formed). The value of k with 2, 3, 4, and 5 were tried\\nbecause it was considered that having clusters more than 5 would\\nnot provide logical risk groups to categorize and would probably\\nnot be easy to name and interpret medically afterwards. The\\nresults for each run are represented in Table 6.\\nThe performances of these entire four approaches with different\\nnumber of clusters (k = 2–5) were compared using intraclass inertia\\nas the performance measure to decide which one to adopt. It is a\\nmeasure which shows how compact each cluster is. Intraclass\\ninertia is the average of the distances between the means and the\\nobservations in each cluster. Eq. (7) indicates this value for given k\\nnumber of clusters [53].\\nFðkÞ ¼ 1\\nn\\nX\\nk\\nX\\ni 2 Ck\\nX\\nm\\nP¼1\\nðXiP \\x02 mkPÞ2\\n(7)\\nwhere n is the number of total observations, CK is the set of kth\\ncluster, XiP is the value of the attribute P for observation i and mkP is\\nthe mean of the attribute P in the kth cluster. Note that in our case\\nthere is only one attribute which is PI, and hence m = 1.\\nThe intraclass inertia values for each possible cluster are also\\nsummarized in Table 6. Prognostic indices were clustered best with\\nk = 3 with k-means clustering algorithm in our case as seen in\\nTable 6 considering its low intraclass inertia value. As seen in\\nTable 6, this classiﬁcation not only gives the lowest intraclass\\ninertia value but also provides an even distribution of the thoracic\\npatients for our nation-wide dataset (38%, 16%, and 46% for low,\\nmedium, and high risk groups of patients, respectively). Although 5\\nclusters with k-means algorithm and 3 clusters in two-step cluster\\nanalysis perform very close to k-means algorithm with 3 clusters,\\nneither of them provides such an even distribution of patients.\\nNote that in addition to considerably higher inertia scores,\\nheuristic calculation with equal-width PIs distribute the nation-\\nwide patients highly skewed to lower tails of risk groups for all ﬁve\\npotential cluster formations. Therefore, we conclude that the k-\\nmeans algorithm based clustering performs better than the other\\npotential groupings in terms of both objective and subjective\\naspects.\\n3.5. Validation of risk groups by Kaplan–Meier survival analysis\\nTo validate the established prognostic indices with 3 clusters\\nand hence the various risk groups in Section 3.4, Kaplan–Meier\\nsurvival analysis [43] was conducted. The corresponding PI\\nclusters were matched with the patients and their predictor\\nvariables from Table 5. In Kaplan–Meier survival analysis the\\npredictor variables were used as explanatory variables and the PI-\\nbased clusters were used as the strata variable to label the patients\\nwith different risks. The main objective here was to compare\\nsurvivor functions for different risk groups of thoracic recipients.\\nIf the survivor function for one risk group is always higher than the\\nsurvivor function for another risk group, than the ﬁrst group\\nclearly lives longer than the second one. The less the survivor\\nfunctions cross, the better the discrimination of the patients\\nwould be. Fig. 2 shows this clear distinction for k-means\\nalgorithm-based PIs.\\nIn order to show that there is a statistically signiﬁcant\\ndifference among these three risk groups, the test of equality\\nover strata was also conducted. Test of equality over strata\\ncontains rank and likelihood-based statistics for testing homoge-\\nneity of survivor functions across strata. The rank tests with the\\nlog-rank test and Wilcoxon test indicate a signiﬁcant difference\\nbetween the risk groups. These results are also supported by\\nlikelihood-based\\nstatistics.\\nThese\\nstatistical\\ntest\\nresults\\nare\\nsummarized in Table 7.\\nTable 5\\nThe variables kept in the Cox regression model.\\nVariable\\nSE\\nChi_square test\\nDF\\nSigniﬁcance\\nexp(b)\\n95% CI for exp(b)\\nLower\\nUpper\\nLOS\\n0.0002\\n385.8701\\n1\\n<.0001\\n1.004\\n1.004\\n1.005\\nEint\\n0.0178\\n56.9447\\n1\\n<.0001\\n0.844\\n0.844\\n0.905\\nGint\\n0.0183\\n11.8644\\n1\\n0.0006\\n0.906\\n0.906\\n0.973\\nAge_Don\\n0.0006\\n247.3162\\n1\\n<.0001\\n1.009\\n1.009\\n1.011\\nWgt_kg_Tcr\\n0.0004\\n5.5091\\n1\\n0.0189\\n0.998\\n0.998\\n1.000\\nWgt_kg_Don\\n0.0005\\n21.3483\\n1\\n<.0001\\n0.997\\n0.997\\n0.999\\nAcyclovir\\n0.0300\\n14.6651\\n1\\n0.0001\\n0.840\\n0.840\\n0.945\\nCitizenship\\n0.0554\\n5.5538\\n1\\n0.0184\\n0.787\\n0.787\\n0.978\\nDayswait_Chron\\n0.0002\\n7.5318\\n1\\n0.0061\\n1.000\\n1.000\\n1.000\\nFluvaccine\\n0.0189\\n15.9915\\n1\\n<.0001\\n1.039\\n1.039\\n1.119\\nIschtime\\n0.0058\\n239.5080\\n1\\n<.0001\\n1.081\\n1.081\\n1.105\\nMed_Cond_Tcr\\n0.0109\\n75.6231\\n1\\n<.0001\\n1.076\\n1.076\\n1.123\\nVad_Tah_Trr\\n0.0002\\n5.7861\\n1\\n0.0162\\n1.000\\n1.000\\n1.001\\nVad_Tah_Tcr\\n0.0077\\n48.9955\\n1\\n<.0001\\n1.040\\n1.040\\n1.072\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n40\\n\\n4. Conclusions and future research directions\\nThis study demonstrates that machine learning-based meth-\\nodology for selecting predictor variables in survivability and\\nprognostic modeling of thoracic organ transplantation is superior\\nto the approaches adopting only expert-selected variables. The\\nstudy showed that of the comprehensive list of predictors, some\\nhave been included in the previous studies (such as gender and age\\nof the recipient, his/her medical condition at registration) while\\nsome others (which are found to be critical) have been absent from\\nthe related literature. These variables (e.g. such as recipient length\\nof stay post-transplant and the interaction of gender and ethnicity\\nbetween the recipient and the donor) should be combined with the\\nfactors identiﬁed in previous studies to better understand and\\nimprove the organ transplantation process.\\nThe study revealed that based on k-means clustering algorithm\\nthe thoracic organ recipients should be allocated into an optimal\\nnumber of ‘‘three’’ risk groups, namely low, medium, and high. This\\nﬁnding conﬁrms the conventional medical discrimination com-\\nmonly used in this ﬁeld of study. However, it also proves that this\\ngrouping should be better performed through a data mining\\nperspective rather than a heuristics-based approach because the\\nlatter one gives more skewed distribution of patients for our US\\nnation-wide\\ndataset.\\nThis\\nis\\nthe\\npoint\\nwhere\\nthe\\nmedical\\nprofessionals should be advised to handle the problem in the\\nfuture.\\nSome of the research extensions to the study reported in this\\narticle includes analysis of other organ types as well as the analysis\\nFig. 2. Kaplan–Meier survival curves for three PIs.\\nTable 7\\nTests of equality over risk groups for k-means based three PI cluster.\\nTest\\nChi-square\\nDF\\nPr > Chi-square\\nLog-rank\\n1002.6135\\n2\\n<.0001\\nWilcoxon\\n939.7492\\n2\\n<.0001\\n\\x022 log(LR)\\n1013.3153\\n2\\n<.0001\\nTable 6\\nThe comparative results for clustering and heuristic-based algorithms.\\nBy clustering algorithms\\nk-Means algorithm\\nTwo-step cluster analysis\\nNumber of clusters\\nRisk group\\nPrognostic index\\nNumber of patients\\nIntraclass inertia\\nPrognostic index\\nNumber of patients\\nIntraclass inertia\\nCluster 1\\nLow\\n0–0.69\\n21163 (58%)\\n12.4 \\x03 10\\x028\\n0–1.09\\n34199 (94%)\\n866.30 \\x03 10\\x028\\nCluster 2\\nHigh\\n0.70–3\\n15262 (41%)\\n1.1–3\\n2226 (6%)\\nCluster 1\\nLow\\n0–0.56\\n13766 (38%)\\n1.68 \\x03 10\\x028\\n0–1.04\\n33529 (92%)\\n2.20 \\x03 10\\x028\\nCluster 2\\nMedium\\n0.57–0.91\\n5834 (16%)\\n1.05–1.83\\n2807 (7.7%)\\nCluster 3\\nHigh\\n0.92–3\\n16825 (46%)\\n1.84–3\\n89(0.3%)\\nCluster 1\\nLow\\n0–0.49\\n15227 (42%)\\n11.2 \\x03 10\\x028\\n0–0.41\\n6410 (17%)\\n445.39 \\x03 10\\x028\\nCluster 2\\nLow–medium\\n0.50–0.77\\n1764 (5%)\\n0.42–0.70\\n15163 (42%)\\nCluster 3\\nMedium–high\\n0.78–1.12\\n9542 (26%)\\n0.71–1.04\\n11892(33%)\\nCluster 4\\nHigh\\n1.13–3\\n9892 (27%)\\n1.05–3\\n2960 (8%)\\nCluster 1\\nVery low\\n0–0.44\\n13266 (36%)\\n3.02 \\x03 10\\x028\\n0–0.36\\n2960 (8%)\\n720.71 \\x03 10\\x028\\nCluster 2\\nLow\\n0.45–0.69\\n451(1%)\\n0.37–0.53\\n10475 (29%)\\nCluster 3\\nMedium\\n0.70–0.95\\n4449 (12%)\\n0.54–0.73\\n10815 (29%)\\nCluster 4\\nHigh\\n0.96–1.39\\n7814 (22%)\\n0.74–1.04\\n4674(13%)\\nCluster 5\\nVery high\\n1.40–3\\n10445 (29%)\\n1.05–3\\n7501 (21%)\\nBy heuristics-based calculation\\nWith equal PI widths\\nWith equal percentiles\\nNumber of clusters\\nRisk group\\nPrognostic index\\nNumber of patients\\nIntraclass inertia\\nPrognostic index\\nNumber of patients\\nIntraclas s inertia\\nCluster 1\\nLow\\n0–1.5\\n36154(99%)\\n713.68 \\x03 10\\x028\\n0–0.64\\n18212(50%)\\n7.02 \\x03 10\\x026\\nCluster 2\\nHigh\\n1.6–3\\n271 (1%)\\n0.65–3\\n18213 (50%)\\nCluster 1\\nLow\\n0–0.9\\n32571 (89%)\\n1678.65 \\x03 10\\x028\\n0–0.53\\n12142 (33.5%;\\n2.01 \\x03 10\\x026\\nCluster 2\\nMedium\\n1–1.9\\n3794 (10%)\\n0.54–0.76\\n12141 (33%)\\nCluster 3\\nHigh\\n2–3.0\\n60 (1%)\\n0.77–3\\n12142 (33.5%;\\nCluster 1\\nLow\\n0–0.7\\n26087 (72%)\\n12961.43 \\x03 10\\x028\\n0–0.47\\n9106 (25%)\\n2755.48 \\x03 10\\x026\\nCluster 2\\nLow–medium\\n0.8–1.5\\n10153 (28%)\\n0.48–0.64\\n9106(25%)\\nCluster 3\\nMedium–high\\n1.6–2.3\\n162(0.4%)\\n0.65–0.82\\n9106 (25%)\\nCluster 4\\nHigh\\n2.4–3\\n23 (0.06%)\\n0.83–3\\n9107 (25%)\\nCluster 1\\nVery low\\n0–0.5\\n15605 (43%)\\n457.67 \\x03 10\\x028\\n0–0.43\\n7285 (20%)\\n3.16 \\x03 10\\x026\\nCluster 2\\nLow\\n0.6–1.1\\n19608 (54%)\\n0.44–0.58\\n7285 (20%)\\nCluster 3\\nMedium\\n1.2–1.7\\n1109(3%)\\n0.59–0.71\\n7285 (20%)\\nCluster 4\\nHigh\\n1.8–2.3\\n80 (0.2%)\\n0.72–0.87\\n7285 (20%)\\nCluster 5\\nVery high\\n2.4–3\\n23 (0.06%)\\n0.88–3\\n7285 (20%)\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n41\\n\\nof multiorgan scenarios where the correlations among the organs\\ncoming from the same donor are also included in the formulation\\nof the problem. Another potential further research direction of this\\nstudy is to validate the patterns obtained from the data mining\\nmodels with a comprehensive simulation model of the organ\\ntransplantation process. Using actual cases, a comprehensive\\ndiscrete-event simulation model can be developed and used as a\\ntest-bed where the potential beneﬁts and limitations of these\\nnovel patterns are tested and validated for a sufﬁciently long\\nperiod of time in the computer simulation environment.\\nReferences\\n[1] Trigt PV, Davis D, Shaeffer GS, Gaynor JW, Landolfo KP, Higginbotham MB, et al.\\nSurvival beneﬁts of heart and lung transplantation. Annals of Surgery\\n1996;223:576–84.\\n[2] Pierson RN, Barr ML, McCullough KP, Egan T, Garrity E, Jessup M, et al. Thoracic\\norgan transplantation. American Journal of Transplantation 2004;4:93–105.\\n[3] Sheppard D, McPhee D, Darke C, Shretha B, Moore R, Jurewitz A, et al. Predicting\\ncytomegalovirus disease after renal transplantation: an artiﬁcial neural network\\napproach. International Journal of Medical Informatics 1999;54:\\n55–76.\\n[4] Lin RS, Horn SD, Hurdle JF, Goldfarb-Rumyantzev S. Single and multiple time-\\npoint prediction models in kidney transplant outcomes. Journal of Biomedical\\nInformatics 2008;41:944–52.\\n[5] Parmar MKB, Machin D. Survival analysis: a practical approach. Cambridge,\\nUK: John Wiley & Sons; 1996.\\n[6] Cox DR. Analysis of survival data. London: Chapman&Hall; 1984.\\n[7] Hariharan S, Johnson CP, Bresnahan BA, Taranto SE, McIntosh MJ, Stablein D.\\nImproved graft survival after renal transplantation in the United States, 1988\\nto 1996. The New England Journal of Medicine 2000;342:605–12.\\n[8] Herrero JI, Lucena JF, Quiroga J, Sangro B, Pardo F, Rotellar F, et al. Liver transplant\\nrecipients older than 60 years have lower survival and higher incidence of\\nmalignancy. American Journal of Transplantation 2003;3:1407–12.\\n[9] Hong Z, Wu J, Smart G, Kaita K, Wen SW, Paton S, et al. Survival analysis of liver\\ntransplant patients in Canada. Transplantation Proceedings 2006;38:2951–6.\\n[10] Kusiak A, Dixon B, Shah S. Predicting survival time for kidney dialysis patients:\\na data mining approach. Computers in Biology and Medicine 2005;35:311–27.\\n[11] Jenkins PC, Flanagan MF, Jenkins KJ, Sargent JD, Canter CE, Chinnock RE, et al.\\nSurvival analysis and risk factors for mortality in transplantation and staged\\nsurgery for hypoplastic left heart syndrome. Journal of the American College of\\nCardiology 2000;36:1178–85.\\n[12] Fernandez-Yanez J, Palomo J, Torrecilla EG, Pascual D, Garrido G, de Diego JJG,\\net al. Prognosis of heart transplant candidates stabilized on medical therapy.\\nRevista Espanola de Cardiologia 2005;58:1162–70.\\n[13] Tjang YS, Heijdan GJMG, Tenderich G, Grobbee D, Korfer R. Survival analysis in\\nheart transplantation: results from an analysis of 1290 Cases in a single center.\\nEuropean Journal of Cardio-Thoracic Surgery 2008;33:856–61.\\n[14] Lin HM, Kaufmann HM, McBride MA, Davies DB, Rosendale JD, Smith CM, et al.\\nCenter-speciﬁc graf and patient survival rates: 1997 UNOS report. JAMA\\n1998;280:1153–60.\\n[15] Cope JT, Kaza AK, Reade CC, Shockey KS, Kern JA, Tribble CG, et al. A cost\\ncomparison of heart transplantation versus alternative operations for cardio-\\nmyopathy. Annual thoracic Surgery 2001;72:1298–305.\\n[16] Aguero J, Almenar L, Martinez-Dolz L, Moro J, Izquierdo MT, Cano O, et al.\\nDifferences in clinical proﬁle and survival after heart transplantation accord-\\ning to prior heart disease. Transplantation Proceedings 2007;39:2350–2.\\n[17] Christensen E, Gunson B, Neuberger J. Optimal timing of liver transplantation\\nfor patients with primary biliary cirrhosis: use of prognostic modeling. Journal\\nof Hepatology 1999;30:285–92.\\n[18] Yoo HY, Galabova V, Edwin D, Thuluvath PJ. Socioeconomicstatus does not affect\\nthe outcome of liver transplantation. Liver Transplantation 2002;8:1133–7.\\n[19] Deng MC, DeMeester MJ, Smiths JMA, Heinecke J, Scheld HH. Effect of receiving\\na heart transplant: analysis of a national cohort entered on to waiting list,\\nstratiﬁed by heart failure severity. British Medical Journal 2000;321:540–5.\\n[20] Ghobrial IM, Habermann TM, Maurer MJ, Geyer SM, Ristow KM, Larson TS,\\net al. Prognostic analysis for survival in adult solid organ transplant recipients\\nwith posy-transplantation lymphoproliferative disorders. Journal of Clinical\\nOncology 2005;23:7574–82.\\n[21] Harper AM, Taranto SE, Edwards EB, Daily OP. An update on a successful\\nsimulation project: the UNOS liver allocation model. In: Joines JA, Barton RR,\\nKang K, Fishwick PA, editors. Proceedings of the winter simulation conference.\\nNew York, NY: ACM Publications; 2000. p. 1955–62.\\n[22] Cupples SA, Ohler L. Transplantation nursing secrets. St. Louis, MO: Hanley &\\nBelfus Publication; 2002.\\n[23] Cristianini N, Shawe-Taylor J. An introduction to support vector machines and\\nother Kernel-based learning methods. London: Cambridge University Press;\\n2000.\\n[24] Mitchell T. Machine learning. New York, NY: McGraw-Hill; 1997.\\n[25] Haykin S. Neural networks: a comprehensive foundation. Upper Saddle River,\\nNJ: Prentice Hall; 1998.\\n[26] Bellazzi R, Zupan B. Predictive data mining in clinical medicine: current issues\\nand guidelines. International Journal of Medical Informatics 2008;77:81–97.\\n[27] Dreiseitl S, Ohno-Machado L. Logistic regression and artiﬁcial neural network\\nclassiﬁcation models: a methodology review. Journal of Biomedical Informat-\\nics 2002;35:352–9.\\n[28] Efron B, Tibshirani R. Statistical data analysis in the computer age. Science\\n1991;253:390–5.\\n[29] Breiman L, Friedman JH, Olshen RA, Stone CJ. Classiﬁcation and regression\\ntrees. Monterey, CA: Wadsworth & Brooks/Cole Advanced Books & Software;\\n1984.\\n[30] Kass GV. An exploratory technique for investigating large quantities of cate-\\ngorical data. Applied Statistics 1980;29:119–27.\\n[31] Quinlan JR. Learning with continuous classes.\\nIn: Adams, Sterling, editors.\\nProceedings of 5th Australian joint conference on artiﬁcial intelligence. Sin-\\ngapore: World Scientiﬁc; 1992. p. 343–8.\\n[32] Makridakis S, Wheelwright SC, Hyndman RJ. Forecasting: methods and appli-\\ncations. New York, NY: John Wiley and Sons; 1998.\\n[33] Everitt BS. Cambridge dictionary of statistics. Cambridge, UK: Cambridge\\nUniversity Press; 2002.\\n[34] Kohavi R. A study of cross-validation and bootstrap for accuracy estimation\\nand model selection. In: Boutilier C, editor. Proceedings of the 14th interna-\\ntional conference on AI (IJCAI). San Mateo, CA: Morgan Kaufmann; 1995. p.\\n1137–45.\\n[35] Olson DL, Delen D. Advanced data mining techniques. New York, NY: Springer;\\n2008.\\n[36] Davis G. Sensitivity analysis in neural net solutions. IEEE Transactions on\\nSystems Man and Cybernetics 1989;19:1078–82.\\n[37] Principe JC, Euliano NR, Lefebvre WC. Neural and adaptive systems. New York,\\nNY: John Wiley and Sons; 2001.\\n[38] Saltelli A. Making best use of model evaluations to compute sensitivity indices.\\nComputer Physics Communications 2002;145:280–97.\\n[39] Saltelli A, Tarantola S, Campolongo F, Ratto M. Sensitivity analysis in practice—\\na guide to assessing scientiﬁc models. New York, NY: John Wiley and Sons;\\n2004.\\n[40] Ohno-Machado L. Modeling medical prognosis: survival analysis techniques.\\nJournal of Biomedical Informatics 2001;34:428–39.\\n[41] Grambsch P, Therneau T. Proportional hazards rates and diagnostics based on\\nweighted residuals. Biometrika 1994;81:515–26.\\n[42] Christensen E. Multivariate survival analysis using Cox’s regression model.\\nHepatology 1987;7:1346–58.\\n[43] Kaplan E, Meier P. Nonparametric estimation from incomplete observations.\\nJournal of the American Statistical Association 1958;53:187–220.\\n[44] MacQueen JB. Some methods for classiﬁcation and analysis of multivariate\\nobservations.\\nIn: Le Cam LM, Neyman J, editors. Proceedings of the ﬁfth\\nsymposium on math, statistics, and probability. Berkeley, CA, USA: University\\nof California Press; 1967. p. 281–97.\\n[45] Krishna K, Murty MN. Genetic k-means algorithm. IEEE Transactions on\\nSystems Man and Cybernetics-Part B Cybernetics 1999;29:433–9.\\n[46] Chiu T, Fang D, Chen J, Wang Y, Jeris C. A robust and scalable clustering\\nalgorithm for mixed type attributes in large database environment. In: Lee D,\\neditor. Proceedings of the seventh ACM SIGKDD international conference on\\nknowledge discovery and data mining. New York, NY: ACM Publications; 2001.\\np. 263.\\n[47] Li ZH, Luo P. Statistical analysis lectures of SPSS for windows. Beijing, China:\\nBeijing Publishing House of Electronics Industry; 2004.\\n[48] SPSS\\nInc.\\nPASW\\nModeler\\nData\\nMining\\nToolkit,\\nVersion\\n13.0,\\nhttp://\\nwww.spss.com/software/modeling/modeler/ 2009 (accessed: June 5, 2009).\\n[49] SAS Institute Inc. Statistical Analysis Systems, Version 9.1.3, http://www.sas.\\ncom/technologies/analytics/statistics/stat/ 2008 (accessed: May 11, 2009).\\n[50] Hair JF, Anderson RE, Tatham RL, Black W. Multivariate data analysis. Upper\\nSaddle River, NJ: Prentice Hall; 1998.\\n[51] Johnson DE. Applied multivariate methods for data analysts. Paciﬁc Grove, CA:\\nDuxbury Press; 1998.\\n[52] Oztekin A, Delen D, Kong ZJ. Predicting the graft survival for heart–lung\\ntransplantation patients: An integrated data mining methodology. Interna-\\ntional Journal of Medical Informatics 2009;78:84–96.\\n[53] Michaud P. Clustering techniques. Future Generation Computer Systems\\n1997;13:135–47.\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n42\\n'}, {'source': '/Users/sir/Downloads/Data/PDF/Decision Support Systems.pdf', 'text': \"Contents lists available at ScienceDirect \\nDecision Support Systems \\njournal homepage: www.elsevier.com/locate/dss \\nA two-stage machine learning framework to predict heart transplantation \\nsurvival probabilities over time with a monotonic probability constraint \\nHamidreza Ahady Dolatsaraa, Ying-Ju Chenb, Christy Evansc, Ashish Guptad, Fadel M. Megahede,⁎ \\na School of Management, Clark University, Worcester, MA 01610, USA \\nb Department of Mathematics, University of Dayton, Dayton, OH 45469, USA \\nc Department of Biological Sciences, Auburn University, Auburn, AL 36849, USA \\nd Harbert College of Business, Auburn University, Auburn, AL 36849, USA \\ne Farmer School of Business, Miami University, Oxford, OH 45056, USA  \\nA R T I C L E  I N F O   \\nKeywords: \\nData mining \\nHeart transplant \\nIsotonic regression \\nMedical informatics \\nMulti-period forecasting \\nUnited network for organ sharing \\nA B S T R A C T   \\nThe overarching goal of this paper is to develop a modeling framework that can be used to obtain personalized, \\ndata-driven and monotonically constrained probability curves. This research is motivated by the important \\nproblem of improving the predictions for organ transplantation outcomes, which can inform updates made to \\norgan allocation protocols, post-transplantation care pathways, and clinical resource utilization. In pursuit of our \\noverarching goal and motivating problem, we propose a novel two-stage machine learning-based framework for \\nobtaining monotonic probabilities over time. The first stage uses the standard approach of using independent \\nmachine learning models to predict transplantation outcomes for each time-period of interest. In the second \\nstage, we calibrate the survival probabilities over time using isotonic regression. To show the utility of our \\nframework, we applied it on a national registry of U.S. heart transplants from 1987 to 2016. The first stage \\nproduces an area under the receiver operating curve (AUC) between 0.60 and 0.71 for years 1–10. While the 1- \\nyear prediction AUC result is comparable to the reported results in the literature, our 10-year AUC of 0.70 is \\nhigher than the current state-of-the-art results. More importantly, we show that the application of isotonic \\nregression to calibrate the survival probabilities for each patient over the 10-year period guarantees mono\\xad\\ntonicity, while capitalizing on the data-driven and individualized nature of machine learning models. To pro\\xad\\nmote future research, our code and analysis are publicly available on GitHub. Furthermore, we created a web \\napp titled “H-TOP: Heart Transplantation Outcome Predictor” to encourage practical applications.   \\n1. Introduction \\nThere exists numerous applications where a decision-maker is in\\xad\\nterested in evaluating the probabilities of observing a terminal state of a \\nprocess or a system at multiple time periods. Recent studies have ex\\xad\\namined applications such as (a) the time-taken to complete a higher \\neducation degree [1], where the probability of completing the degree \\nby the end of the nth year will be less than or equal to the completion \\nprobability by the (n + 1)th year; (b) churn prediction in the energy [2], \\nhigher education [3], and telecommunications [4] sectors, where the \\nprobability of client's churn is montonically increasing with time; (c) \\nsupply-chain servicing probabilities are monotonically increased with \\nincreases in delivery window sizes [5]; and (d) surgical applications, \\ne.g., lung [6], heart [7,8], and kidney [9] transplants where patients' \\nsurvival probabilities monotonically decrease over time. The utility of a \\ndecision support system in such applications hinges on accurate multi- \\nperiod predictions, where the predicted outcome probabilities are \\nmonotonic over time. \\nThere are two common approaches for achieving monotonic sur\\xad\\nvival probabilities. The first approach involves the use of population- \\nbased survival analysis techniques using Kaplan-Meier estimates [10], \\nwhich are not well suited for applications involving a large number of \\npotential predictors and/or when an individualized prediction is im\\xad\\nportant (e.g., organ-allocation decisions). On the other hand, machine \\nlearning methods (ML) are used in the second approach to estimate the \\nsurvival, or any outcome of interest, probability for each time-period \\nbeing investigated. The utility of existing ML applications for multi- \\nperiod prediction is limited since current approaches (a) develop in\\xad\\ndependent ML models for each time-period [8,11], which do not \\nguarantee the expected monotonic probability outcomes over time; (b) \\nhttps://doi.org/10.1016/j.dss.2020.113363 \\nReceived 1 February 2020; Received in revised form 7 June 2020; Accepted 12 July 2020    \\n⁎ Corresponding author. \\nE-mail addresses: hamid@clarku.edu (H. Ahady Dolatsara), chen4@udayton.edu (Y.-J. Chen), cje0010@auburn.edu (C. Evans), azg0074@auburn.edu (A. Gupta), \\nfmegahed@miamioh.edu (F.M. Megahed). \\nDecision Support Systems 137 (2020) 113363\\nAvailable online 31 July 2020\\n0167-9236/ © 2020 Elsevier B.V. All rights reserved.\\nT\\n\\nutilize a sequential prediction approach where the “survival” prob\\xad\\nabilities from time periods 1, 2, 3, …, t are inputs to predict the survival \\nat time t + 1 [12,13], however, such approaches also do not guarantee \\nmonotonicity as the utilized ML models do not include a hard constraint \\non the predicted probabilities due to their assumption-free nature; or \\n(c) apply the ML approach to predict the outcome probability for one \\ntime-period and use the population's survival outcomes to calibrate \\nother periods' probabilities [14]. This particular drawback limits the \\nchanges in the outcome probabilities to those captured by the popula\\xad\\ntion averages, and hence, may not be effective when the case being \\ninvestigated has predictors that deviate significantly from the average \\ncase. This is not uncommon in healthcare applications, and is an im\\xad\\nportant driving factor for the recent developments in personalized \\nmedicine [15]. \\nThe overarching goal of this paper is to develop and comprehen\\xad\\nsively describe a modeling framework that can be used to obtain per\\xad\\nsonalized, data-driven and monotonically constrained probability \\ncurves. Our research is motivated by the important and open-research \\nproblem of predicting organ transplantation outcomes based on solely \\npre-operative data in order to provide decision making assistance for \\norgan-allocation protocols [6,9], post-transplantation clinical and care \\npathways [16], and clinical resource utilization [16]. In pursuit of these \\nobjectives and with the limitations in existing transplantation literature \\nin mind [8,11–14], we propose a novel two-stage machine learning- \\nbased framework for obtaining monotonic “survival” probabilities over \\ntime. The first stage utilizes the standard approach of using in\\xad\\ndependent machine learning models to predict survival outcomes for \\neach time-period of interest [e.g., see [8,11]]. The primary objective in \\nthis stage is to select the most predictive/efficient machine learning \\nmodel based on a predefined performance measure (e.g., area under the \\nreceiving operating characteristics curve, AUC). However, as men\\xad\\ntioned earlier, this approach suffers from the drawback that the ob\\xad\\ntained survival probabilities from stage-one are not guaranteed to be \\nmonotonically decreasing. Therefore, in the second stage, we calibrate \\nthe survival probabilities over time using isotonic regression [17], \\nwhich constrains the survival probabilities (p) such that pt+1 ≤ pt,  ∀ t. \\nThus, isotonic regression ensures that the survival probabilities are \\nmonotonically decreasing and that the results are personalized based on \\nthe donor and recipient's characteristics. \\nThe remainder of this paper is organized as follows. Section 2 pro\\xad\\nvides an overview of our proposed two-stage framework for multi- \\nperiod monotonic probabilistic outcome predictions. Section 3 de\\xad\\nscribes the significance of the heart transplantation problem, the need \\nfor risk stratification, and the decisions made throughout the trans\\xad\\nplantation process. Section 4 explains how the proposed two-stage \\nframework can be applied to multi-period predictions of heart trans\\xad\\nplantation survival probabilities. Section 5 describes the results of our \\napplication. Finally, Section 6 describes the implications of those results \\nto advancing heart transplantation research/practice as well as pro\\xad\\nviding opportunities for future research in applied machine learning. In \\nthe supplementary materials section, we provide a link to a web-based \\ndecision support system, which can facilitate the use of the proposed \\nframework in informing organ-allocation protocols and post-trans\\xad\\nplantation clinical pathways. Furthermore, we share a link of our R \\nMarkdown document that includes our code and analyses to facilitate \\nthe adoption and integration of our proposed methodology in future \\nresearch. \\n2. The two-stage framework for obtaining monotonic prediction \\nprobabilities over time \\nThe data revolution has popularized the use of data-driven pre\\xad\\ndictive models that significantly departed from the traditional empirical \\nmodeling approach in Information Systems [18,19]. Nevertheless, as \\nnoted by Shmueli and Koppius [18, p. 554], data-driven predictive \\nmodeling is a “core scientific activity” that is “useful for generating new \\ntheory, developing new measures, comparing competing theories, im\\xad\\nproving existing theories, assessing the relevance of theories, and as\\xad\\nsessing the predictability of empirical phenomena.” To reap the benefits \\nof predictive models, one must strive to (a) have accurate and precise \\nresults, (b) be able to present the right information with the required \\nlevel of granularity [19], and (c) not capitalize on chance [20]. \\nThis paper proposes a generic framework for multi-period mono\\xad\\ntonic probabilistic outcome predictions. The goals of our framework are \\nto enable: (a) accurate and precise results by enabling the use of any \\nstatistical, machine or deep learning method for predicting the outcome \\nprobability for a given time period; (b) presenting the right information \\nwith the required granularity through the emphasis on data-driven and \\npersonalized/individualized predictions; and (c) not capitalizing on \\nchance (where we avoid over-fitting and ensure that the outcomes are \\nmonotonic through the incorporation of a hard mathematical constraint \\non the magnitude of the probability outcomes for each period). To \\nensure that the framework is “generic”, our framework does not require \\nthe use of a specific machine learning algorithm to obtain the prob\\xad\\nability estimates since their utility may differ by problem (e.g., varia\\xad\\ntions in number of variables, observations, computational processing \\nrequirements and/or requirements for prediction accuracy and inter\\xad\\npretation). Furthermore, we have considered an approach for guaran\\xad\\nteeing monotonic outcomes for any t  >  1 time periods, which makes \\nour approach effective even when only two periods are considered. \\nOur framework addresses two major shortcomings of existing fra\\xad\\nmeworks in the literature; the non-existence of a framework that is \\ncomprehensive enough to be readily applied by analysts over multiple \\ntime periods, yet generic enough to be applicable across various do\\xad\\nmains. These two characteristics of our framework allow for the utili\\xad\\nzation of machine learning results in informing decision making in re\\xad\\nsponse \\nto \\nmulti-period \\ninformation. \\nThe \\nflexibility \\nand \\ncomprehensiveness of a decision-making framework are key requisites \\nfor its successful integration in practice [21,22]. \\nIn the first stage of our proposed framework, independent statistical, \\nmachine and/or deep learning models are applied to predict the prob\\xad\\nability of the binary outcome for each time-period of interest. As such, \\nthis stage will involve the following standard data analytic steps: (a) \\ndata collection, (b) data preparation, (c) choosing a set of candidate \\nmodels, (d) training the models, (e) evaluating the models based on one \\nor more performance metrics of interest, (f) parameter tuning, and (g) \\nmodel selection based on the performance of the tuned-models on one \\nor more holdout/test dataset. For the sake of conciseness, we do not \\ncover these steps in greater detail here since they are well-documented \\nin the literature and standard textbooks [e.g. see 23]. These steps are \\nrepeated for each time-period of interest; therefore, the output from this \\nstage is a computed probability of an event (e.g., a client's churn, an \\norgan's failure, or a patient's survival post surgery) for the different \\ntime-periods. This output can be represented by p = p1, p2, …, pt, where \\npt is the computed outcome probability at period t as obtained from the \\nselected model. \\nIn Stage 2, we propose the use of isotonic regression to calibrate and \\nensure that the elements in p are monotonic. For example, if we were to \\nconsider were the required outcome should be monotonically de\\xad\\ncreasing, this can be achieved by solving the following optimization \\nproblem \\n=\\n…\\n=\\n= w y\\np\\ny\\ny\\ny\\ny\\ny\\nmin\\n(\\n)\\nsubject to\\n,\\ni\\nt\\ni\\ni\\ni\\nmax\\nt\\nmin\\n1\\n2\\n1\\n2\\n(1) \\nwhere wi is a strictly positive weight and yi denotes the calibrated \\nprobability at time i (i.e., the ys are the decision variables obtained from \\nthe optimization model). We use wi = 1,  ∀ i since this corresponds to \\nthe squared Euclidean distance (i.e., the objective function would have \\na physical interpretation). The reader should note that in our frame\\xad\\nwork we preferred isotonic regression to other possible calibration \\napproaches (e.g., the sigmoid smother used in support vector machines \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n2\\n\\n[24]) since it (a) works well for any t  >  1 period; (b) is a non-para\\xad\\nmetric approach, which requires no further tuning; (c) is computa\\xad\\ntionally efficient since the optimization problem can be solved in O n\\n( )\\ntime using the pool adjacent violators algorithm (PAVA) [25]; and (d) \\ncan be easily adapted for non-decreasing probabilities (by changing the \\nconstraint to ymin = y1 ≤ y2 ≤ … ≤ yt = ymax). \\nFig. 1 provides an overview of how isotonic regression can be used \\nto calibrate the outcome probabilities obtained from the Stage 1. In the \\nfigure, we chose to simulate the case where the researcher is interested \\nin computing an organ's failure probabilities over a 20 year time-period \\npost-transplant to illustrate the utility of our approach to a non-de\\xad\\ncreasing prediction problem. Hereafter, we will only consider the op\\xad\\nposite case where one would like to predict the survival probability post \\ntransplant (i.e., the probabilities would be monotonically decreasing \\nover time). \\n3. Problem description \\nHeart failure is a serious medical condition, which can be char\\xad\\nacterized by the heart not being able to pump enough blood and oxygen \\nto support other organs [26]. It is “a global pandemic affecting at least \\n26 million people worldwide and is increasing in prevelance” [26]. For \\nexample, in the U.S., the number of heart failure patients was 5.7 \\nmillion in 2009–2012, which has increased to 6.2 million in 2013–2016 \\nand is expected to increase to 8 million by 2030 [27]. Despite the ad\\xad\\nvancements in treatment protocols, the outlook for heart failure pa\\xad\\ntients remains poor as (a) “heart failure has no cure” [28] and (b) the 5- \\nyear mortality is ≈50% [27], which is worse than the five-year rate for \\npatients suffering from various cancers such as bowl, breast, colon and \\nprostate cancers [29]. \\nHeart transplantation is the most effective treatment for patients \\nwith end-stage heart failure. For adult recipients, the median survival \\ntime after a heart transplant is 9.5 years, which is significantly better \\nthan the median survival of 2.3 years for wait-listed patients who do not \\nreceive a transplant [27]. In the U.S., there were 3552 heart transplants \\nperformed in 2019 [30], with an additional 3527 wait-listed candidates \\nas of May 28, 2020 [31]. Fig. 2 depicts the five phases of the U.S. heart \\ntransplantation process, which are based on the descriptions in the \\nUnited Network for Organ Sharing (UNOS) matching procedure [32]. \\nIn phase I, an eligible end-stage heart failure patient is accepted to a \\ntransplant hospital as a transplant candidate. At this point, the candi\\xad\\ndate is wait-listed and their medical data (e.g., height, weight, blood \\ntype, medical urgency) as well as the hospital's location are added to \\nthe UNOS database [32]. Phase II initiates when (a) a donor heart is \\navailable in a localized geographic area such that preservation and \\ntransport time are feasible, and (b) the candidate is deemed compatible \\nwith donor heart based on blood type, height, weight, and other med\\xad\\nical factors [32]. At this point, the candidate is referred to as a potential \\ntransplant recipient (PTR). The UNOS algorithmic allocation protocol \\n[32] would perform a “match run” of all PTRs and rank them based on \\n(i) medical urgency considerations, (ii) geographic proximity to the \\ndonor's hospital, and (iii) pediatric status. The organ is offered to \\ntransplant centers in the order of the “match run”; it is not uncommon \\nthat a PTR would go back to the first phase of being wait-listed (e.g., if \\nnot highly ranked by the “match run”) [32]. Consequently, the PTR \\nonly advances to the third phase (transplantation) if they were selected \\nby the “match run” and the transplant offer is accepted by the trans\\xad\\nplant hospital (i.e., after additional medical prognoses). The patient, at \\nthis point, is referred to as the recipient. Phase IV is comprised of the in- \\nhospital post-transplant care, where the recovery and discharge are \\nFig. 1. An animated illustration of how isotonic regression is used to calibrate \\nmulti-period ML probabilities. To view the animation, the reader is referred to \\nthe online version of the article. Alternatively, the reader can also view the \\nanimation on our Markdown document (see the supplementary materials). \\nFig. 2. An overview of the decisions made throughout the transplantation process.  \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n3\\n\\ngoverned by the respective policies, procedures and pathways set by the \\nhospital and/or transplantation team. Phase V captures the out-patient \\npost-transplant process, where the recipient adopts the prescribed life \\nstyle and protocols in order to prolong the lifetime of the graft and \\nreturn to society. The recipient is examined regularly in phase V for \\npossible complications; in the case of a graft failure, the recipient would \\nbe evaluated for re-transplantation and is returned to phase I [33]. \\nThe stated goals of the UNOS “policies and computerized network \\n[are to] match donated organs with transplant candidates in ways that \\nsave as many lives as possible and provide transplant recipients with \\nthe best possible chance of long-term survival” [32]. With these goals in \\nmind, proposed changes to the UNOS policies are periodically ex\\xad\\namined [e.g., see 34], and merited proposals are adopted [35] to con\\xad\\ntinuously improve the transplantation outcomes. By analyzing and \\nmodeling the UNOS data, we aim at improving the utility of the existing \\napproaches for multi-period heart transplantation prediction [8,11–14], \\ndescribed in Section 1. Our two-stage framework allows for the use of \\nany machine learning method to obtain individualized, monotonically \\ndecreasing multi-period survival probabilities for any PTR at the \\n“match run” phase. The implications of our approach to risk stratifi\\xad\\ncation are three-fold: (a) improving predictive accuracy (allowing for \\nthe incorporation of best practices to data cleaning, model selection, \\nand tuning) and a more precise calibration of the survival probabilities \\nover time (accounting for the specific parameters' values of evaluated \\ncases); (b) providing UNOS with a benchmark to compare the expected \\noutcomes from a match, which in the long-run can potentially inform \\nthe policies and algorithms governing phase II of the transplantation \\nprocess; and (c) informing the protocols prescribed by the transplan\\xad\\ntation team in phases IV and V based on the estimated survival risk. \\n4. Application of the two-stage framework to predict \\ntransplantation survival outcomes \\nFig. 3 provides an overview of the application of our two-stage \\nframework \\nfor \\nobtaining \\nmonotonically \\ndecreasing \\nsurvival \\nprobabilities. Our goals for Stage I are to select an appropriate model \\nbased on its predictive performance and examine how the selected \\nvariables importance change over time. Stage I is comprised of the \\nfollowing steps: (a) raw data undergoing detailed data preparation that \\nexplains how missing data is handled and how indicator variables are \\ngenerated; (b) use subsampling techniques to handle the imbalance \\nbetween graft survivals and failures at a given time point; (c) use of a \\nstructured variable selection technique to improve the performance of \\nmachine learning models [7,8]; and (d) use of machine learning (ML) \\nmodels to predict survival probabilities for the various time periods. In \\nour application, we examine 11 prediction periods one-month post- \\ntransplant, and 1–10 years post-transplant. These periods were selected \\nto capture the acute rejection period (typically at one-month post- \\ntransplant [36]), short-term survival (< 3 years post-transplant [8]), \\nmedium-term survival (typically at 3–5 years post-transplant [8]), and \\nlong-term survival (typically at 9+ years, post-transplant [8]). In Stage \\nII, we calibrate the survival probabilities such that they are mono\\xad\\ntonically decreasing over time. Stage II has two steps: (a) combining \\noutputs from all time points derived from Stage I models and forming a \\nprobability matrix, and (b) the application of isotonic regression to \\ncalibrate the probability function. \\n4.1. Stage I: Using ML methods to obtain a survival probability at each time \\npoint \\n4.1.1. Data description \\nOur UNOS dataset covered 103,570 heart transplant events up to \\nand including September 30, 2016. The dataset has been anonymized \\nby UNOS and contains 494 variables, capturing various pre-, intra-, and \\npost-transplant information. Note that variables/records within the \\nUNOS dataset have a large number of missing data. In our case, more \\nthan 30% of the “data-table cells” had missing values for various rea\\xad\\nsons. For example, the start date of data collection for 327 of the 494 \\nvariables (66.19%) had a start date of 1990 or after. Moreover, some of \\nthe variables had missing-at-random values due to possible data entry \\nFig. 3. Adopting the proposed two-stage framework to obtain monotonically decreasing heart transplantation survival probabilities over time.  \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n4\\n\\nand collection problems; a variable is not considered to be missing-at- \\nrandom if, e.g., the frequency of missing data changes significantly by \\ntransplant year. Thus, a prerequisite to analyzing the UNOS dataset is a \\ndetailed data cleaning/preparation procedure that will facilitate re\\xad\\nproducible analyses. \\n4.1.2. Data preparation/cleaning \\nOur procedure for cleaning the UNOS dataset was comprised of six \\nsteps. The first step focused on reducing the dimensionality of the data \\nby removing the following sets of variables: (a) intra- and post-transplant \\nvariables, where we have only kept G-TIME (a continuous variable \\ncapturing the time in days from transplant to graft failure) and G- \\nSTATUS (a binary variable, denoting if the graft has failed or not at the \\nlast follow-up time) as they are used in constructing the dependent \\nsurvival outcome at a given time point; (b) variables with an end data- \\ncollection-date since they cannot be used in future PTR evaluations; (c) \\nvariables with a recent data-collection-date, where we only included \\nvariables that were added before 2000; and (d) any variable that had \\n≥90% missing since imputation techniques would produce biased re\\xad\\nsults. We refer the reader to our R Markdown document, which con\\xad\\ntains our code and analysis and whose link is provided in the \\nSupplementary Materials Section of this paper. \\nThe second step involved determining whether a record should be \\nused for analysis (for a given time point). Any transplantation event was \\ncensored if we did not have information pertaining to a patient's sur\\xad\\nvival at the time point of interest (e.g., a recipient was alive at day 300 \\npost-transplant, would not explain whether he/she would have sur\\xad\\nvived to the end of year 1). Our censoring procedure followed the ap\\xad\\nproach of Dag et al. [8], capitalizing on both G-TIME and G-STATUS. \\nSpecifically, the data was censored if and only if the G-TIME was less \\nthan the number of days required for the time-point analysis and the G- \\nSTATUS indicates that the recipient was still alive. Accordingly, the \\ndichotomous outcome for the ith recipient at time t was coded based on \\n=\\no\\ni\\nt\\ni\\nt\\n1, if the\\nrecipient was alive at time\\n0, if the\\nrecipient was dead at time\\n.\\ni t\\nth\\nth\\n,\\n(2)  \\nThe third step in our recommended data preparation procedure \\ngenerated medically-relevant features that have been shown as im\\xad\\nportant/predictive in the literature. These features were computed by \\ncombining information from two or more independent variables. In our \\nanalysis, we created the following features based on [7,8,14]: (a) pul\\xad\\nmonary vascular resistance (PVR), (b) ischemia time in minutes, (c) \\nmismatch in Extracorporeal membrane oxygenation (ECMO) for re\\xad\\ncipient between registration and transplant, (d) percent change in the \\nrecipient's body mass index (BMI) between registration and transplant, \\n(e) percent change in the recipient's weight between registration and \\ntransplant, (f) percent change in the recipient's height between regis\\xad\\ntration and transplant, (g) the absolute difference in age between donor \\nand recipient, and (h) the absolute difference in BMI between donor \\nand recipient. For additional detail on how each feature was generated, \\nwe refer the reader to our R Markdown document. \\nThe fourth step related to the handling of missing data. In the first \\nstep of our data cleaning process, we removed any categorical variable \\nthat has ≥90% missing values, numerical variable that has ≥30% \\nmissing values, and categorical variables which are almost invariant \\n(> 90% of the values are in one level). Note that we have allowed a \\nlarger threshold for categorical variables since we imputed these values \\nas “unknown” in one of our categorical imputation procedures. We do \\nacknowledge that other thresholds could have been used (and their \\nchoice can possibly affect our prediction performance); however, we \\ndid not explore other values in our analysis as our prediction results \\nwere reasonable and we examined a large number of imputation, en\\xad\\ncoding, sub-sampling and algorithm combinations. We utilized the \\nfollowing imputation strategy (a) correctly defining independent vari\\xad\\nable types in the analytical software, and (b) comparing the predictive \\nperformance of the models with no imputation and the models with \\nimputation (median/mean and mode imputation for numeric and ca\\xad\\ntegorical variables, respectively). This performance comparison would \\nsuggest if data imputation can lead to improvements in the prediction \\nresults and whether a more computationally intensive imputation \\ntechnique was warranted. \\nIn the fifth step, the levels of categorical variables were regrouped. \\nWe attempted to reduce the number of levels, by removing any level, \\nwhich has a limited number of observations, and re-coding the values \\nsuch that they are added to the level corresponding to “other”. The \\npurpose of this step was three-fold: (a) avoiding over-fitting, (b) redu\\xad\\ncing the possibility of errors due to factor levels not observed in the \\ntraining of the model, and (c) reducing the dimensionality/complexity \\nof the developed model (especially for tree-based approaches such as \\nrandom forests). Additionally, many implementations of machine \\nlearning algorithms require the categorical variables to be encoded \\neither using label or one-hot encoding. \\nThe sixth step of data preparation involved splitting the data ran\\xad\\ndomly into a training (80%) and a hold-out (20%) set. We used a 5-fold \\ncross validation approach for selecting an appropriate model from the \\ntraining set since (a) it is more computationally efficient than 10-fold \\ncross validation, and (b) both k = 5 and k = 10 “have been shown \\nempirically to yield test error rate estimates that suffer neither from \\nexcessively high bias nor from very high variance” [23, p. 183]. \\n4.1.3. Variable selection \\nVariable selection approaches attempt to (a) reduce the computa\\xad\\ntional burden by reducing the dimensionality of the feature space, (b) \\nimprove the prediction performance by focusing only on important \\nvariables, and (c) improve the generalizability of the developed pre\\xad\\ndiction models. Variable selection approaches can be categorized into \\nthree main groups: [37] (a) filter methods, where univariate statistical \\napproaches are typically used to select features based on their re\\xad\\nlationship to the response, (b) wrapper methods, where the important \\nfeatures are kept based on their prediction performance, and (c) em\\xad\\nbedded methods, which involve the use of methods such as LASSO and \\nrandom forests for selecting the most predictive features. In our case \\nstudy, we compared the performance of fast feature selection, LASSO, \\nand random forests for feature selection. \\n4.1.4. Sub-sampling approaches for imbalanced data \\nOn average, the expected survival probability for heart transplan\\xad\\ntation recipients decreases by about 3–4% per post-transplant year \\n[38]. Moreover, the aforementioned censoring approach would result \\nin the removal of recipients who are currently surviving, but have not \\nreached the time-point of interest. Consequently, we expect that the \\nshort time intervals (e.g., 1 month, 1 year, etc.) will have a much larger \\nnumber of survivals than deaths, and the longer time-periods (e.g., 9- \\nand 10- years post-transplant) will have a higher ratio of deaths to \\nsurvivals. In the data mining literature, sub-sampling methodologies \\nare typically deployed to improve the prediction performance when the \\nunderlying training dataset is imbalanced [39]. We considered five sub- \\nsampling strategies:  \\n(A) No sub-sampling (None), which can be considered as the baseline for \\ncomparison; \\n(B) Random under-sampling (DOWN), where the majority class is sam\\xad\\npled without replacement such that the number of observations in \\nboth classes (i.e. 0 and 1) are equal;  \\n(C) Random over-sampling (UP), where the minority class is sampled \\nwith replacement such that the number of observations in both \\nclasses (i.e. 0 and 1) are equal; \\n(D) Synthetic minority oversampling technique (SMOTE), where the min\\xad\\nority class is over-sampled by “taking each minority class data \\npoint and introducing synthetic examples along the line segments \\njoining any or all of the k-minority class nearest neighbors” [40,p. \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n5\\n\\n342], and repeating the process until the number of observations \\nper class is approximately equal; and \\n(E) ROSE, where a statistical approach based on the smoothed boot\\xad\\nstrap sub-sampling technique of Menardi and Torelli [41] is used \\nfor handling the class imbalance problem. \\nNote that these approaches were selected since they are widely used \\nin machine learning practice due to their implementation within the \\npopular “caret” package in R. \\n4.1.5. Application of statistical and machine learning algorithms \\nAs mentioned in Section 1, our primary goal is to develop a fra\\xad\\nmework that can calibrate the survival probabilities obtained from \\nmachine learning (ML) algorithms over multiple time points. From this \\npoint of view, our approach (in Stage II) is designed to work with any \\nmachine learning methodology that can be used for two-class problems. \\nIn our experience with the UNOS dataset, the utility of a given ML al\\xad\\ngorithm can differ significantly based on the data preparation metho\\xad\\ndology utilized. Algorithms for predicting dichotomous outcomes can \\nbe categorized into:  \\n(A) Statistical models, which are not assumption free. Logistic regression \\n(LR) is the most frequently utilized approach in the literature [see \\ne.g., 8, 11, 42]. Linear discriminant analysis (LDA) also showed \\ngood predictive performance in the literature [43]. \\n(B) Single (data-driven) classifiers, which include a large group of as\\xad\\nsumption-free machine learning approaches. Commonly used al\\xad\\ngorithms include artificial neural networks (ANNs) [8,11,12,42], \\ndecision trees (CART) [7,8], and support vector machines (SVM) \\n[8]; and \\n(C) Ensemble approaches, where single classifiers' predictions are com\\xad\\nbined using voting schemes. In our analysis, we examined random \\nforests (RF) and eXtreme Gradient Boosting (XGB). \\nWe considered all the aforementioned seven ML algorithms (LR, \\nLDA, ANN, CART, SVM, RF and XGB) in the prediction of the heart \\ntransplantation survival probabilities. \\n4.1.6. Evaluation and model selection \\nBased on our Stage I description, we considered six modeling fac\\xad\\ntors, with the following levels:  \\n(A) 2 categorical imputation strategies (drop and impute missing as \\n“unknown”);  \\n(B) 2 numerical imputation strategies (drop and median imputation);  \\n(C) 2 categorical variable encoding approaches (label and one-hot);  \\n(D) 3 variable selection approaches (fast feature selection, LASSO, and \\nrandom forests);  \\n(E) 5 sub-sampling methods (none, DOWN, UP, SMOTE and ROSE); \\nand  \\n(F) 7 machine learning models (LR, LDA, ANN, CART, SVM, RF and \\nXGB). \\nFurthermore, we followed the recommendation of James et al. [23] \\nin using 5-fold cross validation in model selection. To examine short, \\nmid- and long-term survival, we stated that we would examine 11 \\nprediction time-periods to generate the survival probability curve. \\nConsequently, the enumeration of these combinations would result in \\n2 × 2 × 2 × 3 × 5 × 7 × 5 × 11 = 46,200 runs. These runs \\ncorrespond to running a general factorial experiment to compute the \\nmain factor effects and all high order interactions for the settings as\\xad\\nsociated with each step of our framework. \\nWe reduced the computational burden to 4200 runs by comparing \\nthe predictive performance of those combinations based on the 1-year \\nprediction period. We have chosen 1-year since it (i) provided us with \\nthe largest sample size (with the exception of the 1-month time period \\nwhere the focus is on acute organ rejection prediction), and (ii) it is a \\ncommonly used period in the literature [e.g., see 8, 14, 11]. The se\\xad\\nlected modeling approach will then be trained for all other periods for \\nthe sake of demonstrating the need and utility of our isotonic regression \\ncalibration approach in Stage II. \\nFor model selection and evaluation, we reported five common \\nperformance metrics that are suited for two class classification pro\\xad\\nblems. To present these metrics, let us consider the confusion matrix in  \\nTable 1. Based on this confusion matrix, the five metrics can be defined \\nas follows: \\n=\\n+\\n+\\n+\\n+\\naccuracy\\nTN\\nTP\\nTN\\nFP\\nFN\\nTP ,\\n(3)  \\n=\\n+\\nsensitivity\\nTP\\nTP\\nFN ,\\n(4)  \\n=\\n+\\nspecificity\\nTN\\nTN\\nFP ,\\nand\\n(5)  \\n=\\n×\\nG\\nmean\\nsensitivity\\nspecificity .\\n(6)  \\nThe receiver operating characteristics (ROC) curve can be plotted \\nwith the sensitivity on the y-axis versus 1 − specificity on the x-axis. The \\narea under the curve (AUC) captures how well the model predicts actual \\nsurvival as survivals and actual deaths as deaths. AUC is our fifth me\\xad\\ntric. \\nIn this paper, we used G − Mean as the primary metric for model \\nselection since it (a) is suitable for unbalanced classification problems, \\nand (b) penalizes models where there is a large discrepancy between \\nsensitivity and specificity (i.e., more suitable than AUC when similar \\nvalues for sensitivity and specificity performances are preferred). Thus, \\nwe would select the combination of categorical imputation, numerical \\nimputation, categorical variable encoding, sub-sampling and machine \\nlearning model with the highest average G − Mean across the five \\ncross-validation models. \\n4.1.7. Application of selected ML model to remaining 10 time periods \\nFrom our two-stage framework's perspective, isotonic regression can \\nbe applied to survival probabilities obtained from different models. \\nHowever, for the sake of convenience, in this paper we are applying it \\nto probabilities obtained from the selected modeling approach based on  \\nSection 4.1.6. This means that, we would re-train our machine learning \\napproach for each time-period, while maintaining the data preparation \\nprocedure selected from the comparison of the 4200 runs for the first- \\nyear post-transplant analysis. Based on this step, we would compute a \\nsurvival probability for each of the 11 time-periods in order to construct \\nthe survival probability curve. \\n4.2. Stage II: calibrating the stage I prediction probabilities \\nFor a given transplantation event, let p = p0, p1, p2, …, p10 cor\\xad\\nrespond to the obtained survival probabilities from Stage I, where 0 \\nreflects the first month and 1 → 10 denote the number of years post- \\ntransplant. Isotonic regression can be used to ensure that the elements \\nwithin p are non-increasing based on the formulation and solution \\napproach presented in Section 2. \\nTable 1 \\nConfusion matrix for the heart transplantation prediction application.       \\nPredicted outcomes \\n0 \\n1  \\nActual class \\n0 \\ntrue negative (TN) \\nfalse positive (FP) \\n1 \\nfalse negative (FN) \\ntrue positive (TP) \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n6\\n\\n5. Results \\n5.1. Stage I results: using ML methods to obtain survival probabilities for the \\n11 time periods \\n5.1.1. Data cleaning results \\nPer our data cleaning procedure described in Section 4.1.2, we have \\n8 total scenarios (2 categorical imputations × 2 numerical imputations \\n× 2 categorical encoding methods). Based on these scenarios, the \\nnumber of observations and independent variables varied between \\n26,829 − 45,089 and 90 − 269, respectively. In our R Markdown \\ndocument, we provide the following details: (a) number of preoperative \\nvariables in the dataset, (b) distribution/frequency of percent missing \\nin the pre-operative variables, (c) variables that were not included in \\nthe analysis, (d) percent cells changed by imputing unknown, and (e) \\ntotal number of indicator variables after applying one hot encoding. \\n5.1.2. Predictor variable importance over the 11 time periods \\nTable 2 summarizes the important variables according to the \\nnumber of times selected. Twelve variables were selected for 11 per\\xad\\niods: (a) the donor's age; (b) recipient's body mass index; (c) deceased \\ndonor's cause of death; (d) recipient's serum creatinine level at trans\\xad\\nplant; (e) recipient's primary diagnosis; (f) recipient's functional status \\nat transplant; (g) the recipient's calculated height at listing; (h) ischemic \\ntime; (i) recipient's medical condition at transplant; (j) UNOS region \\nwhere transplanted; (k) serum total bilirubin at transplant time; and (l) \\ncandidate diagnosis at listing. \\n5.1.3. Comparison of the ML algorithms' results \\nTable 3 presents an overview of the prediction performance of the \\nseven machine learning algorithms (with the best data preparation/ \\npreprocessing approach for a given algorithm). We have arranged the \\ntable in descending order based on G − Mean since we prefer to have a \\nmodel that penalizes large discrepancies between sensitivity and speci\\xad\\nficity. Note that, irrespective of our pre-algorithm preparation proce\\xad\\ndure, SVM did not converge. Therefore, we do not report any results for \\nSVM. Based on Table 3, logistic regression (LR) presented the best \\nG − Mean performance with (a) median imputation for missing nu\\xad\\nmeric values, (b) imputing missing categories as “unknown”, (c) one- \\nhot encoding for categorical variables, (d) LASSO for feature selection, \\nand (e) UP sampling. While the results are not statistically significant \\nwhen compared to other approaches, we believe LR is the best approach \\nsince it is the quickest to run and most understandable by health-pro\\xad\\nfessionals. We only discuss the LR results, with (a)-(e) preprocessing, \\nhereafter. \\nWe present the ROC curves for Years 1 − 9 in Fig. 4. Our model \\nyields an AUC value between 0.634 − 0.700 and 0.596 − 0.705 for the \\ntraining and test sets, respectively. To present a complete picture of the \\nmodel's hold-out performance, we provide the values for accuracy, \\nspecificity, sensitivity, AUC, and G − Mean in Table 4. These values \\ncapture the performance of the LR modeling approach, with (a)-(e) \\nsettings, for the hold-out dataset, which was limited to those observa\\xad\\ntions where the recipients' survival statuses are available for all time \\nperiods. \\nIn standard machine learning applications, the results shown in  \\nFig. 4 and Table 4 are sufficient to describe the performance of the \\nmodel. However, in our case, we are interested in presenting an in\\xad\\ndividualized survival probability curve for each recipient. As a first step \\ntoward this goal, we have to evaluate the precision of our predicted \\nprobabilities, which we depict in Fig. 5. To ensure that we have a \\nTable 2 \\nImportant variables selected with the corresponding frequency using LASSO.    \\nSelected \\nVariables  \\n11 \\nAGE DON, BMI CALC, COD CAD DON, CREAT TRR, DIAG, FUNC STAT TRR, INIT HGT CM CALC, ISCHTIME, MED COND TRR, REGION, TBILI, TCR DGN \\n10 \\nCMV DON, DAYS STAT1A, DIAB, DRMIS, ETH MAT, ETHCAT, PRI PAYMENT TCR, PRI PAYMENT TRR, THORACIC DGN, TRANSFUSIONS, VENT SUPPORT AFTER \\nLIST \\n9 \\nCARD SURG, GENDER MAT, HCV SEROSTATUS, LIFE SUP TRR, PROC TY HR \\n8 \\nPRIOR CARD SURG TCR, SUD DEATH \\n7 \\nDOPAMINE, FUNC STAT TCR, HBV CORE, HGT CM TCR, INFECT IV DRUG TRR, PT T4 DON, PULM CATH DON \\n6 \\nAMIS, DAYS STAT1, EDUCATION, HEPARIN, HGT CM CALC, INOTROP VASO CO TRR, LIFE SUP TCR, PRIOR CARD SURG TYPE TCR, TBILI DON \\n5 \\nBUN DON, CHEST XRAY DON, HIV SEROSTATUS, IMPL DEFIBRIL, INOTROP VASO PCW TRR, INOTROP VASO SYS TCR, OTHER INF DON, PT DIURETICS DON, \\nSTERNOTOMY TRR \\n4 \\nBRONCHO LT DON, CONTIN CIG DON, EBV SEROSTATUS, ETHCAT DON, HEMO SYS TCR, HEMO SYS TRR, HLAMIS, HTLV2 OLD DON, LAST INACT REASON, \\nSHARE TY \\n3 \\nCORONARY ANGIO, DAYS STAT1B, GENDER DON, HBV SUR ANTIGEN, INIT AGE, INOTROP VASO MN TRR, PULM INF DON, TATTOOS, WGT KG DON CALC \\n2 \\nAGE, ANCEF, ANTIHYPE DON, BMIS, CLIN INFECT DON, CONTIN OTH DRUG DON CRSMATCH DONE, DAYSWAIT CHRON, HGT CM DON CALC, HIST OTH DRUG \\nDON, INIT BMI CALC, INIT STAT, INOTROPIC, PRIOR CARD SURG TRR, PVR, SGOT DON \\n1 \\nABO DON, AGE MAT, BRONCHO RT DON, END STAT, GENDER, HEMO PA DIA TRR, INOTROP VASO SYS TRR, INOTROPES TRR, PROTEIN URINE, VASODIL DON \\nThe UNOS variables are defined at https://www.srtr.org/requesting-srtr-data/saf-data-dictionary/. Due to page limits we do not redefine them here.  \\nTable 3 \\nThe best average holdout performance and the corresponding 95% confidence interval among all scenarios for each algorithm, for the 1-year prediction time-frame.             \\nAlg. \\nNum. Imp. \\nCat. Imp. \\nEncoding \\nFeature Selection \\nSub-sampling \\nG-Mean \\nAUC \\nSpec. \\nSens. \\nAcc.  \\nLR \\nMedian \\nUnknown \\nOne-Hot \\nLASSO \\nUP \\n0.610 \\n0.655 \\n0.593 \\n0.629 \\n0.624 \\n(0.599, 0.621) \\n(0.645, 0.664) \\n(0.575, 0.611) \\n(0.618, 0.639) \\n(0.614, 0.633) \\nXGB \\nMedian \\nDrop \\nOne-Hot \\nLASSO \\nDOWN \\n0.607 \\n0.649 \\n0.585 \\n0.631 \\n0.625 \\n(0.597, 0.618) \\n(0.636, 0.662) \\n(0.567, 0.602) \\n(0.619, 0.642) \\n(0.615, 0.635) \\nLDA \\nMedian \\nUnknown \\nOne-Hot \\nLASSO \\nUP \\n0.606 \\n0.648 \\n0.593 \\n0.621 \\n0.617 \\n(0.598, 0.615) \\n(0.641, 0.656) \\n(0.575, 0.610) \\n(0.611, 0.631) \\n(0.609, 0.625) \\nRF \\nMedian \\nUnknown \\nOne-Hot \\nLASSO \\nDOWN \\n0.606 \\n0.649 \\n0.598 \\n0.614 \\n0.612 \\n(0.600, 0.613) \\n(0.642, 0.657) \\n(0.590, 0.607) \\n(0.597, 0.631) \\n(0.598, 0.626) \\nANN \\nDrop \\nUnknown \\nOne-Hot \\nLASSO \\nUP \\n0.587 \\n0.616 \\n0.619 \\n0.562 \\n0.569 \\n(0.564, 0.609) \\n(0.575, 0.656) \\n(0.537, 0.700) \\n(0.471, 0.653) \\n(0.497, 0.642) \\nCART \\nMedian \\nUnknown \\nLabel \\nLASSO \\nUP \\n0.577 \\n0.599 \\n0.546 \\n0.611 \\n0.602 \\n(0.560, 0.594) \\n(0.578, 0.619) \\n(0.507, 0.584) \\n(0.593, 0.630) \\n(0.589, 0.616) \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n7\\n\\nFig. 4. ROC plots for our logistic regression's training and hold-out datasets for Years 1 − 9. The light blue and dark blue correspond to the training and hold-out \\nperformances, respectively. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) \\nTable 4 \\nHoldout performance of the UP-LASSO-logistic regression implementation.               \\nMonth 1 \\nYear 1 \\nYear 2 \\nYear 3 \\nYear 4 \\nYear 5 \\nYear 6 \\nYear 7 \\nYear 8 \\nYear 9 \\nYear 10  \\nAUC \\n0.608 \\n0.581 \\n0.571 \\n0.594 \\n0.619 \\n0.631 \\n0.654 \\n0.671 \\n0.698 \\n0.703 \\n0.702 \\nAccuracy \\n0.547 \\n0.537 \\n0.541 \\n0.558 \\n0.586 \\n0.594 \\n0.615 \\n0.629 \\n0.64 \\n0.634 \\n0.627 \\nSensitivity \\n0.542 \\n0.522 \\n0.532 \\n0.548 \\n0.599 \\n0.618 \\n0.659 \\n0.684 \\n0.723 \\n0.743 \\n0.752 \\nSpecificity \\n0.589 \\n0.590 \\n0.565 \\n0.579 \\n0.562 \\n0.559 \\n0.558 \\n0.569 \\n0.561 \\n0.541 \\n0.533 \\nG-Mean \\n0.565 \\n0.555 \\n0.548 \\n0.564 \\n0.58 \\n0.588 \\n0.607 \\n0.624 \\n0.636 \\n0.634 \\n0.633    \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n8\\n\\nsufficiently large sample size for inference, we have only plotted \\nprobabilities when the number of observed cases within that prob\\xad\\nability range was ≥100. The plot shows that our forecast survival \\nprobabilities underestimate the observed averages for the first 5 years \\n(based on their respective holdout data sets). For the later years, the \\npredicted probabilities are relatively close to the observed ones. This \\nfinding is consistent with the metrics reported in Table 4. \\n5.2. Stage II results: calibrating the stage I survival probabilities \\nA closer examination of the individualized survival probabilities \\nover time revealed a non-monotonic behavior of the curves (see sample \\nUNOS patients 1, 3, and 4 in Fig. 6). To alleviate this problem, we used \\nisotonic regression in stage II to ensure the monotonicity for each re\\xad\\ncipient's survival probability curve. Fig. 6 shows the survival prob\\xad\\nabilities before and after the calibration with isotonic regression. \\nFig. 5. Plots of the observed versus forecast survival probabilities for Years 1 − 9 (before isotonic regression). The light blue line correspond to the baseline case of a \\nperfect match between both probabilities. The dark blue line correspond to the results obtained from our machine learning model. To create each plot, we grouped \\nthe recipients in our holdout set based on their forecast probability (in 0.1 increments) and we then calculated the observed average as: (#Survivals for year\\xad\\nfromgroup)/(Total # recipients for yearfromgroup). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of \\nthis article.) \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n9\\n\\nWe evaluate the precision of our calibrated probabilities in Fig. 7. \\nSimilar to Fig. 5, this figure shows the observed averages against the \\nforecast survival probability in Year 1, 2…9 after isotonic regression is \\napplied. The overall pattern in each plot is consistent with the corre\\xad\\nsponding plot in Fig. 5. \\nIn Table 5, we quantify the differences in prediction performance \\nafter the application of isotonic regression, where the bold font shows \\nthat the prediction was greater than or equal to the pre-isotonic re\\xad\\ngression values. From the table, one can see that the AUC and \\nG − Mean values were bolded for the majority of the time periods (i.e. \\nthe results typically improved with the application of isotonic regres\\xad\\nsion to calibrate the survival probabilities). However, and more im\\xad\\nportantly, the application of isotonic regression leads to results that are \\nconsistent with the medical expectation of decreasing survival prob\\xad\\nabilities over time, and individualized to each recipient. \\n6. Discussion, contributions and conclusions \\n6.1. Contextualizing the prediction results based on the transplantation \\nliterature \\nPrior to discussing our prediction results, it is important to compare \\nthe consistency between our models' important variables and those \\nfound in the literature. The majority of the literature, where ML \\nmethods were used for predicting transplantation outcomes, examined \\none [e.g., see 7,14,44–46] or at most three time periods [8,11]. Thus, \\nfrom our analysis, we can gain additional insights into the contribution \\nof a variable to predicting graft rejection, short-term, medium-term, \\nand long-term survival. This is not possible for studies that have focused \\non one time-period. Moreover, the study of Dag et al. [8] did not in\\xad\\nclude 1-month acute transplant rejection period and the study of Yoon \\net al. [11] has only investigated 1-, 3-, and 5- year survival outcomes. \\nTo illustrate whether our important predictors have been reported in \\nthe previous literature, let us consider Table 6 where we provide a \\nsummary of whether the variables selected for all 11 time-periods \\nthrough our LASSO implementation have been selected in [7,8,14]. We \\nhave selected these three references as a representative sample of the \\nliterature since (a) Dag et al. [8] examined three time periods, (b) Dag \\net al. [7] focused on long-term survival outcomes, and (c) Medved et al. \\n[14] used state-of-the-art deep learning methods for short-term survival \\npredictions. \\nFrom Table 6, there are three observations to be made. First, the \\ndonor's age and the recipient's medical condition at transplant were the \\ntwo variables where our model agreed with the three papers. Second, \\nour model was the only methodology, where the body mass index for \\nthe recipient was found important. Third, all other variables were also \\nselected by at least one paper. Note that the differences between the \\nmodels can be attributed to (a) differences in data cleaning procedures, \\nFig. 6. Plots of the forecast probabilities for four sample patients at years 1 − 10 post transplant. The light and dark blue lines show the probabilities before and after \\nthe application of isotonic regression, respectively. Note when the dark blue line is not shown, this equates to a perfect overlap with the light blue line. We do not \\nshow month 1 here, for aesthetic purposes, since it is on a different time scale when compared to the yearly data. (For interpretation of the references to colour in this \\nfigure legend, the reader is referred to the web version of this article.) \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n10\\n\\nFig. 7. Plots of the observed versus forecast survival probabilities for Years 1 − 9 (after isotonic regression is applied). The light blue line correspond to the baseline \\ncase of a perfect match between both probabilities. The dark blue line correspond to the results obtained from our machine learning model. To create each plot, we \\ngrouped the recipients in our holdout set based on their forecast probability (in 0.1 increments) and we then calculated the observed average as: (#Survivals for \\nyearfromgroup)/(Total # recipients for yearfromgroup). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version \\nof this article.) \\nTable 5 \\nDifferences (after isotonic regression – before isotonic regression) in Holdout performance of the UP-LASSO-logistic regression implementation.               \\nMonth 1 \\nYear 1 \\nYear 2 \\nYear 3 \\nYear 4 \\nYear 5 \\nYear 6 \\nYear 7 \\nYear 8 \\nYear 9 \\nYear 10  \\nΔ AUC \\n0.008 \\n0.017 \\n0.029 \\n0.021 \\n0.011 \\n0.007 \\n0.001 \\n0.001 \\n0.000 \\n0.002 \\n0.001 \\nΔ Accuracy \\n0.125 \\n0.083 \\n0.061 \\n0.042 \\n0.017 \\n0.007 \\n−0.005 \\n−0.008 \\n−0.006 \\n0.001 \\n0.008 \\nΔ Sensitivity \\n0.150 \\n0.136 \\n0.110 \\n0.083 \\n0.032 \\n0.007 \\n−0.037 \\n−0.061 \\n−0.102 \\n−0.130 \\n−0.161 \\nΔ Specificity \\n−0.114 \\n−0.105 \\n−0.064 \\n−0.047 \\n−0.009 \\n0.008 \\n0.034 \\n0.050 \\n0.087 \\n0.112 \\n0.134 \\nΔ G-Mean \\n0.009 \\n0.010 \\n0.019 \\n0.016 \\n0.010 \\n0.007 \\n0.000 \\n−0.003 \\n−0.003 \\n−0.001 \\n−0.005    \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n11\\n\\n(b) utilization of different variable selection methodologies, (c) differ\\xad\\nences in sample size of dataset used for training (e.g., other papers \\nmight not have used the entire UNOS dataset and/or did not include \\nresults up to 2016), and/or (d) correlation among one or more in\\xad\\ndependent variables in the UNOS dataset. As we show below, our \\nprediction results are consistent with the literature, and thus, the dif\\xad\\nferences shown in Table 6 may not be significant from a prediction \\nperspective. \\nIf we focus on the prediction results in Stage I, our model is (at least) \\ncomparable to the prediction outcomes reported in the literature that \\nutilize the UNOS dataset. For example, for the 1-year survival out\\xad\\ncomes, our utilized model results in specificity, sensitivity, AUC and \\nG − Mean values of 0.590, 0.522, 0.581, and 0.555, respectively (as \\nshown in Table 4). Our AUC value, for the full, non-censored data in  \\nFig. 4, of 0.614 is similar to the AUC values reported in Dag et al. [8] \\n(0.624), Yoon et al. [11] (0.641 for their best model), Medved et al. \\n[14] (0.61 for their IMPACT model which was developed using the \\nUNOS dataset), Miller et al. [45] (who reported AUC values of 0.613, \\n0.59, and 0.663 for three different transplantation eras), and Villela \\net al. [46] (whose auto-ML model had an AUC of 0.66). However, the \\nresults in Dag et al. [8] can be considered much worse in practice since \\ntheir average sensitivity was only 0.128 for their validation data sets, \\nwhich means that their model was not able to predict deaths for the first \\nyear. None of the other cited works reported sensitivity or G − Mean \\nresults and thus, it is not clear if their models suffer from such a per\\xad\\nformance discrepancy as well. To further demonstrate the performance \\nof our model, let us consider the 10-year end-point of our prediction \\nperiod, our model results in specificity, sensitivity, AUC and G − Mean \\nvalues of 0.533, 0.752, 0.702, and 0.633, respectively (as shown in  \\nTable 4). Our AUC value is better than the 0.631 reported by Yoon et al. \\n[11]. Perhaps more importantly, none of the cited papers presented a \\ndetailed description of their data cleaning procedure, which makes \\nreproducing their work difficult in practice. Thus, while our prediction \\nresults are comparable to the published literature, we believe they can \\nbe more useful in practice since we make our code and analysis publicly \\navailable through our R Markdown document. \\nIn the second stage of our application, we constructed a mono\\xad\\ntonically decreasing survival probability curve for each patient using \\nisotonic regression. Overall, the application of isotonic regression did \\nnot affect the overall shape of the curve as evident from Figs. 5 and 7. \\nHowever, the results are medically compelling due to the non-in\\xad\\ncreasing survival probabilities. Furthermore, the individualized nature \\nof the predictions brings us a step closer to personalized medicine [15]. \\n6.2. Contributions to heart transplantation research and practice \\nIn Section 6.1, we have shown that the stage I predictive perfor\\xad\\nmance of the proposed framework is in line or better than the results \\nreported in the literature. The proposed framework has several merits \\nas it (a) provides personalized results to a given “match case”, (b) is \\nguaranteed to be monotonic, (c) is explainable due to the use of logistic \\nregression, (d) is easy to implement through the provided code, and (e) \\nis flexible since it can easily incorporate any data preparation, variable \\nselection and ML modeling procedure in Stage I of the framework. \\nFurthermore, as stated in Section 2, by examining the UNOS data we \\nare able to  \\n• develop a flexible framework, which can result in accurate and \\nprecise heart transplantation outcome predictions;  \\n• provide UNOS with a benchmark to compare the expected outcomes \\nfrom a match, which can inform the policies and algorithms gov\\xad\\nerning phase II of the transplantation process; and  \\n• inform the protocols prescribed by medical professionals in phases \\nIV and V of the transplantation process based on the estimated \\nsurvival risk. \\nThe first aim is achieved based on the obtained results from the \\napplication of our framework. To accomplish the second and third aims, \\nwe have created a web application (app) titled “Heart Transplantation \\nOutcome Predictor (H-TOP)” that allows UNOS and transplant teams to \\nutilize the proposed framework on prospective “match cases”. Our \\nTable 6 \\nA comparison of whether variables selected for all of our 11 time periods have \\nbeen identified as important in previous literature.       \\nImportant \\nPredictors \\nDefinition of variable per UNOS's data \\nsheet [47] \\n[8] \\n[7] \\n[14]  \\nAGE DON \\nDonor's age in years \\n✓ \\n✓ \\n✓ \\nBMI CALC \\nCalculated body mass index for recipient \\n– \\n– \\n– \\nCOD CAD DON \\nDeceased donor's cause of death \\n– \\n– \\n✓ \\nCREAT TRR \\nRecipient's serum creatinine at transplant \\n– \\n– \\n✓ \\nDIAG \\nRecipient's primary diagnosis \\n✓ \\n✓ \\n– \\nFUNC STAT TRR \\nRecipient's functional status at transplant \\n✓ \\n✓ \\n– \\nINIT HGT CM CALC \\nCalculated candidate height in cm at \\nlisting \\n– \\n– \\n✓ \\nISCHTIME \\nIschemic time in hours \\n– \\n– \\n✓ \\nMED COND TRR \\nRecipient's medical condition pre- \\ntransplant at transplant time \\n✓ \\n✓ \\n✓ \\nREGION \\nUNOS region where transplanted/listed \\n✓ \\n✓ \\n– \\nTBILI \\nMost recent serum total bilirubin at \\ntransplant \\n– \\n– \\n✓ \\nTCR DGN \\nCandidate diagnosis at listing \\n✓ \\n– \\n– \\nFig. 8. The workflow of the H-TOP app.  \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n12\\n\\napproach can be used on prospective cases since we (i) do not include \\nany variables that UNOS no longer collects, (ii) do not use transplant \\nyear as a predictor, and (iii) present a comprehensive approach that can \\nbe used for data preparation, which is illustrated by our detailed data \\ncleaning, encoding and imputation procedures. \\nWe have created and deployed the app using the R “shiny” package. \\nThe app can be accessed at http://dataviz.miamioh.edu/Heart- \\nTransplant/monotonic/. To facilitate using the app, it contains an in\\xad\\nstructional video showing how one can use the app to make predictions. \\nThe app allows users to utilize one of two scenarios for inputting data \\n(a) manual entry, where dropdown menus are provided for categorical \\nvariables and text inputs are provided for numeric variables; or (b) a \\nCSV upload, where practitioners should load a CSV file based on a \\nprovided template. Once the data is inputted, the app (a) performs basic \\nchecks on the quality of the imported data, (b) implements the trained \\nlogistic regression model for the 11 time points, (c) calibrates the ob\\xad\\ntained survival probability curves using isotonic regression, and (d) \\nprovides a table and a plot of the calibrated survival probabilities. An \\noverview of the app's workflow is presented in Fig. 8. In our estimation, \\nthe app is user-friendly and it requires no machine learning background \\nfor usage. \\n6.3. Contributions to the data-driven decision-making and support research \\ncommunities \\nWhile we have applied our framework to the problem of predicting \\nheart transplantation outcomes, our proposed two-stage framework \\npresents a generic and flexible methodology that can be applied to any \\nmulti-period prediction application where ML methods are used and \\nmonotonic outcomes are required. In Section 1, we have presented \\nseveral decision-making applications that can benefit from our pro\\xad\\nposed framework. In the context of ML methodologies, our framework \\nrepresents an extension to the use of “hybrid” methodologies. Existing \\n“hybrid” methods typically incorporate two or more approaches for the \\npurposes of improving prediction accuracy [e.g., see 48]. However, our \\nproposed framework introduces the idea of using a hybrid approach to \\nconstrain the predictions from the initial ML modeling stage, which can \\ninfluence the development of other “hybrid” approaches where dif\\xad\\nferent criteria for calibration are to be enforced. \\n6.4. Limitations and future research opportunities \\nThe limitations of our study provides opportunities for future re\\xad\\nsearch. First, the goal of this study was to present a methodological \\nmachine learning based framework that uses isotonic regression to \\ncalibrate and guarantee the monotonicity of outcome probabilities over \\ntime. While we examined a large number of modeling approaches, we \\ndid not attempt to optimize the prediction performance (e.g., through a \\ndetailed investigation of parameter tuning or through examining more \\ncomplex data imputation schemes). We examined these scenarios to \\nshow that even with the “best” modeling approach, non-monotone \\nprobability curves can be obtained. Future research could optimize the \\nfindings of our research to broader decision making domains. Second, \\nwe examined a limited number of few machine learning models and \\ntheir parameterizations in Stage I. Thus, we cannot guarantee that lo\\xad\\ngistic regression will be superior when compared to ensemble or hybrid \\nmodels that were not considered in our analysis. Future research could \\ninvestigate the use of methodologies excluded from this study. The \\nselection of LR could have also differed if other data cleaning proce\\xad\\ndures, time-samples of the UNOS data, and criteria for balancing pre\\xad\\ndictive accuracy and interpretation (which will differ from one appli\\xad\\ncation to another) were used. Third, we did not consider how to \\noptimize the calibration of the survival curve obtained from Stage I. We \\nhave only considered the use of isotonic regression for obtaining \\nmonotonic survival probability curves. Future work can examine the \\nuse of other approaches to achieve monotonicity, while having a \\nsmoother function (e.g., the use of an exponential curve). \\nSpecific to heart transplantation application, there are some addi\\xad\\ntional issues that need to be emphasized. The secondary and retro\\xad\\nspective nature of our analysis from a registry database means that we \\ncannot account for “the quality of the source data, the number of \\nmissing data, and the lack of standardization associated with multi\\xad\\ncenter studies (such as different immunosuppressive regimens and dif\\xad\\nferent matching criteria)” [14,p. 6]. In addition, we have no control \\nover whether the variables included in our model will continue to be \\ncollected in the future. Exclusion of these variables from the UNOS data \\ncollection protocol will require future researcher to retrain our models. \\nBy sharing our detailed code for data cleaning, we explicitly show how \\nwe handled missing data, observations where we identified data quality \\nissues, and how we removed all variables that had an ending data per \\nthe time of our data acquisition. While our efforts cannot guarantee \\nsuitability for future changes in UNOS's data collection protocol, it al\\xad\\nlows researchers to easily build on our analysis if needed. The results \\ndepicted in Figs. 5 and 7, where our approach consistently under\\xad\\nestimates the survival probabilities (when compared to observed rates) \\nfor the first four years. There are two important considerations that \\nneed to be emphasized (a) it is unclear whether the previous methods in \\nthe literature would have similar performance characteristics since this \\ntype of analysis has not been done before (only metrics for the di\\xad\\nchotomous classification are typically reported); and (b) the utility of \\nour approach in practice is not diminished from this limitation. Speci\\xad\\nfically, from a practitioner's perspective knowing with a high degree of \\ncertainty that the prediction probabilities at 5+ years from transplant \\nis accurate, is sufficient to obtain a lower bound on the survival prob\\xad\\nabilities for the earlier time-periods. That being said, future research \\nshould examine how to reduce the bias in the predictions for ≤4 years. \\n6.5. Concluding remarks \\nIn conclusion, through our two-stage machine learning framework, \\nwe obtained data-driven and individualized survival probability curves \\nfor each transplantation event. This represents a significant contribu\\xad\\ntion when compared to previous literature that: (a) had non-monotonic \\npredictions over time [8,11]; (b) attempted to account for the non- \\nmonotone predictions through features in their machine learning model \\n[12,13], which is an improvement over the methods in (a), however, \\nsuch an approach does not guarantee monotonicity; and (c) achieved \\nmonotonic predictions through the use of a population adjustment \\napproach [e.g., see 14], which can potentially limit the utility of the \\napproach (if personalized predictions are important). By providing our \\ndetailed code, we are encouraging future research on improving the \\npredictions obtained from the UNOS heart transplantation dataset. Our \\ndetailed code can also facilitate the adoption of our framework in other \\napplication domains. Our web-based application (app) can inform the \\ncurrent UNOS organ allocation policies and protocols prescribed to \\nheart transplant recipients. Furthermore, we have encouraged the DSS \\ncommunity to use any/all chunks of code from our app (e.g., it can be \\nused in other applications for educational and/or commercial purposes) \\nby making the app's code in the public domain through a CC0 - “No \\nRights Reserved” license. \\nSupplementay materials \\nCode and analysis: Our code, analysis and results are detailed in an \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n13\\n\\nR Markdown document, which can be accessed at https://ying-ju. \\ngithub.io/heart_transplant.github.io/. Our web app can be found online \\nat http://dataviz.miamioh.edu/Heart-Transplant/monotonic/. Note \\nthat both our R Markdown document and app are in the public domain \\nthrough a CCO - “No Rights Reserved” license. \\nFunding \\nThe modeling approach, analysis and computational resources were \\nsupported in part by the Ohio Supercomputer Center [PMIU0166] and \\nthe National Science Foundation [CMMI-1635927]. Dr. Megahed's work \\nwas supported by the Neil R. Anderson Endowed Assistant \\nProfessorship at the Farmer School of Business at Miami University. The \\nauthors would also like to thank the Department of Statistics at Miami \\nUniversity for hosting our web application. \\nDeclaration of Competing Interest \\nNone. \\nAcknowledgments \\nThe UNOS registry was supported in part by Health Resources and \\nServices Administration contract 234-2005-37011C. The content is the \\nresponsibility of the authors alone and does not necessarily reflect the \\nviews or policies of the Department of Health and Human Services. \\nReferences \\n[1] V.L. Miguéis, A. Freitas, P.J. Garcia, A. Silva, Early segmentation of students ac\\xad\\ncording to their academic performance: a predictive modelling approach, Decis. \\nSupport. Syst. 115 (2018) 36–51. \\n[2] J. Moeyersoms, D. Martens, Including high-cardinality attributes in predictive \\nmodels: a case study in churn prediction in the energy sector, Decis. Support. Syst. \\n72 (2015) 72–81. \\n[3] D. Olaya, J. Vásquez, S. Maldonado, J. Miranda, W. Verbeke, Uplift Modeling for \\npreventing student dropout in higher education, Decis. Support. Syst. (2020), \\nhttps://doi.org/10.1016/j.dss.2020.113320 (In Press). \\n[4] K. Coussement, S. Lessmann, G. Verstraeten, A comparative analysis of data pre\\xad\\nparation algorithms for customer churn prediction: a case study in the tele\\xad\\ncommunication industry, Decis. Support. Syst. 95 (2017) 27–36. \\n[5] Z.-H. Hu, Z.-H. Sheng, A decision support system for public logistics information \\nservice management and optimization, Decis. Support. Syst. 59 (2014) 219–229. \\n[6] A. Oztekin, Z.J. Kong, D. Delen, Development of a structural equation modeling- \\nbased decision tree methodology for the analysis of lung transplantations, Decis. \\nSupport. Syst. 51 (1) (2011) 155–166. \\n[7] A. Dag, K. Topuz, A. Oztekin, S. Bulur, F.M. Megahed, A probabilistic data-driven \\nframework for scoring the preoperative recipient-donor heart transplant survival, \\nDecis. Support. Syst. 86 (2016) 1–12. \\n[8] A. Dag, A. Oztekin, A. Yucel, S. Bulur, F.M. Megahed, Predicting heart transplan\\xad\\ntation outcomes through data analytics, Decis. Support. Syst. 94 (2017) 42–52. \\n[9] K. Topuz, F.D. Zengul, A. Dag, A. Almehmi, M.B. Yildirim, Predicting graft survival \\namong kidney transplant recipients: a Bayesian decision support model, Decis. \\nSupport. Syst. 106 (2018) 97–109. \\n[10] B. Efron, Logistic regression, survival analysis, and the Kaplan-Meier curve, J. Am. \\nStat. Assoc. 83 (402) (1988) 414–425. \\n[11] J. Yoon, W.R. Zame, A. Banerjee, M. Cadeiras, A.M. Alaa, M. van der Schaar, \\nPersonalized survival predictions via trees of predictors: an application to cardiac \\ntransplantation, PLoS One 13 (3) (2018) e0194985, , https://doi.org/10.1371/ \\njournal.pone.0194985. \\n[12] L. Ohno-Machado, M.A. Musen, Sequential versus standard neural networks for \\npattern recognition: an example using the domain of coronary heart disease, \\nComput. Biol. Med. 27 (4) (1997) 267–281. \\n[13] L. Ohno-Machado, M.A. Musen, Modular neural networks for medical prognosis: \\nquantifying the benefits of combining neural networks for survival prediction, \\nConnect. Sci. 9 (1) (1997) 71–86. \\n[14] D. Medved, M. Ohlsson, P. Höglund, B. Andersson, P. Nugues, J. Nilsson, Improving \\nprediction of heart transplantation outcome using deep learning techniques, Sci. \\nRep. 8 (1) (2018) 1–9. \\n[15] L.H. Goetz, N.J. Schork, Personalized medicine: motivation, challenges, and pro\\xad\\ngress, Fertil. Steril. 109 (6) (2018) 952–963. \\n[16] M. Pavlakis, D.W. Hanto, Clinical pathways in transplantation: a review and \\nexamples from Beth Israel Deaconess Medical Center, Clin. Transpl. 26 (3) (2012) \\n382–386. \\n[17] R.E. Barlow, H.D. Brunk, The isotonic regression problem and its dual, J. Am. Stat. \\nAssoc. 67 (337) (1972) 140–147. \\n[18] G. Shmueli, O.R. Koppius, Predictive analytics in information systems research, MIS \\nQ. 35 (3) (2011) 553–572. \\n[19] R. Agarwal, V. Dhar, Editorial—big data, data science, and analytics: the oppor\\xad\\ntunity and challenge for IS research, Inf. Syst. Res. 25 (3) (2014) 443–448. \\n[20] A. Gelman, E. Loken, The Garden of Forking Paths: Why Multiple Comparisons Can \\nBe a Problem, Even when there Is no “Fishing Expedition” or “P-Hacking” and the \\nResearch Hypothesis Was Posited Ahead of Time, Department of Statistics, \\nColumbia University, 2013, https://stat.columbia.edu/gelman/research/ \\nunpublished/p_hacking.pdf (Online, last accessed May 26, 2020). \\n[21] S. Nestorov, B. Jukić, N. Jukić, A. Sharma, S. Rossi, Generating insights through \\ndata preparation, visualization, and analysis: framework for combining clustering \\nand data visualization techniques for low-cardinality sequential data, Decis. \\nSupport. Syst. 125 (2019) 113119, , https://doi.org/10.1016/j.dss.2019.113119. \\n[22] J. Kazmaier, J.H. van Vuuren, A generic framework for sentiment analysis: lever\\xad\\naging opinion-bearing data to inform decision making, Decis. Support. Syst. (2020), \\nhttps://doi.org/10.1016/j.dss.2020.113304 In Press. \\n[23] G. James, D. Witten, T. Hastie, R. Tibshirani, An Introduction to Statistical \\nLearning, Springer, 2013 ISBN 978-1-4614-7137-0. \\n[24] J.C. Platt, Probabilistic outputs for support vector machines and comparisons to \\nregularized likelihood methods, in: A.J. Smola, P.J. Bartlett, B. SchÃlkopf, \\nD. Schuurmans (Eds.), Advances in Large Margin Classifiers, Chap. 5, MIT Press, \\n2000, pp. 61–74. \\n[25] M.J. Best, N. Chakravarti, Active set algorithms for isotonic regression; a unifying \\nframework, Math. Program. 47 (1990) 425–439. \\n[26] G. Savarese, L.H. Lund, Global public health burden of heart failure, Cardiac Fail. \\nRev. 3 (1) (2017) 7–11. \\n[27] E.J. Benjamin, P. Muntner, A. Alonso, M.S. Bittencourt, C.W. Callaway, et al., Heart \\ndisease and stroke statistics-2019 update a report from the American Heart \\nAssociation, Circulation 139 (10) (2019) e56–e528. \\n[28] National Heart, Lung, and Blood Institute (NHLBI), Heart Failure (Online, last ac\\xad\\ncessed May 31, 2020),, 2018. https://www.nhlbi.nih.gov/health-topics/heart- \\nfailure. \\n[29] C. Allemani, T. Matsuda, V. Di Carlo, R. Harewood, M. Matz, et al., Global sur\\xad\\nveillance of trends in cancer survival 2000–14 (CONCORD-3): analysis of individual \\nrecords for 37 513 025 patients diagnosed with one of 18 cancers from 322 po\\xad\\npulation-based registries in 71 countries, Lancet 391 (10125) (2018) 1023–1075. \\n[30] United Network for Organ Sharing, Transplant Trends – UNOS (Online, last ac\\xad\\ncessed May 31, 2020),, 2020. https://unos.org/data/transplant-trends/. \\n[31] United Network for Organ Sharing, National Data - UNOS (Online, last accessed \\nMay 31, 2020),, 2020. https://optn.transplant.hrsa.gov/data/view-data-reports/ \\nnational-data/. \\n[32] United Network for Organ Sharing, How We Match Organs, UNOS (Online, last \\naccessed May 31, 2020),, 2020. https://unos.org/transplant/how-we-match- \\norgans/. \\n[33] A. Alba, E. Bain, N. Ng, M. Stein, K. Brien, Complications after heart transplanta\\xad\\ntion: hope for the best, but prepare for the worst, Int. J. Transplant. Res. Med. 2 (2) \\n(2016) 022. \\n[34] The OPTN/UNOS Ad Hoc Geography Committee, OPTN  ׀UNOS Proposed Organ \\nDistribution Frameworks (Online, last accessed May 31, 2020),, 2018. https://unos. \\norg/wp-content/uploads/unos/Proposed-Distribution-Frameworks-v5.pdf. \\n[35] United Network for Organ Sharing, Questions & Answers About Heart Allocation for \\nAdult Transplant Candidates, UNOS Transplant Living (Online, last accessed May \\n31, 2020),, 2018. https://transplantliving.org/organ-facts/heart/heart-faq/. \\n[36] P.J. Thuluvath, H.Y. Yoo, R.E. Thompson, A model to predict survival at one month, \\none year, and five years after liver transplantation based on pretransplant clinical \\ncharacteristics, Liver Transpl. 9 (5) (2003) 527–532. \\n[37] I. Guyon, A. Elisseeff, An introduction to variable and feature selection, J. Mach. \\nLearn. Res. 3 (2003) 1157–1182. \\n[38] M.J. Wilhelm, Long-term outcome following heart transplantation: current per\\xad\\nspective, J. Thoracic Dis. 7 (3) (2015) 549–551. \\n[39] H. He, E.A. Garcia, Learning from imbalanced data, IEEE Trans. Knowl. Data Eng. \\n21 (9) (2009) 1263–1284. \\n[40] N.V. Chawla, K.W. Bowyer, L.O. Hall, W.P. Kegelmeyer, SMOTE: synthetic minority \\nover-sampling technique, J. Artif. Intell. Res. 16 (2002) 321–357. \\n[41] G. Menardi, N. Torelli, Training and assessing classification rules with imbalanced \\ndata, Data Min. Knowl. Disc. 28 (1) (2014) 92–122. \\n[42] J. Lasserre, S. Arnold, M. Vingron, P. Reinke, C. Hinrichs, Predicting the outcome of \\nrenal transplantation, J. Am. Med. Inform. Assoc. 19 (2) (2012) 255–262. \\n[43] A. Decruyenaere, P. Decruyenaere, P. Peeters, F. Vermassen, T. Dhaene, et al., \\nPrediction of delayed graft function after kidney transplantation: comparison be\\xad\\ntween logistic regression and machine learning methods, BMC Med. Inform. \\nDecision Making 15 (1) (2015) (Article No. 83). \\n[44] D. Delen, A. Oztekin, Z.J. Kong, A machine learning-based approach to prognostic \\nanalysis of thoracic transplantations, Artif. Intell. Med. 49 (1) (2010) 33–42. \\n[45] P.E. Miller, S. Pawar, B. Vaccaro, M. McCullough, P. Rao, et al., Predictive abilities \\nof machine learning techniques may be limited by dataset characteristics: insights \\nfrom the UNOS database, J. Card. Fail. 25 (6) (2019) 479–483. \\n[46] M. Villela, C. Bravo, M. Shah, S. Patel, U. Jorde, et al., Prediction of outcomes after \\nheart transplantation using machine learning techniques, J. Heart Lung Transplant. \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n14\\n\\n39 (4) (2020) S295–S296. \\n[47] Scientific Registry of Transplant Recipients, SAF Data Dictionary (Online, last ac\\xad\\ncessed May 31, 2020),, 2020. https://www.srtr.org/requesting-srtr-data/saf-data- \\ndictionary/. \\n[48] B. Weng, W. Martinez, Y.-T. Tsai, C. Li, L. Lu, J.R. Barth, F.M. Megahed, \\nMacroeconomic indicators alone can predict the monthly closing price of major US \\nindices: insights from artificial intelligence, time-series analysis and hybrid models, \\nAppl. Soft Comput. 71 (2018) 685–697.  \\nDr. Hamidreza Ahady Dolatsara is an Assistant Professor in the School of Management \\nat Clark University. He received his Ph.D. and MS degrees in Industrial & Systems \\nEngineering from Auburn University. In addition, he received an MS in Information \\nSystems Management from Auburn University and an MS in Transportation Engineering \\nfrom Western Michigan University. His research interests are in healthcare, transporta\\xad\\ntion, and finance.  \\nDr. Ying-Ju (Tessa) Chen is an Assistant Professor in the Department of Mathematics at \\nthe University of Dayton. Her expertise is in applied machine learning, high performance \\ncomputing, statistical modeling, and survival analysis. Her work has been funded by \\nseveral foundations and government agencies.  \\nChristy Evans graduated from Auburn University with a bachelor's degree in biomedical \\nsciences and then received her master's degree in Industrial and Systems Engineering with \\na focus in occupational safety and ergonomics. During graduate school, Christy worked as \\na graduate research assistant for projects related to burnout in the healthcare field. She is \\nnow an incoming medical student.  \\nDr. Ashish Gupta is an Associate Professor of Analytics in Raymond J. Harbert College of \\nBusiness and Harbert Advisory Council Faculty Fellow at Auburn University. His research \\ninterests are in the areas of artificial intelligence, machine learning, natural language \\nprocessing, healthcare informatics, IoT, sports analytics, organizational and individual \\nperformance. His recent research has appeared in journals such as DSS, EJIS, DSJ, EJOR, \\nJAMIA, etc. He has also published 5 edited research books. Dr. Gupta's research has been \\nsupported by various grant agencies such as THEC, DHS, NSF, DOD, etc.  \\nDr. Fadel M. Megahed is the Neil R. Anderson Endowed Assistant Professor in the \\nFarmer School of Business at Miami University. His current research focuses on creating \\nnew tools to store, organize, analyze, model, and visualize the large heterogeneous data \\nsets associated with modern manufacturing, healthcare and service environments.  \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n15\\n\"}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/Decision Support Systems.pdf', 'text': \"Contents lists available at ScienceDirect \\nDecision Support Systems \\njournal homepage: www.elsevier.com/locate/dss \\nA two-stage machine learning framework to predict heart transplantation \\nsurvival probabilities over time with a monotonic probability constraint \\nHamidreza Ahady Dolatsaraa, Ying-Ju Chenb, Christy Evansc, Ashish Guptad, Fadel M. Megahede,⁎ \\na School of Management, Clark University, Worcester, MA 01610, USA \\nb Department of Mathematics, University of Dayton, Dayton, OH 45469, USA \\nc Department of Biological Sciences, Auburn University, Auburn, AL 36849, USA \\nd Harbert College of Business, Auburn University, Auburn, AL 36849, USA \\ne Farmer School of Business, Miami University, Oxford, OH 45056, USA  \\nA R T I C L E  I N F O   \\nKeywords: \\nData mining \\nHeart transplant \\nIsotonic regression \\nMedical informatics \\nMulti-period forecasting \\nUnited network for organ sharing \\nA B S T R A C T   \\nThe overarching goal of this paper is to develop a modeling framework that can be used to obtain personalized, \\ndata-driven and monotonically constrained probability curves. This research is motivated by the important \\nproblem of improving the predictions for organ transplantation outcomes, which can inform updates made to \\norgan allocation protocols, post-transplantation care pathways, and clinical resource utilization. In pursuit of our \\noverarching goal and motivating problem, we propose a novel two-stage machine learning-based framework for \\nobtaining monotonic probabilities over time. The first stage uses the standard approach of using independent \\nmachine learning models to predict transplantation outcomes for each time-period of interest. In the second \\nstage, we calibrate the survival probabilities over time using isotonic regression. To show the utility of our \\nframework, we applied it on a national registry of U.S. heart transplants from 1987 to 2016. The first stage \\nproduces an area under the receiver operating curve (AUC) between 0.60 and 0.71 for years 1–10. While the 1- \\nyear prediction AUC result is comparable to the reported results in the literature, our 10-year AUC of 0.70 is \\nhigher than the current state-of-the-art results. More importantly, we show that the application of isotonic \\nregression to calibrate the survival probabilities for each patient over the 10-year period guarantees mono\\xad\\ntonicity, while capitalizing on the data-driven and individualized nature of machine learning models. To pro\\xad\\nmote future research, our code and analysis are publicly available on GitHub. Furthermore, we created a web \\napp titled “H-TOP: Heart Transplantation Outcome Predictor” to encourage practical applications.   \\n1. Introduction \\nThere exists numerous applications where a decision-maker is in\\xad\\nterested in evaluating the probabilities of observing a terminal state of a \\nprocess or a system at multiple time periods. Recent studies have ex\\xad\\namined applications such as (a) the time-taken to complete a higher \\neducation degree [1], where the probability of completing the degree \\nby the end of the nth year will be less than or equal to the completion \\nprobability by the (n + 1)th year; (b) churn prediction in the energy [2], \\nhigher education [3], and telecommunications [4] sectors, where the \\nprobability of client's churn is montonically increasing with time; (c) \\nsupply-chain servicing probabilities are monotonically increased with \\nincreases in delivery window sizes [5]; and (d) surgical applications, \\ne.g., lung [6], heart [7,8], and kidney [9] transplants where patients' \\nsurvival probabilities monotonically decrease over time. The utility of a \\ndecision support system in such applications hinges on accurate multi- \\nperiod predictions, where the predicted outcome probabilities are \\nmonotonic over time. \\nThere are two common approaches for achieving monotonic sur\\xad\\nvival probabilities. The first approach involves the use of population- \\nbased survival analysis techniques using Kaplan-Meier estimates [10], \\nwhich are not well suited for applications involving a large number of \\npotential predictors and/or when an individualized prediction is im\\xad\\nportant (e.g., organ-allocation decisions). On the other hand, machine \\nlearning methods (ML) are used in the second approach to estimate the \\nsurvival, or any outcome of interest, probability for each time-period \\nbeing investigated. The utility of existing ML applications for multi- \\nperiod prediction is limited since current approaches (a) develop in\\xad\\ndependent ML models for each time-period [8,11], which do not \\nguarantee the expected monotonic probability outcomes over time; (b) \\nhttps://doi.org/10.1016/j.dss.2020.113363 \\nReceived 1 February 2020; Received in revised form 7 June 2020; Accepted 12 July 2020    \\n⁎ Corresponding author. \\nE-mail addresses: hamid@clarku.edu (H. Ahady Dolatsara), chen4@udayton.edu (Y.-J. Chen), cje0010@auburn.edu (C. Evans), azg0074@auburn.edu (A. Gupta), \\nfmegahed@miamioh.edu (F.M. Megahed). \\nDecision Support Systems 137 (2020) 113363\\nAvailable online 31 July 2020\\n0167-9236/ © 2020 Elsevier B.V. All rights reserved.\\nT\\n\\nutilize a sequential prediction approach where the “survival” prob\\xad\\nabilities from time periods 1, 2, 3, …, t are inputs to predict the survival \\nat time t + 1 [12,13], however, such approaches also do not guarantee \\nmonotonicity as the utilized ML models do not include a hard constraint \\non the predicted probabilities due to their assumption-free nature; or \\n(c) apply the ML approach to predict the outcome probability for one \\ntime-period and use the population's survival outcomes to calibrate \\nother periods' probabilities [14]. This particular drawback limits the \\nchanges in the outcome probabilities to those captured by the popula\\xad\\ntion averages, and hence, may not be effective when the case being \\ninvestigated has predictors that deviate significantly from the average \\ncase. This is not uncommon in healthcare applications, and is an im\\xad\\nportant driving factor for the recent developments in personalized \\nmedicine [15]. \\nThe overarching goal of this paper is to develop and comprehen\\xad\\nsively describe a modeling framework that can be used to obtain per\\xad\\nsonalized, data-driven and monotonically constrained probability \\ncurves. Our research is motivated by the important and open-research \\nproblem of predicting organ transplantation outcomes based on solely \\npre-operative data in order to provide decision making assistance for \\norgan-allocation protocols [6,9], post-transplantation clinical and care \\npathways [16], and clinical resource utilization [16]. In pursuit of these \\nobjectives and with the limitations in existing transplantation literature \\nin mind [8,11–14], we propose a novel two-stage machine learning- \\nbased framework for obtaining monotonic “survival” probabilities over \\ntime. The first stage utilizes the standard approach of using in\\xad\\ndependent machine learning models to predict survival outcomes for \\neach time-period of interest [e.g., see [8,11]]. The primary objective in \\nthis stage is to select the most predictive/efficient machine learning \\nmodel based on a predefined performance measure (e.g., area under the \\nreceiving operating characteristics curve, AUC). However, as men\\xad\\ntioned earlier, this approach suffers from the drawback that the ob\\xad\\ntained survival probabilities from stage-one are not guaranteed to be \\nmonotonically decreasing. Therefore, in the second stage, we calibrate \\nthe survival probabilities over time using isotonic regression [17], \\nwhich constrains the survival probabilities (p) such that pt+1 ≤ pt,  ∀ t. \\nThus, isotonic regression ensures that the survival probabilities are \\nmonotonically decreasing and that the results are personalized based on \\nthe donor and recipient's characteristics. \\nThe remainder of this paper is organized as follows. Section 2 pro\\xad\\nvides an overview of our proposed two-stage framework for multi- \\nperiod monotonic probabilistic outcome predictions. Section 3 de\\xad\\nscribes the significance of the heart transplantation problem, the need \\nfor risk stratification, and the decisions made throughout the trans\\xad\\nplantation process. Section 4 explains how the proposed two-stage \\nframework can be applied to multi-period predictions of heart trans\\xad\\nplantation survival probabilities. Section 5 describes the results of our \\napplication. Finally, Section 6 describes the implications of those results \\nto advancing heart transplantation research/practice as well as pro\\xad\\nviding opportunities for future research in applied machine learning. In \\nthe supplementary materials section, we provide a link to a web-based \\ndecision support system, which can facilitate the use of the proposed \\nframework in informing organ-allocation protocols and post-trans\\xad\\nplantation clinical pathways. Furthermore, we share a link of our R \\nMarkdown document that includes our code and analyses to facilitate \\nthe adoption and integration of our proposed methodology in future \\nresearch. \\n2. The two-stage framework for obtaining monotonic prediction \\nprobabilities over time \\nThe data revolution has popularized the use of data-driven pre\\xad\\ndictive models that significantly departed from the traditional empirical \\nmodeling approach in Information Systems [18,19]. Nevertheless, as \\nnoted by Shmueli and Koppius [18, p. 554], data-driven predictive \\nmodeling is a “core scientific activity” that is “useful for generating new \\ntheory, developing new measures, comparing competing theories, im\\xad\\nproving existing theories, assessing the relevance of theories, and as\\xad\\nsessing the predictability of empirical phenomena.” To reap the benefits \\nof predictive models, one must strive to (a) have accurate and precise \\nresults, (b) be able to present the right information with the required \\nlevel of granularity [19], and (c) not capitalize on chance [20]. \\nThis paper proposes a generic framework for multi-period mono\\xad\\ntonic probabilistic outcome predictions. The goals of our framework are \\nto enable: (a) accurate and precise results by enabling the use of any \\nstatistical, machine or deep learning method for predicting the outcome \\nprobability for a given time period; (b) presenting the right information \\nwith the required granularity through the emphasis on data-driven and \\npersonalized/individualized predictions; and (c) not capitalizing on \\nchance (where we avoid over-fitting and ensure that the outcomes are \\nmonotonic through the incorporation of a hard mathematical constraint \\non the magnitude of the probability outcomes for each period). To \\nensure that the framework is “generic”, our framework does not require \\nthe use of a specific machine learning algorithm to obtain the prob\\xad\\nability estimates since their utility may differ by problem (e.g., varia\\xad\\ntions in number of variables, observations, computational processing \\nrequirements and/or requirements for prediction accuracy and inter\\xad\\npretation). Furthermore, we have considered an approach for guaran\\xad\\nteeing monotonic outcomes for any t  >  1 time periods, which makes \\nour approach effective even when only two periods are considered. \\nOur framework addresses two major shortcomings of existing fra\\xad\\nmeworks in the literature; the non-existence of a framework that is \\ncomprehensive enough to be readily applied by analysts over multiple \\ntime periods, yet generic enough to be applicable across various do\\xad\\nmains. These two characteristics of our framework allow for the utili\\xad\\nzation of machine learning results in informing decision making in re\\xad\\nsponse \\nto \\nmulti-period \\ninformation. \\nThe \\nflexibility \\nand \\ncomprehensiveness of a decision-making framework are key requisites \\nfor its successful integration in practice [21,22]. \\nIn the first stage of our proposed framework, independent statistical, \\nmachine and/or deep learning models are applied to predict the prob\\xad\\nability of the binary outcome for each time-period of interest. As such, \\nthis stage will involve the following standard data analytic steps: (a) \\ndata collection, (b) data preparation, (c) choosing a set of candidate \\nmodels, (d) training the models, (e) evaluating the models based on one \\nor more performance metrics of interest, (f) parameter tuning, and (g) \\nmodel selection based on the performance of the tuned-models on one \\nor more holdout/test dataset. For the sake of conciseness, we do not \\ncover these steps in greater detail here since they are well-documented \\nin the literature and standard textbooks [e.g. see 23]. These steps are \\nrepeated for each time-period of interest; therefore, the output from this \\nstage is a computed probability of an event (e.g., a client's churn, an \\norgan's failure, or a patient's survival post surgery) for the different \\ntime-periods. This output can be represented by p = p1, p2, …, pt, where \\npt is the computed outcome probability at period t as obtained from the \\nselected model. \\nIn Stage 2, we propose the use of isotonic regression to calibrate and \\nensure that the elements in p are monotonic. For example, if we were to \\nconsider were the required outcome should be monotonically de\\xad\\ncreasing, this can be achieved by solving the following optimization \\nproblem \\n=\\n…\\n=\\n= w y\\np\\ny\\ny\\ny\\ny\\ny\\nmin\\n(\\n)\\nsubject to\\n,\\ni\\nt\\ni\\ni\\ni\\nmax\\nt\\nmin\\n1\\n2\\n1\\n2\\n(1) \\nwhere wi is a strictly positive weight and yi denotes the calibrated \\nprobability at time i (i.e., the ys are the decision variables obtained from \\nthe optimization model). We use wi = 1,  ∀ i since this corresponds to \\nthe squared Euclidean distance (i.e., the objective function would have \\na physical interpretation). The reader should note that in our frame\\xad\\nwork we preferred isotonic regression to other possible calibration \\napproaches (e.g., the sigmoid smother used in support vector machines \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n2\\n\\n[24]) since it (a) works well for any t  >  1 period; (b) is a non-para\\xad\\nmetric approach, which requires no further tuning; (c) is computa\\xad\\ntionally efficient since the optimization problem can be solved in O n\\n( )\\ntime using the pool adjacent violators algorithm (PAVA) [25]; and (d) \\ncan be easily adapted for non-decreasing probabilities (by changing the \\nconstraint to ymin = y1 ≤ y2 ≤ … ≤ yt = ymax). \\nFig. 1 provides an overview of how isotonic regression can be used \\nto calibrate the outcome probabilities obtained from the Stage 1. In the \\nfigure, we chose to simulate the case where the researcher is interested \\nin computing an organ's failure probabilities over a 20 year time-period \\npost-transplant to illustrate the utility of our approach to a non-de\\xad\\ncreasing prediction problem. Hereafter, we will only consider the op\\xad\\nposite case where one would like to predict the survival probability post \\ntransplant (i.e., the probabilities would be monotonically decreasing \\nover time). \\n3. Problem description \\nHeart failure is a serious medical condition, which can be char\\xad\\nacterized by the heart not being able to pump enough blood and oxygen \\nto support other organs [26]. It is “a global pandemic affecting at least \\n26 million people worldwide and is increasing in prevelance” [26]. For \\nexample, in the U.S., the number of heart failure patients was 5.7 \\nmillion in 2009–2012, which has increased to 6.2 million in 2013–2016 \\nand is expected to increase to 8 million by 2030 [27]. Despite the ad\\xad\\nvancements in treatment protocols, the outlook for heart failure pa\\xad\\ntients remains poor as (a) “heart failure has no cure” [28] and (b) the 5- \\nyear mortality is ≈50% [27], which is worse than the five-year rate for \\npatients suffering from various cancers such as bowl, breast, colon and \\nprostate cancers [29]. \\nHeart transplantation is the most effective treatment for patients \\nwith end-stage heart failure. For adult recipients, the median survival \\ntime after a heart transplant is 9.5 years, which is significantly better \\nthan the median survival of 2.3 years for wait-listed patients who do not \\nreceive a transplant [27]. In the U.S., there were 3552 heart transplants \\nperformed in 2019 [30], with an additional 3527 wait-listed candidates \\nas of May 28, 2020 [31]. Fig. 2 depicts the five phases of the U.S. heart \\ntransplantation process, which are based on the descriptions in the \\nUnited Network for Organ Sharing (UNOS) matching procedure [32]. \\nIn phase I, an eligible end-stage heart failure patient is accepted to a \\ntransplant hospital as a transplant candidate. At this point, the candi\\xad\\ndate is wait-listed and their medical data (e.g., height, weight, blood \\ntype, medical urgency) as well as the hospital's location are added to \\nthe UNOS database [32]. Phase II initiates when (a) a donor heart is \\navailable in a localized geographic area such that preservation and \\ntransport time are feasible, and (b) the candidate is deemed compatible \\nwith donor heart based on blood type, height, weight, and other med\\xad\\nical factors [32]. At this point, the candidate is referred to as a potential \\ntransplant recipient (PTR). The UNOS algorithmic allocation protocol \\n[32] would perform a “match run” of all PTRs and rank them based on \\n(i) medical urgency considerations, (ii) geographic proximity to the \\ndonor's hospital, and (iii) pediatric status. The organ is offered to \\ntransplant centers in the order of the “match run”; it is not uncommon \\nthat a PTR would go back to the first phase of being wait-listed (e.g., if \\nnot highly ranked by the “match run”) [32]. Consequently, the PTR \\nonly advances to the third phase (transplantation) if they were selected \\nby the “match run” and the transplant offer is accepted by the trans\\xad\\nplant hospital (i.e., after additional medical prognoses). The patient, at \\nthis point, is referred to as the recipient. Phase IV is comprised of the in- \\nhospital post-transplant care, where the recovery and discharge are \\nFig. 1. An animated illustration of how isotonic regression is used to calibrate \\nmulti-period ML probabilities. To view the animation, the reader is referred to \\nthe online version of the article. Alternatively, the reader can also view the \\nanimation on our Markdown document (see the supplementary materials). \\nFig. 2. An overview of the decisions made throughout the transplantation process.  \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n3\\n\\ngoverned by the respective policies, procedures and pathways set by the \\nhospital and/or transplantation team. Phase V captures the out-patient \\npost-transplant process, where the recipient adopts the prescribed life \\nstyle and protocols in order to prolong the lifetime of the graft and \\nreturn to society. The recipient is examined regularly in phase V for \\npossible complications; in the case of a graft failure, the recipient would \\nbe evaluated for re-transplantation and is returned to phase I [33]. \\nThe stated goals of the UNOS “policies and computerized network \\n[are to] match donated organs with transplant candidates in ways that \\nsave as many lives as possible and provide transplant recipients with \\nthe best possible chance of long-term survival” [32]. With these goals in \\nmind, proposed changes to the UNOS policies are periodically ex\\xad\\namined [e.g., see 34], and merited proposals are adopted [35] to con\\xad\\ntinuously improve the transplantation outcomes. By analyzing and \\nmodeling the UNOS data, we aim at improving the utility of the existing \\napproaches for multi-period heart transplantation prediction [8,11–14], \\ndescribed in Section 1. Our two-stage framework allows for the use of \\nany machine learning method to obtain individualized, monotonically \\ndecreasing multi-period survival probabilities for any PTR at the \\n“match run” phase. The implications of our approach to risk stratifi\\xad\\ncation are three-fold: (a) improving predictive accuracy (allowing for \\nthe incorporation of best practices to data cleaning, model selection, \\nand tuning) and a more precise calibration of the survival probabilities \\nover time (accounting for the specific parameters' values of evaluated \\ncases); (b) providing UNOS with a benchmark to compare the expected \\noutcomes from a match, which in the long-run can potentially inform \\nthe policies and algorithms governing phase II of the transplantation \\nprocess; and (c) informing the protocols prescribed by the transplan\\xad\\ntation team in phases IV and V based on the estimated survival risk. \\n4. Application of the two-stage framework to predict \\ntransplantation survival outcomes \\nFig. 3 provides an overview of the application of our two-stage \\nframework \\nfor \\nobtaining \\nmonotonically \\ndecreasing \\nsurvival \\nprobabilities. Our goals for Stage I are to select an appropriate model \\nbased on its predictive performance and examine how the selected \\nvariables importance change over time. Stage I is comprised of the \\nfollowing steps: (a) raw data undergoing detailed data preparation that \\nexplains how missing data is handled and how indicator variables are \\ngenerated; (b) use subsampling techniques to handle the imbalance \\nbetween graft survivals and failures at a given time point; (c) use of a \\nstructured variable selection technique to improve the performance of \\nmachine learning models [7,8]; and (d) use of machine learning (ML) \\nmodels to predict survival probabilities for the various time periods. In \\nour application, we examine 11 prediction periods one-month post- \\ntransplant, and 1–10 years post-transplant. These periods were selected \\nto capture the acute rejection period (typically at one-month post- \\ntransplant [36]), short-term survival (< 3 years post-transplant [8]), \\nmedium-term survival (typically at 3–5 years post-transplant [8]), and \\nlong-term survival (typically at 9+ years, post-transplant [8]). In Stage \\nII, we calibrate the survival probabilities such that they are mono\\xad\\ntonically decreasing over time. Stage II has two steps: (a) combining \\noutputs from all time points derived from Stage I models and forming a \\nprobability matrix, and (b) the application of isotonic regression to \\ncalibrate the probability function. \\n4.1. Stage I: Using ML methods to obtain a survival probability at each time \\npoint \\n4.1.1. Data description \\nOur UNOS dataset covered 103,570 heart transplant events up to \\nand including September 30, 2016. The dataset has been anonymized \\nby UNOS and contains 494 variables, capturing various pre-, intra-, and \\npost-transplant information. Note that variables/records within the \\nUNOS dataset have a large number of missing data. In our case, more \\nthan 30% of the “data-table cells” had missing values for various rea\\xad\\nsons. For example, the start date of data collection for 327 of the 494 \\nvariables (66.19%) had a start date of 1990 or after. Moreover, some of \\nthe variables had missing-at-random values due to possible data entry \\nFig. 3. Adopting the proposed two-stage framework to obtain monotonically decreasing heart transplantation survival probabilities over time.  \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n4\\n\\nand collection problems; a variable is not considered to be missing-at- \\nrandom if, e.g., the frequency of missing data changes significantly by \\ntransplant year. Thus, a prerequisite to analyzing the UNOS dataset is a \\ndetailed data cleaning/preparation procedure that will facilitate re\\xad\\nproducible analyses. \\n4.1.2. Data preparation/cleaning \\nOur procedure for cleaning the UNOS dataset was comprised of six \\nsteps. The first step focused on reducing the dimensionality of the data \\nby removing the following sets of variables: (a) intra- and post-transplant \\nvariables, where we have only kept G-TIME (a continuous variable \\ncapturing the time in days from transplant to graft failure) and G- \\nSTATUS (a binary variable, denoting if the graft has failed or not at the \\nlast follow-up time) as they are used in constructing the dependent \\nsurvival outcome at a given time point; (b) variables with an end data- \\ncollection-date since they cannot be used in future PTR evaluations; (c) \\nvariables with a recent data-collection-date, where we only included \\nvariables that were added before 2000; and (d) any variable that had \\n≥90% missing since imputation techniques would produce biased re\\xad\\nsults. We refer the reader to our R Markdown document, which con\\xad\\ntains our code and analysis and whose link is provided in the \\nSupplementary Materials Section of this paper. \\nThe second step involved determining whether a record should be \\nused for analysis (for a given time point). Any transplantation event was \\ncensored if we did not have information pertaining to a patient's sur\\xad\\nvival at the time point of interest (e.g., a recipient was alive at day 300 \\npost-transplant, would not explain whether he/she would have sur\\xad\\nvived to the end of year 1). Our censoring procedure followed the ap\\xad\\nproach of Dag et al. [8], capitalizing on both G-TIME and G-STATUS. \\nSpecifically, the data was censored if and only if the G-TIME was less \\nthan the number of days required for the time-point analysis and the G- \\nSTATUS indicates that the recipient was still alive. Accordingly, the \\ndichotomous outcome for the ith recipient at time t was coded based on \\n=\\no\\ni\\nt\\ni\\nt\\n1, if the\\nrecipient was alive at time\\n0, if the\\nrecipient was dead at time\\n.\\ni t\\nth\\nth\\n,\\n(2)  \\nThe third step in our recommended data preparation procedure \\ngenerated medically-relevant features that have been shown as im\\xad\\nportant/predictive in the literature. These features were computed by \\ncombining information from two or more independent variables. In our \\nanalysis, we created the following features based on [7,8,14]: (a) pul\\xad\\nmonary vascular resistance (PVR), (b) ischemia time in minutes, (c) \\nmismatch in Extracorporeal membrane oxygenation (ECMO) for re\\xad\\ncipient between registration and transplant, (d) percent change in the \\nrecipient's body mass index (BMI) between registration and transplant, \\n(e) percent change in the recipient's weight between registration and \\ntransplant, (f) percent change in the recipient's height between regis\\xad\\ntration and transplant, (g) the absolute difference in age between donor \\nand recipient, and (h) the absolute difference in BMI between donor \\nand recipient. For additional detail on how each feature was generated, \\nwe refer the reader to our R Markdown document. \\nThe fourth step related to the handling of missing data. In the first \\nstep of our data cleaning process, we removed any categorical variable \\nthat has ≥90% missing values, numerical variable that has ≥30% \\nmissing values, and categorical variables which are almost invariant \\n(> 90% of the values are in one level). Note that we have allowed a \\nlarger threshold for categorical variables since we imputed these values \\nas “unknown” in one of our categorical imputation procedures. We do \\nacknowledge that other thresholds could have been used (and their \\nchoice can possibly affect our prediction performance); however, we \\ndid not explore other values in our analysis as our prediction results \\nwere reasonable and we examined a large number of imputation, en\\xad\\ncoding, sub-sampling and algorithm combinations. We utilized the \\nfollowing imputation strategy (a) correctly defining independent vari\\xad\\nable types in the analytical software, and (b) comparing the predictive \\nperformance of the models with no imputation and the models with \\nimputation (median/mean and mode imputation for numeric and ca\\xad\\ntegorical variables, respectively). This performance comparison would \\nsuggest if data imputation can lead to improvements in the prediction \\nresults and whether a more computationally intensive imputation \\ntechnique was warranted. \\nIn the fifth step, the levels of categorical variables were regrouped. \\nWe attempted to reduce the number of levels, by removing any level, \\nwhich has a limited number of observations, and re-coding the values \\nsuch that they are added to the level corresponding to “other”. The \\npurpose of this step was three-fold: (a) avoiding over-fitting, (b) redu\\xad\\ncing the possibility of errors due to factor levels not observed in the \\ntraining of the model, and (c) reducing the dimensionality/complexity \\nof the developed model (especially for tree-based approaches such as \\nrandom forests). Additionally, many implementations of machine \\nlearning algorithms require the categorical variables to be encoded \\neither using label or one-hot encoding. \\nThe sixth step of data preparation involved splitting the data ran\\xad\\ndomly into a training (80%) and a hold-out (20%) set. We used a 5-fold \\ncross validation approach for selecting an appropriate model from the \\ntraining set since (a) it is more computationally efficient than 10-fold \\ncross validation, and (b) both k = 5 and k = 10 “have been shown \\nempirically to yield test error rate estimates that suffer neither from \\nexcessively high bias nor from very high variance” [23, p. 183]. \\n4.1.3. Variable selection \\nVariable selection approaches attempt to (a) reduce the computa\\xad\\ntional burden by reducing the dimensionality of the feature space, (b) \\nimprove the prediction performance by focusing only on important \\nvariables, and (c) improve the generalizability of the developed pre\\xad\\ndiction models. Variable selection approaches can be categorized into \\nthree main groups: [37] (a) filter methods, where univariate statistical \\napproaches are typically used to select features based on their re\\xad\\nlationship to the response, (b) wrapper methods, where the important \\nfeatures are kept based on their prediction performance, and (c) em\\xad\\nbedded methods, which involve the use of methods such as LASSO and \\nrandom forests for selecting the most predictive features. In our case \\nstudy, we compared the performance of fast feature selection, LASSO, \\nand random forests for feature selection. \\n4.1.4. Sub-sampling approaches for imbalanced data \\nOn average, the expected survival probability for heart transplan\\xad\\ntation recipients decreases by about 3–4% per post-transplant year \\n[38]. Moreover, the aforementioned censoring approach would result \\nin the removal of recipients who are currently surviving, but have not \\nreached the time-point of interest. Consequently, we expect that the \\nshort time intervals (e.g., 1 month, 1 year, etc.) will have a much larger \\nnumber of survivals than deaths, and the longer time-periods (e.g., 9- \\nand 10- years post-transplant) will have a higher ratio of deaths to \\nsurvivals. In the data mining literature, sub-sampling methodologies \\nare typically deployed to improve the prediction performance when the \\nunderlying training dataset is imbalanced [39]. We considered five sub- \\nsampling strategies:  \\n(A) No sub-sampling (None), which can be considered as the baseline for \\ncomparison; \\n(B) Random under-sampling (DOWN), where the majority class is sam\\xad\\npled without replacement such that the number of observations in \\nboth classes (i.e. 0 and 1) are equal;  \\n(C) Random over-sampling (UP), where the minority class is sampled \\nwith replacement such that the number of observations in both \\nclasses (i.e. 0 and 1) are equal; \\n(D) Synthetic minority oversampling technique (SMOTE), where the min\\xad\\nority class is over-sampled by “taking each minority class data \\npoint and introducing synthetic examples along the line segments \\njoining any or all of the k-minority class nearest neighbors” [40,p. \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n5\\n\\n342], and repeating the process until the number of observations \\nper class is approximately equal; and \\n(E) ROSE, where a statistical approach based on the smoothed boot\\xad\\nstrap sub-sampling technique of Menardi and Torelli [41] is used \\nfor handling the class imbalance problem. \\nNote that these approaches were selected since they are widely used \\nin machine learning practice due to their implementation within the \\npopular “caret” package in R. \\n4.1.5. Application of statistical and machine learning algorithms \\nAs mentioned in Section 1, our primary goal is to develop a fra\\xad\\nmework that can calibrate the survival probabilities obtained from \\nmachine learning (ML) algorithms over multiple time points. From this \\npoint of view, our approach (in Stage II) is designed to work with any \\nmachine learning methodology that can be used for two-class problems. \\nIn our experience with the UNOS dataset, the utility of a given ML al\\xad\\ngorithm can differ significantly based on the data preparation metho\\xad\\ndology utilized. Algorithms for predicting dichotomous outcomes can \\nbe categorized into:  \\n(A) Statistical models, which are not assumption free. Logistic regression \\n(LR) is the most frequently utilized approach in the literature [see \\ne.g., 8, 11, 42]. Linear discriminant analysis (LDA) also showed \\ngood predictive performance in the literature [43]. \\n(B) Single (data-driven) classifiers, which include a large group of as\\xad\\nsumption-free machine learning approaches. Commonly used al\\xad\\ngorithms include artificial neural networks (ANNs) [8,11,12,42], \\ndecision trees (CART) [7,8], and support vector machines (SVM) \\n[8]; and \\n(C) Ensemble approaches, where single classifiers' predictions are com\\xad\\nbined using voting schemes. In our analysis, we examined random \\nforests (RF) and eXtreme Gradient Boosting (XGB). \\nWe considered all the aforementioned seven ML algorithms (LR, \\nLDA, ANN, CART, SVM, RF and XGB) in the prediction of the heart \\ntransplantation survival probabilities. \\n4.1.6. Evaluation and model selection \\nBased on our Stage I description, we considered six modeling fac\\xad\\ntors, with the following levels:  \\n(A) 2 categorical imputation strategies (drop and impute missing as \\n“unknown”);  \\n(B) 2 numerical imputation strategies (drop and median imputation);  \\n(C) 2 categorical variable encoding approaches (label and one-hot);  \\n(D) 3 variable selection approaches (fast feature selection, LASSO, and \\nrandom forests);  \\n(E) 5 sub-sampling methods (none, DOWN, UP, SMOTE and ROSE); \\nand  \\n(F) 7 machine learning models (LR, LDA, ANN, CART, SVM, RF and \\nXGB). \\nFurthermore, we followed the recommendation of James et al. [23] \\nin using 5-fold cross validation in model selection. To examine short, \\nmid- and long-term survival, we stated that we would examine 11 \\nprediction time-periods to generate the survival probability curve. \\nConsequently, the enumeration of these combinations would result in \\n2 × 2 × 2 × 3 × 5 × 7 × 5 × 11 = 46,200 runs. These runs \\ncorrespond to running a general factorial experiment to compute the \\nmain factor effects and all high order interactions for the settings as\\xad\\nsociated with each step of our framework. \\nWe reduced the computational burden to 4200 runs by comparing \\nthe predictive performance of those combinations based on the 1-year \\nprediction period. We have chosen 1-year since it (i) provided us with \\nthe largest sample size (with the exception of the 1-month time period \\nwhere the focus is on acute organ rejection prediction), and (ii) it is a \\ncommonly used period in the literature [e.g., see 8, 14, 11]. The se\\xad\\nlected modeling approach will then be trained for all other periods for \\nthe sake of demonstrating the need and utility of our isotonic regression \\ncalibration approach in Stage II. \\nFor model selection and evaluation, we reported five common \\nperformance metrics that are suited for two class classification pro\\xad\\nblems. To present these metrics, let us consider the confusion matrix in  \\nTable 1. Based on this confusion matrix, the five metrics can be defined \\nas follows: \\n=\\n+\\n+\\n+\\n+\\naccuracy\\nTN\\nTP\\nTN\\nFP\\nFN\\nTP ,\\n(3)  \\n=\\n+\\nsensitivity\\nTP\\nTP\\nFN ,\\n(4)  \\n=\\n+\\nspecificity\\nTN\\nTN\\nFP ,\\nand\\n(5)  \\n=\\n×\\nG\\nmean\\nsensitivity\\nspecificity .\\n(6)  \\nThe receiver operating characteristics (ROC) curve can be plotted \\nwith the sensitivity on the y-axis versus 1 − specificity on the x-axis. The \\narea under the curve (AUC) captures how well the model predicts actual \\nsurvival as survivals and actual deaths as deaths. AUC is our fifth me\\xad\\ntric. \\nIn this paper, we used G − Mean as the primary metric for model \\nselection since it (a) is suitable for unbalanced classification problems, \\nand (b) penalizes models where there is a large discrepancy between \\nsensitivity and specificity (i.e., more suitable than AUC when similar \\nvalues for sensitivity and specificity performances are preferred). Thus, \\nwe would select the combination of categorical imputation, numerical \\nimputation, categorical variable encoding, sub-sampling and machine \\nlearning model with the highest average G − Mean across the five \\ncross-validation models. \\n4.1.7. Application of selected ML model to remaining 10 time periods \\nFrom our two-stage framework's perspective, isotonic regression can \\nbe applied to survival probabilities obtained from different models. \\nHowever, for the sake of convenience, in this paper we are applying it \\nto probabilities obtained from the selected modeling approach based on  \\nSection 4.1.6. This means that, we would re-train our machine learning \\napproach for each time-period, while maintaining the data preparation \\nprocedure selected from the comparison of the 4200 runs for the first- \\nyear post-transplant analysis. Based on this step, we would compute a \\nsurvival probability for each of the 11 time-periods in order to construct \\nthe survival probability curve. \\n4.2. Stage II: calibrating the stage I prediction probabilities \\nFor a given transplantation event, let p = p0, p1, p2, …, p10 cor\\xad\\nrespond to the obtained survival probabilities from Stage I, where 0 \\nreflects the first month and 1 → 10 denote the number of years post- \\ntransplant. Isotonic regression can be used to ensure that the elements \\nwithin p are non-increasing based on the formulation and solution \\napproach presented in Section 2. \\nTable 1 \\nConfusion matrix for the heart transplantation prediction application.       \\nPredicted outcomes \\n0 \\n1  \\nActual class \\n0 \\ntrue negative (TN) \\nfalse positive (FP) \\n1 \\nfalse negative (FN) \\ntrue positive (TP) \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n6\\n\\n5. Results \\n5.1. Stage I results: using ML methods to obtain survival probabilities for the \\n11 time periods \\n5.1.1. Data cleaning results \\nPer our data cleaning procedure described in Section 4.1.2, we have \\n8 total scenarios (2 categorical imputations × 2 numerical imputations \\n× 2 categorical encoding methods). Based on these scenarios, the \\nnumber of observations and independent variables varied between \\n26,829 − 45,089 and 90 − 269, respectively. In our R Markdown \\ndocument, we provide the following details: (a) number of preoperative \\nvariables in the dataset, (b) distribution/frequency of percent missing \\nin the pre-operative variables, (c) variables that were not included in \\nthe analysis, (d) percent cells changed by imputing unknown, and (e) \\ntotal number of indicator variables after applying one hot encoding. \\n5.1.2. Predictor variable importance over the 11 time periods \\nTable 2 summarizes the important variables according to the \\nnumber of times selected. Twelve variables were selected for 11 per\\xad\\niods: (a) the donor's age; (b) recipient's body mass index; (c) deceased \\ndonor's cause of death; (d) recipient's serum creatinine level at trans\\xad\\nplant; (e) recipient's primary diagnosis; (f) recipient's functional status \\nat transplant; (g) the recipient's calculated height at listing; (h) ischemic \\ntime; (i) recipient's medical condition at transplant; (j) UNOS region \\nwhere transplanted; (k) serum total bilirubin at transplant time; and (l) \\ncandidate diagnosis at listing. \\n5.1.3. Comparison of the ML algorithms' results \\nTable 3 presents an overview of the prediction performance of the \\nseven machine learning algorithms (with the best data preparation/ \\npreprocessing approach for a given algorithm). We have arranged the \\ntable in descending order based on G − Mean since we prefer to have a \\nmodel that penalizes large discrepancies between sensitivity and speci\\xad\\nficity. Note that, irrespective of our pre-algorithm preparation proce\\xad\\ndure, SVM did not converge. Therefore, we do not report any results for \\nSVM. Based on Table 3, logistic regression (LR) presented the best \\nG − Mean performance with (a) median imputation for missing nu\\xad\\nmeric values, (b) imputing missing categories as “unknown”, (c) one- \\nhot encoding for categorical variables, (d) LASSO for feature selection, \\nand (e) UP sampling. While the results are not statistically significant \\nwhen compared to other approaches, we believe LR is the best approach \\nsince it is the quickest to run and most understandable by health-pro\\xad\\nfessionals. We only discuss the LR results, with (a)-(e) preprocessing, \\nhereafter. \\nWe present the ROC curves for Years 1 − 9 in Fig. 4. Our model \\nyields an AUC value between 0.634 − 0.700 and 0.596 − 0.705 for the \\ntraining and test sets, respectively. To present a complete picture of the \\nmodel's hold-out performance, we provide the values for accuracy, \\nspecificity, sensitivity, AUC, and G − Mean in Table 4. These values \\ncapture the performance of the LR modeling approach, with (a)-(e) \\nsettings, for the hold-out dataset, which was limited to those observa\\xad\\ntions where the recipients' survival statuses are available for all time \\nperiods. \\nIn standard machine learning applications, the results shown in  \\nFig. 4 and Table 4 are sufficient to describe the performance of the \\nmodel. However, in our case, we are interested in presenting an in\\xad\\ndividualized survival probability curve for each recipient. As a first step \\ntoward this goal, we have to evaluate the precision of our predicted \\nprobabilities, which we depict in Fig. 5. To ensure that we have a \\nTable 2 \\nImportant variables selected with the corresponding frequency using LASSO.    \\nSelected \\nVariables  \\n11 \\nAGE DON, BMI CALC, COD CAD DON, CREAT TRR, DIAG, FUNC STAT TRR, INIT HGT CM CALC, ISCHTIME, MED COND TRR, REGION, TBILI, TCR DGN \\n10 \\nCMV DON, DAYS STAT1A, DIAB, DRMIS, ETH MAT, ETHCAT, PRI PAYMENT TCR, PRI PAYMENT TRR, THORACIC DGN, TRANSFUSIONS, VENT SUPPORT AFTER \\nLIST \\n9 \\nCARD SURG, GENDER MAT, HCV SEROSTATUS, LIFE SUP TRR, PROC TY HR \\n8 \\nPRIOR CARD SURG TCR, SUD DEATH \\n7 \\nDOPAMINE, FUNC STAT TCR, HBV CORE, HGT CM TCR, INFECT IV DRUG TRR, PT T4 DON, PULM CATH DON \\n6 \\nAMIS, DAYS STAT1, EDUCATION, HEPARIN, HGT CM CALC, INOTROP VASO CO TRR, LIFE SUP TCR, PRIOR CARD SURG TYPE TCR, TBILI DON \\n5 \\nBUN DON, CHEST XRAY DON, HIV SEROSTATUS, IMPL DEFIBRIL, INOTROP VASO PCW TRR, INOTROP VASO SYS TCR, OTHER INF DON, PT DIURETICS DON, \\nSTERNOTOMY TRR \\n4 \\nBRONCHO LT DON, CONTIN CIG DON, EBV SEROSTATUS, ETHCAT DON, HEMO SYS TCR, HEMO SYS TRR, HLAMIS, HTLV2 OLD DON, LAST INACT REASON, \\nSHARE TY \\n3 \\nCORONARY ANGIO, DAYS STAT1B, GENDER DON, HBV SUR ANTIGEN, INIT AGE, INOTROP VASO MN TRR, PULM INF DON, TATTOOS, WGT KG DON CALC \\n2 \\nAGE, ANCEF, ANTIHYPE DON, BMIS, CLIN INFECT DON, CONTIN OTH DRUG DON CRSMATCH DONE, DAYSWAIT CHRON, HGT CM DON CALC, HIST OTH DRUG \\nDON, INIT BMI CALC, INIT STAT, INOTROPIC, PRIOR CARD SURG TRR, PVR, SGOT DON \\n1 \\nABO DON, AGE MAT, BRONCHO RT DON, END STAT, GENDER, HEMO PA DIA TRR, INOTROP VASO SYS TRR, INOTROPES TRR, PROTEIN URINE, VASODIL DON \\nThe UNOS variables are defined at https://www.srtr.org/requesting-srtr-data/saf-data-dictionary/. Due to page limits we do not redefine them here.  \\nTable 3 \\nThe best average holdout performance and the corresponding 95% confidence interval among all scenarios for each algorithm, for the 1-year prediction time-frame.             \\nAlg. \\nNum. Imp. \\nCat. Imp. \\nEncoding \\nFeature Selection \\nSub-sampling \\nG-Mean \\nAUC \\nSpec. \\nSens. \\nAcc.  \\nLR \\nMedian \\nUnknown \\nOne-Hot \\nLASSO \\nUP \\n0.610 \\n0.655 \\n0.593 \\n0.629 \\n0.624 \\n(0.599, 0.621) \\n(0.645, 0.664) \\n(0.575, 0.611) \\n(0.618, 0.639) \\n(0.614, 0.633) \\nXGB \\nMedian \\nDrop \\nOne-Hot \\nLASSO \\nDOWN \\n0.607 \\n0.649 \\n0.585 \\n0.631 \\n0.625 \\n(0.597, 0.618) \\n(0.636, 0.662) \\n(0.567, 0.602) \\n(0.619, 0.642) \\n(0.615, 0.635) \\nLDA \\nMedian \\nUnknown \\nOne-Hot \\nLASSO \\nUP \\n0.606 \\n0.648 \\n0.593 \\n0.621 \\n0.617 \\n(0.598, 0.615) \\n(0.641, 0.656) \\n(0.575, 0.610) \\n(0.611, 0.631) \\n(0.609, 0.625) \\nRF \\nMedian \\nUnknown \\nOne-Hot \\nLASSO \\nDOWN \\n0.606 \\n0.649 \\n0.598 \\n0.614 \\n0.612 \\n(0.600, 0.613) \\n(0.642, 0.657) \\n(0.590, 0.607) \\n(0.597, 0.631) \\n(0.598, 0.626) \\nANN \\nDrop \\nUnknown \\nOne-Hot \\nLASSO \\nUP \\n0.587 \\n0.616 \\n0.619 \\n0.562 \\n0.569 \\n(0.564, 0.609) \\n(0.575, 0.656) \\n(0.537, 0.700) \\n(0.471, 0.653) \\n(0.497, 0.642) \\nCART \\nMedian \\nUnknown \\nLabel \\nLASSO \\nUP \\n0.577 \\n0.599 \\n0.546 \\n0.611 \\n0.602 \\n(0.560, 0.594) \\n(0.578, 0.619) \\n(0.507, 0.584) \\n(0.593, 0.630) \\n(0.589, 0.616) \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n7\\n\\nFig. 4. ROC plots for our logistic regression's training and hold-out datasets for Years 1 − 9. The light blue and dark blue correspond to the training and hold-out \\nperformances, respectively. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) \\nTable 4 \\nHoldout performance of the UP-LASSO-logistic regression implementation.               \\nMonth 1 \\nYear 1 \\nYear 2 \\nYear 3 \\nYear 4 \\nYear 5 \\nYear 6 \\nYear 7 \\nYear 8 \\nYear 9 \\nYear 10  \\nAUC \\n0.608 \\n0.581 \\n0.571 \\n0.594 \\n0.619 \\n0.631 \\n0.654 \\n0.671 \\n0.698 \\n0.703 \\n0.702 \\nAccuracy \\n0.547 \\n0.537 \\n0.541 \\n0.558 \\n0.586 \\n0.594 \\n0.615 \\n0.629 \\n0.64 \\n0.634 \\n0.627 \\nSensitivity \\n0.542 \\n0.522 \\n0.532 \\n0.548 \\n0.599 \\n0.618 \\n0.659 \\n0.684 \\n0.723 \\n0.743 \\n0.752 \\nSpecificity \\n0.589 \\n0.590 \\n0.565 \\n0.579 \\n0.562 \\n0.559 \\n0.558 \\n0.569 \\n0.561 \\n0.541 \\n0.533 \\nG-Mean \\n0.565 \\n0.555 \\n0.548 \\n0.564 \\n0.58 \\n0.588 \\n0.607 \\n0.624 \\n0.636 \\n0.634 \\n0.633    \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n8\\n\\nsufficiently large sample size for inference, we have only plotted \\nprobabilities when the number of observed cases within that prob\\xad\\nability range was ≥100. The plot shows that our forecast survival \\nprobabilities underestimate the observed averages for the first 5 years \\n(based on their respective holdout data sets). For the later years, the \\npredicted probabilities are relatively close to the observed ones. This \\nfinding is consistent with the metrics reported in Table 4. \\n5.2. Stage II results: calibrating the stage I survival probabilities \\nA closer examination of the individualized survival probabilities \\nover time revealed a non-monotonic behavior of the curves (see sample \\nUNOS patients 1, 3, and 4 in Fig. 6). To alleviate this problem, we used \\nisotonic regression in stage II to ensure the monotonicity for each re\\xad\\ncipient's survival probability curve. Fig. 6 shows the survival prob\\xad\\nabilities before and after the calibration with isotonic regression. \\nFig. 5. Plots of the observed versus forecast survival probabilities for Years 1 − 9 (before isotonic regression). The light blue line correspond to the baseline case of a \\nperfect match between both probabilities. The dark blue line correspond to the results obtained from our machine learning model. To create each plot, we grouped \\nthe recipients in our holdout set based on their forecast probability (in 0.1 increments) and we then calculated the observed average as: (#Survivals for year\\xad\\nfromgroup)/(Total # recipients for yearfromgroup). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of \\nthis article.) \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n9\\n\\nWe evaluate the precision of our calibrated probabilities in Fig. 7. \\nSimilar to Fig. 5, this figure shows the observed averages against the \\nforecast survival probability in Year 1, 2…9 after isotonic regression is \\napplied. The overall pattern in each plot is consistent with the corre\\xad\\nsponding plot in Fig. 5. \\nIn Table 5, we quantify the differences in prediction performance \\nafter the application of isotonic regression, where the bold font shows \\nthat the prediction was greater than or equal to the pre-isotonic re\\xad\\ngression values. From the table, one can see that the AUC and \\nG − Mean values were bolded for the majority of the time periods (i.e. \\nthe results typically improved with the application of isotonic regres\\xad\\nsion to calibrate the survival probabilities). However, and more im\\xad\\nportantly, the application of isotonic regression leads to results that are \\nconsistent with the medical expectation of decreasing survival prob\\xad\\nabilities over time, and individualized to each recipient. \\n6. Discussion, contributions and conclusions \\n6.1. Contextualizing the prediction results based on the transplantation \\nliterature \\nPrior to discussing our prediction results, it is important to compare \\nthe consistency between our models' important variables and those \\nfound in the literature. The majority of the literature, where ML \\nmethods were used for predicting transplantation outcomes, examined \\none [e.g., see 7,14,44–46] or at most three time periods [8,11]. Thus, \\nfrom our analysis, we can gain additional insights into the contribution \\nof a variable to predicting graft rejection, short-term, medium-term, \\nand long-term survival. This is not possible for studies that have focused \\non one time-period. Moreover, the study of Dag et al. [8] did not in\\xad\\nclude 1-month acute transplant rejection period and the study of Yoon \\net al. [11] has only investigated 1-, 3-, and 5- year survival outcomes. \\nTo illustrate whether our important predictors have been reported in \\nthe previous literature, let us consider Table 6 where we provide a \\nsummary of whether the variables selected for all 11 time-periods \\nthrough our LASSO implementation have been selected in [7,8,14]. We \\nhave selected these three references as a representative sample of the \\nliterature since (a) Dag et al. [8] examined three time periods, (b) Dag \\net al. [7] focused on long-term survival outcomes, and (c) Medved et al. \\n[14] used state-of-the-art deep learning methods for short-term survival \\npredictions. \\nFrom Table 6, there are three observations to be made. First, the \\ndonor's age and the recipient's medical condition at transplant were the \\ntwo variables where our model agreed with the three papers. Second, \\nour model was the only methodology, where the body mass index for \\nthe recipient was found important. Third, all other variables were also \\nselected by at least one paper. Note that the differences between the \\nmodels can be attributed to (a) differences in data cleaning procedures, \\nFig. 6. Plots of the forecast probabilities for four sample patients at years 1 − 10 post transplant. The light and dark blue lines show the probabilities before and after \\nthe application of isotonic regression, respectively. Note when the dark blue line is not shown, this equates to a perfect overlap with the light blue line. We do not \\nshow month 1 here, for aesthetic purposes, since it is on a different time scale when compared to the yearly data. (For interpretation of the references to colour in this \\nfigure legend, the reader is referred to the web version of this article.) \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n10\\n\\nFig. 7. Plots of the observed versus forecast survival probabilities for Years 1 − 9 (after isotonic regression is applied). The light blue line correspond to the baseline \\ncase of a perfect match between both probabilities. The dark blue line correspond to the results obtained from our machine learning model. To create each plot, we \\ngrouped the recipients in our holdout set based on their forecast probability (in 0.1 increments) and we then calculated the observed average as: (#Survivals for \\nyearfromgroup)/(Total # recipients for yearfromgroup). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version \\nof this article.) \\nTable 5 \\nDifferences (after isotonic regression – before isotonic regression) in Holdout performance of the UP-LASSO-logistic regression implementation.               \\nMonth 1 \\nYear 1 \\nYear 2 \\nYear 3 \\nYear 4 \\nYear 5 \\nYear 6 \\nYear 7 \\nYear 8 \\nYear 9 \\nYear 10  \\nΔ AUC \\n0.008 \\n0.017 \\n0.029 \\n0.021 \\n0.011 \\n0.007 \\n0.001 \\n0.001 \\n0.000 \\n0.002 \\n0.001 \\nΔ Accuracy \\n0.125 \\n0.083 \\n0.061 \\n0.042 \\n0.017 \\n0.007 \\n−0.005 \\n−0.008 \\n−0.006 \\n0.001 \\n0.008 \\nΔ Sensitivity \\n0.150 \\n0.136 \\n0.110 \\n0.083 \\n0.032 \\n0.007 \\n−0.037 \\n−0.061 \\n−0.102 \\n−0.130 \\n−0.161 \\nΔ Specificity \\n−0.114 \\n−0.105 \\n−0.064 \\n−0.047 \\n−0.009 \\n0.008 \\n0.034 \\n0.050 \\n0.087 \\n0.112 \\n0.134 \\nΔ G-Mean \\n0.009 \\n0.010 \\n0.019 \\n0.016 \\n0.010 \\n0.007 \\n0.000 \\n−0.003 \\n−0.003 \\n−0.001 \\n−0.005    \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n11\\n\\n(b) utilization of different variable selection methodologies, (c) differ\\xad\\nences in sample size of dataset used for training (e.g., other papers \\nmight not have used the entire UNOS dataset and/or did not include \\nresults up to 2016), and/or (d) correlation among one or more in\\xad\\ndependent variables in the UNOS dataset. As we show below, our \\nprediction results are consistent with the literature, and thus, the dif\\xad\\nferences shown in Table 6 may not be significant from a prediction \\nperspective. \\nIf we focus on the prediction results in Stage I, our model is (at least) \\ncomparable to the prediction outcomes reported in the literature that \\nutilize the UNOS dataset. For example, for the 1-year survival out\\xad\\ncomes, our utilized model results in specificity, sensitivity, AUC and \\nG − Mean values of 0.590, 0.522, 0.581, and 0.555, respectively (as \\nshown in Table 4). Our AUC value, for the full, non-censored data in  \\nFig. 4, of 0.614 is similar to the AUC values reported in Dag et al. [8] \\n(0.624), Yoon et al. [11] (0.641 for their best model), Medved et al. \\n[14] (0.61 for their IMPACT model which was developed using the \\nUNOS dataset), Miller et al. [45] (who reported AUC values of 0.613, \\n0.59, and 0.663 for three different transplantation eras), and Villela \\net al. [46] (whose auto-ML model had an AUC of 0.66). However, the \\nresults in Dag et al. [8] can be considered much worse in practice since \\ntheir average sensitivity was only 0.128 for their validation data sets, \\nwhich means that their model was not able to predict deaths for the first \\nyear. None of the other cited works reported sensitivity or G − Mean \\nresults and thus, it is not clear if their models suffer from such a per\\xad\\nformance discrepancy as well. To further demonstrate the performance \\nof our model, let us consider the 10-year end-point of our prediction \\nperiod, our model results in specificity, sensitivity, AUC and G − Mean \\nvalues of 0.533, 0.752, 0.702, and 0.633, respectively (as shown in  \\nTable 4). Our AUC value is better than the 0.631 reported by Yoon et al. \\n[11]. Perhaps more importantly, none of the cited papers presented a \\ndetailed description of their data cleaning procedure, which makes \\nreproducing their work difficult in practice. Thus, while our prediction \\nresults are comparable to the published literature, we believe they can \\nbe more useful in practice since we make our code and analysis publicly \\navailable through our R Markdown document. \\nIn the second stage of our application, we constructed a mono\\xad\\ntonically decreasing survival probability curve for each patient using \\nisotonic regression. Overall, the application of isotonic regression did \\nnot affect the overall shape of the curve as evident from Figs. 5 and 7. \\nHowever, the results are medically compelling due to the non-in\\xad\\ncreasing survival probabilities. Furthermore, the individualized nature \\nof the predictions brings us a step closer to personalized medicine [15]. \\n6.2. Contributions to heart transplantation research and practice \\nIn Section 6.1, we have shown that the stage I predictive perfor\\xad\\nmance of the proposed framework is in line or better than the results \\nreported in the literature. The proposed framework has several merits \\nas it (a) provides personalized results to a given “match case”, (b) is \\nguaranteed to be monotonic, (c) is explainable due to the use of logistic \\nregression, (d) is easy to implement through the provided code, and (e) \\nis flexible since it can easily incorporate any data preparation, variable \\nselection and ML modeling procedure in Stage I of the framework. \\nFurthermore, as stated in Section 2, by examining the UNOS data we \\nare able to  \\n• develop a flexible framework, which can result in accurate and \\nprecise heart transplantation outcome predictions;  \\n• provide UNOS with a benchmark to compare the expected outcomes \\nfrom a match, which can inform the policies and algorithms gov\\xad\\nerning phase II of the transplantation process; and  \\n• inform the protocols prescribed by medical professionals in phases \\nIV and V of the transplantation process based on the estimated \\nsurvival risk. \\nThe first aim is achieved based on the obtained results from the \\napplication of our framework. To accomplish the second and third aims, \\nwe have created a web application (app) titled “Heart Transplantation \\nOutcome Predictor (H-TOP)” that allows UNOS and transplant teams to \\nutilize the proposed framework on prospective “match cases”. Our \\nTable 6 \\nA comparison of whether variables selected for all of our 11 time periods have \\nbeen identified as important in previous literature.       \\nImportant \\nPredictors \\nDefinition of variable per UNOS's data \\nsheet [47] \\n[8] \\n[7] \\n[14]  \\nAGE DON \\nDonor's age in years \\n✓ \\n✓ \\n✓ \\nBMI CALC \\nCalculated body mass index for recipient \\n– \\n– \\n– \\nCOD CAD DON \\nDeceased donor's cause of death \\n– \\n– \\n✓ \\nCREAT TRR \\nRecipient's serum creatinine at transplant \\n– \\n– \\n✓ \\nDIAG \\nRecipient's primary diagnosis \\n✓ \\n✓ \\n– \\nFUNC STAT TRR \\nRecipient's functional status at transplant \\n✓ \\n✓ \\n– \\nINIT HGT CM CALC \\nCalculated candidate height in cm at \\nlisting \\n– \\n– \\n✓ \\nISCHTIME \\nIschemic time in hours \\n– \\n– \\n✓ \\nMED COND TRR \\nRecipient's medical condition pre- \\ntransplant at transplant time \\n✓ \\n✓ \\n✓ \\nREGION \\nUNOS region where transplanted/listed \\n✓ \\n✓ \\n– \\nTBILI \\nMost recent serum total bilirubin at \\ntransplant \\n– \\n– \\n✓ \\nTCR DGN \\nCandidate diagnosis at listing \\n✓ \\n– \\n– \\nFig. 8. The workflow of the H-TOP app.  \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n12\\n\\napproach can be used on prospective cases since we (i) do not include \\nany variables that UNOS no longer collects, (ii) do not use transplant \\nyear as a predictor, and (iii) present a comprehensive approach that can \\nbe used for data preparation, which is illustrated by our detailed data \\ncleaning, encoding and imputation procedures. \\nWe have created and deployed the app using the R “shiny” package. \\nThe app can be accessed at http://dataviz.miamioh.edu/Heart- \\nTransplant/monotonic/. To facilitate using the app, it contains an in\\xad\\nstructional video showing how one can use the app to make predictions. \\nThe app allows users to utilize one of two scenarios for inputting data \\n(a) manual entry, where dropdown menus are provided for categorical \\nvariables and text inputs are provided for numeric variables; or (b) a \\nCSV upload, where practitioners should load a CSV file based on a \\nprovided template. Once the data is inputted, the app (a) performs basic \\nchecks on the quality of the imported data, (b) implements the trained \\nlogistic regression model for the 11 time points, (c) calibrates the ob\\xad\\ntained survival probability curves using isotonic regression, and (d) \\nprovides a table and a plot of the calibrated survival probabilities. An \\noverview of the app's workflow is presented in Fig. 8. In our estimation, \\nthe app is user-friendly and it requires no machine learning background \\nfor usage. \\n6.3. Contributions to the data-driven decision-making and support research \\ncommunities \\nWhile we have applied our framework to the problem of predicting \\nheart transplantation outcomes, our proposed two-stage framework \\npresents a generic and flexible methodology that can be applied to any \\nmulti-period prediction application where ML methods are used and \\nmonotonic outcomes are required. In Section 1, we have presented \\nseveral decision-making applications that can benefit from our pro\\xad\\nposed framework. In the context of ML methodologies, our framework \\nrepresents an extension to the use of “hybrid” methodologies. Existing \\n“hybrid” methods typically incorporate two or more approaches for the \\npurposes of improving prediction accuracy [e.g., see 48]. However, our \\nproposed framework introduces the idea of using a hybrid approach to \\nconstrain the predictions from the initial ML modeling stage, which can \\ninfluence the development of other “hybrid” approaches where dif\\xad\\nferent criteria for calibration are to be enforced. \\n6.4. Limitations and future research opportunities \\nThe limitations of our study provides opportunities for future re\\xad\\nsearch. First, the goal of this study was to present a methodological \\nmachine learning based framework that uses isotonic regression to \\ncalibrate and guarantee the monotonicity of outcome probabilities over \\ntime. While we examined a large number of modeling approaches, we \\ndid not attempt to optimize the prediction performance (e.g., through a \\ndetailed investigation of parameter tuning or through examining more \\ncomplex data imputation schemes). We examined these scenarios to \\nshow that even with the “best” modeling approach, non-monotone \\nprobability curves can be obtained. Future research could optimize the \\nfindings of our research to broader decision making domains. Second, \\nwe examined a limited number of few machine learning models and \\ntheir parameterizations in Stage I. Thus, we cannot guarantee that lo\\xad\\ngistic regression will be superior when compared to ensemble or hybrid \\nmodels that were not considered in our analysis. Future research could \\ninvestigate the use of methodologies excluded from this study. The \\nselection of LR could have also differed if other data cleaning proce\\xad\\ndures, time-samples of the UNOS data, and criteria for balancing pre\\xad\\ndictive accuracy and interpretation (which will differ from one appli\\xad\\ncation to another) were used. Third, we did not consider how to \\noptimize the calibration of the survival curve obtained from Stage I. We \\nhave only considered the use of isotonic regression for obtaining \\nmonotonic survival probability curves. Future work can examine the \\nuse of other approaches to achieve monotonicity, while having a \\nsmoother function (e.g., the use of an exponential curve). \\nSpecific to heart transplantation application, there are some addi\\xad\\ntional issues that need to be emphasized. The secondary and retro\\xad\\nspective nature of our analysis from a registry database means that we \\ncannot account for “the quality of the source data, the number of \\nmissing data, and the lack of standardization associated with multi\\xad\\ncenter studies (such as different immunosuppressive regimens and dif\\xad\\nferent matching criteria)” [14,p. 6]. In addition, we have no control \\nover whether the variables included in our model will continue to be \\ncollected in the future. Exclusion of these variables from the UNOS data \\ncollection protocol will require future researcher to retrain our models. \\nBy sharing our detailed code for data cleaning, we explicitly show how \\nwe handled missing data, observations where we identified data quality \\nissues, and how we removed all variables that had an ending data per \\nthe time of our data acquisition. While our efforts cannot guarantee \\nsuitability for future changes in UNOS's data collection protocol, it al\\xad\\nlows researchers to easily build on our analysis if needed. The results \\ndepicted in Figs. 5 and 7, where our approach consistently under\\xad\\nestimates the survival probabilities (when compared to observed rates) \\nfor the first four years. There are two important considerations that \\nneed to be emphasized (a) it is unclear whether the previous methods in \\nthe literature would have similar performance characteristics since this \\ntype of analysis has not been done before (only metrics for the di\\xad\\nchotomous classification are typically reported); and (b) the utility of \\nour approach in practice is not diminished from this limitation. Speci\\xad\\nfically, from a practitioner's perspective knowing with a high degree of \\ncertainty that the prediction probabilities at 5+ years from transplant \\nis accurate, is sufficient to obtain a lower bound on the survival prob\\xad\\nabilities for the earlier time-periods. That being said, future research \\nshould examine how to reduce the bias in the predictions for ≤4 years. \\n6.5. Concluding remarks \\nIn conclusion, through our two-stage machine learning framework, \\nwe obtained data-driven and individualized survival probability curves \\nfor each transplantation event. This represents a significant contribu\\xad\\ntion when compared to previous literature that: (a) had non-monotonic \\npredictions over time [8,11]; (b) attempted to account for the non- \\nmonotone predictions through features in their machine learning model \\n[12,13], which is an improvement over the methods in (a), however, \\nsuch an approach does not guarantee monotonicity; and (c) achieved \\nmonotonic predictions through the use of a population adjustment \\napproach [e.g., see 14], which can potentially limit the utility of the \\napproach (if personalized predictions are important). By providing our \\ndetailed code, we are encouraging future research on improving the \\npredictions obtained from the UNOS heart transplantation dataset. Our \\ndetailed code can also facilitate the adoption of our framework in other \\napplication domains. Our web-based application (app) can inform the \\ncurrent UNOS organ allocation policies and protocols prescribed to \\nheart transplant recipients. Furthermore, we have encouraged the DSS \\ncommunity to use any/all chunks of code from our app (e.g., it can be \\nused in other applications for educational and/or commercial purposes) \\nby making the app's code in the public domain through a CC0 - “No \\nRights Reserved” license. \\nSupplementay materials \\nCode and analysis: Our code, analysis and results are detailed in an \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n13\\n\\nR Markdown document, which can be accessed at https://ying-ju. \\ngithub.io/heart_transplant.github.io/. Our web app can be found online \\nat http://dataviz.miamioh.edu/Heart-Transplant/monotonic/. Note \\nthat both our R Markdown document and app are in the public domain \\nthrough a CCO - “No Rights Reserved” license. \\nFunding \\nThe modeling approach, analysis and computational resources were \\nsupported in part by the Ohio Supercomputer Center [PMIU0166] and \\nthe National Science Foundation [CMMI-1635927]. Dr. Megahed's work \\nwas supported by the Neil R. Anderson Endowed Assistant \\nProfessorship at the Farmer School of Business at Miami University. The \\nauthors would also like to thank the Department of Statistics at Miami \\nUniversity for hosting our web application. \\nDeclaration of Competing Interest \\nNone. \\nAcknowledgments \\nThe UNOS registry was supported in part by Health Resources and \\nServices Administration contract 234-2005-37011C. The content is the \\nresponsibility of the authors alone and does not necessarily reflect the \\nviews or policies of the Department of Health and Human Services. \\nReferences \\n[1] V.L. Miguéis, A. Freitas, P.J. Garcia, A. Silva, Early segmentation of students ac\\xad\\ncording to their academic performance: a predictive modelling approach, Decis. \\nSupport. Syst. 115 (2018) 36–51. \\n[2] J. Moeyersoms, D. Martens, Including high-cardinality attributes in predictive \\nmodels: a case study in churn prediction in the energy sector, Decis. Support. Syst. \\n72 (2015) 72–81. \\n[3] D. Olaya, J. Vásquez, S. Maldonado, J. Miranda, W. Verbeke, Uplift Modeling for \\npreventing student dropout in higher education, Decis. Support. Syst. (2020), \\nhttps://doi.org/10.1016/j.dss.2020.113320 (In Press). \\n[4] K. Coussement, S. Lessmann, G. Verstraeten, A comparative analysis of data pre\\xad\\nparation algorithms for customer churn prediction: a case study in the tele\\xad\\ncommunication industry, Decis. Support. Syst. 95 (2017) 27–36. \\n[5] Z.-H. Hu, Z.-H. Sheng, A decision support system for public logistics information \\nservice management and optimization, Decis. Support. Syst. 59 (2014) 219–229. \\n[6] A. Oztekin, Z.J. Kong, D. Delen, Development of a structural equation modeling- \\nbased decision tree methodology for the analysis of lung transplantations, Decis. \\nSupport. Syst. 51 (1) (2011) 155–166. \\n[7] A. Dag, K. Topuz, A. Oztekin, S. Bulur, F.M. Megahed, A probabilistic data-driven \\nframework for scoring the preoperative recipient-donor heart transplant survival, \\nDecis. Support. Syst. 86 (2016) 1–12. \\n[8] A. Dag, A. Oztekin, A. Yucel, S. Bulur, F.M. Megahed, Predicting heart transplan\\xad\\ntation outcomes through data analytics, Decis. Support. Syst. 94 (2017) 42–52. \\n[9] K. Topuz, F.D. Zengul, A. Dag, A. Almehmi, M.B. Yildirim, Predicting graft survival \\namong kidney transplant recipients: a Bayesian decision support model, Decis. \\nSupport. Syst. 106 (2018) 97–109. \\n[10] B. Efron, Logistic regression, survival analysis, and the Kaplan-Meier curve, J. Am. \\nStat. Assoc. 83 (402) (1988) 414–425. \\n[11] J. Yoon, W.R. Zame, A. Banerjee, M. Cadeiras, A.M. Alaa, M. van der Schaar, \\nPersonalized survival predictions via trees of predictors: an application to cardiac \\ntransplantation, PLoS One 13 (3) (2018) e0194985, , https://doi.org/10.1371/ \\njournal.pone.0194985. \\n[12] L. Ohno-Machado, M.A. Musen, Sequential versus standard neural networks for \\npattern recognition: an example using the domain of coronary heart disease, \\nComput. Biol. Med. 27 (4) (1997) 267–281. \\n[13] L. Ohno-Machado, M.A. Musen, Modular neural networks for medical prognosis: \\nquantifying the benefits of combining neural networks for survival prediction, \\nConnect. Sci. 9 (1) (1997) 71–86. \\n[14] D. Medved, M. Ohlsson, P. Höglund, B. Andersson, P. Nugues, J. Nilsson, Improving \\nprediction of heart transplantation outcome using deep learning techniques, Sci. \\nRep. 8 (1) (2018) 1–9. \\n[15] L.H. Goetz, N.J. Schork, Personalized medicine: motivation, challenges, and pro\\xad\\ngress, Fertil. Steril. 109 (6) (2018) 952–963. \\n[16] M. Pavlakis, D.W. Hanto, Clinical pathways in transplantation: a review and \\nexamples from Beth Israel Deaconess Medical Center, Clin. Transpl. 26 (3) (2012) \\n382–386. \\n[17] R.E. Barlow, H.D. Brunk, The isotonic regression problem and its dual, J. Am. Stat. \\nAssoc. 67 (337) (1972) 140–147. \\n[18] G. Shmueli, O.R. Koppius, Predictive analytics in information systems research, MIS \\nQ. 35 (3) (2011) 553–572. \\n[19] R. Agarwal, V. Dhar, Editorial—big data, data science, and analytics: the oppor\\xad\\ntunity and challenge for IS research, Inf. Syst. Res. 25 (3) (2014) 443–448. \\n[20] A. Gelman, E. Loken, The Garden of Forking Paths: Why Multiple Comparisons Can \\nBe a Problem, Even when there Is no “Fishing Expedition” or “P-Hacking” and the \\nResearch Hypothesis Was Posited Ahead of Time, Department of Statistics, \\nColumbia University, 2013, https://stat.columbia.edu/gelman/research/ \\nunpublished/p_hacking.pdf (Online, last accessed May 26, 2020). \\n[21] S. Nestorov, B. Jukić, N. Jukić, A. Sharma, S. Rossi, Generating insights through \\ndata preparation, visualization, and analysis: framework for combining clustering \\nand data visualization techniques for low-cardinality sequential data, Decis. \\nSupport. Syst. 125 (2019) 113119, , https://doi.org/10.1016/j.dss.2019.113119. \\n[22] J. Kazmaier, J.H. van Vuuren, A generic framework for sentiment analysis: lever\\xad\\naging opinion-bearing data to inform decision making, Decis. Support. Syst. (2020), \\nhttps://doi.org/10.1016/j.dss.2020.113304 In Press. \\n[23] G. James, D. Witten, T. Hastie, R. Tibshirani, An Introduction to Statistical \\nLearning, Springer, 2013 ISBN 978-1-4614-7137-0. \\n[24] J.C. Platt, Probabilistic outputs for support vector machines and comparisons to \\nregularized likelihood methods, in: A.J. Smola, P.J. Bartlett, B. SchÃlkopf, \\nD. Schuurmans (Eds.), Advances in Large Margin Classifiers, Chap. 5, MIT Press, \\n2000, pp. 61–74. \\n[25] M.J. Best, N. Chakravarti, Active set algorithms for isotonic regression; a unifying \\nframework, Math. Program. 47 (1990) 425–439. \\n[26] G. Savarese, L.H. Lund, Global public health burden of heart failure, Cardiac Fail. \\nRev. 3 (1) (2017) 7–11. \\n[27] E.J. Benjamin, P. Muntner, A. Alonso, M.S. Bittencourt, C.W. Callaway, et al., Heart \\ndisease and stroke statistics-2019 update a report from the American Heart \\nAssociation, Circulation 139 (10) (2019) e56–e528. \\n[28] National Heart, Lung, and Blood Institute (NHLBI), Heart Failure (Online, last ac\\xad\\ncessed May 31, 2020),, 2018. https://www.nhlbi.nih.gov/health-topics/heart- \\nfailure. \\n[29] C. Allemani, T. Matsuda, V. Di Carlo, R. Harewood, M. Matz, et al., Global sur\\xad\\nveillance of trends in cancer survival 2000–14 (CONCORD-3): analysis of individual \\nrecords for 37 513 025 patients diagnosed with one of 18 cancers from 322 po\\xad\\npulation-based registries in 71 countries, Lancet 391 (10125) (2018) 1023–1075. \\n[30] United Network for Organ Sharing, Transplant Trends – UNOS (Online, last ac\\xad\\ncessed May 31, 2020),, 2020. https://unos.org/data/transplant-trends/. \\n[31] United Network for Organ Sharing, National Data - UNOS (Online, last accessed \\nMay 31, 2020),, 2020. https://optn.transplant.hrsa.gov/data/view-data-reports/ \\nnational-data/. \\n[32] United Network for Organ Sharing, How We Match Organs, UNOS (Online, last \\naccessed May 31, 2020),, 2020. https://unos.org/transplant/how-we-match- \\norgans/. \\n[33] A. Alba, E. Bain, N. Ng, M. Stein, K. Brien, Complications after heart transplanta\\xad\\ntion: hope for the best, but prepare for the worst, Int. J. Transplant. Res. Med. 2 (2) \\n(2016) 022. \\n[34] The OPTN/UNOS Ad Hoc Geography Committee, OPTN  ׀UNOS Proposed Organ \\nDistribution Frameworks (Online, last accessed May 31, 2020),, 2018. https://unos. \\norg/wp-content/uploads/unos/Proposed-Distribution-Frameworks-v5.pdf. \\n[35] United Network for Organ Sharing, Questions & Answers About Heart Allocation for \\nAdult Transplant Candidates, UNOS Transplant Living (Online, last accessed May \\n31, 2020),, 2018. https://transplantliving.org/organ-facts/heart/heart-faq/. \\n[36] P.J. Thuluvath, H.Y. Yoo, R.E. Thompson, A model to predict survival at one month, \\none year, and five years after liver transplantation based on pretransplant clinical \\ncharacteristics, Liver Transpl. 9 (5) (2003) 527–532. \\n[37] I. Guyon, A. Elisseeff, An introduction to variable and feature selection, J. Mach. \\nLearn. Res. 3 (2003) 1157–1182. \\n[38] M.J. Wilhelm, Long-term outcome following heart transplantation: current per\\xad\\nspective, J. Thoracic Dis. 7 (3) (2015) 549–551. \\n[39] H. He, E.A. Garcia, Learning from imbalanced data, IEEE Trans. Knowl. Data Eng. \\n21 (9) (2009) 1263–1284. \\n[40] N.V. Chawla, K.W. Bowyer, L.O. Hall, W.P. Kegelmeyer, SMOTE: synthetic minority \\nover-sampling technique, J. Artif. Intell. Res. 16 (2002) 321–357. \\n[41] G. Menardi, N. Torelli, Training and assessing classification rules with imbalanced \\ndata, Data Min. Knowl. Disc. 28 (1) (2014) 92–122. \\n[42] J. Lasserre, S. Arnold, M. Vingron, P. Reinke, C. Hinrichs, Predicting the outcome of \\nrenal transplantation, J. Am. Med. Inform. Assoc. 19 (2) (2012) 255–262. \\n[43] A. Decruyenaere, P. Decruyenaere, P. Peeters, F. Vermassen, T. Dhaene, et al., \\nPrediction of delayed graft function after kidney transplantation: comparison be\\xad\\ntween logistic regression and machine learning methods, BMC Med. Inform. \\nDecision Making 15 (1) (2015) (Article No. 83). \\n[44] D. Delen, A. Oztekin, Z.J. Kong, A machine learning-based approach to prognostic \\nanalysis of thoracic transplantations, Artif. Intell. Med. 49 (1) (2010) 33–42. \\n[45] P.E. Miller, S. Pawar, B. Vaccaro, M. McCullough, P. Rao, et al., Predictive abilities \\nof machine learning techniques may be limited by dataset characteristics: insights \\nfrom the UNOS database, J. Card. Fail. 25 (6) (2019) 479–483. \\n[46] M. Villela, C. Bravo, M. Shah, S. Patel, U. Jorde, et al., Prediction of outcomes after \\nheart transplantation using machine learning techniques, J. Heart Lung Transplant. \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n14\\n\\n39 (4) (2020) S295–S296. \\n[47] Scientific Registry of Transplant Recipients, SAF Data Dictionary (Online, last ac\\xad\\ncessed May 31, 2020),, 2020. https://www.srtr.org/requesting-srtr-data/saf-data- \\ndictionary/. \\n[48] B. Weng, W. Martinez, Y.-T. Tsai, C. Li, L. Lu, J.R. Barth, F.M. Megahed, \\nMacroeconomic indicators alone can predict the monthly closing price of major US \\nindices: insights from artificial intelligence, time-series analysis and hybrid models, \\nAppl. Soft Comput. 71 (2018) 685–697.  \\nDr. Hamidreza Ahady Dolatsara is an Assistant Professor in the School of Management \\nat Clark University. He received his Ph.D. and MS degrees in Industrial & Systems \\nEngineering from Auburn University. In addition, he received an MS in Information \\nSystems Management from Auburn University and an MS in Transportation Engineering \\nfrom Western Michigan University. His research interests are in healthcare, transporta\\xad\\ntion, and finance.  \\nDr. Ying-Ju (Tessa) Chen is an Assistant Professor in the Department of Mathematics at \\nthe University of Dayton. Her expertise is in applied machine learning, high performance \\ncomputing, statistical modeling, and survival analysis. Her work has been funded by \\nseveral foundations and government agencies.  \\nChristy Evans graduated from Auburn University with a bachelor's degree in biomedical \\nsciences and then received her master's degree in Industrial and Systems Engineering with \\na focus in occupational safety and ergonomics. During graduate school, Christy worked as \\na graduate research assistant for projects related to burnout in the healthcare field. She is \\nnow an incoming medical student.  \\nDr. Ashish Gupta is an Associate Professor of Analytics in Raymond J. Harbert College of \\nBusiness and Harbert Advisory Council Faculty Fellow at Auburn University. His research \\ninterests are in the areas of artificial intelligence, machine learning, natural language \\nprocessing, healthcare informatics, IoT, sports analytics, organizational and individual \\nperformance. His recent research has appeared in journals such as DSS, EJIS, DSJ, EJOR, \\nJAMIA, etc. He has also published 5 edited research books. Dr. Gupta's research has been \\nsupported by various grant agencies such as THEC, DHS, NSF, DOD, etc.  \\nDr. Fadel M. Megahed is the Neil R. Anderson Endowed Assistant Professor in the \\nFarmer School of Business at Miami University. His current research focuses on creating \\nnew tools to store, organize, analyze, model, and visualize the large heterogeneous data \\nsets associated with modern manufacturing, healthcare and service environments.  \\nH. Ahady Dolatsara, et al.   \\nDecision Support Systems 137 (2020) 113363\\n15\\n\"}, {'source': '/Users/sir/Downloads/Data/PDF/Problem identification in data science projects | by Guy Maskall | Towards Data Science.pdf', 'text': 'Problem identification in data\\nscience projects\\nThree questions to ask yourself to achieve SMART success.\\nGuy Maskall · Follow\\nPublished in Towards Data Science · 3 min read · Jul 15, 2020\\n65\\nImage by Gerd Altmann from Pixabay\\nSolve the right problem!\\nMany of you will be familiar with Kaggle, the home of machine learning\\ncompetitions. It’s the place you can can go to hone your data science skills,\\nright? Wrong. It’s the place you can go to hone your machine learning skills.\\nAt least that’s how it used to be. There’d be a training data set complete with\\na column of labels and a test data set lacking said labels. You train and\\nimprove your best machine learning models, predict the labels on the test\\nset, submit, and wait and watch your name/team rise up the leader board!\\nWell sorry, that’s not data science. Not the full gamut of data science anyway.\\nIn fact, really just about the smallest part. A fun part, sure. A key part,\\narguably. But definitely not the be all and end all. It counts for nothing if\\nyour best machine learning model predicts the wrong thing, or simply\\nsomething not directly useful to the business. Indeed, it’s possible to identify\\na problem (or, perhaps rather, a metric) that’s positively bad for your\\nbusiness, as Microsoft’s Bing team found out (eventually). Your business\\nproblem is to build out a great search engine. A great search engine is one\\nthat users want to use more often, right? So you correctly identify the\\nbusiness problem to be one of driving up the number of searches that users\\nmake, right? Nope. Your correct problem identification for a search engine is\\nto optimize the finding, not the searching!\\nWith that example above, and I do encourage you to read the linked blog, I’m\\nalmost tempted to end this article here, because it’s such a great lesson that\\nsuccinctly encapsulates the key message. Identifying the wrong problem\\nreally will set you off in the wrong direction, and any subsequent attempts to\\noptimize will simply get you more wrong faster. But, salutary war stories\\naside, what constructive techniques are there to guide you towards success?\\nBe SMART!\\nA good checklist to run through, when drafting your problem statement, is\\nSMART:\\nSpecific, not general\\nMeasurable\\nAction-oriented\\nRelevant (to the key problem)\\nTime-bound\\nIf you find yourself writing a problem statement with phrases such as\\n“improve sales” or “reduce churn”, you should catch yourself and review the\\nSMART checklist. Improve by how much? By when? The end of the second\\nquarter? Third? Next year? What is “sales” — units sold or revenue? What is\\n“churn”? When the team managing “pay as you go” customers look at\\ncustomers who left their system, should it really be called churn if a\\ncustomer upgrades to a 12 month contract with your company (and so just\\nmoves onto another team’s books)? Perhaps your company would actually\\nlike more of that sort of thing!\\nIs SMART enough? Arguably, the Bing team, as highlighted above, thought\\nthey were being SMART. Unfortunately, they’d picked the wrong “specific”.\\nYou can start to deconstruct a SMART problem statement. You can start to\\ntalk about business context and stakeholders and constraints and risks and…\\nIf you’re new to this and your head is starting to spin, I can’t blame you.\\nWhen you feel there are too many factors to think about, I offer you three\\nsimple questions that will help orient you:\\n1. who is it for?\\n2. what do they want?\\n3. how will you know when you’ve got what they want?\\nFor all the fancy talk of SMART measures and context and stakeholders, this\\nis what it’s about. That and “keeping it real”. There’s little point delivering a\\ngreat solution a week after it was needed — would a good-enough solution\\ndelivered in time be better? Did you really think things through when you\\ncreated a model to predict customer churn a month in advance using\\nfeatures that include information only known a week in advance?\\nUnderstanding what is wanted, and how you’re going to measure your\\nsolution in the context of delivering that want, and in a way that is realistic,\\nreally is the key to setting up a data science project for success.\\nAbout this article\\nThis is the second article of a linked series written to provide a\\nstraightforward introduction to getting started with the data science process.\\nYou can find the introduction here, and the next article in the series here.\\n65\\nWritten by Guy Maskall\\n86 Followers · Writer for Towards Data Science\\nHead of data science at Rezatec. My views are my own, but you’re welcome to them too.\\nFollow\\nMore from Guy Maskall and Towards Data Science\\nSee all from Guy Maskall\\nSee all from Towards Data Science\\nRecommended from Medium\\nLists\\nPredictive Modeling w/\\nPython\\n20 stories · 1323 saves\\nPractical Guides to Machine\\nLearning\\n10 stories · 1592 saves\\nCoding & Development\\n11 stories · 671 saves\\nChatGPT prompts\\n48 stories · 1718 saves\\nSee more recommendations\\nHelp\\nStatus\\nAbout\\nCareers\\nPress\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams\\nData Science\\nin\\nThe Data Science Method (DSM)\\nIntroduction\\nJul 15, 2020\\nin\\nWhat 10 Years at Uber, Meta and\\nStartups Taught Me About Data…\\nAdvice for Data Scientists and Managers\\nMay 30\\nin\\nHow I Use ChatGPT As A Data\\nScientist\\nHow ChatGPT improved my productivity as a\\ndata scientist\\nJun 2\\nin\\nPre-processing and training data\\nPre-processing\\nJul 15, 2020\\nin\\nThe resume that got a software\\nengineer a $300,000 job at Google.\\n1-page. Well-formatted.\\nJun 1\\nin\\nWhat Happens When You Start\\nReading Every Day\\nThink before you speak. Read before you\\nthink.\\u200a—\\u200aFran Lebowitz\\nMar 11\\nin\\nShould You Join FAANG or a\\nStartup as a Data Scientist?\\nLessons from working at Uber + Meta, a\\ngrowth stage company and a tiny startup\\nJun 20\\nin\\nI Did Yale’s Happiness Challenge\\nfor 28 Days To Improve My Mental…\\nA comprehensive overview of my experience\\nwith this Yale-inspired challenge, including…\\nJun 19\\nin\\nLinear Algebra Concepts Every\\nData Scientist Should Know\\nDo you know Linear Algebra well enough?\\nJun 18\\nin\\n4 Must Read Python Books To\\nBoost Your Skills By 10000%\\nStart learning Python easily, fast, and while\\nhaving fun\\nMay 9\\nGuy Maskall\\nTowards Data Science\\n52\\nTorsten Walbaum\\nTowards Data Science\\n5.4K\\n83\\nEgor Howell\\nTowards Data Science\\n654\\n21\\nGuy Maskall\\nTowards Data Science\\n86\\nAlexander Nguyen\\nLevel Up Coding\\n9.2K\\n111\\nSufyan Maan, M.Eng\\nILLUMINATION\\n23K\\n503\\nTorsten Walbaum\\nTowards Data Science\\n556\\n10\\nAlexa V.S.\\nIn Fitness And In Health\\n1.6K\\n27\\nBenedict Neo\\nbitgrit Data Science Publication\\n474\\n2\\nAxel Casas, PhD Candid…\\nPython in Plain Engli…\\n1.5K\\n9\\n'}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/Problem identification in data science projects | by Guy Maskall | Towards Data Science.pdf', 'text': 'Problem identification in data\\nscience projects\\nThree questions to ask yourself to achieve SMART success.\\nGuy Maskall · Follow\\nPublished in Towards Data Science · 3 min read · Jul 15, 2020\\n65\\nImage by Gerd Altmann from Pixabay\\nSolve the right problem!\\nMany of you will be familiar with Kaggle, the home of machine learning\\ncompetitions. It’s the place you can can go to hone your data science skills,\\nright? Wrong. It’s the place you can go to hone your machine learning skills.\\nAt least that’s how it used to be. There’d be a training data set complete with\\na column of labels and a test data set lacking said labels. You train and\\nimprove your best machine learning models, predict the labels on the test\\nset, submit, and wait and watch your name/team rise up the leader board!\\nWell sorry, that’s not data science. Not the full gamut of data science anyway.\\nIn fact, really just about the smallest part. A fun part, sure. A key part,\\narguably. But definitely not the be all and end all. It counts for nothing if\\nyour best machine learning model predicts the wrong thing, or simply\\nsomething not directly useful to the business. Indeed, it’s possible to identify\\na problem (or, perhaps rather, a metric) that’s positively bad for your\\nbusiness, as Microsoft’s Bing team found out (eventually). Your business\\nproblem is to build out a great search engine. A great search engine is one\\nthat users want to use more often, right? So you correctly identify the\\nbusiness problem to be one of driving up the number of searches that users\\nmake, right? Nope. Your correct problem identification for a search engine is\\nto optimize the finding, not the searching!\\nWith that example above, and I do encourage you to read the linked blog, I’m\\nalmost tempted to end this article here, because it’s such a great lesson that\\nsuccinctly encapsulates the key message. Identifying the wrong problem\\nreally will set you off in the wrong direction, and any subsequent attempts to\\noptimize will simply get you more wrong faster. But, salutary war stories\\naside, what constructive techniques are there to guide you towards success?\\nBe SMART!\\nA good checklist to run through, when drafting your problem statement, is\\nSMART:\\nSpecific, not general\\nMeasurable\\nAction-oriented\\nRelevant (to the key problem)\\nTime-bound\\nIf you find yourself writing a problem statement with phrases such as\\n“improve sales” or “reduce churn”, you should catch yourself and review the\\nSMART checklist. Improve by how much? By when? The end of the second\\nquarter? Third? Next year? What is “sales” — units sold or revenue? What is\\n“churn”? When the team managing “pay as you go” customers look at\\ncustomers who left their system, should it really be called churn if a\\ncustomer upgrades to a 12 month contract with your company (and so just\\nmoves onto another team’s books)? Perhaps your company would actually\\nlike more of that sort of thing!\\nIs SMART enough? Arguably, the Bing team, as highlighted above, thought\\nthey were being SMART. Unfortunately, they’d picked the wrong “specific”.\\nYou can start to deconstruct a SMART problem statement. You can start to\\ntalk about business context and stakeholders and constraints and risks and…\\nIf you’re new to this and your head is starting to spin, I can’t blame you.\\nWhen you feel there are too many factors to think about, I offer you three\\nsimple questions that will help orient you:\\n1. who is it for?\\n2. what do they want?\\n3. how will you know when you’ve got what they want?\\nFor all the fancy talk of SMART measures and context and stakeholders, this\\nis what it’s about. That and “keeping it real”. There’s little point delivering a\\ngreat solution a week after it was needed — would a good-enough solution\\ndelivered in time be better? Did you really think things through when you\\ncreated a model to predict customer churn a month in advance using\\nfeatures that include information only known a week in advance?\\nUnderstanding what is wanted, and how you’re going to measure your\\nsolution in the context of delivering that want, and in a way that is realistic,\\nreally is the key to setting up a data science project for success.\\nAbout this article\\nThis is the second article of a linked series written to provide a\\nstraightforward introduction to getting started with the data science process.\\nYou can find the introduction here, and the next article in the series here.\\n65\\nWritten by Guy Maskall\\n86 Followers · Writer for Towards Data Science\\nHead of data science at Rezatec. My views are my own, but you’re welcome to them too.\\nFollow\\nMore from Guy Maskall and Towards Data Science\\nSee all from Guy Maskall\\nSee all from Towards Data Science\\nRecommended from Medium\\nLists\\nPredictive Modeling w/\\nPython\\n20 stories · 1323 saves\\nPractical Guides to Machine\\nLearning\\n10 stories · 1592 saves\\nCoding & Development\\n11 stories · 671 saves\\nChatGPT prompts\\n48 stories · 1718 saves\\nSee more recommendations\\nHelp\\nStatus\\nAbout\\nCareers\\nPress\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams\\nData Science\\nin\\nThe Data Science Method (DSM)\\nIntroduction\\nJul 15, 2020\\nin\\nWhat 10 Years at Uber, Meta and\\nStartups Taught Me About Data…\\nAdvice for Data Scientists and Managers\\nMay 30\\nin\\nHow I Use ChatGPT As A Data\\nScientist\\nHow ChatGPT improved my productivity as a\\ndata scientist\\nJun 2\\nin\\nPre-processing and training data\\nPre-processing\\nJul 15, 2020\\nin\\nThe resume that got a software\\nengineer a $300,000 job at Google.\\n1-page. Well-formatted.\\nJun 1\\nin\\nWhat Happens When You Start\\nReading Every Day\\nThink before you speak. Read before you\\nthink.\\u200a—\\u200aFran Lebowitz\\nMar 11\\nin\\nShould You Join FAANG or a\\nStartup as a Data Scientist?\\nLessons from working at Uber + Meta, a\\ngrowth stage company and a tiny startup\\nJun 20\\nin\\nI Did Yale’s Happiness Challenge\\nfor 28 Days To Improve My Mental…\\nA comprehensive overview of my experience\\nwith this Yale-inspired challenge, including…\\nJun 19\\nin\\nLinear Algebra Concepts Every\\nData Scientist Should Know\\nDo you know Linear Algebra well enough?\\nJun 18\\nin\\n4 Must Read Python Books To\\nBoost Your Skills By 10000%\\nStart learning Python easily, fast, and while\\nhaving fun\\nMay 9\\nGuy Maskall\\nTowards Data Science\\n52\\nTorsten Walbaum\\nTowards Data Science\\n5.4K\\n83\\nEgor Howell\\nTowards Data Science\\n654\\n21\\nGuy Maskall\\nTowards Data Science\\n86\\nAlexander Nguyen\\nLevel Up Coding\\n9.2K\\n111\\nSufyan Maan, M.Eng\\nILLUMINATION\\n23K\\n503\\nTorsten Walbaum\\nTowards Data Science\\n556\\n10\\nAlexa V.S.\\nIn Fitness And In Health\\n1.6K\\n27\\nBenedict Neo\\nbitgrit Data Science Publication\\n474\\n2\\nAxel Casas, PhD Candid…\\nPython in Plain Engli…\\n1.5K\\n9\\n'}, {'source': '/Users/sir/Downloads/Data/PDF/Editorial-Board_2010_Artificial-Intelligence-in-Medicine.pdf', 'text': 'AIIM\\nAims and scope\\nArtificial Intelligence in Medicine is an international journal and publishes original articles from a variety of interdisciplinary perspectives concerning the theory and\\npractice of artificial intelligence (AI) in medicine, human biology, and health care.\\nAreas of particular theoretical interest include: knowledge representation, automated reasoning, intelligent communication, computational theories of learning, as\\nwell as signal, image, speech and natural language understanding. The theory, engineering, and practice of computational, knowledge-based, and agent-based\\nintelligent systems in clinical medicine, biomedicine, and health care and of software intelligence built into medical instruments, equipment, robotic or prosthetic\\ndevices are of special practical interest. Further topics are methodological, philosophical, ethical, psychological, and social aspects of medical AI.\\nThe journal features original research contributions, tutorials on new methods, research area reviews and bibliographies, editorials, book reviews, and letters to the\\neditor.\\nEditor-in-Chief\\nKLAUS-PETER ADLASSNIG, Section for Medical Expert and Knowledge-Based Systems, Center for Medical Statistics, Informatics, and Intelligent Systems, Medical\\nUniversity of Vienna, Spitalgasse 23, A-1090 Vienna, Austria. E-mail: klaus-peter.adlassnig@meduniwien.ac.at\\nEditorial office\\nANDREA RAPPELSBERGER, Section for Medical Expert and Knowledge-Based Systems, Center for Medical Statistics, Informatics, and Intelligent Systems, Medical\\nUniversity of Vienna, Spitalgasse 23, A-1090 Vienna, Austria. Tel.: +43-1-40400-6668; Fax: +43-1-40400-6625; E-mail: aiim-office@meduniwien.ac.at\\nFounding editor\\nKAZEM SADEGH-ZADEH, Theory of Medicine Department, University of Münster Clinicum, Münster, Germany.\\nAssociate\\neditors\\nE. COIERA\\nCentre for Health Informatics\\nThe University of New\\nSouth Wales\\nSydney 2055, Australia\\newc@pobox.com\\nE. KERAVNOU-PAPAILIOU\\nDepartment of\\nComputer Science\\nUniversity of Cyprus\\n75 Kallipoleos Str.\\nP.O. Box 20537\\nCY-1678 Nicosia, Cyprus\\nelpida@ucy.ac.cy\\nC.A. KULIKOWSKI\\nDepartment of\\nComputer Science\\nRutgers - The State\\nUniversity of New Jersey\\nHill Center, Busch Campus\\nNew Brunswick, New Jersey\\n08903, USA\\nkulikows@cs.rutgers.edu\\nT.Y. LEONG\\nDepartment of\\nComputer Science\\nSchool of Computing\\nNational University of\\nSingapore\\n3 Science Drive 2\\nSingapore 117543\\nRepublic of Singapore\\nleongty@comp.nus.edu.sg\\nBook review\\neditors\\nP.J.F. LUCAS\\nInstitute for Computer\\nand Information Science\\nUniversity of Nijmegen\\nToernooiveld 1, 6525\\nED Nijmegen\\nThe Netherlands\\npeterl@cs.kun.nl\\nS. MIKSCH\\nDepartment of Information\\nand Knowledge Engineering\\nDanube University Krems\\nDr.-Karl-Dorrek-Straße 30\\nA-3500 Krems, Austria\\nsilvia.miksch@donau-uni.ac.at\\nEditorial\\nboard\\nB.G. BUCHANAN\\nDepartment of\\nComputer Science\\nUniversity of Pittsburgh\\n206 Mineral Industries\\nBuilding\\nPittsburgh, PA 15260, USA\\nbuchanan@cs.pitt.edu\\nC. COMBI\\nDepartment of\\nComputer Science\\nUniversity of Verona\\nCa’ Vignal 2\\nStrada le Grazie 15\\nI-37134 Verona, Italy\\ncombi@sci.univr.it\\nG.F. COOPER\\nCenter for Biomedical\\nInformatics\\nSuite 8084, Forbes Tower\\nUniversity of Pittsburgh\\nPittsburgh, PA 15213-2582\\ngfc@cbmi.upmc.edu\\nM. DOJAT\\nGrenoble Institut des\\nNeurosciences (GIN)\\nCentre de Recherche\\nInserm U836-UJF-CEA-CHU\\nUniversité Joseph\\nFourier - Site Santé\\nBP170, F38042 Grenoble\\nCedex 9, France\\nmichel.dojat@ujf-grenoble.fr\\nG. DORFFNER\\nDepartment of Medical\\nCybernetics and Artificial\\nIntelligence\\nCenter for Brain\\nResearch\\nMedical University of\\nVienna\\nFreyung 6/2\\nA-1010 Vienna, Austria\\ngeorg.dorffner@meduniwien.\\nac.at\\nP. EKLUND\\nDepartment of\\nComputing Science\\nUmeå University\\nSE-90187 Umeå, Sweden\\npeklund@cs.umu.se\\nC. GARBAY\\nLaboratoire TIMC-IMAG\\nInstitut Albert Bonniot\\nFaculté de Médicine\\nDomaine de la Merci\\n38706 La Tronche Cedex\\nFrance\\ncatherine.garbay@imag.fr\\nD.A. GIUSE\\nDepartment of\\nBiomedical Informatics\\nVanderbilt University\\nEskind Biomedical Library\\n2209 Garland Avenue\\nNashville\\nTN 37232-8340\\ndario.giuse@vanderbilt.edu\\nR. HAUX\\nInstitute for Medical\\nInformatics\\nTechnical University of\\nBraunschweig\\nMuehlenpfordtstraße 23\\nD-38106 Braunschweig,\\nGermany\\nr.haux@mi.tu-bs.de\\nY. HAYASHI\\nDepartment of\\nComputer Science\\nMeiji University\\n1-1-1 Higashimita, Tamu-ku\\nKawasaki 214-8571, Japan\\nhayashiy@cs.meiji.ac.jp\\nW. HORN\\nDepartment of Medical\\nCybernetics and Artificial\\nIntelligence\\nCenter for Brain Research\\nMedical University of Vienna\\nFreyung 6/2\\nA-1010 Vienna, Austria\\nwerner.horn@meduniwien.\\nac.at\\nJ. HUNTER\\nDepartment of\\nComputing Science\\nUniversity of Aberdeen\\nKing’s College\\nAberdeen AB 24 3UE\\nUnited Kingdom\\njhunter@csd.abdn.ac.uk\\nN. LAVRAC\\nDepartment of Intelligent\\nSystems\\nJozef Stefan Institute\\nJamova 39, 1000 Ljubljana\\nSlovenia\\nnada.lavrac@ijs.si\\nP. MILLER\\nCenter for Medical\\nInformatics\\nYale University School of\\nMedicine\\n333 Cedar Street\\nPO Box 208009\\nNew Haven, CT 06520-8009\\nUSA\\nperry.miller@yale.edu\\nM.A. MUSEN\\nStanford Medical\\nInformatics\\nStanford University\\nStanford, CA 94305-5479\\nUSA\\nmusen@smi.stanford.edu\\nV.L. PATEL\\nDepartments of Medical\\nInformatics and Psychiatry\\nColumbia University\\nVanderbilt Clinic, 5th Floor\\n622 West 168th Street\\nNew York, NY 10032-3720\\nUSA\\npatel@dmi.columbia.edu\\nR. RADA\\nDepartment of Information\\nSystems\\nUniversity of Maryland\\nBaltimore County\\n1000 Hilltop Circle\\nBaltimore, MD 21250, USA\\nrada@umbc.edu\\nA. RECTOR\\nMedical Informatics Group\\nDepartment of Computer\\nScience\\nUniversity of Manchester\\nOxford Road\\nManchester, M13 9PL\\nUnited Kingdom\\nrector@cs.man.ac.uk\\nJ.A. REGGIA\\nDepartments of Computer\\nScience and Neurology\\nInstitute for Advanced\\nComputer Studies\\nUniversity of Maryland\\nA.V. Williams Building\\nCollege Park, MD 20742\\nUSA\\nreggia@cs.umd.edu\\nE. RUPPIN\\nSchool of Computer Science\\nand Sackler Faculty of\\nMedicine\\nTel Aviv University\\nTel Aviv 69978, Israel\\nruppin@math.tau.ac.il\\nE. SANCHEZ\\nUniv. Mediterranee\\nFaculte de Medecine\\nLab. d’Informatique\\nMedicale\\n27 Bd Jean Moulin\\n13385 Marseille Cedex 5\\nFrance\\nelie.sanchez@medecine.\\nuniv-mrs.fr\\nY. SHAHAR\\nMedical Informatics\\nResearch Center\\nDepartment of Information\\nSystems Engineering\\nBen-Gurion University of the\\nNegev\\nBeer-Sheva 84105, Israel\\nyshahar@bgumail.bgu.ac.il\\nE.H. SHORTLIFFE\\nSchool of Health\\nInformation Sciences \\nThe University of Texas\\nHealth Science Center \\n7000 Fannin\\nSuite 600\\nHouston, TX 77030 \\nedward.shortliffe@uth.tmc.\\nedu\\nC.D. SPYROPOULOS\\nSoftware and Knowledge\\nEngineering Laboratory\\nInstitute of Informatics and\\nTelecommunications\\nN.C.S.R. Demokritos\\n15310 Aghia Paraskevi\\nGreece\\ncostass@iit.demokritos.gr\\nM. STEFANELLI\\nDepartment of Computer\\nand Systems Sciences\\nLaboratory for Medical\\nInformatics\\nUniversity of Pavia\\nVia Ferrata 1\\nI-27100 Pavia, Italy\\nmstefa@aim.unipv.it\\nP.S. SZCZEPANIAK\\nInstitute of Computer\\nScience\\nTechnical University of Lodz\\nul. Wolczanska 215\\n93-005 Lodz, Poland\\noffice@ics.p.lodz.pl\\nT. TIMPKA\\nDepartment of Social\\nMedicine and Public Health\\nDepartment of Computer\\nScience\\nLinköping University\\nSE-581 83 Linköping\\nSweden\\ntti@ida.liu.se\\nT. WETTER\\nInstitute for Medical\\nBiometry and Informatics\\nDepartment of Medical\\nInformatics\\nUniversity of Heidelberg\\nIm Neuenheimer Feld 400\\nD-69120 Heidelberg\\nGermany\\nthomas_wetter@med.uni-hei -\\ndelberg.de\\nL.A. ZADEH\\nDepartment of Electrical\\nEngineering and Computer\\nSciences\\nComputer Science Division\\nUniversity of California\\nBerkeley, CA 94720-1776\\nUSA\\nzadeh@cs.berkeley.edu\\nZ.-H. ZHOU\\nNational Lab for Novel\\nSoftware Technology\\nNanjing University\\nHankou Road 22\\nNanjing 210093, P.R. China\\nzhouzh@nju.edu.cn\\ndoi:10.1016/S0933-3657(10)00042-4\\n'}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/Editorial-Board_2010_Artificial-Intelligence-in-Medicine.pdf', 'text': 'AIIM\\nAims and scope\\nArtificial Intelligence in Medicine is an international journal and publishes original articles from a variety of interdisciplinary perspectives concerning the theory and\\npractice of artificial intelligence (AI) in medicine, human biology, and health care.\\nAreas of particular theoretical interest include: knowledge representation, automated reasoning, intelligent communication, computational theories of learning, as\\nwell as signal, image, speech and natural language understanding. The theory, engineering, and practice of computational, knowledge-based, and agent-based\\nintelligent systems in clinical medicine, biomedicine, and health care and of software intelligence built into medical instruments, equipment, robotic or prosthetic\\ndevices are of special practical interest. Further topics are methodological, philosophical, ethical, psychological, and social aspects of medical AI.\\nThe journal features original research contributions, tutorials on new methods, research area reviews and bibliographies, editorials, book reviews, and letters to the\\neditor.\\nEditor-in-Chief\\nKLAUS-PETER ADLASSNIG, Section for Medical Expert and Knowledge-Based Systems, Center for Medical Statistics, Informatics, and Intelligent Systems, Medical\\nUniversity of Vienna, Spitalgasse 23, A-1090 Vienna, Austria. E-mail: klaus-peter.adlassnig@meduniwien.ac.at\\nEditorial office\\nANDREA RAPPELSBERGER, Section for Medical Expert and Knowledge-Based Systems, Center for Medical Statistics, Informatics, and Intelligent Systems, Medical\\nUniversity of Vienna, Spitalgasse 23, A-1090 Vienna, Austria. Tel.: +43-1-40400-6668; Fax: +43-1-40400-6625; E-mail: aiim-office@meduniwien.ac.at\\nFounding editor\\nKAZEM SADEGH-ZADEH, Theory of Medicine Department, University of Münster Clinicum, Münster, Germany.\\nAssociate\\neditors\\nE. COIERA\\nCentre for Health Informatics\\nThe University of New\\nSouth Wales\\nSydney 2055, Australia\\newc@pobox.com\\nE. KERAVNOU-PAPAILIOU\\nDepartment of\\nComputer Science\\nUniversity of Cyprus\\n75 Kallipoleos Str.\\nP.O. Box 20537\\nCY-1678 Nicosia, Cyprus\\nelpida@ucy.ac.cy\\nC.A. KULIKOWSKI\\nDepartment of\\nComputer Science\\nRutgers - The State\\nUniversity of New Jersey\\nHill Center, Busch Campus\\nNew Brunswick, New Jersey\\n08903, USA\\nkulikows@cs.rutgers.edu\\nT.Y. LEONG\\nDepartment of\\nComputer Science\\nSchool of Computing\\nNational University of\\nSingapore\\n3 Science Drive 2\\nSingapore 117543\\nRepublic of Singapore\\nleongty@comp.nus.edu.sg\\nBook review\\neditors\\nP.J.F. LUCAS\\nInstitute for Computer\\nand Information Science\\nUniversity of Nijmegen\\nToernooiveld 1, 6525\\nED Nijmegen\\nThe Netherlands\\npeterl@cs.kun.nl\\nS. MIKSCH\\nDepartment of Information\\nand Knowledge Engineering\\nDanube University Krems\\nDr.-Karl-Dorrek-Straße 30\\nA-3500 Krems, Austria\\nsilvia.miksch@donau-uni.ac.at\\nEditorial\\nboard\\nB.G. BUCHANAN\\nDepartment of\\nComputer Science\\nUniversity of Pittsburgh\\n206 Mineral Industries\\nBuilding\\nPittsburgh, PA 15260, USA\\nbuchanan@cs.pitt.edu\\nC. COMBI\\nDepartment of\\nComputer Science\\nUniversity of Verona\\nCa’ Vignal 2\\nStrada le Grazie 15\\nI-37134 Verona, Italy\\ncombi@sci.univr.it\\nG.F. COOPER\\nCenter for Biomedical\\nInformatics\\nSuite 8084, Forbes Tower\\nUniversity of Pittsburgh\\nPittsburgh, PA 15213-2582\\ngfc@cbmi.upmc.edu\\nM. DOJAT\\nGrenoble Institut des\\nNeurosciences (GIN)\\nCentre de Recherche\\nInserm U836-UJF-CEA-CHU\\nUniversité Joseph\\nFourier - Site Santé\\nBP170, F38042 Grenoble\\nCedex 9, France\\nmichel.dojat@ujf-grenoble.fr\\nG. DORFFNER\\nDepartment of Medical\\nCybernetics and Artificial\\nIntelligence\\nCenter for Brain\\nResearch\\nMedical University of\\nVienna\\nFreyung 6/2\\nA-1010 Vienna, Austria\\ngeorg.dorffner@meduniwien.\\nac.at\\nP. EKLUND\\nDepartment of\\nComputing Science\\nUmeå University\\nSE-90187 Umeå, Sweden\\npeklund@cs.umu.se\\nC. GARBAY\\nLaboratoire TIMC-IMAG\\nInstitut Albert Bonniot\\nFaculté de Médicine\\nDomaine de la Merci\\n38706 La Tronche Cedex\\nFrance\\ncatherine.garbay@imag.fr\\nD.A. GIUSE\\nDepartment of\\nBiomedical Informatics\\nVanderbilt University\\nEskind Biomedical Library\\n2209 Garland Avenue\\nNashville\\nTN 37232-8340\\ndario.giuse@vanderbilt.edu\\nR. HAUX\\nInstitute for Medical\\nInformatics\\nTechnical University of\\nBraunschweig\\nMuehlenpfordtstraße 23\\nD-38106 Braunschweig,\\nGermany\\nr.haux@mi.tu-bs.de\\nY. HAYASHI\\nDepartment of\\nComputer Science\\nMeiji University\\n1-1-1 Higashimita, Tamu-ku\\nKawasaki 214-8571, Japan\\nhayashiy@cs.meiji.ac.jp\\nW. HORN\\nDepartment of Medical\\nCybernetics and Artificial\\nIntelligence\\nCenter for Brain Research\\nMedical University of Vienna\\nFreyung 6/2\\nA-1010 Vienna, Austria\\nwerner.horn@meduniwien.\\nac.at\\nJ. HUNTER\\nDepartment of\\nComputing Science\\nUniversity of Aberdeen\\nKing’s College\\nAberdeen AB 24 3UE\\nUnited Kingdom\\njhunter@csd.abdn.ac.uk\\nN. LAVRAC\\nDepartment of Intelligent\\nSystems\\nJozef Stefan Institute\\nJamova 39, 1000 Ljubljana\\nSlovenia\\nnada.lavrac@ijs.si\\nP. MILLER\\nCenter for Medical\\nInformatics\\nYale University School of\\nMedicine\\n333 Cedar Street\\nPO Box 208009\\nNew Haven, CT 06520-8009\\nUSA\\nperry.miller@yale.edu\\nM.A. MUSEN\\nStanford Medical\\nInformatics\\nStanford University\\nStanford, CA 94305-5479\\nUSA\\nmusen@smi.stanford.edu\\nV.L. PATEL\\nDepartments of Medical\\nInformatics and Psychiatry\\nColumbia University\\nVanderbilt Clinic, 5th Floor\\n622 West 168th Street\\nNew York, NY 10032-3720\\nUSA\\npatel@dmi.columbia.edu\\nR. RADA\\nDepartment of Information\\nSystems\\nUniversity of Maryland\\nBaltimore County\\n1000 Hilltop Circle\\nBaltimore, MD 21250, USA\\nrada@umbc.edu\\nA. RECTOR\\nMedical Informatics Group\\nDepartment of Computer\\nScience\\nUniversity of Manchester\\nOxford Road\\nManchester, M13 9PL\\nUnited Kingdom\\nrector@cs.man.ac.uk\\nJ.A. REGGIA\\nDepartments of Computer\\nScience and Neurology\\nInstitute for Advanced\\nComputer Studies\\nUniversity of Maryland\\nA.V. Williams Building\\nCollege Park, MD 20742\\nUSA\\nreggia@cs.umd.edu\\nE. RUPPIN\\nSchool of Computer Science\\nand Sackler Faculty of\\nMedicine\\nTel Aviv University\\nTel Aviv 69978, Israel\\nruppin@math.tau.ac.il\\nE. SANCHEZ\\nUniv. Mediterranee\\nFaculte de Medecine\\nLab. d’Informatique\\nMedicale\\n27 Bd Jean Moulin\\n13385 Marseille Cedex 5\\nFrance\\nelie.sanchez@medecine.\\nuniv-mrs.fr\\nY. SHAHAR\\nMedical Informatics\\nResearch Center\\nDepartment of Information\\nSystems Engineering\\nBen-Gurion University of the\\nNegev\\nBeer-Sheva 84105, Israel\\nyshahar@bgumail.bgu.ac.il\\nE.H. SHORTLIFFE\\nSchool of Health\\nInformation Sciences \\nThe University of Texas\\nHealth Science Center \\n7000 Fannin\\nSuite 600\\nHouston, TX 77030 \\nedward.shortliffe@uth.tmc.\\nedu\\nC.D. SPYROPOULOS\\nSoftware and Knowledge\\nEngineering Laboratory\\nInstitute of Informatics and\\nTelecommunications\\nN.C.S.R. Demokritos\\n15310 Aghia Paraskevi\\nGreece\\ncostass@iit.demokritos.gr\\nM. STEFANELLI\\nDepartment of Computer\\nand Systems Sciences\\nLaboratory for Medical\\nInformatics\\nUniversity of Pavia\\nVia Ferrata 1\\nI-27100 Pavia, Italy\\nmstefa@aim.unipv.it\\nP.S. SZCZEPANIAK\\nInstitute of Computer\\nScience\\nTechnical University of Lodz\\nul. Wolczanska 215\\n93-005 Lodz, Poland\\noffice@ics.p.lodz.pl\\nT. TIMPKA\\nDepartment of Social\\nMedicine and Public Health\\nDepartment of Computer\\nScience\\nLinköping University\\nSE-581 83 Linköping\\nSweden\\ntti@ida.liu.se\\nT. WETTER\\nInstitute for Medical\\nBiometry and Informatics\\nDepartment of Medical\\nInformatics\\nUniversity of Heidelberg\\nIm Neuenheimer Feld 400\\nD-69120 Heidelberg\\nGermany\\nthomas_wetter@med.uni-hei -\\ndelberg.de\\nL.A. ZADEH\\nDepartment of Electrical\\nEngineering and Computer\\nSciences\\nComputer Science Division\\nUniversity of California\\nBerkeley, CA 94720-1776\\nUSA\\nzadeh@cs.berkeley.edu\\nZ.-H. ZHOU\\nNational Lab for Novel\\nSoftware Technology\\nNanjing University\\nHankou Road 22\\nNanjing 210093, P.R. China\\nzhouzh@nju.edu.cn\\ndoi:10.1016/S0933-3657(10)00042-4\\n'}, {'source': '/Users/sir/Downloads/Data/PDF/Intelligent-visualization-and-exploration-of-time-_2010_Artificial-Intellige.pdf', 'text': 'Intelligent visualization and exploration of time-oriented data of multiple\\npatients\\nDenis Klimov *, Yuval Shahar, Meirav Taieb-Maimon\\nDepartment of Information Systems Engineering, Faculty of Engineering Sciences, Ben-Gurion University of the Negev, P.O. Box 653, Beer-Sheva 84105, Israel\\n1. Introduction: intelligent visualization of time-oriented data\\nof multiple patients\\nA key task facing clinicians and medical researchers is the\\nanalysis of time-stamped, longitudinal medical records, parti-\\ncularly, records of multiple patients. This capability is necessary\\nto support, for example, quality assessment tasks, analysis of\\nclinical trials, and the discovery of new clinical knowledge.\\nAlthough the task of accessing patient data has been solved\\nmostly through the increasing use of electronic medical record\\n(EMR) systems, there still remains the task of intelligent\\nprocessing\\nof\\ntime-oriented\\nrecords\\nof\\nmultiple\\npatients,\\nincluding the capability for interactive exploration of the results.\\nFor this task, standard means, such as tables, time-oriented\\nstatistical tools, or even more advanced temporal data-mining\\ntechniques, are often not adequate, since their use may demand\\nArtiﬁcial Intelligence in Medicine 49 (2010) 11–31\\nA R T I C L E\\nI N F O\\nArticle history:\\nReceived 20 August 2008\\nReceived in revised form 31 January 2010\\nAccepted 16 February 2010\\nKeywords:\\nMultiple patients\\nIntelligent user interfaces\\nTemporal databases\\nTemporal abstraction\\nKnowledge-based systems\\nInformation visualization\\nInteractive data mining\\nOncology\\nA B S T R A C T\\nObjective: Clinicians and medical researchers alike require useful, intuitive, and intelligent tools to\\nprocess large amounts of time-oriented multiple-patient data from multiple sources. For analyzing the\\nresults of clinical trials or for quality assessment purposes, an aggregated view of a group of patients is\\noften required. To meet this need, we designed and developed the VISualizatIon of Time-Oriented RecordS\\n(VISITORS) system, which combines intelligent temporal analysis and information visualization\\ntechniques. The VISITORS system includes tools for intelligent retrieval, visualization, exploration, and\\nanalysis of raw time-oriented data and derived (abstracted) concepts for multiple patient records. To\\nderive meaningful interpretations from raw time-oriented data (known as temporal abstractions), we\\nused the knowledge-based temporal-abstraction method.\\nMethods: The main module of the VISITORS system is an interactive, ontology-based exploration\\nmodule, which enables the user to visualize raw data and abstract (derived) concepts for multiple patient\\nrecords, at several levels of temporal granularity; to explore these concepts; and to display associations\\namong raw and abstract concepts. A knowledge-based delegate function is used to convert multiple data\\npoints into one delegate value representing each temporal granule. To select the population of patients\\nto explore, the VISITORS system includes an ontology-based temporal-aggregation speciﬁcation\\nlanguage and a graphical expression-speciﬁcation module. The expressions, applied by an external\\ntemporal mediator, retrieve a list of patients, a list of relevant time intervals, and a list of time-oriented\\npatients’ data sets, by using an expressive set of time and value constraints.\\nResults: Functionality and usability evaluation of the interactive exploration module was performed on a\\ndatabase of more than 1000 oncology patients by a group of 10 users—ﬁve clinicians and ﬁve medical\\ninformaticians. Both types of users were able in a short time (mean of 2.5 \\x02 0.2 min per question) to\\nanswer a set of clinical questions, including questions that require the use of specialized operators for ﬁnding\\nassociations among derived temporal abstractions, with high accuracy (mean of 98.7 \\x02 2.4 on a predeﬁned\\nscale from 0 to 100). There were no signiﬁcant differences between the response times and between accuracy\\nlevels of the exploration of the data using different time lines, i.e., absolute (i.e., calendrical) versus relative\\n(referring to some clinical key event). A system usability scale (SUS) questionnaire ﬁlled out by the users\\ndemonstrated the VISITORS system to be usable (mean score for the overall group: 69.3), but the clinicians’\\nusability assessment was signiﬁcantly lower than that of the medical informaticians.\\nConclusions: We conclude that intelligent visualization and exploration of longitudinal data of multiple\\npatients with the VISITORS system is feasible, functional, and usable.\\n\\x02 2010 Elsevier B.V. All rights reserved.\\n* Corresponding author at: Medical Informatics Research Center, Department of\\nInformation Systems Engineering, Ben-Gurion University of the Negev, P.O. Box\\n653, Beer-Sheva 84105, Israel. Tel.: +972 8 6477160; fax: +972 8 6477161.\\nE-mail address: klimov@bgu.ac.il (D. Klimov).\\nContents lists available at ScienceDirect\\nArtificial Intelligence in Medicine\\njournal homepage: www.elsevier.com/locate/aiim\\n0933-3657/$ – see front matter \\x02 2010 Elsevier B.V. All rights reserved.\\ndoi:10.1016/j.artmed.2010.02.001\\n\\nspecialized, advanced knowledge, or they may be applicable only\\nin particular cases.\\nTo derive meaningful patterns and interpretations, known as\\ntemporal abstractions (or abstract concepts), from raw time-\\noriented patient data, we use a knowledge-based temporal-\\nabstraction\\n(KBTA)\\nmethod\\n[1].\\nThrough\\na\\ndomain-speciﬁc\\ntemporal-abstraction knowledge base acquired from a domain\\nexpert, this method derives interval-based temporal abstractions,\\nfor example, the pattern, ‘‘a period of more than two months of\\ngrade 1 or higher bone-marrow toxicity, followed within three\\nmonths by a decrease in liver-function’’ (these concepts are\\ndeﬁned in the context of a particular oncology therapy protocol).\\nThe temporal abstractions computed by the KBTA method for an\\nindividual patient or small number of patients can be efﬁciently\\nvisualized through an ontology-driven interface that we developed\\npreviously, known as KNAVE-II [2], which has been shown to be\\nfunctional and usable [3]. However, analysis of the data for large\\npatient\\npopulations,\\nsuch as\\nclinical\\ntrial\\nresults\\nor\\nquality\\nassessments of clinical-management processes, requires a new tool\\nthat provides aggregate views of time-oriented data and abstrac-\\ntions of groups of patient records. In addition, certain patterns can be\\ndiscovered only through the analysis of multiple patient records.\\nTherefore, as part of the current study, we designed and developed a\\ngreatly enhanced extension of the KNAVE-II system, designated the\\nVISualization of Time-Oriented RecordS (VISITORS) system, which\\ncombines intelligent temporal reasoning computational mechan-\\nisms with information visualization methods for display and\\nexploration of time-oriented records of multiple patients. Fig. 1\\npresents an overview of the main interface of raw and derived\\nconcepts and the semantics of VISITORS.\\nFurthermore, the VISITORS system also enables users to\\ninteractively specify temporal and knowledge-based constraints,\\nthrough\\na\\ngraphical\\nexpression-speciﬁcation\\nmodule,\\nwhich\\nenables users to deﬁne the patient subsets selected for exploration\\n(e.g., the lists of patients displayed in panel A of Fig. 1). Underlying\\nthe expression-speciﬁcation module is the ontology-based tem-\\nporal-aggregation (OBTAIN) speciﬁcation language, which includes\\na set of operators and constraints that enable unsophisticated\\nusers to graphically construct (i.e., to specify) three types of\\nexpression: Select Patients (Who?), Select Time Intervals (When?)\\nand Get Patient Data (What?). The goal of these expressions is to\\nretrieve a list of patients, a list of relevant time intervals, or a list of\\ntime-oriented patient data sets, respectively; combinations of\\nthese lists may be manipulated by the user according to the\\npatients, time periods and/or data values that are to be analyzed\\nfurther. A full exposition of the OBTAIN language and the graphical\\nexpression-speciﬁcation module is beyond the scope of this study\\nand can be found elsewhere [4].\\nAggregation of the longitudinal data of raw and abstract\\nconcepts of a group of patients is another unique aspect of the\\nVISITORS system. We have deﬁned and implemented in VISITORS\\nthe ‘‘delegate value’’ method, namely, given a single patient’s time-\\noriented data for a speciﬁc concept (raw or abstract) over a speciﬁc\\ntime interval (including a predeﬁned granularity level), we\\ncalculate the delegate value of an individual patient’s data at each\\ntime granule (or for another speciﬁc time period) using a function\\nspeciﬁc to the concept and the temporal granularity. For example,\\nassume that on 1 January 2007 there were three Platelet values for\\na particular patient: 17,700 cells/ml at 5 a.m., 38,900 cells/ml at 11\\na.m., and 43,250 cells/ml at 8 p.m. Thus, if we select the mean as the\\nFig. 1. The VISITORS main interface, this case in the hematological oncology domain. The two top panels display lists of patients (denoted by A) and lists of time intervals\\n(denoted by B), retrieved by computing the previous population-expressions. The graphs (denoted by C) show the data for a group of 58 patients for the White blood cell\\n(WBC) count raw concept (graph 1) and for the monthly distribution of the values of the Platelet-state abstract concept during 1995 (graph 2). Graph 3 shows the monthly\\ndistribution of the values of the Hemoglobin (HGB)-state concept during the ﬁrst year (relative time line) following BMT (see Sections 4.4 and 4.5 further details of the\\nvisualization and exploration operators). The left panel (denoted by D) of the interface includes a knowledge-based browser showing the domain’s ontology.\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n12\\n\\ndelegate (representative) function, then the daily average Platelet\\ncount for that patient was 33,283 cells/ml. However, during the\\nexploration time, the user can choose any other suitable delegate\\nfunction (such as mode or maximum).\\nThe population delegate value represents the aggregated value of\\na group of patients calculated by the concept- and the temporal\\ngranularity-speciﬁc statistical function within a speciﬁc time\\ngranule (or for another speciﬁc time period), e.g., the maximal\\nvalue of the raw Hemoglobin values for a group of patients over an\\nentire month. This function can also be manipulated at will.\\nThe following features distinguish the VISITORS system from\\nother data exploration tools:\\n\\x03 The system provides a 5-step iterative loop for intelligent\\ninvestigation of multiple time-oriented patient records: (1)\\nspeciﬁcation, (2) retrieval, (3) visualization, (4) interactive\\nexploration, and (5) knowledge-based temporal analysis.\\n\\x03 Time-oriented data are graphically displayed and explored in an\\nintuitively similar fashion for both individual and multiple patient\\nrecords.\\n\\x03 Particular consideration is given to the temporal aspect of the\\nconceptual and graphical representations: The data can be\\naggregated at and explored within various temporal granula-\\nrities, such as hour, day, and month. Support is also provided for a\\ncalendrical (absolute) timeline and for a timeline relative to a\\nspecial event [e.g., the months following a bone-marrow\\ntransplantation (BMT) event], or to another clinically signiﬁcant\\ntime point (e.g., start of high fever).\\n\\x03 The computational reasoning supports not only a view of raw\\ntime-oriented data and its statistics but also a meaningful\\ndisplay of various knowledge-based interpretations of the raw\\ndata, based on temporal-abstraction domain ontology, the KBTA\\ncomputational\\nmechanisms,\\nand\\nspecialized\\ntime-oriented\\naggregation operators. The exploration interface is also based\\non the same ontology, which supports a semantic exploration of\\nthe data (using semantic relations such as ‘‘derived from’’ or\\n‘‘part of’’) and enables navigation of semantically related raw and\\nabstract concepts.\\nThe ﬁrst two steps of the iterative loop, i.e., speciﬁcation and\\nretrieval of the multiple time-oriented patient records, are\\ndescribed in detail in another study [4]. In the current study, we\\nfocus on the other three capabilities: visualization, interactive\\nexploration, and knowledge-based temporal analysis of multiple\\npatient records.\\nThe main contributions of the current study lie in: (1) providing\\nformal deﬁnitions for the operators that perform visualization,\\nexploration, and knowledge-based temporal analysis of long-\\nitudinal data for multiple patients; (2) implementation of a\\ncomplete architecture developed using formal deﬁnitions; and (3)\\na functionality and usability evaluation of the implemented\\nsystem.\\n2. Related work\\n2.1. Combining domain knowledge, temporal abstraction and\\ninformation visualization in medical informatics\\nThe use of a domain knowledge base can both support an in-\\ndepth analysis of longitudinal patient records and simplify and\\nfacilitate the data exploration process, since the user can explore\\nonly high-level concepts based on complex temporal patterns (or,\\nin general, on any abstract concepts) previously deﬁned in a\\ndomain-speciﬁc knowledge base and detected in the patients’ data.\\nIn addition, it has been demonstrated that visual representation\\ncan often communicate information much more rapidly and\\neffectively than any other method [5]. Thus, the combination of\\nthese two approaches could signiﬁcantly improve the exploration\\nof patient data, as has been suggested by a clinical survey [6].\\nIn the current study, using a domain-speciﬁc knowledge-base,\\nwe applied the KBTA method [1] for automated derivation of\\nmeaningful\\ncontext-speciﬁc\\ninterpretations\\nand\\nconclusions\\n(temporal abstractions) from raw time-oriented patient data. In\\ngeneral, the KBTA method may be described as follows: the input\\nincludes a set of time-stamped measurable concepts (e.g., Platelet\\ncount, Red blood cell count) and external events (e.g., bone-\\nmarrow transplantation). The events typically create the necessary\\ninterpretation contexts (e.g., the therapy protocol used), which\\ncould change the interpretations of the data. The output includes a\\nset of interval-based, context-speciﬁc concepts at the same or a\\nhigher level of abstraction and their respective values (e.g., a period\\nof two months of grade 1 bone-marrow toxicity in the context of\\nparticular chemotherapy protocol).\\nOther\\nstudies\\nhave\\ninvestigated\\ndifferent\\ntechniques\\nfor\\ntemporal abstraction. Silvent et al. [7], for example, proposed\\ncombining temporal data abstraction techniques with data mining\\napproaches, a method that would update the prior domain\\nknowledge. The temporal-abstraction mechanisms put forward\\nby Miksch et al. [8] do not require predeﬁned domain knowledge\\nand\\ncan\\nprocess\\nhigh-frequency\\ntemporal\\nquantitative\\nand\\nqualitative data. Another method for calculation of temporal\\nabstractions has been applied to ECG data by using a fast Fourier\\ntransform [9,10]. In that method, a curve made up of a series of data\\npoints was transformed to a set of bends and lines in between these\\ndata points. However, the above frameworks concentrate on the\\nmethodology\\nof\\nthe\\ntemporal\\nabstractions\\nrather\\nthan\\non\\nvisualization and exploration issues. Moreover, the interfaces\\nused do not fulﬁll most of the desiderata deﬁned below (in Section\\n3) for exploration of longitudinal patient data.\\nUnlike the KBTA method, in which the domain expert deﬁnes\\nthe temporal patterns for temporal abstraction derivations, Combi\\nand Chittaro [11] used, as the temporal ontology, an object-\\noriented model based on the calculus of events [12]. They validated\\ntheir approach by building the CARDIOlogic Temporal Abstraction\\nSystem (CARDIOTABS) for the abstraction of temporal cardiology\\ndata, which supports nonvisual construction of queries regarding\\npatient data and very simple graphical interfaces for patient data\\nvisualization. They did not, however, evaluate the usability of the\\ninterfaces.\\nTo date, most visual exploration systems in medicine have\\nfocused solely on the visualization and exploration of static raw\\npatient data, as reviewed by Chittaro [13]. The LifeLines project\\n[14,15], for example, provides a general visualization environment\\nfor personal histories, which can be applied to medical and court\\nrecords, professional histories and other types of biographical data.\\nThe extended version, LifeLine2 [16], includes the option of\\naligning the patient data according to key medical events, i.e., a\\nrelative timeline view.\\nIn the area of visualization of data for multiple patients, a\\nnumber of systems have been developed. The InfoZoom system\\n[17] uses a novel technique to display data sets as a highly\\ncompressed table that always ﬁts completely onto the screen. The\\noverall goal of the MedView project [18] is to develop models,\\nmethods and tools to support clinicians in their daily diagnostic\\nwork. As part of MedView, two information visualization tools\\nwere developed and tested as a solution to the problem of\\nvisualizing insights derived from large amounts of clinical data.\\nThe ﬁrst tool, The Cube, enables interactive recognition of patient\\npatterns through a 3D display of a set of 2D parallel diagrams (each\\nusing a horizontal time axis and a vertical value axis), where each\\ndiagram represents a single patient attribute (e.g., diagnoses,\\nallergies). Thus, patients who have similar values for several\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n13\\n\\nattributes might have parallel lines connecting the different 2D\\ndiagrams. The second tool, SimVis, is based on a similarity\\nassessment-based interaction model for exploring data; the tool\\nwas designed to help clinicians to classify and cluster clinical-test\\ndata.A third system, the Interactive Parallel Bar Charts(IPBC) system\\n[19] adopts 3D bar charts as its basic visualization technique and\\naugments them with several interactive features, thereby exploiting\\n3D space to signiﬁcantly increase both the number of time-series\\nthat can be simultaneously analyzed in a convenient way and the\\nnumber of values associated with each series.\\nAs noted earlier, the above exploration systems focus mostly on\\nthe visualization of raw longitudinal data or only partially support\\nhigh-level meaningful interpretations of these data as abstract\\nclinical concepts. There is, however, a growing realization of the\\nimportance of integration of the visual, analytical and user-centered\\nmethods, as is shown in the review of Aigner et al. [20] of different\\nvisualizations of time-oriented data, in which the exploration task is\\nsolved by providing complex integrated interfaces. However, we\\nview the integration somewhat differently: the visualization must\\nbe simple and must represent the results of a complex temporal\\nreasoning computational process, namely, various types of users\\nmust be able to explore the data in an equally efﬁcient fashion.\\nMoreover, such integration must be modular, accessible, and\\nindependent of a particular medical domain.\\nTo address the issues discussed above, we devoted the current\\nstudy to extending our previously developed KNAVE-II system [2]\\nfor visualization and exploration of individual patient data to\\nsupport the visualization and exploration of the time-oriented\\nrecords of multiple patients. This effort required signiﬁcant\\nmodiﬁcation of the original tools, as explained in the Methods\\nsection below.\\n2.2. Other temporal aspects relevant to the study\\n2.2.1. Temporal and visual-temporal query languages\\nRecall that the OBTAIN speciﬁcation language and its graphical\\nexpression-speciﬁcation tool are used by end-users to specify the\\nrelevant patients or time periods to be retrieved and explored.\\nAlthough the OBTAIN speciﬁcation language is not intended to be,\\nin any way, a general temporal query language, the OBTAIN\\nexpressions speciﬁed by users could be considered as types of\\nqueries, since their goal is to retrieve a set of patients, time-\\nintervals, or time-oriented data. Thus, it is relevant to compare the\\nOBTAIN language to existing temporal (visual) query languages.\\nThe main difference between the OBTAIN speciﬁcation lan-\\nguage and the previous pure time-oriented query languages, such\\nas TSQL [21], HSQL [22], TSQL2 [23], GCH-OSQL [24–26], TLSQL\\n[27] and most recently t4sql [28,29], is that according to these\\nlanguages SQL queries are applied directly to raw-data databases;\\nthus, the expressivity of the language is limited by the pure SQL\\nand by the temporal-extension capabilities of these languages. In\\ncontrast, a broad set of temporal [knowledge-based] constraints\\nfor speciﬁcation of either patients or time intervals can be used in\\nOBTAIN expressions, since to answer the OBTAIN expressions,\\nadditional computational modules (such as the module for\\ncalculation of delegate values that is described in the Methods\\nsection), and a runtime capability for computation, on the ﬂy, of\\nknowledge-based temporal abstractions, are provided. Thus, on\\none hand, the semantic expressivity of the OBTAIN language,\\naccording to our desiderata, is enhanced relative to standard query\\nlanguages. On the other hand, recall that OBTAIN is not a general\\nquery language, thus, important features such as nested queries,\\njoin, and other query operators are not supported in the OBTAIN\\nlanguage, unlike most of the query languages mentioned above.\\nWith respect to visual temporal query languages, the TVQL [30],\\nTVQE [31], and MQuery [32] temporal visual query languages and\\ntheir appropriate visual query environments use a series of sliders,\\ncheckboxes, and other widgets to specify the temporal constraints\\non the start or end boundary time points of temporal intervals or\\nrelationships between temporal intervals. The TimeFinder system\\n[33,34] is a visual exploration and query system for exploring time\\npoint-based data sets, based on a direct manipulation metaphor.\\nThe main disadvantage of these systems is the lack of access to\\ndomain knowledge, and thus meaningful temporal abstraction\\ncannot be speciﬁed. Moreover, these systems focus only on\\nretrieving time-oriented data and do not support the retrieval of a\\nset of subjects (e.g., patients). Chittaro and Combi [35] provide a\\nframework for visual representation of temporal intervals and of\\ntheir interrelations. However, the proposed techniques are focused\\non the visual deﬁnition of temporal queries regarding the time-\\noriented data, rather than on the speciﬁcation of groups of patients.\\nSeveral proposals have been made to enhance query capabil-\\nities through the use of domain-speciﬁc knowledge, in a manner\\nthat somewhat resembles this aspect of our research framework\\n[36,37], but they are speciﬁc to only one domain (do not have\\nunderlying generic temporal abstraction ontology) and cannot be\\nused in other medical domains.\\n2.2.2. Temporal aggregation\\nIn historical databases, temporal aggregation (or temporal\\ngrouping) is a process in which a time line is partitioned over\\ntime and the values of various attributes in the database are\\ngrouped over these partitions [38]. As such, temporal aggregation\\ncan be seen as a temporal extension of the standard SQL operator\\ngroup by. The temporal grouping algebra and its application to the\\nSQL language were well deﬁned by Clifford et al. [39]. A typical\\nexample of temporal aggregation is the monthly accumulation of\\nsalary payment. Due to the large number of temporal data and\\ntheir distribution over the time line, efﬁcient algorithms to\\nperform temporal grouping are required, as mentioned by Moon\\net al. [40], who proposed several methods for large-scale temporal\\naggregation.\\nThe temporal aggregation process is also relevant to our study.\\nWe perform a type of temporal aggregation during the visualiza-\\ntion of longitudinal data for multiple patients (actually, of the\\ndelegate values of these data: see Section 4.2) or during the retrieval\\nof patient groups. However, our algorithms for aggregation of the\\ntime-oriented data for each individual patient in a group are\\napplied separately to the data for each patient so as to produce\\ndelegate values for each concept type at each temporal granularity\\n(see Section 4.2). For example, the VISITORS system and its\\nunderlying computational modules enable users to aggregate\\nindividual patient data by interval-based temporal abstractions\\n(using the KBTA methodology: see Section 2.1) and to further\\naggregate these abstractions into distributions at various temporal\\ngranularities (e.g., monthly distribution of the state of bone\\nmarrow following a bone-marrow transplant). However, these\\noperations are quite different from the semantics of standard\\ntemporal aggregation methods in temporal databases. Thus, such\\nmethods are less relevant to our study.\\n2.2.3. Temporal granularity\\nVarious medical domains require representation and explora-\\ntion of longitudinal patients’ data within different time granula-\\nrities. For example, in intensive care units, medical parameters are\\nmeasured using a granularity of minutes (or even seconds); diabetic\\npatients perform glucose tests three times a day, and the clinical\\nhistory of most chronic patients spans several years. Another\\nproblem is that even for the same patient, temporal clinical\\ninformation is represented at different time granularities; e.g.\\n(1)‘‘the patient has taken statin medications during the past three\\nmonths’’, and (2) ‘‘his glucose level rose sharply from 8:00 a.m. to\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n14\\n\\n6:00 p.m. on September 18th, 1996’’. Thus, the need to support\\nvarious time granularities is very clear.\\nIn the case of the VISITORS system, we approach these two\\nproblems in the following fashion: (1) we perform a summariza-\\ntion of each individual patient’s time-oriented data (i.e., calculat-\\ning delegate values as described in Section 4.2) according to the\\ntemporal\\ngranularity\\nlevel,\\nand\\n(2)\\nwe\\nsupport\\ninteractive\\nexploration of longitudinal data for individual/multiple patients\\nwithin different time granularities (see Sections 4.3 and 4.4).\\nOur intention here is not to propose a general approach to the\\ntreatment of temporal granularities, even though such approaches\\ndo, indeed, exist. Goralwalla et al. [41] showed an approach to the\\nhandling of granularity in temporal data. They separated temporal\\ndata into two groups: anchored (calendrical day or month, e.g.,\\nJanuary 1st, 2008 or May 1978), and unanchored (time intervals,\\ne.g., 2 months, 5 h 20 min, etc.) data. Thus, a temporal granule is a\\nspecial kind of unanchored temporal data [42].\\nIn our framework, a special kind of unanchored temporal data\\nis a relative time line (e.g., the period before or after bone-marrow\\ntransplantation). A relative time line is a determinate time span\\n[42], in which one boundary time point (start or end) is\\nreferenced\\nto\\na\\nspecial\\nclinical\\nevent\\n(e.g.,\\nbone-marrow\\ntransplantation) or another clinically signiﬁcant time point\\n(e.g., start of high fever). Clifford and Rao [43] have proven the\\nimportance of using integral time granularities that can be\\ncomposed of lower level granules. Thus, we deﬁne a relative\\nmonth as 30 days, and a relative year as 12 months, or 360 days.\\nNote that: (1) these periods do not correspond to any anchored\\ndurations, such as calendrical months; and (2) weeks are not\\nallowed as unanchored units in our framework, according to the\\nreasoning of Clifford and Rao [43].\\nOther techniques for the treatment of the temporal granularity\\nissue include the work of Dal Lago and Montanari [44] and other\\napproaches that were demonstrated within clinical domains\\n[26,45]. The GSTP system [46], for example, provides access to a\\nset of implemented algorithms (such as to the AC-G [47], and to an\\nalgorithm for converting calendrical expressions into periodical\\ngranularities [48]). These algorithms support a solution to the\\nmulti-granularity temporal constraint satisfaction problem (TCSP).\\nThe GSTP system supports a rich set of predeﬁned temporal\\ngranularities: common temporal granularities (e.g., days, months),\\nand\\nspecial\\ngranularities\\n(e.g.,\\nweeks,\\nacademic\\nsemesters).\\nMoreover, new user-deﬁned granularities can be added.\\nNote, however, that in our study, we focus on interactive\\nexploration of time-oriented data for multiple patients (actually of\\ntheir delegate values) within a predeﬁned set of temporal\\ngranularities (seconds, minutes, hours, days, months, and years):\\nthus, developing theoretical algorithms for dealing with temporal\\ngranularities was not relevant for the current study.\\n3. Desiderata for effective exploration of time-oriented data for\\nmultiple patients\\nOur study of the problem of effective and usable visualization\\nand exploration of raw clinical data and, especially, of derived\\nmeaningful abstractions from these data, revealed the following\\nset of desiderata for the intelligent interface and exploration\\noperators supporting the task of exploration of time-oriented data\\nfor multiple patients.\\n1. Evaluation of the functionality and usability of the KNAVE-II\\nsystem for exploration of longitudinal data of individual patients\\n[2] demonstrated the importance of the visualization and\\nexploration of meaningful temporal interpretations that were\\nderived from raw clinical data using context-sensitive domain-\\nspeciﬁc knowledge. Thus, the visualization and exploration of\\ntime-oriented interpretations of raw data is also an important\\nrequirement for a system whose goal is to explore longitudinal\\ndata for multiple patients.\\n2. Representation and computation of raw and abstract concepts at\\nvarious temporal granularities, especially for interval-based\\nconcepts [45], is very important for the clinical domain. Recall in\\nparticular the two requirements mentioned in Section 2.2.3: (1)\\nthe need to handle different temporal granularities for different\\nmedical domains, and (2) the need to support multiple types of\\nclinical\\ndata\\nthat\\nare\\nrepresented\\nat\\ndifferent\\ntemporal\\ngranularities within the same longitudinal patient record. The\\nsystem should be able to compute statistical aggregations of\\nboth raw and abstract concepts at any temporal granularity (e.g.,\\nday, month) or for any arbitrary time period (say, from 8 August\\nto 27 September 2007) for either individual or multiple patients.\\nFulﬁlling such a requirement constitutes a novel aspect of this\\nstudy.\\n3. Before exploring the time-oriented data of a group of patients,\\nthe clinician must select the patients that will comprise the\\npatient population to be analyzed. Current selection tools\\nsupport the speciﬁcation of patient groups by using demo-\\ngraphic (e.g., age, sex) or raw-data time and value constraints\\n(e.g., Hemoglobin values) [49,50]. However, to enable clinicians\\nto quickly and efﬁciently specify patient populations, there is a\\npressing need for the ability to specify the relevant patients by\\nusing knowledge-based and statistical aggregation constraints,\\ne.g., ‘‘select patients whose most frequent Hemoglobin-state\\nvalue (i.e., the mode of Hemoglobin-state values) during the ﬁrst\\ntwo months following bone-marrow transplantation was higher\\nthan the mode of the Hemoglobin-state of whole patient\\npopulation’’.\\n4. Current information-visualization systems proposed for multi-\\nple patients focus solely on the display of patients’ raw\\nlongitudinal data [14–18]. Moreover, such systems do not\\nsupport either population aggregation capabilities for raw data\\nor visualization of temporal abstractions for multiple patients.\\nThus, we must investigate and implement suitable visualization\\ntechniques for effective display of interval-based temporal\\nabstractions (not only of raw data) for multiple patients.\\n5. The system should be able to support the following tasks for\\ninteractive exploration of the data for multiple patients:\\n\\x03 Performing interactive dynamic exploration of the data of\\nmultiple patients at different temporal granularities (i.e.,\\nzooming in and out of the time line). This is a basic necessary\\ntask, which has been solved in several systems for interactive\\nexploration of raw data for the individual patient [14–16] and\\nfor abstract concepts [2]. However, the visualization and\\ninteractive exploration of both raw and abstract concepts for\\nmultiple patients require an additional extension.\\n\\x03 Changing dynamically the method used for the aggregation of\\npatients’ data at different temporal granularities (i.e., aggre-\\ngating differently blood glucose values at a granularity of days\\nversus months). There is no standard solution available for this\\ntask.\\n\\x03 Supporting both an absolute (i.e., calendrical) time line and a\\nrelative time line (which refers to some clinical key event) and\\nperforming an alignment of the patients’ data along signiﬁcant\\nclinical\\nreference\\nevent\\n(or\\ntime\\npoint),\\nincluding\\nthe\\ncomputation of the appropriate aggregation within the\\nrelative time line.\\n6. One of the more important tasks in the analysis of multiple\\npatient data is an investigation of potentially meaningful\\ninterrelations, especially temporal interrelations such as asso-\\nciations among subsets of raw patient data and abstract\\nconcepts. For such purposes different techniques of data mining\\n(e.g., temporal association rules), are often used [51], although\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n15\\n\\nthey typically focus only on temporal precedence relations.\\nCurrent visual data mining systems in the medical domain focus\\nmostly on raw data [18,19]. What is required is an interactive\\nuser-driven method to compute, display and explore temporal\\nassociations for the data of multiple patients among raw and\\nabstract concepts.\\n4. Methods\\nThe speciﬁc methods that we used are presented below in six\\nsubsections, each in the context of its relevant desideratum as\\npresented in Section 3.\\n4.1. Knowledge-based abstraction of raw time-oriented data for\\nmultiple patients\\nThe VISITORS system is an intelligent interface to a distributed\\narchitecture speciﬁc to the tasks of retrieval, knowledge-based\\nvisualization, and interactive exploration of the time-oriented\\nrecords of multiple patients. Fig. 2 describes the overall architec-\\nture of the VISITORS system. The architecture capitalizes on what\\nwe denote as a temporal abstraction services (see details below). In\\nparticular, we are assuming that all the necessary elements for the\\ntemporal abstraction framework (shown in Fig. 2 by dotted lines),\\nsuch as those designed in our previous studies or equivalent\\nservices, are available.\\nThe OBTAIN expressions constructed by end-users (clinicians)\\nare submitted to a SQL- and C#-based multiple-patients retrieval\\nmodule, which we refer to as Multiple-Patient Time-Oriented\\nQuery\\n(the\\nMulti-TOQ)\\nmodule.\\nThe\\nMulti-TOQ\\nmodule\\nis\\nresponsible for the retrieval of a set of patients, a set of\\ntemporal intervals, or a set of time-oriented patient data\\naccording to the following three types of OBTAIN expressions,\\nrespectively: Select Patients, Select Time Intervals, and Get Patients\\nData. The Multi-TOQ module combines specialized computa-\\ntional methods, such as calculation of delegate values (see\\nSection 4.2), with database operators, such as the selection of\\npatients by demographic (e.g., sex, age) values. It also creates\\nand ﬁlls the virtual databases used for retrieval purposes, for\\nexample, calculation of the population mean values for concepts\\nmentioned in the OBTAIN expression.\\nThe Multi-TOQ module interacts with the intelligent temporal\\nmediator IDAN [52] that integrates relevant time-oriented data\\nand knowledge, to obtain answers regarding the temporal\\nabstractions. IDAN uses several temporal abstraction services,\\nwhich differ in the mode in which they are used for processing\\ntime-oriented data. If the abstractions do not exist as yet, they are\\ncomputed on the ﬂy by a C#-based, goal-driven temporal\\nabstraction module, which we refer to as Tempura (a variation\\nof the Resume temporal abstraction system [53]), or by the prolog-\\nbased ALMA temporal-abstraction module [52] (in the case of\\ncomplex clinical patterns). Otherwise, if the abstractions have\\nalready been previously computed, in a data-driven manner, by\\nthe MOMENTUM system [54], they are simply retrieved by the\\nMulti-TOQ\\nmodule.\\nThe\\nMOMENTUM\\nsystem\\nis\\nan\\nactive\\nmiddleware (which is built on the concept of an active knowl-\\nedge-based time-oriented database) speciﬁc to solving the\\ntemporal abstraction task for large groups of subjects (e.g.,\\npatients) by using an incremental version of the KBTA method\\n[55]. MOMENTUM is a data-driven system, which generates\\ntemporal abstractions deﬁned in its knowledge base incremen-\\ntally, as new patient data arrive, and saves the abstractions in a\\nspecial layer in the database.\\n4.2. Statistical aggregation of patient time-oriented data and their\\nabstractions\\nIt is often the case that for a particular patient there are several\\nmeasurements of the same raw concept during the time granule of\\ninterest. For example, several blood-glucose level values during the\\nsame day or several Hemoglobin values during the same month or\\nsame year. However, the exploration analysis might require a\\nsingle value at each desired temporal granularity, and the problem\\nmight refer to either raw data or abstract concepts.\\nTo aggregate patient data at arbitrary temporal granularities or\\nduring speciﬁc time periods, we deﬁned the concepts of a delegate\\n(representative) value that is calculated by a delegate (representa-\\ntive) function: given a single patient’s time-oriented data for a\\nspeciﬁc concept (raw or abstract) over a particular time interval\\n(including a predeﬁned temporal granularity level), we calculate\\nthe delegate value of the patient’s data at each time granule (or for\\na particular time period) using a function speciﬁc to the concept\\nand the temporal granularity. Such a calculation is performed for\\neach patient in the group.\\nIn the VISITORS framework, we make the following four\\nassumptions:\\nAssumption 1. We need to support exactly six temporal granu-\\nlarity levels: seconds, minutes, hours, days, months and years, with\\nthe lowest granularity being seconds and the highest being years.\\nFig. 2. Overall VISITORS architecture. End users (clinicians) use the expression-speciﬁcation module of VISITORS to create and submit an OBTAIN expression to the Multiple-\\nPatient Time-Oriented Query (Multi-TOQ) module. The Multi-TOQ module interacts with the IDAN temporal abstraction mediator regarding the requested patients’ data and/\\nor temporal abstractions (TAs). The IDAN mediator integrates the relevant data and knowledge from the appropriate sources – indicated by the user in the OBTAIN expression\\n– to retrieve the raw data or to derive, using a temporal abstraction service (the Momentum, Tempura or ALMA temporal abstraction computational modules), a set of abstract\\ntime-oriented (interval-based) concepts from these data. The derived temporal concepts are returned to Multi-TOQ for further processing, if needed. The resultant data\\n(patients, time intervals or time-oriented data) answering the OBTAIN expression are returned to the exploration interface.\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n16\\n\\nAssumption 2. The patient has only one concept value at the\\nlowest given temporal granularity level (e.g., for data provided\\nat the temporal granularity level of months and days, no concept\\nhas more than one value per day).\\nAssumption 3. A single delegate value can be requested and\\ncomputed for any time granule (e.g., second day following bone-\\nmarrow transplantation) or arbitrary time period (e.g., January 7th,\\n2007–February 2nd, 2007), but a series of delegate values at a\\ndesired temporal granularity level for a particular concept can be\\nrepresented for a time period that contains only a whole number of\\ntime granules, e.g., a monthly summary can be requested for a\\nperiod of a whole number of calendrical months.\\nAssumption 4. The input data for our approach has the following\\ndata structure:\\nin put data \\x04 < Patientn; Conce ptc; T-Startc;n;m; T-Endc;n;m;valuec;n;m > \\x05; 1\\n\\x06 n \\x06 N; 1 \\x06 m \\x06 Mn;\\nwhere N is the number of patients, Mn is the number of values of\\nConceptc for Patientn; T-Startc,n,m and T-Endc,n,m are the start and end\\ntimes of the mth laboratory test (or temporal abstraction) for\\nPatientn, with value valuec,n,m. The symbol * denotes 0 or more\\nrepetitions.\\n4.2.1. Computing a single delegate value of a raw-data concept for a\\nspeciﬁc aggregation time period\\nThe delegate value for Patientn of raw concept Conceptc for a\\nspeciﬁc aggregation time period [T-Startaggreg, T-Endaggreg] is\\ncomputed by the delegate function DFc (which is possibly speciﬁc\\nto each combination of concept and temporal granularity level)\\nfrom the input_data as follows:delegate valuec;n;T-Startaggreg;T-Endaggreg ¼\\nDFc½ðT-Startn;1;T-Endn;1;valuec;n;1Þ; . . .ðT-Startn;i;T-Endn;i;valu ec;n;iÞ; . . .\\nðT-Startn;K; T-Endn;K;valuec;n;KÞ\\x07; T- Startaggreg \\x06 T-Startn;i; T-Endn;i \\x06\\nT-Endaggreg; 1 \\x06 i \\x06 K ¼ Kc;n;T-Startaggreg;T-Endaggreg where\\nK = Kc,n,T-Star-\\ntaggreg,T-Endaggreg is the number of instances of Conceptc for Patientn\\nmeasured within the desired [T-Startaggreg, T-Endaggreg] period, i.e., K\\nvaries for each concept, patient, and aggregation period.\\nThe delegate function is deﬁned in the knowledge base or is\\nchosen at runtime by the user from several predeﬁned default\\nfunctions.\\nFor\\nexample,\\nassume\\nthat\\nafter\\na\\nbone-marrow\\ntransplantation procedure, a patient’s Platelet counts on two\\nseparate occasions during the same day were 22,000 cells/ml at 10\\na.m. and 17,000 cells/ml at 9 p.m. If we specify, in the domain\\nknowledge base, the mean as the default delegate daily function for\\nthe Platelet count (raw-data) concept, then the patient had a daily\\ndelegate value of 19,500 cells/ml for the Platelet count (raw)\\nconcept. However, during the interactive exploration time, the\\nuser can also select any other suitable delegate function (such as\\nthe mode or the maximum).\\nIndeed, in theory, almost any function from multiple time-\\nstamped values into one value, of the same concept that has the\\nsame domain and units as the possible values of the original\\nconcept, can serve as the delegate function. However, unlike\\nstandard statistical functions, it must be applied to each time\\ngranule in the relevant aggregation temporal granularity (e.g., day).\\nOf course, the selected function must make clinical sense, and is\\nthus speciﬁc to each clinical concept and aggregation time\\ngranularity level.\\n4.2.2. Computing a series of delegate values of a raw-data concept at a\\ndesired temporal granularity for a given overall aggregation time\\nperiod\\nThe computing of a series of delegate values is performed for\\neach time granule at the desired temporal granularity level for an\\noverall aggregation time period. Thus, a series of delegate values for\\nPatientn of raw concept Conceptc for an overall aggregation time\\nperiod [T-Startoverall, T-Endoverall] has the following data structure:\\ndelegate valuesc;n;T-Startoverall;T-Endoverall ¼ < Patientn; Conce ptc;\\n-Startaggreg n; j; T-Endaggreg n; j; delegate valuec;n; j > \\x05; 1 \\x06 n \\x06 N;\\n1 \\x06 j \\x06 Jc;n;\\nwhere N is the number of patients; Jc,n is the number of delegate\\nvalues of Conceptc for Patientn; T-Startaggreg n,j and T-Endaggreg n,j are\\nthe start and end times of the time granule that is speciﬁc to the jth\\ndelegate value for Patientn; and Jc,n varies with each patient,\\nconcept and the particular overall time period of a series of\\ndelegate values.\\nThe values for boundary time granules of Conceptc for Patientn of\\nthe start point T-Startaggreg n,1 of the ﬁrst time granule and of the\\nend point T-Endaggreg n,Jc,n of the last time granule are:\\nT-Startaggreg n;1 ¼ Begin O f Time GranuleðT-Startc;n;1Þ \\x08 T-Startoverall;\\nT-Endaggreg n;Jc;n ¼ End O f Time GranuleðT-Endc;n;MnÞ \\x06 T-Endoverall:\\nThe jth index denotes the jth delegate value of the speciﬁc\\nPatientn. For example, assume that laboratory tests for Conceptc\\nwere performed for the patient several times on each of the days\\nJanuary 3rd, January 15th, and January 20th, then the T-Granaggreg is\\nday, and the DF is mean. Thus, the ﬁrst delegate value will be the\\nmean value of all of the concept-measurement results on January\\n3rd, the second delegate value will be the mean value of all of the\\nresults on January 15th and so on. In this case J = 1.3 for the overall\\naggregation period of January. If T-Granaggreg is month and the\\noverall aggregation time period is 1 January–31 August for the\\nsame year, then the ﬁrst delegate value will be the mean value of all\\nthe tests during January. Note that several of the months during the\\nrequested overall period might have no measurements, and hence\\nno delegate values representing them.\\n4.2.3. Computing a single delegate value for a speciﬁc aggregation\\ntime period and a series of delegate values concepts at a desired\\ntemporal granularity for a given overall aggregation time period of an\\nabstract concept\\nThe computing of a delegate value for a speciﬁc aggregation time\\nperiod ora series of delegatevalues at a desiredtemporal granularity\\nfor a given overall aggregation time period in the case of interval-\\nbased abstract concepts [such as intervals of different (discrete)\\ngrades of bone-marrow toxicity] is similar to the aggregation of raw\\n(time point based) patient’s data, as explained in Sections 4.2.1 and\\n4.2.2, but requires the use of more complex delegate functions,\\nbecause we must refer to both the value of the temporal abstraction\\ninterval and to its duration. Thus, standard statistical functions are\\nnot sufﬁcient. In this case, we provide additional delegate functions,\\nsuch as the value of the abstraction that has the maximal cumulative\\nduration during the relevant aggregation time period or the value\\nassociated with the interval that has the longest duration.\\nSince values of abstract concepts often span time intervals (i.e.,\\na period and not just a time point), we consider the case of an\\noverlap between the interval during which the value holds, and the\\nseveral time granules, which are at the time granularity level that\\nhad been requested for the computation of the delegate value.\\nAssume that the abstract concept computed for the relevant\\npatient spans four time intervals (denoted by i1–i4) within the\\nrange of three time units of the desired time granularity level\\n(Fig. 3a). First, we extend the temporal intervals by performing an\\nextrapolation operation that is an extension of the knowledge-\\nbased interpolation operation [56], according to the concept-\\nsensitive relevant interpolation function (the extension of the\\nintervals denoted by dotted lines in Fig. 3b). The extension is\\nnecessary to enable application of various statistical functions,\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n17\\n\\nsince abstractions that hold at a particular time point must be\\nextended so as to hold over a sufﬁciently long time interval.\\nSimilarly, we extend longer precomputed intervals beyond their\\ntwo edges, assuming that the abstract value persists beyond the\\ncrisp temporal points that deﬁne the time interval. The local\\nextrapolation of a point or an interval’s boundary point uses the\\nD(0,0) interpolation function for the relevant concept [56], i.e., the\\nmaximal gap allowed between two single time points, at each of\\nwhich holds the value of the abstract concept. We actually extend\\neach boundary point to the past and the future by amount of\\nD(0,0)/2. In the case of adjacent intervals that have different values\\nfor the same abstract concept, the extrapolation is proportional to\\nthe duration of the intervals. Second, we segment the extended\\nintervals according to the time granules of the desired time\\ngranularity (Fig. 3c). Third, we apply the default delegate function\\nfor that concept and time granularity. The default for all concepts is\\ncurrently the value that has maximal cumulative duration within\\nthe time granule. In Fig. 3c, within the ﬁrst time granule the\\ndelegate value is ‘‘normal’’ (note that the sum of extended\\ndurations of intervals i10 and i20 is more than the duration of the\\nextended interval i30 within the ﬁrst time granule). The ‘‘high’’\\nvalue is the delegate value computed for the second and third time\\ngranules. Note, that interval i3000 (part of extended i3 within the\\nthird time granule) has a higher duration in that granule than\\nthe extended interval i40; thus, the delegate value computed for the\\nthird granule is ‘‘high’’; although the original duration of interval i4\\nwas longer than the part of i3 within the third time granule. Using\\nother delegate functions could change the computed delegate\\nvalues.\\n4.2.4. Computing a single population delegate value of raw-data and\\nabstract concepts for a speciﬁc aggregation time period\\nThe population delegate value represents the aggregated value of\\na group of patients calculated by the concept-speciﬁc and the\\ntemporal-granularity-speciﬁc statistical function within a speciﬁc\\ntime granule (or for another speciﬁc time period), e.g., the maximal\\nvalue of the raw Hemoglobin values for a group of patients over a\\nwhole month. The population delegate value of a group of patients\\nfor Conceptc within a speciﬁc aggregation time period [T-Startaggreg,\\nT-Endaggreg] is calculated by the population delegate function PDFc\\nfrom the set of original patient data, input_data* as follows:\\npo pulation delegate valuec;T-Startaggreg;T-Endaggreg\\n¼ PDFc½ðT-Startn;1; T-Endn;1;valuec;n;1Þ; . . . ðT-Startn;i; T-Endn;i;\\nvaluec;n;iÞ; . . . ðT-Startn;K; T-Endn;K;valuec;n;KÞ\\x07; 1 \\x06 n \\x06 N;\\n1 \\x06 i \\x06 K; T-Startaggreg \\x06 T-Startn;i; T-Endn;i\\n¼ T-Endaggreg; 1 \\x06 i \\x06 K \\x06 Kc;n;T\\tStartaggreg;T-Endaggreg\\nwhere N is the number of patients in the group, and K = Kc,n,T-\\nStartaggreg,T-Endaggreg is the number of instances of Conceptc for\\nPatientn measured within the [T-Startaggreg, T-Endaggreg] time period.\\nAn example of a population delegate value is the maximal value\\nof the concept across all patients within a speciﬁc month (see the\\nline denoted by 2 in Fig. 4).\\nSimilarly to the delegate function DFc, the population delegate\\nfunction PDFc calculates the delegate value for Conceptc over each\\nrelevant time granule. However, through the population delegate\\nfunction PDFc, we calculate a ‘‘delegate value’’ that represents a\\npopulation value for Conceptc for a group of patients during that\\nFig. 3. Computation of a series of delegate values for an abstract concept (see text for details).\\nFig. 4. Visualization of the data for the Red blood cell (RBC) count raw concept for a group of 58 patients (retrieved earlier by using a Select Patients expression) from April 1995\\nto March 1996. The individual raw data are represented at a resolution level of seconds with respect to time granularity, but the population statistics are aggregated at a\\ngranularity of months, according to the user’s current request. All laboratory test results for the RBC count for patients treated during this period are displayed as (blue) X’s\\n(denoted by region 1). Through the density of the points, the user can judge the number of data instances belonging to each value or time period range. The top (red) line\\n(denoted by 2) represents the monthly maximal value of the whole group. The tooltip provides detailed information about the maximal value (4520 cells/ml), the ID of the\\npatient (703) with that value, and the observation time. The bottom (blue) line (denoted by 4) and the middle (green) line (denoted by 3) represent the monthly minimal and\\nmean values, respectively. On the left-hand side (denoted by 5) are displayed statistics for the whole time period (April 1995 to March 1996). The three dotted lines drawn\\ninside the panel indicate the mean value \\x02 standard deviation for this period (For interpretation of the references to color in this ﬁgure legend, the reader is referred to the web\\nversion of the article).\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n18\\n\\ntime granule. The population delegate function can be any\\nstatistical function, such as the mean or the maximal value, which\\ncan be applied to the values of all of the patients within a speciﬁc\\ntime granule and that returns a single output value from the\\ndomain of values and units of Conceptc.\\n4.2.5. Computing a series of population delegate values of raw-data\\nand abstract concepts at a desired temporal granularity for a given\\noverall aggregation time period\\nThe process of computing a series of population delegate values\\nis similar to computing a series of individual delegate values: we\\ncompute a population delegate value for each time granule at a\\ndesired temporal granularity level for an overall aggregation time\\nperiod. Thus, a series of population delegate values of Conceptc for an\\noverall aggregation time period [T-Startoverall, T-Endoverall] has the\\nfollowing data structure:\\npo pulation delegate valuesc;T-Startoverall;T-Endoverall \\x04 < Conce ptc;\\n-Startaggreg j; T-Endaggreg j; po pulation delegate valuec; j > \\x05;\\n1 \\x06 n \\x06 N; 1 \\x06 j \\x06 Jc;\\nwhere Jc is the number of population delegate values of Conceptc for\\nthe overall aggregation time period; and T-Startaggreg j and T-\\nEndaggreg j are the start and end times of the time granule that is\\nspeciﬁc to the jth population delegate value.\\n4.3. The OBTAIN speciﬁcation language\\nThe VISITORS system includes a graphical module, which is used\\nfor speciﬁcation of the relevant patients, time intervals and values.\\nUnderlyingtheVISITORSgraphicalexpression-speciﬁcationmodule,\\nis an ontology-based temporal-aggregation (OBTAIN) speciﬁcation\\nlanguage. The OBTAIN speciﬁcation language enables the speciﬁca-\\ntion of three types of expression: Select Patients, Select Time Intervals\\nand Get Patients Data. A full exposition of the OBTAIN language and\\nthegraphicalexpression-speciﬁcationmoduleisbeyondthescopeof\\nthis study and can be found elsewhere [4]. We will brieﬂy explain,\\nhowever, these three expression types. The Appendix A includes the\\nBackus Normal Form (BNF) syntax of all three expressions.\\n4.3.1. Select Patients expression\\nThe Select Patients expression retrieves from a selected database\\na list of patients who satisfy a set of either demographic (i.e., non-\\ntemporal) constraints, for example, ‘‘select male patients [cur-\\nrently] insured by a certain health maintenance organization’’, or\\ntime and value [knowledge-based] constraints, for example: ‘‘ﬁnd\\npatients who had, during the ﬁrst month following bone-marrow\\ntransplantation, at least one episode of bone-marrow toxicity (an\\nabstraction deﬁned by the relevant clinical protocol) of ‘‘grade1’’ or\\nhigher that lasted at least two days’’; or [time and value]\\nproportional and statistical constraints, for example, ‘‘ﬁnd patients\\ntaking a Statin-type medication whose mean Low density lipid\\n(LDL) cholesterol value exceeded that of the mean value for all\\npatients taking Statin-type medications’’.\\n4.3.2. Select Time Intervals expression\\nGiven a set of time-oriented patient data, this expression\\nreturns a list of time intervals that satisfy the constraints deﬁned\\nby the user for some portion of the patients. The goal of this\\nexpression is to ﬁnd when a certain portion of the patients had a\\nspeciﬁc value of some raw or abstract concept or value within a\\npredeﬁned value range. For example, a typical Select Time Intervals\\nexpression is ‘‘Find periods [relative to an allogenic bone-marrow\\ntransplantation event] during which the White blood cell state\\nvalue was less than ‘‘normal’’, and the Platelet value was between\\n2000 and 10,000 cells/ml for at least 50% of the patients.’’\\n4.3.3. Get Patients Data expression\\nGiven a concept, a list of patient IDs and, optionally, a list of\\ntime intervals, the expression retrieves the values of the concept\\nwithin the selected time intervals for the selected patients. The\\ndefault patient list is all the patients in the database, and by\\ndefault there are no time-interval constraints, i.e., values are\\nreturned for the entire timeline. For example, a typical Get Patients\\nData expression is ‘‘Get the state values of Hemoglobin for\\npatients #1–#10 during the ﬁrst two weeks following bone-\\nmarrow transplantation.’’\\nThe Get Patients Data expression is not one of the expression\\ntypes\\nspeciﬁed\\nwithin\\nthe\\nexpression-speciﬁcation\\nmodule.\\nRather, it is generated by an interactive exploration process. Once\\na patient list has been deﬁned by a Select Patients expression (or by\\nthe explicit setting of the ID list in the exploration interface),\\nselecting a concept from the ontology browser (see Fig. 1) opens a\\nnew panel with the values of that concept for all of the patients in\\nthe list.\\n4.3.4. Evaluation of the graphical expression-speciﬁcation module\\nWe have previously evaluated both the functionality and the\\nusability of the expression-speciﬁcation module by a group of 10\\nusers—ﬁve clinicians and ﬁve medical informaticians. Results have\\nshown that both types of users were able, in a short time and with\\nhigh accuracy, to graphically construct complex expressions,\\nalthough the accuracy of the speciﬁcation of time-range con-\\nstraints\\nregarding\\nthe\\nstart\\nor\\nend\\nof\\ntime\\nintervals\\nwas\\nsigniﬁcantly lower than that of the rest of the constraints. The\\ndetails of this evaluation appear in a previous study [4].\\n4.4. Visualization of the data for both raw and abstract concepts\\nThe data set retrieved by the Get Patients Data expression can be\\nvisualized and explored. In general, we provide a two-dimensional\\nvisualization, in which the horizontal axis is the time dimension,\\nand the vertical axis is the value dimension.\\n4.4.1. Visualization of the data of raw concepts\\nFor the basic visualization of the data of raw concepts for\\nmultiple patients, we use the line plot visualization technique,\\nwhich plots the data points on the screen according to the X and Y\\ncoordinates (i.e., according to the time and values coordinates in\\nour case) and either connects or does not connect the plotted\\npoints by lines (Fig. 4). In this visualization, three data types are\\nrepresented:\\n(1) The individual patient’s raw concept data for all patients in a group\\n(denoted as 1 in Fig. 4). In fact, the delegate values of the data\\nfor the current time granularity are represented.\\n(2) The time-oriented population statistics of the whole group of\\npatients. In fact, the population delegate values of the group of\\npatients for the current time granularity are represented. We\\nrepresent the following three time-oriented statistics, each\\naggregated (calculated) within a potentially different time\\ngranularity: maximum value at each time granule (top red line\\ndenoted as 2 in Fig. 4), minimum value at each time granule\\n(bottom blue line denoted as 4 in Fig. 4), and mean value for\\neach time granule (the middle green line denoted as 3 in Fig. 4)\\npopulation values. Each population delegate value is a function\\nof all of the population values in the relevant time granule (e.g.,\\nmonth in this example).\\n(3) The value statistics, which are sensitive to the particular time\\nwindow of the displayed data (denoted by the number 5 in\\nFig. 4). The dynamically changing content of the displayed data\\n(e.g., by panning) causes recalculation of these statistics.\\nDefault statistics for a raw data concept include descriptive\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n19\\n\\nstatistics, such as the mean, maximum, and minimum values,\\nand the standard deviation.\\nTo formally deﬁne the conﬁguration of the display for raw\\nindividual patient data and to understand the semantics of the\\nexploration operators, we introduce the Graph Computational\\nManager (GCM), which stores both the concepts to be displayed\\nand several important computational parameters relevant to these\\nconcepts. The GCM has the following data structure:\\n< Conce ptc; in put data\\x05; T-Granaggreg; DFc > ; 1 \\x06 c \\x06 C;\\nwhere C is the number of concepts in the knowledge base;\\ninput_data* is the original source data accessed by the temporal\\nmediator and retrieved by the Get Patient Data expression; T-\\nGranaggreg deﬁnes the temporal granularity level of the individual\\npatient’s aggregated data (e.g., one data point per day); and DFc is\\nthe delegate function, appropriate to Conceptc, used for calculating\\nthe delegate values for each patient at the T-Granaggreg granularity\\n(e.g., the daily mean).\\nTo intelligently and more efﬁciently explore the Mn data points\\nof Conceptc for Patientn, we use an aggregation of the values\\nthrough a delegate-value function speciﬁc to each concept and\\ntime granularity level (as described in Section 4.2). Thus, for\\nexample, several daily values out of the Mn values might be\\nrepresented by a single delegate value, e.g., the maximum.\\nThe Graph Display Manager (GDM) controls the actual data to be\\ndisplayed and has the following data structure: < Conce ptc;\\ndelegate values\\x05; T-Startexplor; T-Endexplor; T-Granexplor; ½RefP\\x07> ;1 \\x06\\nc \\x06 C;\\nwhere C is the number of concepts in the knowledge\\nbase; delegate_values* were calculated by the delegate function\\nappropriate to the temporal aggregation level T-Granaggreg, as\\nstored by the GCM; T-Startexplor and T-Endexplor deﬁne the temporal\\nrange of the current display time window, which includes 0 or\\nmore delegate values of Conceptc for each of the relevant patients;\\nT-Granexplor denotes the current temporal granularity level of the\\nexploration; and RefP is an optional parameter that deﬁnes the\\nreference symbolic point (e.g., bone-marrow transplantation)\\nwhen a relative time line is being used.\\nThe GCM of the time-oriented population statistics is similar to\\nthe GCM of the raw data. However, it uses a population delegate\\nfunction, instead of an individual delegate function:\\n< Conce ptc; in put data\\x05; T-Granaggreg; PDFc > ; 1 \\x06 c \\x06 C;\\nwhere C is the number of concepts in the knowledge base;\\ninput_data* is the original source data accessed by the temporal\\nmediator and retrieved by the Get Patient Data expression; T-\\nGranaggreg deﬁnes the temporal granularity level of the patient’s\\nstatistical data; and PDFc is the population delegate function,\\nappropriate to Conceptc, used for calculating a population delegate\\nvalues at the T-Granaggreg granularity (as described in Section 4.2).\\nThe GDM of the time-oriented population statistics has the\\nfollowing data structure:\\n< Conce ptc; po pulation delegate values\\x05; T-Startexplor; T-Endexplor;\\nT-Granexplor; ½RefP\\x07 > ; 1 \\x06 c \\x06 C;\\nwhere C is the number of concepts in the knowledge base; the\\npopulation_delegate_values* structure stores the values of the\\npopulation\\nstatistics\\ncalculated\\nfor\\nall\\ntime\\ngranules\\nby\\nthe appropriate PDFc function; T-Startexplor and T-Endexplor deﬁne\\nthe temporal range of the current display time window, which\\nincludes 0 or more population delegate values; T-Granexplor denotes\\nthe current temporal granularity level of the exploration; and RefP\\nis an optional parameter that deﬁnes the reference point (e.g., bone-\\nmarrow transplantation) when a relative time line is being used.\\nT-Granaggreg and T-Granexplor can be different or they can be the\\nsame. For example (see Fig. 4), the individual aggregation temporal\\ngranularity level might be seconds, while the exploration temporal\\ngranularity might be months. If both temporal granularities are the\\nsame, each patient will be represented by only one delegate-value\\npoint for the displayed concept for each time granule.\\nT-Granexplor deﬁnes the exploration granularity of the graphical\\npanel for all patient data displayed in the panel. It might be\\ndifferent for each panel, even in a case of the exploration of the data\\nof an equal concept twice. The default is to use the same\\nexploration granularity for all panels, which enables the synchro-\\nnization of the explored data among different concepts.\\nIf both the individual patient data and population statistics are\\ndisplayed in the panel, then T-Startexplor, T-Endexplor, T-Granexplor\\nparameters are same in each of the GDMs.\\n4.4.2. Visualization of the data of abstract concepts\\nTemporal abstractions for multiple patients are displayed as a\\ndistribution of the delegate values of the abstract concept of the\\npatients at the desired (interactively modiﬁed) time granularity.\\nFor the visualization, we use a modiﬁed version of the bar chart\\nvisualization technique. The modiﬁcation includes providing\\nseparate [0.100%] scales for each of the possible values of the\\ntemporal abstractions (e.g., for each toxicity grade), which is useful\\nfor discovering trends in the distribution. We assume a ﬁnite\\nnumber of [symbolic] values for each abstract concept. The\\nFig. 5. Visualization of the values of the Platelet-state abstract concept for a group of 58 patients (retrieved earlier by using a Select Patients expression) during 1995. The user\\nsees the monthly distribution of the patients’ values during this period (e.g., the dotted rectangle denotes the distribution of the concept’s values during May). Note that the\\nvertical axis has two scales: an external ordinal scale of possible values and an internal percentage scale, from 0 to 100%, within each concept value. The tooltip provides\\ndetailed information about the contents of the displayed area: the value (low) and its proportion (43.75%), the start and end time points and the actual number of patients\\nwith the selected value (7) out of all patients (16 patients). Note that only 16 patients in the group of 58 patients had the necessary raw data (in this example, the Platelet\\ncount) during May 1995.\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n20\\n\\nvisualization of the values of an abstract concept for multiple\\npatients is shown in Fig. 5.\\nThe GCM of a temporal abstraction graph has the following data\\nstructure:\\n< Conce ptc; in put data\\x05; T-Granaggreg; DFc > ; 1 \\x06 c \\x06 C;\\nwhere C is the number of concepts in the knowledge base;\\ninput_data* is the original source data accessed by the temporal\\nmediator and retrieved by the Get Patient Data expression; T-\\nGranaggreg deﬁnes the temporal granularity level of the patient’s\\naggregated data; and DFc is the delegate function, appropriate to\\nConceptc, used for calculating the delegate values for each patient\\nin the T-Granaggreg granularity.\\nSince the data of an abstract concept for multiple patients are\\ndisplayed as a set of distributions of the abstract-concept values, one\\nforeachofthe temporal granules ofthe T-Granaggreg level,the GDMof\\nthe temporal abstraction differs from the GDM of the raw data:\\n< Conce ptc; distribution\\x05; T-Startexplor; T-Endexplor; T-Granexplor;\\n½RefP\\x07 > ; 1 \\x06 c \\x06 C;\\nwhere C is the number of concepts in the knowledge base;\\ndistribution* is the data structure {[vall\\nc, Prevl\\nc]1 . . . [vall\\nc, Prevl\\nc]J},\\nwhere vall\\nc is the 1st value of Conceptc (usually measured on ordinal\\nsymbolic scale) and Prevl\\nc is the prevalence of patients having that\\nvalue; and J is the number of temporal granules at the aggregation\\ngranularity level T-Granaggreg, during the exploration time interval\\n[T-Startexplor, T-Endexplor]. The delegate values are calculated by the\\ndelegate function appropriate to the T-Granaggreg as stored by\\nthe GCM. T-Startexplor and T-Endexplor deﬁne the temporal range of the\\ncurrent display time window, which includes 0 or more delegate\\nvalues of the Conceptc for each relevant patient. T-Granexplor denotes\\nthe current temporal granularity level of the exploration. RefP is an\\noptional parameter that deﬁnes the reference point (e.g., bone-\\nmarrow transplantation) when a relative time line is being used. In\\nthe case of temporal abstractions, T-Granaggreg and T-Granexplor are\\nalways the same; for example (see Fig. 5), the aggregation\\ngranularity and exploration granularity could both be months.\\n4.5. Exploration operators of time-oriented data of multiple patients\\nIn this section, we describe the formal deﬁnition and semantics\\nof the general exploration operators for the exploration of time-\\noriented data for multiple patients, i.e., the input, output, and\\nnecessary calculations performed on the patients’ data. Each of the\\noperators can be applied on three different possible types of data\\ndisplayed in the graph:\\n\\x03 Individual raw data, such as Hemoglobin values, White blood cell\\ncount, and other laboratory tests, interventions and medications.\\n\\x03 Time-oriented\\npopulation\\nstatistical\\nvalues\\n(e.g.,\\nmaximal\\nmonthly value of Hemoglobin), calculated from the data of\\nraw concepts for multiple patients.\\n\\x03 Temporal abstractions, such as the values of the Hemoglobin-\\nstate abstract concept, derived from the data of raw concepts by\\nusing the domain-speciﬁc context-sensitive knowledge base.\\nThe application of the exploration operators to different data\\ntypes differs with the performed calculation and the output.\\nSeveral of the operators act in a ‘‘syntactic’’ fashion, i.e., perform\\nsimple magniﬁcation or miniﬁcation of the displayed data without\\nadditional calculation; thus the input and output data are same.\\nOther operators return output data that are different from the\\ninput data (e.g., by computing the mean values), but that are still in\\nthe input concept’s domain of values and units.\\nFor conciseness, we introduce the following annotation, which\\nwe use to formally deﬁne the semantics of all three operators:\\n\\x03 current_values*: denotes the current data that are displayed by\\nthe GDM before the application of the operator (i.e., delegate_-\\nvalues* in the case of individual raw data, population_delegate_-\\nvalues* in the case of time-oriented population statistics, and\\ndistribution* in the case of temporal abstractions).\\n\\x03 D-function: denotes the delegate function DFc in the case of\\nindividual raw data and temporal abstractions or the population\\ndelegate function PDFc in the case of time-oriented population\\nstatistics.\\n\\x03 new_values*: denotes the new actual data to be displayed by the\\nGDM after the application of the operator. The new_values are\\ncalculated by the function accordingly to the aggregation time\\ngranularity from the original data accessed by the temporal\\nmediator.\\n4.5.1. The Temporal Exploration Operator\\nThe VISITORS system enables the user to manipulate the data\\ninteractively, e.g., to pan the patient data within various time\\nintervals or to zoom-in and zoom-out of the patient data within\\ndifferent time granularities ranging from seconds to decades.\\nFor individual values of raw concepts for multiple patients the\\nTime Exploration Operator operates as a syntactic zoom. It\\nperforms a magniﬁcation or miniﬁcation of the displayed data\\nduring the speciﬁc predeﬁned period of time (i.e., a particular time\\ngranule or a speciﬁc time period). This zoom function facilitates the\\ndisplay of large amounts of patient data (i.e., an overview mode) or\\na reduction of the time granularity to support in-depth exploration\\nof the patients’ data for a particular time granule of an arbitrary\\ntime range. The individual delegate and population delegate values\\nof a raw data concept are not recalculated during the application of\\nthe Time Exploration Operator; thus, this operator is indeed purely\\nFig. 6. Application of the Time Exploration Operator (in this case a zoom-in) to the data for the White blood cell (WBC) raw concept for a group of 58 patients selected earlier\\nby the user. Clicking on the ‘‘Mar’’ (March) month widget in panel A results in a display of the data throughout the month of March, as shown in panel B. Note, clicking on the\\n‘‘Mar 95’’ granule in the panel B would again provide panel A.\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n21\\n\\nsyntactic, as opposed to the semantic operators, which signiﬁ-\\ncantly modify the displayed data.\\nFor an individual patient’s data and for population statistics of\\nraw concepts the Temporal Exploration Operator is applied\\nthrough the GDM and may be formally deﬁned as follows:\\nTEOðGDM : < Conce ptc; current values\\x05; T-Startexplor; T-Endexplor;\\nT-Granexplor; ½RefP\\x07 > ; T-Start0\\nexplor; T-End0\\nexplor; T-Gran0\\nexplorÞ\\n) GDM0 : < Conce ptc; new values\\x05; T-Start0\\nexplor; T-End0\\nexplor;\\nT-Gran0\\nexplor; ½RefP\\x07 > ;\\nwhere T-Start0explor, T-End0explor, and T-Gran0explor are user-deter-\\nmined parameters of the new earliest and latest time points for\\ndisplay, and the new exploration temporal granularity, respec-\\ntively. Fig. 6 shows the application of the Temporal Exploration\\nOperator to the data of a raw concept for a group of patients.\\nNote that the default delegate function is the identity function.\\nThus, the output from the mediator will be recalculated according\\nto the input temporal granularity of the data and represented using\\nan identical temporal granularity.\\nApplication of the Temporal Exploration Operator to the values\\nof abstract concepts calculated for multiple patients is more\\ncomplex and quite different. To apply the Temporal Exploration\\nOperator in this case, we must recalculate the distributions\\naccording to the new time granularity level; thus, the application\\nof the operator is, in fact, semi-semantic and is applied through\\nboth the GCM and the GDM:\\nTEOðGCM : < Conce ptc; in put data\\x05; T-Granaggreg; DFc > ;\\nGDM : < Conce ptc; distribution\\x05; T-Startexplor; T-Endexplor;\\nT-Granexplor; ½RefP\\x07 > T-Gran0\\naggreg; T-Start0\\nexplor; T-End0\\nexplor;\\nT-Gran0\\nexplorÞ ) ðGCM0 : < Conce ptc; in put data\\x05;\\nT-Gran0\\naggreg; DFc > GDM0 : < Conce ptc; distribution0\\x05;\\nT-Start0\\nexplor; T-End0\\nexplor; T-Gran0\\nexplor; ½RefP\\x07 > Þ;\\nwhere T-Start0explor, T-End0explor, and T-Gran0explor are user-deter-\\nmined parameters of the new earliest and latest time points for\\ndisplay, and the new exploration temporal granularity, respec-\\ntively; and distribution0* is the new distributions of delegate values\\nthat were calculated by the delegate function appropriate to the T-\\nGranaggreg as stored by the GCM. Fig. 7 shows the application of the\\nTemporal Exploration Operator to the data of abstract concept for a\\ngroup of patients.\\nThe user-determined parameters T-Start0explor, T-End0explor, and\\nT-Gran0explor can be derived in several ways: panning (i.e., shifting\\nthe displayed data to the right or left) changes the T-Start0explor and\\nT-End0explor parameters; using a calendaric-range zoom (i.e.,\\nstandard calendar function) enables the user to specify the T-\\nStart0explor and the T-End0explor time points to zoom-in to a speciﬁc\\ntime range; and clicking the speciﬁc temporal granule at the\\nbottom part of the graphs (see Figs. 6 and 7) performs a zoom-in or\\nzoom-out operation according to the selected time granule; thus,\\nall three parameters may be changed. A detailed exposition of all of\\nthe ways for manipulating the temporal granularity can be found\\nin the description of the KNAVE-II system, which focuses on\\nexploration of individual patient data [2].\\n4.5.2. The Change Delegate Value Operator\\nAs explained earlier, the VISITORS system supports aggregation\\nof patient data according to a given time granularity. Such\\naggregate values are designated the ‘‘delegate values’’ for that\\ngranule (for example, a representative blood-glucose value for the\\nwhole day or even the whole month). To apply another delegate\\nfunction or to change the granularity level of the aggregation, the\\nuser can use the Change Delegate Value Operator (CDVO). Note\\nthat the data values are changed, but the data still maintain the\\nsame type and domain of values.\\nFor all three types of displayed data, the operator is applied\\nthrough the GCM. Thus, the GDM will display the new values within\\nthe same display conﬁguration, i.e., the earliest and latest times for\\ndisplay and exploration granularity are the same before and after\\napplication of the Change Delegate Value Operator. The formal\\ndeﬁnition of the Change Delegate Value Operator is as follows:\\nCDVOðGCM : < Conce ptc; in put data\\x05; T-Granaggreg; D- function > ;\\nGDM : < Conce ptc; current values\\x05; T-Startexplor; T-Endexplor;\\nT-Granexplor; ½RefP\\x07 > ; T-Gran0\\naggreg; D- function0Þ ) GCM0 :\\n< Conce ptc; in put data\\x05; T-Gran0\\naggreg; D- function0 > ;\\nGDM0 : < Conce ptc; new values\\x05; T-Startexplor; T-Endexplor;\\nT-Granexplor; ½RefP\\x07 > ;\\nwhere T-Gran0aggreg, and D-function0 are the user-determined\\nparameters of the new temporal aggregation granularity and D-\\nfunction, respectively. The new_values* are derived from the input\\ndata as explained in Section 4.2.\\nFigs. 8 and 9 show the application of the Change Delegate Value\\nOperator to the values of raw and abstract concepts for a group of\\npatients.\\nFor both cases of the visualizations, i.e., for the raw and abstract\\nconcepts, we provide a Graph Manager interface (not shown here),\\nin which the user can change the delegate function and the\\naggregation temporal granularity for each graph in the panel; e.g.,\\nthe maximal and mean population statistics might be represented\\nFig. 7. Application of the Temporal Exploration Operator (in this case a zoom-out) to the data for the White blood cell (WBC) state abstract concept for a group of 58 patients\\nselected earlier by the user. Clicking on the ‘‘May 95’’ month widget in panel A produces a display of the distributions of data throughout the year of 1995 according to the\\nmonths time granularity, as shown in panel B. Note that the required raw data (in this example, the WBC count) might not necessarily have been available for all 58 patients\\nduring May 1995. Clicking on the ‘‘May’’ granule in the panel B would again provide panel A.\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n22\\n\\nat a monthly granularity level, and the individual raw data might be\\nrepresented at a resolution level of seconds, as is shown in Fig. 4.\\n4.5.3. The Set Relative Time Operator\\nChanging dynamically the point of view from an absolute\\n(calendar-based) time line to a relative time line is a key operator\\nin the VISITORS system. The relative time line is set by identifying\\nclinically signiﬁcant events, or another clinically signiﬁcant time\\npoint in the domain’s temporal-abstraction ontology (e.g., start of\\ntherapy, birth of the child, start of high fever), which serve as a date\\nof reference (time zero) for all patient data.\\nSeveral patients in the group might not have experienced the\\nreference event (e.g., a particular type of intervention). Thus, only\\nthe data points of patients who have experienced the reference\\nevent are displayed. Since each patient may have experienced the\\nintervention at a different time from the other patients, we must\\nalign the data of all of the patients according to the reference time\\npoint, which is set as the new (relative) zero-time point. Moreover,\\nin the case of a temporal abstraction, we must calculate the\\ndistribution of abstract concept values by including only patients\\nwho had experienced the selected event or intervention.\\nFor uniformity, each ‘‘month’’ in the relative time line includes\\nby deﬁnition 30 days and each ‘‘year’’ comprises 360 days (or 12\\nmonths) (see Section 2.2.3). These are, of course, only labels for\\ndurations, and no longer bear any relation to absolute calendrical\\nmonths.\\nNote that the Set Relative Time Operator is not purely a\\nsyntactic operator, because, although the data type and domain of\\nvalues remain the same, the output data values are typically quite\\ndifferent, all values and statistics being recalculated according to\\nthe new zero point.\\nThe Set Relative Time Operator (SRTO) is formally deﬁned as\\nfollows:\\nSRTOðGCM : < Conce ptc; in put data\\x05; T-Granaggreg; function > ;\\nGDM : < Conce ptc; current values\\x05; T-Startexplor; T-Endexplor;\\nT-Granexplor; ½RefP\\x07 > ; RefP0Þ ) GDM0 : < Conce ptc; new values0\\x05;\\nT-Startexplor; T-Endexplor; T-Granexplor; RefP0 > ;\\nwhere RefP is the user-determined parameter of the zero-point of\\nthe displayed data. Note that the GCM supports the computation of\\nthe\\nnew\\nvalues;\\nthe\\naggregation\\n(T-Granaggreg)\\nand\\ndisplay\\nparameters (T-Startexplor, T-Endexplor, T-Granexplor) are not changed.\\nOnly the visualized data are changed.\\nBefore application of the Set Relative Time Operator, the T-\\nStartexplor and T-Endexplor time points have an absolute time value\\n(e.g., 01.07.1995), but after application they have a relative time\\nformat (e.g., +1 month following the RefP, for example, a bone-\\nmarrow transplantation intervention). Figs. 10 and 11 show the\\napplication of the Set Relative Time Operator to the values of a raw\\nand abstract concepts for a group of patients.\\n4.5.4. Other general exploration and documentation operators\\nOther visualization and exploration operators that are well\\ndescribed in our previous work regarding exploration of individual\\npatient data [2] are also implemented in the VISITORS system.\\nThese operators include: (1) export of data to a spreadsheet, (2)\\nperformance of dynamic sensitivity analysis of possible changes to\\nthe values (‘What-if’’ dynamic simulation) of the raw data, and\\ntheir effect on the derived temporal abstraction for the individual\\npatient (i.e., the user is able to simulate the effect of changing the\\nderived temporal abstractions by modifying, deleting or adding\\nFig. 9. Visualization of the data for the White blood cell (WBC) state abstract concept for group of patients before (panel A) and after (panel B) application the Change Delegate\\nValue Operator. Panel A displays the distribution of the values by representing the value of the abstraction that has the maximal cumulative duration. By using such a function,\\nthe state of the WBC count for all patients was abstracted as ‘‘normal’’ during April 1995 (see tooltip in the panel A). The resultant visualization (panel B) displays the new\\ndistribution of the values, this time using a delegate function that selects the value associated with the longest-duration interval. Thus, during April 1995, the state of the WBC\\ncount for half of the patients was abstracted as ‘‘normal’’, and for half of the patients was abstracted as ‘‘very-low’’.\\nFig. 10. Application of the Set Relative Time Operator to the data for the White blood cell (WBC) count raw concept for a group of 58 patients. Panel A displays the data for the\\nyear 1995 (the absolute time-line navigation bottom bars), and the panel B displays the data after application of the Set Relative Time Operator, showing the ﬁrst year\\nfollowing the allogenic bone-marrow transplantation (selected by user), as shown in the new relative time-line navigation bottom bars. Note that absolute calendrical labels\\nsuch as May 1995 no longer appear; instead, only relative temporal labels such as 5m (+5 months) can be seen.\\nFig. 8. Visualization of the data for the Red blood cell (RBC) count raw concept for the 58 patients (selected earlier by the user) before (panel A) and after (panel B) application\\nof the Change Delegate Value Operator. Panel A displays the RBC data at a granularity of seconds and minimal- and maximal-value population delegate values (population\\nstatistics) at a granularity of months (the mean is not displayed). The resultant visualization (panel B) displays the monthly mean RBC values for each patient. The green graph\\nrepresents the yearly mean value (one value during year 1995) for all patients for whom data had been collected during the year 1995.\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n23\\n\\nraw data), (3) the exploration of the knowledge-based ontology\\n(i.e., exploration of the properties of both the raw concepts and\\ntheir abstractions, using meaningful domain-speciﬁc semantic\\nrelations such as derived-from, part-of), (4) support for the use of\\nclinical proﬁles (e.g., a diabetic proﬁle includes a set of diabetes-\\nrelated raw and abstract concepts such as Blood-glucose value,\\nGlucose-state level, HbA1C value, etc.), (5) support of intra-group\\ncollaboration capabilities, and (6) documentation operators.\\n4.6. Interactive visual exploration of temporal associations among\\ntime-oriented data of multiple patients\\nA temporal association chart (TAC) [57] is a new, user-driven,\\ninteractive knowledge-based visualization technique that supports\\nthe investigation of temporal and statistical associations within\\nmultiple patient records among both raw and abstract temporal\\nconcepts. A complete exposition of the computational semantics\\nunderlyingTACS,aswellasadetailedevaluationoftheirfunctionality\\nandusability,appearselsewhere[57].Here,weprovideasummaryof\\nthe core functionality that is relevant to the current study.\\nThe core of the TAC is an ordered list of raw and/or abstract\\ndomain concepts (e.g., Platelet-state, Hemoglobin value, White\\nblood cell count), designated a temporal association template (TAT),\\nin a particular order determined by the user. Each concept is\\nmeasured (or computed, in the case of an abstract concept) for a\\nparticular patient group during a particular concept-speciﬁc time\\nperiod. The period can be different for each concept. Given the data\\nfor a group of patients, between each consecutive pair of concepts\\nin the list, a relationship will be computed based on the delegate\\nvalues of the concepts for each patient. If one of the concepts is raw,\\nthe result will be a set of relations, each relation being between a\\nvalue of the ﬁrst concept and a value of the second concept for a\\nparticular patient. If both concepts are abstract, the result will be\\naggregated into a set of extended relations—temporal association\\nrules, one rule per each combination of values from both concepts,\\neach rule representing the set of patients who have this particular\\ncombination of values for the two abstract concepts.\\nTACs are created by the user in two steps. First, the user creates\\nthe TAT, by the selection of two or more concepts, using an\\nappropriate interface (not shown here), possibly changing the order\\nas necessary; and second, the user selects the group of patients (e.g.,\\nfrom a list of groups retrieved earlier by Select Patients expressions).\\nAlthough full exposition of TACs is beyond the scope of the current\\npaper, we will brieﬂy explain their core semantics.\\n4.6.1. Temporal association templates\\nA TAT is an ordered list of time-oriented concepts (TOCs)\\n(jTOCsj \\x08 2), in which each TOC denotes a combination of a raw or\\nabstract domain concept (such as a Hemoglobin value or a bone-\\nmarrow toxicity grade) and a time interval <T-Start, T-End>. A\\nspeciﬁc concept can appear more than once in the TAT, but only\\nwithin different time intervals. An example of a TAT listing the\\nHemoglobin-state and White blood cell-state abstract concepts\\nand the Platelet count raw-data concepts, and their respective time\\nperiods, would be <(Hemoglobin-state, 1/1/95, 31/1/95), (WBC-\\nstate, 1/1/95, 31/1/95), (Platelet count, 1/1/95, 31/1/95), (WBC-\\nstate, 1/2/95, 28/2/95)>. Note that once a TAT is deﬁned, it can be\\napplied to different patient groups.\\n4.6.2. Application of a TAT to a set of patient records\\nWhen applying a TAT to a set P of patient records that includes N\\npatients, we get a TAC. A TAC is a list of instantiated TOCs and of\\nassociation relations (ARs), in which each instantiated TOC is\\ncomposed of the original TOC of the TAT upon which it is based and\\nthe patient-speciﬁc delegate values for that TOC within its\\nrespective time interval, based on the actual values of the records\\nin P. To be included in a TAC, a patient Pn (1 \\x06 n \\x06 N) must have at\\nleast one value from each TOC of the TAT deﬁning the TAC. The\\ngroup of such patients is the relevant group (or relevant patients). In\\nthe resulting TAC, each instantiated TOCi includes the original TAT\\nTOCi and the set of delegate values (one delegate value for each\\npatient) of the concept Ci, computed using the delegate function\\nappropriate to Ci from the set of patient data included within the\\nrespective time interval [T-Start i, T-Endi], as deﬁned in the TAT.\\n4.6.3. Association relations\\nThe relationship between the values of consecutive instantiated\\nTOCs <TOCi, TOCi+1>, 1 \\x06 i < I (I is the number of concepts in the\\nTAT) are denoted by ARs.\\nWhen at least one of the consecutive concepts is raw, the\\nnumber of ARs between each pair of TOCs is equal to the number of\\nrelevant patients. Each AR connects the delegate values vali\\nn and\\nvaliþ1\\nn\\nof the pair of concepts Ci and Ci+1, during the relevant period\\nof each concept, for one speciﬁc patient Pn.\\nIn the case of an abstract-abstract concept pair, we aggregate\\nthe ARs between two consecutive TOCs into groups, where each\\ngroup includes a set of identical pairs of delegate values (one pair\\nfor each concept). Each such group denotes a temporal association\\nrule (TAR) and includes:\\n\\x03 Support: the proportion of relevant patients who have the\\ncombination of delegate values < vali; j\\nn , valiþ1;k\\nn\\n>, 1 \\x06 j \\x06 J,\\n1 \\x06 k \\x06 K, where vali; j\\nn , valiþ1;k\\nn\\nare the jth and kth allowed values of\\nCi and Ci+1, respectively; and J and K are the numbers of different\\nvalues of the concepts Ci and Ci+1, respectively. (We assume a\\nﬁnite number of [symbolic] values for each abstract concept.)\\n\\x03 Conﬁdence: the fraction of the relevant patients who, given a\\ndelegatevaluevali; j\\nn ofconceptCiforpatientPn,haveadelegatevalue\\nof concept Ci+1 that is valiþ1;k\\nn\\n; i.e., the probability P½valiþ1;k\\nn\\njvali; j\\nn \\x07.\\n\\x03 Actual number of patients: the number of patients who have this\\ncombination of values.\\nThe number of possible TARs between two consecutive TOCs is\\nthus J*K.\\nNote, that the deﬁnitions of the support and conﬁdence\\nproperties are equal to support and conﬁdence in the context of\\nassociation rules, where performing general data mining.\\n4.7. Display of TACs and interactive data mining using TACs\\nFig. 12 presents an example of a TAC computed by applying a\\nTAT [user-deﬁned on the ﬂy, using another interface (not shown\\nFig. 11. Application of the Set Relative Time Operator to the data for the Platelet-state abstract concept for a group of 58 patients. Panel A displays the data for May 1995 (the\\nabsolute time-line navigation bottom bars), and panel B displays the data after application of the Set Relative Time Operator, showing the ﬁrst month following the allogenic\\nbone-marrow transplantation (selected by user), as shown in the new relative time-line navigation bottom bars. Note that absolute calendrical labels such as March 15th,\\n1995 no longer appear; instead, only relative temporal labels such as 8d (+8 days) can be seen.\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n24\\n\\nhere) that enables the user to select TAT concepts]. The TAT\\nincludes three hematological concepts (Platelet-state, White blood\\ncells (WBC)-state abstract concepts, and the Hemoglobin (HGB)\\nraw concept) and two hepatic concepts (Total bilirubin (T-Bili) and\\nAlkaline-phosphatase (Alk-Phos)-state abstract concepts) applied\\nto a group of 58 patients selected earlier by the user. For the\\nabstract concepts, WBC-, Platelet-, and Alkaline-phosphatase\\nstates, the visualization in Fig. 12 shows the relative proportion\\n(i.e., distribution) of all the values of the speciﬁc abstract concept\\nfor the relevant patients, within the speciﬁc time interval. It also\\nshows each patient’s mean values for Hemoglobin and Total\\nbilirubin during the year 1995. Delegate values of all adjacent\\nconcept pairs for each patient are connected by lines, denoting the\\nARs. Only 44 patients in this particular group had data for all\\nconcepts during 1995.\\nAs described above, ARs among values of abstract concepts\\nprovide additional statistical information. For example, the AR’s\\nwidth indicates to the user the support for each combination of\\nvalues,\\nwhile\\nthe\\ncolor\\nsaturation\\nrepresents\\nthe\\nlevel\\nof\\nconﬁdence: a deep shade of red signiﬁes high conﬁdence, while\\npink denotes lower conﬁdence. The support, conﬁdence, and the\\nnumber of patients in each association are displayed numerically\\non the edge. For example, the widest edge among two distributions\\non the left hand side in Fig. 12 represents the relation between the\\n‘‘low’’ value of the Platelet-state concept and the ‘‘normal’’ value of\\nthe WBC-state concept during 1995. The edge shows that 50.0% of\\nall of the patients in the relevant patient group had this, particular,\\ncombination of values during 1995 (i.e., support = 0.500). 75.9% of\\nthe patients who had a ‘‘low’’ Platelet-state value have also had a\\n‘‘normal’’ WBC-state value during 1995 (i.e., conﬁdence = 0.759).\\nThis association was valid for 22 patients. Note that in this\\nparticular case, the two periods were identical (the year 1995).\\nHowever, the user could have asked to visualize ARs that link\\ndifferent time periods (e.g., different years or different months\\nwithin the same or another year).\\nUsing this visualization interface, the user can dynamically\\napply a value and time lens to interactively analyze the time and\\nvalue associations among multiple patients’ data:\\n\\x03 Dynamic application of a value lens enables the user to answer\\nthe question ‘‘how does constraining the value of one concept\\nduring a particular time period affect the association between\\nmultiple concepts during that and/or during additional time\\nperiods’’. The user can either select another range of values for\\nthe data of the raw concepts, using trackbars, or select a subset\\nof the relevant values in the case of an abstract concept. In the\\nfuture versions of VISITORS, we are planning to allow the user\\nto vary\\nalso\\nthe\\ndelegate function\\nto\\nenable\\nadditional\\nanalyses.\\n\\x03 The system also supports the application of a time lens by\\nchanging the range of the time interval for each instantiated TOC,\\nincluding ranges on the relative time line. The time lens can be\\nespecially useful for clinical research involving longitudinal\\nmonitoring (e.g., clinical trials). For example, the researcher can\\ninvestigate the relation between the values among several\\nconcepts before and after treatment.\\nIn addition, the user can change the order of the displayed\\nconcepts, export all the visualized data and associations to an\\nelectronic spreadsheet, and add or remove displayed concepts.\\nThe main limitation of TACs in the current version of the\\nVISITORS system is that the system does not recommend which\\nconcepts to select, nor the time periods in which to examine the\\nedges (ARs) among them. However, we intend to combine the\\nVISITORS system with temporal data mining tools (that we have\\nbeen developing [58]) for automated detection of sufﬁciently\\nfrequent temporal associations.\\n5. Example of a clinical scenario\\nIn this section, we present an example of an exploration clinical\\nscenario for application of the VISITORS system and an analysis\\nusing a TAC. The example, which relates to a retrospective database\\nof bone-marrow transplantation (BMT) patients (see evaluation\\nSection 6), comprises an investigation of the bone-marrow recovery\\ncharacteristics of patients, who are either young [<20 years] or old\\n[>70 years], following an autologous BMT procedure.\\nFig. 12. Visualization of associations among three hematological and two hepatic concepts for 44 patients during the year 1995. Association rules are displayed between the\\nPlatelet-state and White blood cells (WBC)-state abstract concepts. The conﬁdence and support scales are represented on the left.\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n25\\n\\n(1) In a previous study [4] we introduced the graphical interfaces\\nfor construction of the Select Patients expression ‘‘Select all\\nmale patients, either younger than 20 years or older than 70\\nyears, whose value of the Hemoglobin (HGB) state abstract\\nconcept was at least ‘‘moderately-low’’ or higher, for at least\\nten days, during the ﬁrst month following an autologous BMT\\nand whose White blood cell (WBC) counts were abstracted as\\n‘‘increasing’’ during those ten days’’. As a result of applying this\\nSelect Patients expression, a group of patients who recovered\\nfrom the autologous BMT (designated as BMT_Au Recovering by\\nthe clinician) was returned and saved. Our current scenario\\ncontinues from that point.\\n(2) In the second stage, the clinician explores the data of 124\\npatients in the ‘‘BMT_Au Recovering’’ group of Myelotoxicity-,\\nWBC-, Platelet-, and HGB-state abstract concepts (Fig. 13)\\nduring the ﬁrst month following the autologous BMT.\\nIt is clear from the ﬁrst (top) panel, i.e., Myelotoxicity-state\\nconcept panel, that until the sixth day after the BMT, the\\nportion of patients with a high value of the myelotoxicity-state\\nconcept (grade_3) was relatively high and increased from day 1\\nto day 6. During these ﬁrst six days, the portion of patients with\\nthe ‘‘very-low’’ value of the WBC-state concept also increased\\neach day (up to 87.78% of patients in the group by the sixth\\nday), as shown in the tooltip of the second panel from the top.\\nStarting from the sixth day following the transplantation,\\nthe portion of patients in the group with values of the WBC-\\nstate concept higher than ‘‘very-low’’ began to increase, and by\\nthe twelfth day, most patients had ‘‘normal’’ or higher values of\\nthe WBC-state, as shown in panel 2.\\nFrom the third panel, i.e., the Platelet-state concept panel,\\nwe can visually conclude that during the ﬁrst month following\\nthe transplantation at least 50% of the patients had a daily\\n‘‘low’’ or ‘‘moderately-low’’ value of the Platelet-state concept.\\nFrom the tenth day onwards, the portion of patients with\\nvalues of Platelet-state higher than ‘‘low’’ appeared to increase.\\nExploration of the fourth panel, i.e., the HGB-state concept\\npanel, shows that during the ﬁrst month following the\\ntransplantation most of the patients had a ‘‘moderately-low’’\\nHGB-state concept.\\nThus, given the data for the selected 124 patients, by\\nfollowing a simple exploration process, we can conclude that:\\n(1) the post-BMT recovery process for most patients seemed to\\nhave started on the tenth day following the BMT; and (2) the\\nHGB-state concept did not seem to be affected by the BMT\\nprocedure or by the recovery of the WBC and Platelet counts.\\n(3) Finally, the clinician creates a TAC to examine the relationships\\namong the delegate values of the 124 patients for the above four\\nabstract concepts during the ﬁrst two weeks following the BMT\\n(Fig. 14). Note that for only 91 patients in the group were there\\nvalues for all four concepts during the desired time period.\\nDuring the period of the ﬁrst two weeks following the BMT,\\nmost patients had a high-grade Myelotoxicity level, a very-low\\nWBC-state, and a low Platelet-state. Moreover, for a patient with a\\n‘‘grade_3’’ value of the Myelotoxicity-state concept, there was a\\n92.3% probability of a ‘‘very-low’’ value of the WBC-state abstract\\nconcept. Similarly, given a ‘‘very-low’’ value of the WBC-state\\nconcept, 86.8% of the patients had a ‘‘low’’ value of the Platelet-\\nstate concept.\\nFrom the TAC in Fig. 14, we can conclude that the characteristic\\n‘‘proﬁle’’ of most of the 124 patients was a combination of a\\n‘‘grade_3’’ value of the Myelotoxicity-state, a ‘‘very-low’’ value of\\nthe\\nWBC-state, a\\n‘‘low’’ value of the Platelet-state, and a\\n‘‘moderately-low’’ value of the HGB-state. If each concept were\\nFig. 13. Exploration of the data of 124 patients for the Myelotoxicity-, WBC-, Platelet-, and HGB-state abstract concepts (denoted by 1–4 from the top to the bottom panel). See\\nthe text for details.\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n26\\n\\nmeasured at a different period, we would get a characteristic\\ntemporal path.\\n6. Evaluation of the functionality and usability of the VISITORS\\nsystem\\n6.1. Research questions\\nWe designed and developed the VISITORS system according to\\nthe desiderata listed in Section 3, and envision it as potentially\\nuseful for two types of users: clinicians and medical informati-\\ncians. We also envision that the system can be used to answer\\ndifferent\\nclinically\\nmotivated\\nquestions.\\nWe\\nconducted\\nan\\nevaluation of the system with the aim of answering the following\\nfour research questions:\\n1. Overall functionality and usability: are the interactive explora-\\ntion operators of the VISITORS system feasible, functional, and\\nusable?\\n2. Effect of the interaction mode: it there a signiﬁcant difference in\\nthe accuracy or in the time to answer between the answers\\nobtained using the operators of the general exploration mode\\nand those obtained using the operators of the TACs?\\n3. Effect of the time line: is there a signiﬁcant difference in the\\naccuracy of the clinical scenarios or in the time to answer using\\nthe two types of time line (an absolute, i.e., calendar time line,\\nversus a relative time line, i.e., a time line referenced to an\\nexternal signiﬁcant clinical time point, such as a therapeutic\\nintervention), and if so, which time line leads to a lower or a\\nhigher level of accuracy and time to answer?\\n4. Effect of the user group: can both medical informaticians\\n(e.g.,\\ninformation\\nsystem\\nengineers\\nwho\\nwork\\nin\\nthe\\nmedical informatics domain) and clinicians use the system\\neffectively?\\n6.2. Measurement methods and data collection\\nTo the best of our knowledge, there is no known method that is\\nfunctionally equivalent to the VISITORS system. Furthermore, with\\ncurrent methods users can answer the complex questions for\\nwhich VISITORS was designed only through the performing the\\nlaborious computations. Thus, we have chosen an objective-based\\napproach [59] for evaluation of the VISITORS system. In such an\\napproach, certain reasonable objectives are deﬁned for a new\\nsystem, and the evaluation strives to demonstrate that these\\nobjectives have been achieved. In this case, we set out to prove\\ncertain functionality and usability objectives of the VISITORS\\nsystem.\\nThe evaluation of the VISITORS system was performed in the\\noncology domain. In all the tests, we used a retrospective database\\nof more than 1000 oncology patients who had received bone-\\nmarrow transplants and who had been followed-up for 2–4 years.\\nThe knowledge source used for the evaluation was an oncology\\nknowledge base speciﬁc to the bone-marrow transplantation\\ndomain; the source was acquired with the help of a domain expert,\\nas reported in a previous study [60].\\nTen participants, ﬁve medical informaticians, i.e., the informa-\\ntion system engineers who work in the medical informatics\\ndomain, and ﬁve clinicians with different medical training levels,\\nwere asked to answer ten clinical questions: ﬁve questions\\nFig. 14. Visualization of associations among the myelotoxicity-, WBC-, Platelet-, and HGB-state abstract concepts (denoted by 1–4 from the left to the right) for 124 patients\\nduring the ﬁrst two weeks following BMT. See the text for details.\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n27\\n\\nrequired the use of the general exploration operators of VISITORS,\\nand ﬁve questions required the use of TACs. None of the study\\nparticipants was a member of the VISITORS development team.\\nThe ten questions were selected in consultation with oncology\\ndomain experts. They represented typical questions relevant to the\\nmonitoring of a group of oncology patients or to the analysis of an\\nexperimental protocol in oncology. Examples of the questions are\\npresented in Table 1. The order of the questions (in each of the two\\ncategories according to the interaction mode) was permuted\\nrandomly across participants. Each evaluation session with a\\nparticipant started with a 20-min tutorial, which included a brief\\ndescription of the KBTA methodology [1] and of the general\\nexploration and TAC operators. A demonstration of the general and\\nTAC operators was given, showing how several typical clinical\\nquestions can be answered. The scope of the instruction was\\npredetermined and included (after the demo) testing each\\nparticipant by asking him/her to answer three clinical questions,\\none of which included the use of TACs. When the participant could\\nanswer the questions correctly, he/she was considered ready to\\nperform for the evaluation.\\nThe functionality was assessed using two parameters: the\\ntime in minutes needed to answer the question, and the\\naccuracy of the answer. The scale determining the accuracy of\\neach answer was pre-deﬁned before the start of the study with\\nthe help of a medical expert. The accuracy was measured on a\\nscale from 0 (completely wrong value) to 100 (completely\\ncorrect value).\\nTo test the usability of VISITORS system, we used the system\\nusability scale (SUS) [61], a common validated method to evaluate\\ninterface usability. The SUS is a questionnaire that includes ten\\npredeﬁned questions regarding the effectiveness, efﬁciency, and\\nsatisfaction from an interface. SUS scores have a range of 0–100.\\nInformally, a score higher than 50 is considered to indicate a usable\\nsystem.\\n7. Results\\nThis section summarizes the evaluation results in terms of the\\nfour research questions deﬁned in Section 6.1.\\n7.1. Overall functionality and usability\\n7.1.1. Method of measurement\\nThe effectiveness of the users in answering clinical questions\\nusing the general exploration operators and TACs was assessed by\\ncalculating the overall means and standard deviations of the\\nanswer accuracy and of the answer response time. To test the\\nusability of the system, the SUS questionnaire was used. To\\ncompare the usability scores of the two groups of participants, a t-\\ntest was performed.\\n7.1.2. Results\\nTable 2 summarizes the response times and answer accuracy\\nlevels for all participants.\\nMost of the participants (9 of 10) successfully answered the\\nclinical questions with a mean accuracy of more than 96 (out of\\n100); 6 of them had a mean accuracy of 100. One participant had a\\nmean accuracy 90.5. All participants answered the clinical\\nquestions in a mean time of less than 3 min.\\nThe mean SUS score for all operators, across all participants,\\nwas 69.3 (over 50 is usable). The results of a t-test analysis showed\\nthat the mean SUS score of the medical informaticians (80.5) was\\nsigniﬁcantly higher than that of the clinicians (58): [t(8) = 3.88,\\np < 0.01].\\n7.1.3. Conclusion\\nBased on the results of the VISITORS evaluation, we may\\nconclude that, after a short training period, the participants were\\nable to answer the clinical questions with high accuracy and within\\nshort period of time. The SUS scores showed that VISITORS system\\nis usable but still needs to be improved.\\n7.2. Effects of the interaction mode, time line and user group\\nTo answer research questions 2, 3 and 4, i.e., questions\\nregarding the interaction mode effect, effect of the time line,\\nand the user group effect, a joint analysis was performed, as\\nexplained below.\\nTable 1\\nExamples of clinical questions used in the evaluation, ordered informally by level of\\ncomplexity.\\nCategory\\nExamples of questions\\nGeneral\\nexploration\\noperators\\n\\x03 What was the mean (i.e., average) annual value of the\\nHemoglobin (HGB) raw data concept during each of\\nthe years 1995, 1996, and 1997?\\n\\x03 What was the distribution of the values of the\\nPlatelet-state abstract concept during the ﬁrst and\\nsecond months (i.e., relative time) following the\\nbone-marrow transplantation (BMT) procedure?\\n\\x03 What was the distribution of the aggregate\\nvalues of the Platelet-state abstract concept\\nduring the ﬁrst month after BMT? Who (i.e., which\\npatient) had the maximal delegate value of the\\nWhite blood cell (WBC) count raw concept and\\nwhich patient had the minimal delegate value\\nof Red blood cells (RBC) count?\\nTemporal\\nassociation\\ncharts\\n\\x03 What delegate value of the HGB-state abstract\\nconcept was the most frequent among the\\npatients who previously had a ‘‘low’’ aggregate\\nvalue of the Platelet-state abstract concept?\\n\\x03 What was the distribution of the delegate values\\nof the Platelet-state in patients whose minimal\\ndelegate value of the WBC count raw concept was\\n5000 cells/ml (instead of the previous minimal value)?\\nWhat were the new maximal and minimal\\ndelegate values of the RBC?\\n\\x03 What percentage of the patients had previously had a\\n‘‘low’’ delegate value of the Platelet-state abstract\\nconcept during both the ﬁrst and second month\\nfollowing the BMT?\\nTable 2\\nAccuracy and response times for all participants.\\nGeneral exploration\\noperators\\nTemporal association\\ncharts\\nOverall\\nAccuracy score (0–100)\\nMean accuracy\\n99.5 \\x02 1.6\\n97.9 \\x02 3.4\\n98.7 \\x02 2.4\\nRange of mean accuracy per question across all participants\\n[97.5. . .100]\\n[95.0. . .100]\\n[95.0. . .100]\\nRange of mean accuracy per participant across all questions\\n[95.0. . .100]\\n[90.5. . .100]\\n[93. . .100]\\nResponse time (min)\\nMean time\\n2.2 \\x02 0.2\\n2.7 \\x02 0.4\\n2.5 \\x02 0.2\\nRange of mean time per question across all participants\\n[2.2. . .2.4]\\n[2.4. . .3.0]\\n[2.2. . .3.0]\\nRange of mean time per participant across all questions\\n[2.0. . .2.6]\\n[2.2. . .3.6]\\n[2.2. . .3.0]\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n28\\n\\n7.2.1. Method of measurements\\nThe effects of the interaction mode (i.e., general exploration\\noperators and TACs), time line (i.e., absolute and relative times),\\nand the group of participants (i.e., medical informaticians and\\nclinicians) on the dependent variables of response time and\\naccuracy of answering were examined using two different three-\\nway ANOVA tests with repeated measures (one for each dependent\\nvariable). The interaction mode and the time-line type were\\nwithin-subject independent variables, and the group of subjects\\nwas a between-subjects independent variable. Since we did not\\nﬁnd statistically signiﬁcant differences among the response times\\nto different clinical questions (and among resultant accuracy levels\\nof answering these questions) of the same time line using the same\\ninteraction mode, the mean value of the response time (and of the\\naccuracy) of the clinical questions of the same time line was used\\nas the dependent variable.\\n7.2.2. Results\\nThe results of the analysis of the accuracy showed no signiﬁcant\\neffects (p > 0.05) of the interaction mode, time line, or of the group\\nof participants. With respect to the response time, the results of the\\nanalysis showed that the only signiﬁcant effect was the main effect\\nof the type of interaction mode [F(1, 8) = 11.08, p < 0.05], i.e., a\\nmean of 2.2 \\x02 0.2 min for answering the clinical questions when\\nusing the general exploration operators of VISITORS, and a mean of\\n2.7 \\x02 0.4 min for answering the clinical questions using the TACs.\\nThere was no signiﬁcant difference in the response time either of the\\ntime line or of the group of participants.\\n7.2.3. Conclusion\\nInteraction mode, time line and user type did not affect the\\naccuracy of the answers to clinical questions. The mean time\\nneeded to answer the clinical scenarios using the TACs was\\nsigniﬁcantly higher than that for the general exploration operators\\nof VISITORS, but it was still less than 3 min. The time line and the\\nuser type did not affect the response time of the answers to the\\nclinical questions.\\n8. Discussion\\n8.1. Contributions and advantages\\nThe major contribution of the VISITORS system is the provision\\nof a comprehensive environment for intelligent, i.e., knowledge-\\nbased, investigation of time-oriented data for multiple patients,\\nincluding the speciﬁcation and retrieval, visualization, exploration,\\nand analysis of the time and value associations among both raw\\nand abstract clinical concepts. Based on the results of the current\\nstudy, the VISITORS system might be described as an ‘‘intelligent\\nequalizer’’ for data interpretation and exploration, which results in\\na uniform performance level, regardless of the patient-speciﬁca-\\ntion complexity, the exploration question or mode, or the user\\ntype. A similar insight has emerged from our previous study,\\nregarding assessment of the usability and functionality of the\\nOBTAIN\\nlanguage\\nexpression-speciﬁcation\\ntool\\n[4].\\nIn\\nthat\\nexperiment, both medical informaticians and clinicians have\\nconstructed complex expressions quickly and efﬁciently, al-\\nthough the clinicians found the interface less usable than the\\ninformaticians.\\nThe VISITORS system can access diverse types of temporal\\nabstraction knowledge and clinical data. Moreover, the VISITORS\\nsystem is quite generic and can support exploration within non-\\nmedical domains, for example, in the information security domain\\n[62].\\nSomewhat\\nsurprisingly,\\nexploration\\nof\\ndata\\nof\\nmultiple\\npatients using a relative time line (i.e., relative to a meaningful\\nreference event, or to another clinically signiﬁcant time point) did\\nnot seem to be more difﬁcult to either clinicians and informa-\\nticians than exploration using an absolute (i.e., calendrical) time\\nline. A possible explanation might be that there is no semantic\\ndifference between ‘‘the ﬁrst day of December’’ and ‘‘the ﬁrst day\\nafter bone-marrow transplantation’’; thus, it is equally easy for\\npeople to explore data within absolute and relative time lines.\\nFurthermore, clinicians are trained to think along relative time\\nlines, such as ‘‘the fourth month of pregnancy’’ or the ‘‘second day\\nof therapy’’. However, using TACs to answer questions, although\\nresulting in equal accuracy levels, required signiﬁcantly higher\\nresponse times. These results probably reﬂect the highly unusual\\nTACs interface and data mining parameters (i.e., support and\\nconﬁdence), which unlike the relative time line, are qualitatively\\ndifferent from the general exploration operators and exploration\\ninterfaces. A possible reason for the lack of signiﬁcant differences\\nin the accuracy scores when using different interaction modes was\\nthat\\nthe\\nevaluation\\nincluded\\na\\nrelatively\\nsmall\\ngroup\\nof\\nparticipants and questions. However, it should be noted that\\nthe variance among accuracy scores was quite low for both\\ninteraction modes, all of the participants achieving scores above\\n90. Thus, the absence of a signiﬁcant effect could not be attributed\\nto random differences and high variability in each interaction\\nmode.\\n8.2. Limitations and future work\\nAlthough the user interfaces of the VISITORS system were\\ninitially\\nconsidered\\nby\\nthe\\nusers\\nas\\nquite\\ncomplex,\\nmost\\nparticipants, during the population speciﬁcation and exploration\\nevaluations successfully ﬁnished all the evaluation tasks. After the\\ntraining session and the learning of the principles of the system,\\nthe system was considered less complex than before. However, the\\nparticipants noted that the system provides more functionality\\nthan will usually be exploited by typical clinicians or even clinical\\nresearchers. Thus, in the future, the main user interface should be\\nsimpliﬁed by providing only the basic functions, while more\\ncomplex features (such as time-range constraints regarding the\\nstart or end of time intervals) would be enabled only in an\\nadvanced mode.\\nThe VISITORS system can also be improved by adding\\ncapabilities for interactive speciﬁcation of the patient popula-\\ntion during the exploration process (in the current state,\\nselection and retrieval of patients is a separate process). The\\nvisual exploration operators can be enhanced by adding more\\nintelligence to the semantic zoom, e.g., increasing the temporal\\ngranularity level of the display (e.g., from the day to month) will\\ncause a display of higher level abstractions when appropriate.\\nFurthermore, ‘‘typical day’’ (modal day) or other granularity\\nvisualizations can be added, which is very important for periodic\\ndomains such as diabetes. Finally, a visual comparison of several\\npopulation groups, based on one or more speciﬁc concepts (for\\nclinical trials or quality assessment tasks), would be quite\\nuseful. We intend to explore all of these options in our future\\nwork.\\nTo summarize, we conclude that intelligent retrieval and\\nexploration of longitudinal data for multiple patients using the\\nVISITORS system is feasible, functional, and usable. Future work is\\nneeded to extend the visual exploration operators and to assess the\\nvalue of such extensions to the users.\\nAcknowledgments\\nThis research was supported by Deutsche Telekom Labs at Ben-\\nGurion University of the Negev and the Israel Ministry of Defense,\\nBGU award No. #89357628-01. We thank all the clinicians and\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n29\\n\\nmedical\\ninformaticians\\nwho\\ncontributed\\ntheir\\ntime\\nto\\nthe\\nevaluation. We thank Ms. Efrat German for her work on the\\nTempura system, and Mr. Ido Hacham and Mr. Shahar Albia for\\ntheir work on the Multi-TOQ system.\\nAppendix A. BNF syntax for a Select Patients expression\\nselect-patients-expression: <data-base> <knowledge-base>\\n<patient-constraints>\\npatient-constraints:\\n<demographic-constraints>*\\n<time-\\nand-value-constraints>+\\nj\\ndemographic-constraints>+\\n<time-\\nand-value-constraints>*\\ndemographic-constraints: <selection-condition>+\\ntime-and-value-constraints: <local-constraints>+ [<global-\\npairwise-constraints>+]\\nselection-condition: <attribute-name> <min value> <max\\nvalue>\\nlocal-constraints:\\n<concept-name>\\n<value-constraints\\n>\\n<time point-constraints> [<duration-constraints>] [<relative-\\ntime-constraints>] [<proportion-constraints>] [<statistical-con-\\nstraints>]\\nglobal-pairwise-constraints:\\n<value-pairwise-constraints>*\\n[<temporal-pairwise-constraints>*]\\nvalue-constraints: <min-value> <max-value>\\nnecessary-context: <context-name>\\ntime point-constraints: <start-point, end-point> [<earliest-\\nstart-point> <latest-start-point>] [<earliest-end-point> <latest-\\nend-point>]\\nduration-constraints: <min-duration> <max-duration>\\nrelative-time-constraints:\\n<relative-start-point>\\n<relative\\nend\\npoint>\\n[<relative-earliest-start-point>\\n<relative-latest-\\nstart-point>]\\n[<relative-earliest-end-point>\\n<relative-latest-\\nend-point>]\\nproportion constraints: <min-threshold> <max-threshold>\\nstatistical\\nconstraints:\\n<individual-patient-delegate-\\nfunction>\\n<population-delegate-function>\\n<value-relation>\\n[<delta>]\\nvalue-pairwise-constraints: <ﬁrst-concept name> <second-\\nconcept name> <time-relation> < individual-patient-delegate-\\nfunction>\\ntemporal-pairwise-constraints:\\n<ﬁrst-concept-name>\\n<second-concept-name>\\n<ﬁrst-boundary-time\\npoint>\\n<second-boundary-time point> <value-relation> [<difference>]\\nindividual-patient-delegate-function:\\n<maximal>\\nj\\n<minimal> j <mean>\\npopulation-delegate-function: <maximal> j <minimal> j\\n<mean>\\nvalue-relation: <great-than-equal> j <great-than> j <less-\\nthan> j <less-than-equal>\\ntime-relation: <before> j <after> j <starts> j <ends> j\\n<within> j <meets> j <overlaps> j <equal>\\ndifference: <min-difference> <max-difference>\\nAppendix B. BNF syntax for a Select Time Intervals expression\\nselect-time-intervals-expression: <data-base> <knowledge-\\nbase> <interval-constraints>\\ninterval-constraints: <granularity> <concept-constraints>*\\n[<time-constraints> j <relative-time-constraints>]\\ngranularity: <seconds> j <minutes> j <hours> j <days> j\\n<months> j <years>\\nconcept-constraints:\\n<concept-name><population-thresh-\\nolds>\\n<value-constraints>\\n<individual-patient-delegate-\\nfunction>\\ntime point-constraints: <start-point, end-point> [<earliest-\\nstart-point> <latest-start-point>] [<earliest-end-point> <latest-\\nend-point>]\\nrelative-time-constraints:\\n<relative-start-point>\\n<relative\\nend\\npoint>\\n[<relative-earliest-start-point>\\n<relative-latest-\\nstart-point>]\\n[<relative-earliest-end-point>\\n<relative-latest-\\nend-point>]\\npopulation-thresholds: <min-threshold> <max-threshold>\\nvalue-constraints: <min-value> <max-value>\\nindividual-patient-delegate-function:\\n<maximal>\\nj\\n<minimal> j <mean>\\nAppendix C. BNF syntax for a Get Patient Data expression\\nget-patients-data-expression:\\n<data-base>\\n<knowledge-\\nbase> < concept-name > <patient-list> [<interval-list>]\\nReferences\\n[1] Shahar Y. A framework for knowledge-based temporal abstraction. Artif Intell\\nMed 1997;90(1–2):79–133.\\n[2] Shahar Y, Goren-Bar D, Boaz D, Tahan G. Distributed, intelligent, interactive\\nvisualization and exploration of time-oriented clinical data and their abstrac-\\ntions. Artif Intell Med 2006;38(2):115–35.\\n[3] Martins S, Shahar Y, Goren-Bar D, Galperin M, Kaizer H, Basso L, et al.\\nEvaluation of an architecture for intelligent query and exploration of time-\\noriented clinical data. Artif Intell Med 2008;43(1):17–34.\\n[4] Klimov D, Shahar Y, Taieb-Maimon M. Intelligent selection and retrieval of\\nmultiple time-oriented records. Journal of Intelligent Information Systems; in\\npress. doi:10.1007/s10844-009r-r0100-0.\\n[5] Hearst M. User interfaces and visualization. In: Baeza Yates R, Ribeiro-Neto B,\\neditors. Modern information retrieval. NY: ACM Press; 2000 . p. 257–324\\n[Chapter 1].\\n[6] Goldstein M, Hoffman B. Graphical displays to improve guideline-based\\ntherapy of hypertension. In: Izzo Jr JL, Black HR, editors. Hypertension primer.\\n3rd ed., Baltimore: Lippincot, Williams & Wikkins; 2003.\\n[7] Silvent A-S, Dojat M, Garbay C. Multi-level temporal abstraction for medical\\nscenario construction. Int J Adapt Control 2005;19:377–94.\\n[8] Miksch S, Horn W, Popow C, Paky F. Utilizing temporal data abstraction for\\ndata validation and therapy planning for artiﬁcially ventilated newborn\\ninfants. Artif Intell Med 1996;8:543–76.\\n[9] Cooley J, Tukey J. An algorithm for the machine calculation of complex Fourier\\nseries. Math Comput 1965;19(90):297–301.\\n[10] Miksch S, Seyfang A, Popow C. Abstraction and representation of repeated\\npatterns in high-frequency data. In: The ﬁfth workshop on intelligent data\\nanalysis in medicine and pharmacology (IDAMAP-2000), workshop notes of\\nthe 14th European conference on artiﬁcial intelligence (ECAI-2000);\\n2000.p. 32–9.\\n[11] Combi C, Chittaro L. Abstraction on clinical data sequences: an object-oriented\\ndata model and a query language based on the event calculus. Artif Intell Med\\n1999;17:271–301.\\n[12] Kowalaski R, Sergot M. A logic-based calculus of events. New Gener Comput\\n1986;4:67–95.\\n[13] Chittaro L. Information visualization and its application to medicine. Artif\\nIntell Med 2001;22(2):81–8.\\n[14] Plaisant C, Milash B, Rose A, Widoff S, Shneiderman B. LifeLines: visualizing\\npersonal histories. In: Proceedings of ACM CHI ‘96 Conference. New York, NY,\\nUSA: ACM Publisher; 1996. p. 221–7. ISBN:0-89791r-r777-4.\\n[15] Plaisant C., Mushlin R., Snyder A., Li J., Heller D., Shneiderman B. LifeLines:\\nusing visualization to enhance navigation and analysis of patient records.\\nRevised version appeared in American Medical Informatics Association Annual\\nFall Symposium, 1998: 76–80.\\n[16] Wang T, Plaisant C, Quinn A, Stanchak R, Shneiderman B, Murphy S. Aligning\\ntemporal data by sentinel events: discovering patterns in electronic health\\nrecords. In: Proc. ACM Conf. on human factors in computing systems\\n(CHI2008), ACM. New York, NY, USA: ACM Publisher; 2008 . p. 457–66.\\nISBN:978-1-60558r-r011-1.\\n[17] Spenke M. Visualization and interactive analysis of blood parameters with\\nInfoZoom. Artif Intell Med 2001;22(2):159–72.\\n[18] Falkman G. Information visualisation in clinical Odontology: multidimen-\\nsional\\nanalysis\\nand\\ninteractive\\ndata\\nexploration.\\nArtif\\nIntell\\nMed\\n2001;22(2):133–58.\\n[19] Chittaro L, Combi C, Trapasso G. Data mining on temporal data: a visual\\napproach and its clinical application to hemodialysis. J Vis Lang Comput\\n2003;14(6):591–620.\\n[20] Aigner W, Miksch S, Mu¨ ller W, Schumann H, Tominski C. Visual methods for\\nanalyzing time-oriented data. IEEE Trans Visual Comput Graph 2008;14(1):\\n47–60.\\n[21] Navathe S, Ahmed R. Temporal extensions to the relational model and SQL.\\nIn: Tansel AU, Clifford J, Gadia S, Segev A, Snodgrass R, editors. Temporal\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n30\\n\\nDatabases: Theory, Design, and Implementation. Redwood City, CA: Benja-\\nmin/Cummings; 1993 . p. 6–27.\\n[22] Sarda N. HSQL: A historical query language. In: Tansel A, Clifford J, Gadia S,\\nJajodia S, Segev A, Snodgrass R, editors. Temporal Databases: Theory, Design,\\nand Implementation. Redwood City, CA: Benjamin/Cummings; 1993. p. 110–\\n40.\\n[23] Snodgrass R. The TSQL2 temporal query language. Hingham, MA: Kluwer;\\n1995, ISBN 0-7923-9614-6.\\n[24] Combi C, Pinciroli F, Cavallaro M, Cucchi G. Querying temporal clinical\\ndatabases with different time granularities: the GCH-OSQL language.\\nIn:\\nGardner RM, editor. 19 Annual symposium on computer applications in\\nmedical care. Philadelphia: Hanley & Belfus; 1995. p. 326–30.\\n[25] Combi C, Cucchi G. GCH-OSQL: a temporally-oriented object-oriented query\\nlanguage based on a three-valued logic. In: Proceedings of the Fourth Inter-\\nnational Workshop on Temporal Representation and Reasoning, 1997\\n(TIME’97), 119-126, 1997. ISBN:0-8186-7937-9.\\n[26] Combi C, Cucchi G, Pinciroli F. Applying object-oriented technologies in\\nmodeling and querying temporally-oriented clinical databases dealing with\\ntemporal granularity and indeterminacy. IEEE Trans Inform Technol Biomed\\n1997;1(2):100–27.\\n[27] Das A, Musen M. A temporal query system for protocol directed decision\\nsupport. Methods Inform Med 1994;33(4):358–70.\\n[28] Combi C, Montanari A, Pozzi G. The t4sql temporal query language. In: Mario J,\\nSilva, Alberto HF, Laender, Ricardo A, Baeza-Yates, Deborah L, McGuinness,\\nBjorn Olstad, Oystein Haug Olsen, Andre O, Falcao, editors. Proceedings of the\\nsixteenth ACM conference on information and knowledge management, CIKM\\n2007. 2007. p. 193–202.\\n[29] Combi C, Montanari A. Data models with multiple temporal dimensions:\\ncompleting the picture. In: Klaus R i, Andreas Geppert, Moira C o, editors.\\nCAiSE, volume 2068 of LNCS:187-202, 2001.\\n[30] Hibino S, Rudensteiner E. A visual multimedia query language for temporal\\nanalysis of video data. Multimedia database systems: design and implemen-\\ntation strategies. Kluwer Academic Publishers; 1995, 123–59.\\n[31] Silva S, Catarci T, Schiel U. Formalizing visual interaction with historical\\ndatabases. Inform Systs; 2002;27(7):487–521.\\n[32] Dionisio J, Cardenas A. MQuery: a visual query language for multimedia,\\ntimeline and simulation data. J Visual Lang Comput 1996;7:377–401.\\n[33] HochheiserH,Shneiderman B.Visualspeciﬁcation ofqueries forﬁnding patterns\\nin time-series data. In: Proceedings of discovery science; 2001. p. 441–6.\\n[34] Hochheiser H, Shneiderman B. Dynamic query tools for time series data sets:\\ntimebox widgets for interactive exploration. Inform Visual Spring 2004;3(1):\\n1–18.\\n[35] Chittaro L, Combi C. Visualizing queries on databases of temporal histories:\\nnew metaphors and their evaluation. Data Knowl Eng 2003;44(2):239–64.\\n[36] Chu W, Chih-Cheng H, Cardenas A, Taira R. Knowledge-based image retrieval\\nwith\\nspatial\\nand\\ntemporal\\nconstructs.\\nIEEE\\nTrans\\nKnowl\\nData\\nEng\\n1998;10(6):872–88.\\n[37] Bresciani P, Nori M, Pedot N. QueloDB: a knowledge based visual query system.\\nIn: Proceedings of the 2000 international conference on artiﬁcial intelligence\\nIC-AI 2000, vol. III; 2000.\\n[38] Dumas M, Fauvet M-C, Scholl P-C. Handling temporal grouping and pattern-\\nmatching queries in a temporal object model. In: Georges Gardarin, James C,\\nFrench, Niki Pissinou, Kia Makki, Luc Bouganim, editors. Proceedings of the\\n1998 ACM CIKM international conference on information and knowledge\\nmanagement. 1998. p. 424–31.\\n[39] Clifford J, Croker A, Grandi F, Tuzhilin A. On Temporal Grouping. Temporal\\nDatabases.\\nIn: Clifford J, Tuzhilin A, editors. Recent Advances in Temporal\\nDatabases, Proceedings of the International Workshop on Temporal Data-\\nbases, Zurich, Switzerland, 17-18 September 1995. Springer, Workshops in\\nComputing; 1995. p. 194–213.\\n[40] Moon B, Lupez F, Vijaykumar I. Efﬁcient algorithms for large-scale temporal\\naggregation. IEEE Trans Knowl Data Eng 2003;15(3):744–59.\\n[41] Goralwalla I, Leontiev Y, O¨ zsu T, Szafron D, Combi C. Temporal granularity:\\ncompleting the puzzle. J Intell Inf Syst 2001;16(1):41–63.\\n[42] Goralwalla I, Leontiev Y, O¨ zsu T, Szafron D, Combi C. Temporal granularity for\\nunanchored temporal data. Proceedings of the seventh international confer-\\nence on Information and knowledge management, November 02–07, 1998,\\nBethesda, Maryland, United States. New York, NY, USA: ACM Publisher, 414–\\n23, ISBN:1-58113r-r061-9.\\n[43] Clifford J, Rao A. A simple, general structure for temporal domains. Temporal\\nAspects Inform Systs 1987;17–28.\\n[44] Dal Lago U, Montanari A. Calendars. Time granularities, and automata.\\nProceedings of the 7th international symposium on advances in spatial\\nand temporal databases, 12–15, 2001, Lecture notes in computer science,\\nvolume 2121/2001. Springer Berlin/Heidelberg, 279–98, ISBN:978-3-540-\\n42301-0.\\n[45] Combi C, Pinciroli F, Pozzi G. Managing different time granularities of clinical\\ninformation by an interval-based temporal data model. Methods Inform Med\\n1995;34:458–74.\\n[46] Bettini C, Mascetti S, Pupillo V. A system prototype for solving multi-granu-\\nlarity temporal CSP. In: Recent advances in constraints, revised selected papers\\nfrom the workshop on constraint solving and constraint logic programming\\n(CSCLP) volume 3419 of lecture notes in computer science. Springer; 2005. p.\\n142–56.\\n[47] Bettini C, Wang X, Jajodia S. Solving multi-granularity temporal constraint\\nnetworks. Artif Intell 2002;140(1/2):107–52.\\n[48] Bettini C, Mascetti S, Wang X. Mapping calendar expressions into periodical\\ngranularities. Proc. of the 11th international symposium on temporal repre-\\nsentation and reasoning (TIME). IEEE Computer Society; 2004, p. 96–102,\\nISBN:0-8186-7937-9.\\n[49] Nigrin D, Kohane IS. Temporal expressiveness in querying a time-stamp–based\\nclinical database. J Am Med Inform Assoc 2000;7:152–63.\\n[50] Narayanan A, Shaman T. Iconic SQL: practical issues in the querying of\\ndatabase through structured iconic expressions. J Visual Lang Comput\\n2002;13(6):623–47.\\n[51] Sacchi L, Larizza C, Combi C, Bellazzi R. Data mining with temporal abstrac-\\ntions: learning rules from time series. Data Min Knowl Discov 2007;15(2):\\n217–47.\\n[52] Boaz D, Shahar Y. A distributed temporal-abstraction mediation architecture\\nfor medical databases. Artif Intell Med 2005;34(1):3–24.\\n[53] Shahar Y, Musen M. Knowledge-based temporal abstraction in clinical\\ndomains. Artif Intell Med 1996;8(3):267–98.\\n[54] Spokoiny A, Shahar Y. Active database architecture for knowledge-based\\nincremental abstraction of complex concepts from continuously arriving\\ntime-oriented raw data. J Intell Inform Syst 2007;28(3):199–231.\\n[55] Spokoiny A, Shahar Y. Incremental application of knowledge to continuously\\narriving time-oriented data. J Intell Inform Syst 2008;31(1):1–33.\\n[56] Shahar Y. Knowledge-based temporal interpolation. J Exp Theor Artif Intell\\n1996;11(1):123–44.\\n[57] Klimov D, Shahar Y, Taieb-Maimon M. Intelligent interactive visual explora-\\ntion of temporal associations among multiple time-oriented patient records.\\nMethods Inform Med 2009;48(3):254–62.\\n[58] Moskovitch R, Shahar Y. Temporal data mining based on temporal abstrac-\\ntions. In: ICDM-05 workshop on temporal data mining; 2005.\\n[59] Friedman C, Wyatt J. Evaluation methods in medical informatics. New York:\\nSpringer; 1997.\\n[60] Chakravarty S, Shahar Y. Speciﬁcation and detection of periodicity in clinical\\ndata. Methods Inform Med 2001, 40(5):410–20. [Reprinted in: Haux R, Kuli-\\nkowski C., editors. Yearbook of Medical Informatics 2003, Stuttgart: F.K.\\nSchattauer and The International Medical Informatics Association, forthcom-\\ning].\\n[61] Brooke J. SUS: a ‘‘quick and dirty’’ usability scale. In: Jordan PW, Thomas B,\\nWeerdmeester BA, McClelland AL, editors. Usability evaluation in industry.\\nLondon: Taylor and Francis; 1996.\\n[62] Shabtai A, Klimov D, Shahar Y, Elovici Y. An intelligent, interactive tool for\\nexploration and visualization of time-oriented security data. In: Conference on\\ncomputer and communications security. Proceedings of the 3rd international\\nworkshop on visualization for computer security; 2006. p. 15–22.\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n31\\n'}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/Intelligent-visualization-and-exploration-of-time-_2010_Artificial-Intellige.pdf', 'text': 'Intelligent visualization and exploration of time-oriented data of multiple\\npatients\\nDenis Klimov *, Yuval Shahar, Meirav Taieb-Maimon\\nDepartment of Information Systems Engineering, Faculty of Engineering Sciences, Ben-Gurion University of the Negev, P.O. Box 653, Beer-Sheva 84105, Israel\\n1. Introduction: intelligent visualization of time-oriented data\\nof multiple patients\\nA key task facing clinicians and medical researchers is the\\nanalysis of time-stamped, longitudinal medical records, parti-\\ncularly, records of multiple patients. This capability is necessary\\nto support, for example, quality assessment tasks, analysis of\\nclinical trials, and the discovery of new clinical knowledge.\\nAlthough the task of accessing patient data has been solved\\nmostly through the increasing use of electronic medical record\\n(EMR) systems, there still remains the task of intelligent\\nprocessing\\nof\\ntime-oriented\\nrecords\\nof\\nmultiple\\npatients,\\nincluding the capability for interactive exploration of the results.\\nFor this task, standard means, such as tables, time-oriented\\nstatistical tools, or even more advanced temporal data-mining\\ntechniques, are often not adequate, since their use may demand\\nArtiﬁcial Intelligence in Medicine 49 (2010) 11–31\\nA R T I C L E\\nI N F O\\nArticle history:\\nReceived 20 August 2008\\nReceived in revised form 31 January 2010\\nAccepted 16 February 2010\\nKeywords:\\nMultiple patients\\nIntelligent user interfaces\\nTemporal databases\\nTemporal abstraction\\nKnowledge-based systems\\nInformation visualization\\nInteractive data mining\\nOncology\\nA B S T R A C T\\nObjective: Clinicians and medical researchers alike require useful, intuitive, and intelligent tools to\\nprocess large amounts of time-oriented multiple-patient data from multiple sources. For analyzing the\\nresults of clinical trials or for quality assessment purposes, an aggregated view of a group of patients is\\noften required. To meet this need, we designed and developed the VISualizatIon of Time-Oriented RecordS\\n(VISITORS) system, which combines intelligent temporal analysis and information visualization\\ntechniques. The VISITORS system includes tools for intelligent retrieval, visualization, exploration, and\\nanalysis of raw time-oriented data and derived (abstracted) concepts for multiple patient records. To\\nderive meaningful interpretations from raw time-oriented data (known as temporal abstractions), we\\nused the knowledge-based temporal-abstraction method.\\nMethods: The main module of the VISITORS system is an interactive, ontology-based exploration\\nmodule, which enables the user to visualize raw data and abstract (derived) concepts for multiple patient\\nrecords, at several levels of temporal granularity; to explore these concepts; and to display associations\\namong raw and abstract concepts. A knowledge-based delegate function is used to convert multiple data\\npoints into one delegate value representing each temporal granule. To select the population of patients\\nto explore, the VISITORS system includes an ontology-based temporal-aggregation speciﬁcation\\nlanguage and a graphical expression-speciﬁcation module. The expressions, applied by an external\\ntemporal mediator, retrieve a list of patients, a list of relevant time intervals, and a list of time-oriented\\npatients’ data sets, by using an expressive set of time and value constraints.\\nResults: Functionality and usability evaluation of the interactive exploration module was performed on a\\ndatabase of more than 1000 oncology patients by a group of 10 users—ﬁve clinicians and ﬁve medical\\ninformaticians. Both types of users were able in a short time (mean of 2.5 \\x02 0.2 min per question) to\\nanswer a set of clinical questions, including questions that require the use of specialized operators for ﬁnding\\nassociations among derived temporal abstractions, with high accuracy (mean of 98.7 \\x02 2.4 on a predeﬁned\\nscale from 0 to 100). There were no signiﬁcant differences between the response times and between accuracy\\nlevels of the exploration of the data using different time lines, i.e., absolute (i.e., calendrical) versus relative\\n(referring to some clinical key event). A system usability scale (SUS) questionnaire ﬁlled out by the users\\ndemonstrated the VISITORS system to be usable (mean score for the overall group: 69.3), but the clinicians’\\nusability assessment was signiﬁcantly lower than that of the medical informaticians.\\nConclusions: We conclude that intelligent visualization and exploration of longitudinal data of multiple\\npatients with the VISITORS system is feasible, functional, and usable.\\n\\x02 2010 Elsevier B.V. All rights reserved.\\n* Corresponding author at: Medical Informatics Research Center, Department of\\nInformation Systems Engineering, Ben-Gurion University of the Negev, P.O. Box\\n653, Beer-Sheva 84105, Israel. Tel.: +972 8 6477160; fax: +972 8 6477161.\\nE-mail address: klimov@bgu.ac.il (D. Klimov).\\nContents lists available at ScienceDirect\\nArtificial Intelligence in Medicine\\njournal homepage: www.elsevier.com/locate/aiim\\n0933-3657/$ – see front matter \\x02 2010 Elsevier B.V. All rights reserved.\\ndoi:10.1016/j.artmed.2010.02.001\\n\\nspecialized, advanced knowledge, or they may be applicable only\\nin particular cases.\\nTo derive meaningful patterns and interpretations, known as\\ntemporal abstractions (or abstract concepts), from raw time-\\noriented patient data, we use a knowledge-based temporal-\\nabstraction\\n(KBTA)\\nmethod\\n[1].\\nThrough\\na\\ndomain-speciﬁc\\ntemporal-abstraction knowledge base acquired from a domain\\nexpert, this method derives interval-based temporal abstractions,\\nfor example, the pattern, ‘‘a period of more than two months of\\ngrade 1 or higher bone-marrow toxicity, followed within three\\nmonths by a decrease in liver-function’’ (these concepts are\\ndeﬁned in the context of a particular oncology therapy protocol).\\nThe temporal abstractions computed by the KBTA method for an\\nindividual patient or small number of patients can be efﬁciently\\nvisualized through an ontology-driven interface that we developed\\npreviously, known as KNAVE-II [2], which has been shown to be\\nfunctional and usable [3]. However, analysis of the data for large\\npatient\\npopulations,\\nsuch as\\nclinical\\ntrial\\nresults\\nor\\nquality\\nassessments of clinical-management processes, requires a new tool\\nthat provides aggregate views of time-oriented data and abstrac-\\ntions of groups of patient records. In addition, certain patterns can be\\ndiscovered only through the analysis of multiple patient records.\\nTherefore, as part of the current study, we designed and developed a\\ngreatly enhanced extension of the KNAVE-II system, designated the\\nVISualization of Time-Oriented RecordS (VISITORS) system, which\\ncombines intelligent temporal reasoning computational mechan-\\nisms with information visualization methods for display and\\nexploration of time-oriented records of multiple patients. Fig. 1\\npresents an overview of the main interface of raw and derived\\nconcepts and the semantics of VISITORS.\\nFurthermore, the VISITORS system also enables users to\\ninteractively specify temporal and knowledge-based constraints,\\nthrough\\na\\ngraphical\\nexpression-speciﬁcation\\nmodule,\\nwhich\\nenables users to deﬁne the patient subsets selected for exploration\\n(e.g., the lists of patients displayed in panel A of Fig. 1). Underlying\\nthe expression-speciﬁcation module is the ontology-based tem-\\nporal-aggregation (OBTAIN) speciﬁcation language, which includes\\na set of operators and constraints that enable unsophisticated\\nusers to graphically construct (i.e., to specify) three types of\\nexpression: Select Patients (Who?), Select Time Intervals (When?)\\nand Get Patient Data (What?). The goal of these expressions is to\\nretrieve a list of patients, a list of relevant time intervals, or a list of\\ntime-oriented patient data sets, respectively; combinations of\\nthese lists may be manipulated by the user according to the\\npatients, time periods and/or data values that are to be analyzed\\nfurther. A full exposition of the OBTAIN language and the graphical\\nexpression-speciﬁcation module is beyond the scope of this study\\nand can be found elsewhere [4].\\nAggregation of the longitudinal data of raw and abstract\\nconcepts of a group of patients is another unique aspect of the\\nVISITORS system. We have deﬁned and implemented in VISITORS\\nthe ‘‘delegate value’’ method, namely, given a single patient’s time-\\noriented data for a speciﬁc concept (raw or abstract) over a speciﬁc\\ntime interval (including a predeﬁned granularity level), we\\ncalculate the delegate value of an individual patient’s data at each\\ntime granule (or for another speciﬁc time period) using a function\\nspeciﬁc to the concept and the temporal granularity. For example,\\nassume that on 1 January 2007 there were three Platelet values for\\na particular patient: 17,700 cells/ml at 5 a.m., 38,900 cells/ml at 11\\na.m., and 43,250 cells/ml at 8 p.m. Thus, if we select the mean as the\\nFig. 1. The VISITORS main interface, this case in the hematological oncology domain. The two top panels display lists of patients (denoted by A) and lists of time intervals\\n(denoted by B), retrieved by computing the previous population-expressions. The graphs (denoted by C) show the data for a group of 58 patients for the White blood cell\\n(WBC) count raw concept (graph 1) and for the monthly distribution of the values of the Platelet-state abstract concept during 1995 (graph 2). Graph 3 shows the monthly\\ndistribution of the values of the Hemoglobin (HGB)-state concept during the ﬁrst year (relative time line) following BMT (see Sections 4.4 and 4.5 further details of the\\nvisualization and exploration operators). The left panel (denoted by D) of the interface includes a knowledge-based browser showing the domain’s ontology.\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n12\\n\\ndelegate (representative) function, then the daily average Platelet\\ncount for that patient was 33,283 cells/ml. However, during the\\nexploration time, the user can choose any other suitable delegate\\nfunction (such as mode or maximum).\\nThe population delegate value represents the aggregated value of\\na group of patients calculated by the concept- and the temporal\\ngranularity-speciﬁc statistical function within a speciﬁc time\\ngranule (or for another speciﬁc time period), e.g., the maximal\\nvalue of the raw Hemoglobin values for a group of patients over an\\nentire month. This function can also be manipulated at will.\\nThe following features distinguish the VISITORS system from\\nother data exploration tools:\\n\\x03 The system provides a 5-step iterative loop for intelligent\\ninvestigation of multiple time-oriented patient records: (1)\\nspeciﬁcation, (2) retrieval, (3) visualization, (4) interactive\\nexploration, and (5) knowledge-based temporal analysis.\\n\\x03 Time-oriented data are graphically displayed and explored in an\\nintuitively similar fashion for both individual and multiple patient\\nrecords.\\n\\x03 Particular consideration is given to the temporal aspect of the\\nconceptual and graphical representations: The data can be\\naggregated at and explored within various temporal granula-\\nrities, such as hour, day, and month. Support is also provided for a\\ncalendrical (absolute) timeline and for a timeline relative to a\\nspecial event [e.g., the months following a bone-marrow\\ntransplantation (BMT) event], or to another clinically signiﬁcant\\ntime point (e.g., start of high fever).\\n\\x03 The computational reasoning supports not only a view of raw\\ntime-oriented data and its statistics but also a meaningful\\ndisplay of various knowledge-based interpretations of the raw\\ndata, based on temporal-abstraction domain ontology, the KBTA\\ncomputational\\nmechanisms,\\nand\\nspecialized\\ntime-oriented\\naggregation operators. The exploration interface is also based\\non the same ontology, which supports a semantic exploration of\\nthe data (using semantic relations such as ‘‘derived from’’ or\\n‘‘part of’’) and enables navigation of semantically related raw and\\nabstract concepts.\\nThe ﬁrst two steps of the iterative loop, i.e., speciﬁcation and\\nretrieval of the multiple time-oriented patient records, are\\ndescribed in detail in another study [4]. In the current study, we\\nfocus on the other three capabilities: visualization, interactive\\nexploration, and knowledge-based temporal analysis of multiple\\npatient records.\\nThe main contributions of the current study lie in: (1) providing\\nformal deﬁnitions for the operators that perform visualization,\\nexploration, and knowledge-based temporal analysis of long-\\nitudinal data for multiple patients; (2) implementation of a\\ncomplete architecture developed using formal deﬁnitions; and (3)\\na functionality and usability evaluation of the implemented\\nsystem.\\n2. Related work\\n2.1. Combining domain knowledge, temporal abstraction and\\ninformation visualization in medical informatics\\nThe use of a domain knowledge base can both support an in-\\ndepth analysis of longitudinal patient records and simplify and\\nfacilitate the data exploration process, since the user can explore\\nonly high-level concepts based on complex temporal patterns (or,\\nin general, on any abstract concepts) previously deﬁned in a\\ndomain-speciﬁc knowledge base and detected in the patients’ data.\\nIn addition, it has been demonstrated that visual representation\\ncan often communicate information much more rapidly and\\neffectively than any other method [5]. Thus, the combination of\\nthese two approaches could signiﬁcantly improve the exploration\\nof patient data, as has been suggested by a clinical survey [6].\\nIn the current study, using a domain-speciﬁc knowledge-base,\\nwe applied the KBTA method [1] for automated derivation of\\nmeaningful\\ncontext-speciﬁc\\ninterpretations\\nand\\nconclusions\\n(temporal abstractions) from raw time-oriented patient data. In\\ngeneral, the KBTA method may be described as follows: the input\\nincludes a set of time-stamped measurable concepts (e.g., Platelet\\ncount, Red blood cell count) and external events (e.g., bone-\\nmarrow transplantation). The events typically create the necessary\\ninterpretation contexts (e.g., the therapy protocol used), which\\ncould change the interpretations of the data. The output includes a\\nset of interval-based, context-speciﬁc concepts at the same or a\\nhigher level of abstraction and their respective values (e.g., a period\\nof two months of grade 1 bone-marrow toxicity in the context of\\nparticular chemotherapy protocol).\\nOther\\nstudies\\nhave\\ninvestigated\\ndifferent\\ntechniques\\nfor\\ntemporal abstraction. Silvent et al. [7], for example, proposed\\ncombining temporal data abstraction techniques with data mining\\napproaches, a method that would update the prior domain\\nknowledge. The temporal-abstraction mechanisms put forward\\nby Miksch et al. [8] do not require predeﬁned domain knowledge\\nand\\ncan\\nprocess\\nhigh-frequency\\ntemporal\\nquantitative\\nand\\nqualitative data. Another method for calculation of temporal\\nabstractions has been applied to ECG data by using a fast Fourier\\ntransform [9,10]. In that method, a curve made up of a series of data\\npoints was transformed to a set of bends and lines in between these\\ndata points. However, the above frameworks concentrate on the\\nmethodology\\nof\\nthe\\ntemporal\\nabstractions\\nrather\\nthan\\non\\nvisualization and exploration issues. Moreover, the interfaces\\nused do not fulﬁll most of the desiderata deﬁned below (in Section\\n3) for exploration of longitudinal patient data.\\nUnlike the KBTA method, in which the domain expert deﬁnes\\nthe temporal patterns for temporal abstraction derivations, Combi\\nand Chittaro [11] used, as the temporal ontology, an object-\\noriented model based on the calculus of events [12]. They validated\\ntheir approach by building the CARDIOlogic Temporal Abstraction\\nSystem (CARDIOTABS) for the abstraction of temporal cardiology\\ndata, which supports nonvisual construction of queries regarding\\npatient data and very simple graphical interfaces for patient data\\nvisualization. They did not, however, evaluate the usability of the\\ninterfaces.\\nTo date, most visual exploration systems in medicine have\\nfocused solely on the visualization and exploration of static raw\\npatient data, as reviewed by Chittaro [13]. The LifeLines project\\n[14,15], for example, provides a general visualization environment\\nfor personal histories, which can be applied to medical and court\\nrecords, professional histories and other types of biographical data.\\nThe extended version, LifeLine2 [16], includes the option of\\naligning the patient data according to key medical events, i.e., a\\nrelative timeline view.\\nIn the area of visualization of data for multiple patients, a\\nnumber of systems have been developed. The InfoZoom system\\n[17] uses a novel technique to display data sets as a highly\\ncompressed table that always ﬁts completely onto the screen. The\\noverall goal of the MedView project [18] is to develop models,\\nmethods and tools to support clinicians in their daily diagnostic\\nwork. As part of MedView, two information visualization tools\\nwere developed and tested as a solution to the problem of\\nvisualizing insights derived from large amounts of clinical data.\\nThe ﬁrst tool, The Cube, enables interactive recognition of patient\\npatterns through a 3D display of a set of 2D parallel diagrams (each\\nusing a horizontal time axis and a vertical value axis), where each\\ndiagram represents a single patient attribute (e.g., diagnoses,\\nallergies). Thus, patients who have similar values for several\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n13\\n\\nattributes might have parallel lines connecting the different 2D\\ndiagrams. The second tool, SimVis, is based on a similarity\\nassessment-based interaction model for exploring data; the tool\\nwas designed to help clinicians to classify and cluster clinical-test\\ndata.A third system, the Interactive Parallel Bar Charts(IPBC) system\\n[19] adopts 3D bar charts as its basic visualization technique and\\naugments them with several interactive features, thereby exploiting\\n3D space to signiﬁcantly increase both the number of time-series\\nthat can be simultaneously analyzed in a convenient way and the\\nnumber of values associated with each series.\\nAs noted earlier, the above exploration systems focus mostly on\\nthe visualization of raw longitudinal data or only partially support\\nhigh-level meaningful interpretations of these data as abstract\\nclinical concepts. There is, however, a growing realization of the\\nimportance of integration of the visual, analytical and user-centered\\nmethods, as is shown in the review of Aigner et al. [20] of different\\nvisualizations of time-oriented data, in which the exploration task is\\nsolved by providing complex integrated interfaces. However, we\\nview the integration somewhat differently: the visualization must\\nbe simple and must represent the results of a complex temporal\\nreasoning computational process, namely, various types of users\\nmust be able to explore the data in an equally efﬁcient fashion.\\nMoreover, such integration must be modular, accessible, and\\nindependent of a particular medical domain.\\nTo address the issues discussed above, we devoted the current\\nstudy to extending our previously developed KNAVE-II system [2]\\nfor visualization and exploration of individual patient data to\\nsupport the visualization and exploration of the time-oriented\\nrecords of multiple patients. This effort required signiﬁcant\\nmodiﬁcation of the original tools, as explained in the Methods\\nsection below.\\n2.2. Other temporal aspects relevant to the study\\n2.2.1. Temporal and visual-temporal query languages\\nRecall that the OBTAIN speciﬁcation language and its graphical\\nexpression-speciﬁcation tool are used by end-users to specify the\\nrelevant patients or time periods to be retrieved and explored.\\nAlthough the OBTAIN speciﬁcation language is not intended to be,\\nin any way, a general temporal query language, the OBTAIN\\nexpressions speciﬁed by users could be considered as types of\\nqueries, since their goal is to retrieve a set of patients, time-\\nintervals, or time-oriented data. Thus, it is relevant to compare the\\nOBTAIN language to existing temporal (visual) query languages.\\nThe main difference between the OBTAIN speciﬁcation lan-\\nguage and the previous pure time-oriented query languages, such\\nas TSQL [21], HSQL [22], TSQL2 [23], GCH-OSQL [24–26], TLSQL\\n[27] and most recently t4sql [28,29], is that according to these\\nlanguages SQL queries are applied directly to raw-data databases;\\nthus, the expressivity of the language is limited by the pure SQL\\nand by the temporal-extension capabilities of these languages. In\\ncontrast, a broad set of temporal [knowledge-based] constraints\\nfor speciﬁcation of either patients or time intervals can be used in\\nOBTAIN expressions, since to answer the OBTAIN expressions,\\nadditional computational modules (such as the module for\\ncalculation of delegate values that is described in the Methods\\nsection), and a runtime capability for computation, on the ﬂy, of\\nknowledge-based temporal abstractions, are provided. Thus, on\\none hand, the semantic expressivity of the OBTAIN language,\\naccording to our desiderata, is enhanced relative to standard query\\nlanguages. On the other hand, recall that OBTAIN is not a general\\nquery language, thus, important features such as nested queries,\\njoin, and other query operators are not supported in the OBTAIN\\nlanguage, unlike most of the query languages mentioned above.\\nWith respect to visual temporal query languages, the TVQL [30],\\nTVQE [31], and MQuery [32] temporal visual query languages and\\ntheir appropriate visual query environments use a series of sliders,\\ncheckboxes, and other widgets to specify the temporal constraints\\non the start or end boundary time points of temporal intervals or\\nrelationships between temporal intervals. The TimeFinder system\\n[33,34] is a visual exploration and query system for exploring time\\npoint-based data sets, based on a direct manipulation metaphor.\\nThe main disadvantage of these systems is the lack of access to\\ndomain knowledge, and thus meaningful temporal abstraction\\ncannot be speciﬁed. Moreover, these systems focus only on\\nretrieving time-oriented data and do not support the retrieval of a\\nset of subjects (e.g., patients). Chittaro and Combi [35] provide a\\nframework for visual representation of temporal intervals and of\\ntheir interrelations. However, the proposed techniques are focused\\non the visual deﬁnition of temporal queries regarding the time-\\noriented data, rather than on the speciﬁcation of groups of patients.\\nSeveral proposals have been made to enhance query capabil-\\nities through the use of domain-speciﬁc knowledge, in a manner\\nthat somewhat resembles this aspect of our research framework\\n[36,37], but they are speciﬁc to only one domain (do not have\\nunderlying generic temporal abstraction ontology) and cannot be\\nused in other medical domains.\\n2.2.2. Temporal aggregation\\nIn historical databases, temporal aggregation (or temporal\\ngrouping) is a process in which a time line is partitioned over\\ntime and the values of various attributes in the database are\\ngrouped over these partitions [38]. As such, temporal aggregation\\ncan be seen as a temporal extension of the standard SQL operator\\ngroup by. The temporal grouping algebra and its application to the\\nSQL language were well deﬁned by Clifford et al. [39]. A typical\\nexample of temporal aggregation is the monthly accumulation of\\nsalary payment. Due to the large number of temporal data and\\ntheir distribution over the time line, efﬁcient algorithms to\\nperform temporal grouping are required, as mentioned by Moon\\net al. [40], who proposed several methods for large-scale temporal\\naggregation.\\nThe temporal aggregation process is also relevant to our study.\\nWe perform a type of temporal aggregation during the visualiza-\\ntion of longitudinal data for multiple patients (actually, of the\\ndelegate values of these data: see Section 4.2) or during the retrieval\\nof patient groups. However, our algorithms for aggregation of the\\ntime-oriented data for each individual patient in a group are\\napplied separately to the data for each patient so as to produce\\ndelegate values for each concept type at each temporal granularity\\n(see Section 4.2). For example, the VISITORS system and its\\nunderlying computational modules enable users to aggregate\\nindividual patient data by interval-based temporal abstractions\\n(using the KBTA methodology: see Section 2.1) and to further\\naggregate these abstractions into distributions at various temporal\\ngranularities (e.g., monthly distribution of the state of bone\\nmarrow following a bone-marrow transplant). However, these\\noperations are quite different from the semantics of standard\\ntemporal aggregation methods in temporal databases. Thus, such\\nmethods are less relevant to our study.\\n2.2.3. Temporal granularity\\nVarious medical domains require representation and explora-\\ntion of longitudinal patients’ data within different time granula-\\nrities. For example, in intensive care units, medical parameters are\\nmeasured using a granularity of minutes (or even seconds); diabetic\\npatients perform glucose tests three times a day, and the clinical\\nhistory of most chronic patients spans several years. Another\\nproblem is that even for the same patient, temporal clinical\\ninformation is represented at different time granularities; e.g.\\n(1)‘‘the patient has taken statin medications during the past three\\nmonths’’, and (2) ‘‘his glucose level rose sharply from 8:00 a.m. to\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n14\\n\\n6:00 p.m. on September 18th, 1996’’. Thus, the need to support\\nvarious time granularities is very clear.\\nIn the case of the VISITORS system, we approach these two\\nproblems in the following fashion: (1) we perform a summariza-\\ntion of each individual patient’s time-oriented data (i.e., calculat-\\ning delegate values as described in Section 4.2) according to the\\ntemporal\\ngranularity\\nlevel,\\nand\\n(2)\\nwe\\nsupport\\ninteractive\\nexploration of longitudinal data for individual/multiple patients\\nwithin different time granularities (see Sections 4.3 and 4.4).\\nOur intention here is not to propose a general approach to the\\ntreatment of temporal granularities, even though such approaches\\ndo, indeed, exist. Goralwalla et al. [41] showed an approach to the\\nhandling of granularity in temporal data. They separated temporal\\ndata into two groups: anchored (calendrical day or month, e.g.,\\nJanuary 1st, 2008 or May 1978), and unanchored (time intervals,\\ne.g., 2 months, 5 h 20 min, etc.) data. Thus, a temporal granule is a\\nspecial kind of unanchored temporal data [42].\\nIn our framework, a special kind of unanchored temporal data\\nis a relative time line (e.g., the period before or after bone-marrow\\ntransplantation). A relative time line is a determinate time span\\n[42], in which one boundary time point (start or end) is\\nreferenced\\nto\\na\\nspecial\\nclinical\\nevent\\n(e.g.,\\nbone-marrow\\ntransplantation) or another clinically signiﬁcant time point\\n(e.g., start of high fever). Clifford and Rao [43] have proven the\\nimportance of using integral time granularities that can be\\ncomposed of lower level granules. Thus, we deﬁne a relative\\nmonth as 30 days, and a relative year as 12 months, or 360 days.\\nNote that: (1) these periods do not correspond to any anchored\\ndurations, such as calendrical months; and (2) weeks are not\\nallowed as unanchored units in our framework, according to the\\nreasoning of Clifford and Rao [43].\\nOther techniques for the treatment of the temporal granularity\\nissue include the work of Dal Lago and Montanari [44] and other\\napproaches that were demonstrated within clinical domains\\n[26,45]. The GSTP system [46], for example, provides access to a\\nset of implemented algorithms (such as to the AC-G [47], and to an\\nalgorithm for converting calendrical expressions into periodical\\ngranularities [48]). These algorithms support a solution to the\\nmulti-granularity temporal constraint satisfaction problem (TCSP).\\nThe GSTP system supports a rich set of predeﬁned temporal\\ngranularities: common temporal granularities (e.g., days, months),\\nand\\nspecial\\ngranularities\\n(e.g.,\\nweeks,\\nacademic\\nsemesters).\\nMoreover, new user-deﬁned granularities can be added.\\nNote, however, that in our study, we focus on interactive\\nexploration of time-oriented data for multiple patients (actually of\\ntheir delegate values) within a predeﬁned set of temporal\\ngranularities (seconds, minutes, hours, days, months, and years):\\nthus, developing theoretical algorithms for dealing with temporal\\ngranularities was not relevant for the current study.\\n3. Desiderata for effective exploration of time-oriented data for\\nmultiple patients\\nOur study of the problem of effective and usable visualization\\nand exploration of raw clinical data and, especially, of derived\\nmeaningful abstractions from these data, revealed the following\\nset of desiderata for the intelligent interface and exploration\\noperators supporting the task of exploration of time-oriented data\\nfor multiple patients.\\n1. Evaluation of the functionality and usability of the KNAVE-II\\nsystem for exploration of longitudinal data of individual patients\\n[2] demonstrated the importance of the visualization and\\nexploration of meaningful temporal interpretations that were\\nderived from raw clinical data using context-sensitive domain-\\nspeciﬁc knowledge. Thus, the visualization and exploration of\\ntime-oriented interpretations of raw data is also an important\\nrequirement for a system whose goal is to explore longitudinal\\ndata for multiple patients.\\n2. Representation and computation of raw and abstract concepts at\\nvarious temporal granularities, especially for interval-based\\nconcepts [45], is very important for the clinical domain. Recall in\\nparticular the two requirements mentioned in Section 2.2.3: (1)\\nthe need to handle different temporal granularities for different\\nmedical domains, and (2) the need to support multiple types of\\nclinical\\ndata\\nthat\\nare\\nrepresented\\nat\\ndifferent\\ntemporal\\ngranularities within the same longitudinal patient record. The\\nsystem should be able to compute statistical aggregations of\\nboth raw and abstract concepts at any temporal granularity (e.g.,\\nday, month) or for any arbitrary time period (say, from 8 August\\nto 27 September 2007) for either individual or multiple patients.\\nFulﬁlling such a requirement constitutes a novel aspect of this\\nstudy.\\n3. Before exploring the time-oriented data of a group of patients,\\nthe clinician must select the patients that will comprise the\\npatient population to be analyzed. Current selection tools\\nsupport the speciﬁcation of patient groups by using demo-\\ngraphic (e.g., age, sex) or raw-data time and value constraints\\n(e.g., Hemoglobin values) [49,50]. However, to enable clinicians\\nto quickly and efﬁciently specify patient populations, there is a\\npressing need for the ability to specify the relevant patients by\\nusing knowledge-based and statistical aggregation constraints,\\ne.g., ‘‘select patients whose most frequent Hemoglobin-state\\nvalue (i.e., the mode of Hemoglobin-state values) during the ﬁrst\\ntwo months following bone-marrow transplantation was higher\\nthan the mode of the Hemoglobin-state of whole patient\\npopulation’’.\\n4. Current information-visualization systems proposed for multi-\\nple patients focus solely on the display of patients’ raw\\nlongitudinal data [14–18]. Moreover, such systems do not\\nsupport either population aggregation capabilities for raw data\\nor visualization of temporal abstractions for multiple patients.\\nThus, we must investigate and implement suitable visualization\\ntechniques for effective display of interval-based temporal\\nabstractions (not only of raw data) for multiple patients.\\n5. The system should be able to support the following tasks for\\ninteractive exploration of the data for multiple patients:\\n\\x03 Performing interactive dynamic exploration of the data of\\nmultiple patients at different temporal granularities (i.e.,\\nzooming in and out of the time line). This is a basic necessary\\ntask, which has been solved in several systems for interactive\\nexploration of raw data for the individual patient [14–16] and\\nfor abstract concepts [2]. However, the visualization and\\ninteractive exploration of both raw and abstract concepts for\\nmultiple patients require an additional extension.\\n\\x03 Changing dynamically the method used for the aggregation of\\npatients’ data at different temporal granularities (i.e., aggre-\\ngating differently blood glucose values at a granularity of days\\nversus months). There is no standard solution available for this\\ntask.\\n\\x03 Supporting both an absolute (i.e., calendrical) time line and a\\nrelative time line (which refers to some clinical key event) and\\nperforming an alignment of the patients’ data along signiﬁcant\\nclinical\\nreference\\nevent\\n(or\\ntime\\npoint),\\nincluding\\nthe\\ncomputation of the appropriate aggregation within the\\nrelative time line.\\n6. One of the more important tasks in the analysis of multiple\\npatient data is an investigation of potentially meaningful\\ninterrelations, especially temporal interrelations such as asso-\\nciations among subsets of raw patient data and abstract\\nconcepts. For such purposes different techniques of data mining\\n(e.g., temporal association rules), are often used [51], although\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n15\\n\\nthey typically focus only on temporal precedence relations.\\nCurrent visual data mining systems in the medical domain focus\\nmostly on raw data [18,19]. What is required is an interactive\\nuser-driven method to compute, display and explore temporal\\nassociations for the data of multiple patients among raw and\\nabstract concepts.\\n4. Methods\\nThe speciﬁc methods that we used are presented below in six\\nsubsections, each in the context of its relevant desideratum as\\npresented in Section 3.\\n4.1. Knowledge-based abstraction of raw time-oriented data for\\nmultiple patients\\nThe VISITORS system is an intelligent interface to a distributed\\narchitecture speciﬁc to the tasks of retrieval, knowledge-based\\nvisualization, and interactive exploration of the time-oriented\\nrecords of multiple patients. Fig. 2 describes the overall architec-\\nture of the VISITORS system. The architecture capitalizes on what\\nwe denote as a temporal abstraction services (see details below). In\\nparticular, we are assuming that all the necessary elements for the\\ntemporal abstraction framework (shown in Fig. 2 by dotted lines),\\nsuch as those designed in our previous studies or equivalent\\nservices, are available.\\nThe OBTAIN expressions constructed by end-users (clinicians)\\nare submitted to a SQL- and C#-based multiple-patients retrieval\\nmodule, which we refer to as Multiple-Patient Time-Oriented\\nQuery\\n(the\\nMulti-TOQ)\\nmodule.\\nThe\\nMulti-TOQ\\nmodule\\nis\\nresponsible for the retrieval of a set of patients, a set of\\ntemporal intervals, or a set of time-oriented patient data\\naccording to the following three types of OBTAIN expressions,\\nrespectively: Select Patients, Select Time Intervals, and Get Patients\\nData. The Multi-TOQ module combines specialized computa-\\ntional methods, such as calculation of delegate values (see\\nSection 4.2), with database operators, such as the selection of\\npatients by demographic (e.g., sex, age) values. It also creates\\nand ﬁlls the virtual databases used for retrieval purposes, for\\nexample, calculation of the population mean values for concepts\\nmentioned in the OBTAIN expression.\\nThe Multi-TOQ module interacts with the intelligent temporal\\nmediator IDAN [52] that integrates relevant time-oriented data\\nand knowledge, to obtain answers regarding the temporal\\nabstractions. IDAN uses several temporal abstraction services,\\nwhich differ in the mode in which they are used for processing\\ntime-oriented data. If the abstractions do not exist as yet, they are\\ncomputed on the ﬂy by a C#-based, goal-driven temporal\\nabstraction module, which we refer to as Tempura (a variation\\nof the Resume temporal abstraction system [53]), or by the prolog-\\nbased ALMA temporal-abstraction module [52] (in the case of\\ncomplex clinical patterns). Otherwise, if the abstractions have\\nalready been previously computed, in a data-driven manner, by\\nthe MOMENTUM system [54], they are simply retrieved by the\\nMulti-TOQ\\nmodule.\\nThe\\nMOMENTUM\\nsystem\\nis\\nan\\nactive\\nmiddleware (which is built on the concept of an active knowl-\\nedge-based time-oriented database) speciﬁc to solving the\\ntemporal abstraction task for large groups of subjects (e.g.,\\npatients) by using an incremental version of the KBTA method\\n[55]. MOMENTUM is a data-driven system, which generates\\ntemporal abstractions deﬁned in its knowledge base incremen-\\ntally, as new patient data arrive, and saves the abstractions in a\\nspecial layer in the database.\\n4.2. Statistical aggregation of patient time-oriented data and their\\nabstractions\\nIt is often the case that for a particular patient there are several\\nmeasurements of the same raw concept during the time granule of\\ninterest. For example, several blood-glucose level values during the\\nsame day or several Hemoglobin values during the same month or\\nsame year. However, the exploration analysis might require a\\nsingle value at each desired temporal granularity, and the problem\\nmight refer to either raw data or abstract concepts.\\nTo aggregate patient data at arbitrary temporal granularities or\\nduring speciﬁc time periods, we deﬁned the concepts of a delegate\\n(representative) value that is calculated by a delegate (representa-\\ntive) function: given a single patient’s time-oriented data for a\\nspeciﬁc concept (raw or abstract) over a particular time interval\\n(including a predeﬁned temporal granularity level), we calculate\\nthe delegate value of the patient’s data at each time granule (or for\\na particular time period) using a function speciﬁc to the concept\\nand the temporal granularity. Such a calculation is performed for\\neach patient in the group.\\nIn the VISITORS framework, we make the following four\\nassumptions:\\nAssumption 1. We need to support exactly six temporal granu-\\nlarity levels: seconds, minutes, hours, days, months and years, with\\nthe lowest granularity being seconds and the highest being years.\\nFig. 2. Overall VISITORS architecture. End users (clinicians) use the expression-speciﬁcation module of VISITORS to create and submit an OBTAIN expression to the Multiple-\\nPatient Time-Oriented Query (Multi-TOQ) module. The Multi-TOQ module interacts with the IDAN temporal abstraction mediator regarding the requested patients’ data and/\\nor temporal abstractions (TAs). The IDAN mediator integrates the relevant data and knowledge from the appropriate sources – indicated by the user in the OBTAIN expression\\n– to retrieve the raw data or to derive, using a temporal abstraction service (the Momentum, Tempura or ALMA temporal abstraction computational modules), a set of abstract\\ntime-oriented (interval-based) concepts from these data. The derived temporal concepts are returned to Multi-TOQ for further processing, if needed. The resultant data\\n(patients, time intervals or time-oriented data) answering the OBTAIN expression are returned to the exploration interface.\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n16\\n\\nAssumption 2. The patient has only one concept value at the\\nlowest given temporal granularity level (e.g., for data provided\\nat the temporal granularity level of months and days, no concept\\nhas more than one value per day).\\nAssumption 3. A single delegate value can be requested and\\ncomputed for any time granule (e.g., second day following bone-\\nmarrow transplantation) or arbitrary time period (e.g., January 7th,\\n2007–February 2nd, 2007), but a series of delegate values at a\\ndesired temporal granularity level for a particular concept can be\\nrepresented for a time period that contains only a whole number of\\ntime granules, e.g., a monthly summary can be requested for a\\nperiod of a whole number of calendrical months.\\nAssumption 4. The input data for our approach has the following\\ndata structure:\\nin put data \\x04 < Patientn; Conce ptc; T-Startc;n;m; T-Endc;n;m;valuec;n;m > \\x05; 1\\n\\x06 n \\x06 N; 1 \\x06 m \\x06 Mn;\\nwhere N is the number of patients, Mn is the number of values of\\nConceptc for Patientn; T-Startc,n,m and T-Endc,n,m are the start and end\\ntimes of the mth laboratory test (or temporal abstraction) for\\nPatientn, with value valuec,n,m. The symbol * denotes 0 or more\\nrepetitions.\\n4.2.1. Computing a single delegate value of a raw-data concept for a\\nspeciﬁc aggregation time period\\nThe delegate value for Patientn of raw concept Conceptc for a\\nspeciﬁc aggregation time period [T-Startaggreg, T-Endaggreg] is\\ncomputed by the delegate function DFc (which is possibly speciﬁc\\nto each combination of concept and temporal granularity level)\\nfrom the input_data as follows:delegate valuec;n;T-Startaggreg;T-Endaggreg ¼\\nDFc½ðT-Startn;1;T-Endn;1;valuec;n;1Þ; . . .ðT-Startn;i;T-Endn;i;valu ec;n;iÞ; . . .\\nðT-Startn;K; T-Endn;K;valuec;n;KÞ\\x07; T- Startaggreg \\x06 T-Startn;i; T-Endn;i \\x06\\nT-Endaggreg; 1 \\x06 i \\x06 K ¼ Kc;n;T-Startaggreg;T-Endaggreg where\\nK = Kc,n,T-Star-\\ntaggreg,T-Endaggreg is the number of instances of Conceptc for Patientn\\nmeasured within the desired [T-Startaggreg, T-Endaggreg] period, i.e., K\\nvaries for each concept, patient, and aggregation period.\\nThe delegate function is deﬁned in the knowledge base or is\\nchosen at runtime by the user from several predeﬁned default\\nfunctions.\\nFor\\nexample,\\nassume\\nthat\\nafter\\na\\nbone-marrow\\ntransplantation procedure, a patient’s Platelet counts on two\\nseparate occasions during the same day were 22,000 cells/ml at 10\\na.m. and 17,000 cells/ml at 9 p.m. If we specify, in the domain\\nknowledge base, the mean as the default delegate daily function for\\nthe Platelet count (raw-data) concept, then the patient had a daily\\ndelegate value of 19,500 cells/ml for the Platelet count (raw)\\nconcept. However, during the interactive exploration time, the\\nuser can also select any other suitable delegate function (such as\\nthe mode or the maximum).\\nIndeed, in theory, almost any function from multiple time-\\nstamped values into one value, of the same concept that has the\\nsame domain and units as the possible values of the original\\nconcept, can serve as the delegate function. However, unlike\\nstandard statistical functions, it must be applied to each time\\ngranule in the relevant aggregation temporal granularity (e.g., day).\\nOf course, the selected function must make clinical sense, and is\\nthus speciﬁc to each clinical concept and aggregation time\\ngranularity level.\\n4.2.2. Computing a series of delegate values of a raw-data concept at a\\ndesired temporal granularity for a given overall aggregation time\\nperiod\\nThe computing of a series of delegate values is performed for\\neach time granule at the desired temporal granularity level for an\\noverall aggregation time period. Thus, a series of delegate values for\\nPatientn of raw concept Conceptc for an overall aggregation time\\nperiod [T-Startoverall, T-Endoverall] has the following data structure:\\ndelegate valuesc;n;T-Startoverall;T-Endoverall ¼ < Patientn; Conce ptc;\\n-Startaggreg n; j; T-Endaggreg n; j; delegate valuec;n; j > \\x05; 1 \\x06 n \\x06 N;\\n1 \\x06 j \\x06 Jc;n;\\nwhere N is the number of patients; Jc,n is the number of delegate\\nvalues of Conceptc for Patientn; T-Startaggreg n,j and T-Endaggreg n,j are\\nthe start and end times of the time granule that is speciﬁc to the jth\\ndelegate value for Patientn; and Jc,n varies with each patient,\\nconcept and the particular overall time period of a series of\\ndelegate values.\\nThe values for boundary time granules of Conceptc for Patientn of\\nthe start point T-Startaggreg n,1 of the ﬁrst time granule and of the\\nend point T-Endaggreg n,Jc,n of the last time granule are:\\nT-Startaggreg n;1 ¼ Begin O f Time GranuleðT-Startc;n;1Þ \\x08 T-Startoverall;\\nT-Endaggreg n;Jc;n ¼ End O f Time GranuleðT-Endc;n;MnÞ \\x06 T-Endoverall:\\nThe jth index denotes the jth delegate value of the speciﬁc\\nPatientn. For example, assume that laboratory tests for Conceptc\\nwere performed for the patient several times on each of the days\\nJanuary 3rd, January 15th, and January 20th, then the T-Granaggreg is\\nday, and the DF is mean. Thus, the ﬁrst delegate value will be the\\nmean value of all of the concept-measurement results on January\\n3rd, the second delegate value will be the mean value of all of the\\nresults on January 15th and so on. In this case J = 1.3 for the overall\\naggregation period of January. If T-Granaggreg is month and the\\noverall aggregation time period is 1 January–31 August for the\\nsame year, then the ﬁrst delegate value will be the mean value of all\\nthe tests during January. Note that several of the months during the\\nrequested overall period might have no measurements, and hence\\nno delegate values representing them.\\n4.2.3. Computing a single delegate value for a speciﬁc aggregation\\ntime period and a series of delegate values concepts at a desired\\ntemporal granularity for a given overall aggregation time period of an\\nabstract concept\\nThe computing of a delegate value for a speciﬁc aggregation time\\nperiod ora series of delegatevalues at a desiredtemporal granularity\\nfor a given overall aggregation time period in the case of interval-\\nbased abstract concepts [such as intervals of different (discrete)\\ngrades of bone-marrow toxicity] is similar to the aggregation of raw\\n(time point based) patient’s data, as explained in Sections 4.2.1 and\\n4.2.2, but requires the use of more complex delegate functions,\\nbecause we must refer to both the value of the temporal abstraction\\ninterval and to its duration. Thus, standard statistical functions are\\nnot sufﬁcient. In this case, we provide additional delegate functions,\\nsuch as the value of the abstraction that has the maximal cumulative\\nduration during the relevant aggregation time period or the value\\nassociated with the interval that has the longest duration.\\nSince values of abstract concepts often span time intervals (i.e.,\\na period and not just a time point), we consider the case of an\\noverlap between the interval during which the value holds, and the\\nseveral time granules, which are at the time granularity level that\\nhad been requested for the computation of the delegate value.\\nAssume that the abstract concept computed for the relevant\\npatient spans four time intervals (denoted by i1–i4) within the\\nrange of three time units of the desired time granularity level\\n(Fig. 3a). First, we extend the temporal intervals by performing an\\nextrapolation operation that is an extension of the knowledge-\\nbased interpolation operation [56], according to the concept-\\nsensitive relevant interpolation function (the extension of the\\nintervals denoted by dotted lines in Fig. 3b). The extension is\\nnecessary to enable application of various statistical functions,\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n17\\n\\nsince abstractions that hold at a particular time point must be\\nextended so as to hold over a sufﬁciently long time interval.\\nSimilarly, we extend longer precomputed intervals beyond their\\ntwo edges, assuming that the abstract value persists beyond the\\ncrisp temporal points that deﬁne the time interval. The local\\nextrapolation of a point or an interval’s boundary point uses the\\nD(0,0) interpolation function for the relevant concept [56], i.e., the\\nmaximal gap allowed between two single time points, at each of\\nwhich holds the value of the abstract concept. We actually extend\\neach boundary point to the past and the future by amount of\\nD(0,0)/2. In the case of adjacent intervals that have different values\\nfor the same abstract concept, the extrapolation is proportional to\\nthe duration of the intervals. Second, we segment the extended\\nintervals according to the time granules of the desired time\\ngranularity (Fig. 3c). Third, we apply the default delegate function\\nfor that concept and time granularity. The default for all concepts is\\ncurrently the value that has maximal cumulative duration within\\nthe time granule. In Fig. 3c, within the ﬁrst time granule the\\ndelegate value is ‘‘normal’’ (note that the sum of extended\\ndurations of intervals i10 and i20 is more than the duration of the\\nextended interval i30 within the ﬁrst time granule). The ‘‘high’’\\nvalue is the delegate value computed for the second and third time\\ngranules. Note, that interval i3000 (part of extended i3 within the\\nthird time granule) has a higher duration in that granule than\\nthe extended interval i40; thus, the delegate value computed for the\\nthird granule is ‘‘high’’; although the original duration of interval i4\\nwas longer than the part of i3 within the third time granule. Using\\nother delegate functions could change the computed delegate\\nvalues.\\n4.2.4. Computing a single population delegate value of raw-data and\\nabstract concepts for a speciﬁc aggregation time period\\nThe population delegate value represents the aggregated value of\\na group of patients calculated by the concept-speciﬁc and the\\ntemporal-granularity-speciﬁc statistical function within a speciﬁc\\ntime granule (or for another speciﬁc time period), e.g., the maximal\\nvalue of the raw Hemoglobin values for a group of patients over a\\nwhole month. The population delegate value of a group of patients\\nfor Conceptc within a speciﬁc aggregation time period [T-Startaggreg,\\nT-Endaggreg] is calculated by the population delegate function PDFc\\nfrom the set of original patient data, input_data* as follows:\\npo pulation delegate valuec;T-Startaggreg;T-Endaggreg\\n¼ PDFc½ðT-Startn;1; T-Endn;1;valuec;n;1Þ; . . . ðT-Startn;i; T-Endn;i;\\nvaluec;n;iÞ; . . . ðT-Startn;K; T-Endn;K;valuec;n;KÞ\\x07; 1 \\x06 n \\x06 N;\\n1 \\x06 i \\x06 K; T-Startaggreg \\x06 T-Startn;i; T-Endn;i\\n¼ T-Endaggreg; 1 \\x06 i \\x06 K \\x06 Kc;n;T\\tStartaggreg;T-Endaggreg\\nwhere N is the number of patients in the group, and K = Kc,n,T-\\nStartaggreg,T-Endaggreg is the number of instances of Conceptc for\\nPatientn measured within the [T-Startaggreg, T-Endaggreg] time period.\\nAn example of a population delegate value is the maximal value\\nof the concept across all patients within a speciﬁc month (see the\\nline denoted by 2 in Fig. 4).\\nSimilarly to the delegate function DFc, the population delegate\\nfunction PDFc calculates the delegate value for Conceptc over each\\nrelevant time granule. However, through the population delegate\\nfunction PDFc, we calculate a ‘‘delegate value’’ that represents a\\npopulation value for Conceptc for a group of patients during that\\nFig. 3. Computation of a series of delegate values for an abstract concept (see text for details).\\nFig. 4. Visualization of the data for the Red blood cell (RBC) count raw concept for a group of 58 patients (retrieved earlier by using a Select Patients expression) from April 1995\\nto March 1996. The individual raw data are represented at a resolution level of seconds with respect to time granularity, but the population statistics are aggregated at a\\ngranularity of months, according to the user’s current request. All laboratory test results for the RBC count for patients treated during this period are displayed as (blue) X’s\\n(denoted by region 1). Through the density of the points, the user can judge the number of data instances belonging to each value or time period range. The top (red) line\\n(denoted by 2) represents the monthly maximal value of the whole group. The tooltip provides detailed information about the maximal value (4520 cells/ml), the ID of the\\npatient (703) with that value, and the observation time. The bottom (blue) line (denoted by 4) and the middle (green) line (denoted by 3) represent the monthly minimal and\\nmean values, respectively. On the left-hand side (denoted by 5) are displayed statistics for the whole time period (April 1995 to March 1996). The three dotted lines drawn\\ninside the panel indicate the mean value \\x02 standard deviation for this period (For interpretation of the references to color in this ﬁgure legend, the reader is referred to the web\\nversion of the article).\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n18\\n\\ntime granule. The population delegate function can be any\\nstatistical function, such as the mean or the maximal value, which\\ncan be applied to the values of all of the patients within a speciﬁc\\ntime granule and that returns a single output value from the\\ndomain of values and units of Conceptc.\\n4.2.5. Computing a series of population delegate values of raw-data\\nand abstract concepts at a desired temporal granularity for a given\\noverall aggregation time period\\nThe process of computing a series of population delegate values\\nis similar to computing a series of individual delegate values: we\\ncompute a population delegate value for each time granule at a\\ndesired temporal granularity level for an overall aggregation time\\nperiod. Thus, a series of population delegate values of Conceptc for an\\noverall aggregation time period [T-Startoverall, T-Endoverall] has the\\nfollowing data structure:\\npo pulation delegate valuesc;T-Startoverall;T-Endoverall \\x04 < Conce ptc;\\n-Startaggreg j; T-Endaggreg j; po pulation delegate valuec; j > \\x05;\\n1 \\x06 n \\x06 N; 1 \\x06 j \\x06 Jc;\\nwhere Jc is the number of population delegate values of Conceptc for\\nthe overall aggregation time period; and T-Startaggreg j and T-\\nEndaggreg j are the start and end times of the time granule that is\\nspeciﬁc to the jth population delegate value.\\n4.3. The OBTAIN speciﬁcation language\\nThe VISITORS system includes a graphical module, which is used\\nfor speciﬁcation of the relevant patients, time intervals and values.\\nUnderlyingtheVISITORSgraphicalexpression-speciﬁcationmodule,\\nis an ontology-based temporal-aggregation (OBTAIN) speciﬁcation\\nlanguage. The OBTAIN speciﬁcation language enables the speciﬁca-\\ntion of three types of expression: Select Patients, Select Time Intervals\\nand Get Patients Data. A full exposition of the OBTAIN language and\\nthegraphicalexpression-speciﬁcationmoduleisbeyondthescopeof\\nthis study and can be found elsewhere [4]. We will brieﬂy explain,\\nhowever, these three expression types. The Appendix A includes the\\nBackus Normal Form (BNF) syntax of all three expressions.\\n4.3.1. Select Patients expression\\nThe Select Patients expression retrieves from a selected database\\na list of patients who satisfy a set of either demographic (i.e., non-\\ntemporal) constraints, for example, ‘‘select male patients [cur-\\nrently] insured by a certain health maintenance organization’’, or\\ntime and value [knowledge-based] constraints, for example: ‘‘ﬁnd\\npatients who had, during the ﬁrst month following bone-marrow\\ntransplantation, at least one episode of bone-marrow toxicity (an\\nabstraction deﬁned by the relevant clinical protocol) of ‘‘grade1’’ or\\nhigher that lasted at least two days’’; or [time and value]\\nproportional and statistical constraints, for example, ‘‘ﬁnd patients\\ntaking a Statin-type medication whose mean Low density lipid\\n(LDL) cholesterol value exceeded that of the mean value for all\\npatients taking Statin-type medications’’.\\n4.3.2. Select Time Intervals expression\\nGiven a set of time-oriented patient data, this expression\\nreturns a list of time intervals that satisfy the constraints deﬁned\\nby the user for some portion of the patients. The goal of this\\nexpression is to ﬁnd when a certain portion of the patients had a\\nspeciﬁc value of some raw or abstract concept or value within a\\npredeﬁned value range. For example, a typical Select Time Intervals\\nexpression is ‘‘Find periods [relative to an allogenic bone-marrow\\ntransplantation event] during which the White blood cell state\\nvalue was less than ‘‘normal’’, and the Platelet value was between\\n2000 and 10,000 cells/ml for at least 50% of the patients.’’\\n4.3.3. Get Patients Data expression\\nGiven a concept, a list of patient IDs and, optionally, a list of\\ntime intervals, the expression retrieves the values of the concept\\nwithin the selected time intervals for the selected patients. The\\ndefault patient list is all the patients in the database, and by\\ndefault there are no time-interval constraints, i.e., values are\\nreturned for the entire timeline. For example, a typical Get Patients\\nData expression is ‘‘Get the state values of Hemoglobin for\\npatients #1–#10 during the ﬁrst two weeks following bone-\\nmarrow transplantation.’’\\nThe Get Patients Data expression is not one of the expression\\ntypes\\nspeciﬁed\\nwithin\\nthe\\nexpression-speciﬁcation\\nmodule.\\nRather, it is generated by an interactive exploration process. Once\\na patient list has been deﬁned by a Select Patients expression (or by\\nthe explicit setting of the ID list in the exploration interface),\\nselecting a concept from the ontology browser (see Fig. 1) opens a\\nnew panel with the values of that concept for all of the patients in\\nthe list.\\n4.3.4. Evaluation of the graphical expression-speciﬁcation module\\nWe have previously evaluated both the functionality and the\\nusability of the expression-speciﬁcation module by a group of 10\\nusers—ﬁve clinicians and ﬁve medical informaticians. Results have\\nshown that both types of users were able, in a short time and with\\nhigh accuracy, to graphically construct complex expressions,\\nalthough the accuracy of the speciﬁcation of time-range con-\\nstraints\\nregarding\\nthe\\nstart\\nor\\nend\\nof\\ntime\\nintervals\\nwas\\nsigniﬁcantly lower than that of the rest of the constraints. The\\ndetails of this evaluation appear in a previous study [4].\\n4.4. Visualization of the data for both raw and abstract concepts\\nThe data set retrieved by the Get Patients Data expression can be\\nvisualized and explored. In general, we provide a two-dimensional\\nvisualization, in which the horizontal axis is the time dimension,\\nand the vertical axis is the value dimension.\\n4.4.1. Visualization of the data of raw concepts\\nFor the basic visualization of the data of raw concepts for\\nmultiple patients, we use the line plot visualization technique,\\nwhich plots the data points on the screen according to the X and Y\\ncoordinates (i.e., according to the time and values coordinates in\\nour case) and either connects or does not connect the plotted\\npoints by lines (Fig. 4). In this visualization, three data types are\\nrepresented:\\n(1) The individual patient’s raw concept data for all patients in a group\\n(denoted as 1 in Fig. 4). In fact, the delegate values of the data\\nfor the current time granularity are represented.\\n(2) The time-oriented population statistics of the whole group of\\npatients. In fact, the population delegate values of the group of\\npatients for the current time granularity are represented. We\\nrepresent the following three time-oriented statistics, each\\naggregated (calculated) within a potentially different time\\ngranularity: maximum value at each time granule (top red line\\ndenoted as 2 in Fig. 4), minimum value at each time granule\\n(bottom blue line denoted as 4 in Fig. 4), and mean value for\\neach time granule (the middle green line denoted as 3 in Fig. 4)\\npopulation values. Each population delegate value is a function\\nof all of the population values in the relevant time granule (e.g.,\\nmonth in this example).\\n(3) The value statistics, which are sensitive to the particular time\\nwindow of the displayed data (denoted by the number 5 in\\nFig. 4). The dynamically changing content of the displayed data\\n(e.g., by panning) causes recalculation of these statistics.\\nDefault statistics for a raw data concept include descriptive\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n19\\n\\nstatistics, such as the mean, maximum, and minimum values,\\nand the standard deviation.\\nTo formally deﬁne the conﬁguration of the display for raw\\nindividual patient data and to understand the semantics of the\\nexploration operators, we introduce the Graph Computational\\nManager (GCM), which stores both the concepts to be displayed\\nand several important computational parameters relevant to these\\nconcepts. The GCM has the following data structure:\\n< Conce ptc; in put data\\x05; T-Granaggreg; DFc > ; 1 \\x06 c \\x06 C;\\nwhere C is the number of concepts in the knowledge base;\\ninput_data* is the original source data accessed by the temporal\\nmediator and retrieved by the Get Patient Data expression; T-\\nGranaggreg deﬁnes the temporal granularity level of the individual\\npatient’s aggregated data (e.g., one data point per day); and DFc is\\nthe delegate function, appropriate to Conceptc, used for calculating\\nthe delegate values for each patient at the T-Granaggreg granularity\\n(e.g., the daily mean).\\nTo intelligently and more efﬁciently explore the Mn data points\\nof Conceptc for Patientn, we use an aggregation of the values\\nthrough a delegate-value function speciﬁc to each concept and\\ntime granularity level (as described in Section 4.2). Thus, for\\nexample, several daily values out of the Mn values might be\\nrepresented by a single delegate value, e.g., the maximum.\\nThe Graph Display Manager (GDM) controls the actual data to be\\ndisplayed and has the following data structure: < Conce ptc;\\ndelegate values\\x05; T-Startexplor; T-Endexplor; T-Granexplor; ½RefP\\x07> ;1 \\x06\\nc \\x06 C;\\nwhere C is the number of concepts in the knowledge\\nbase; delegate_values* were calculated by the delegate function\\nappropriate to the temporal aggregation level T-Granaggreg, as\\nstored by the GCM; T-Startexplor and T-Endexplor deﬁne the temporal\\nrange of the current display time window, which includes 0 or\\nmore delegate values of Conceptc for each of the relevant patients;\\nT-Granexplor denotes the current temporal granularity level of the\\nexploration; and RefP is an optional parameter that deﬁnes the\\nreference symbolic point (e.g., bone-marrow transplantation)\\nwhen a relative time line is being used.\\nThe GCM of the time-oriented population statistics is similar to\\nthe GCM of the raw data. However, it uses a population delegate\\nfunction, instead of an individual delegate function:\\n< Conce ptc; in put data\\x05; T-Granaggreg; PDFc > ; 1 \\x06 c \\x06 C;\\nwhere C is the number of concepts in the knowledge base;\\ninput_data* is the original source data accessed by the temporal\\nmediator and retrieved by the Get Patient Data expression; T-\\nGranaggreg deﬁnes the temporal granularity level of the patient’s\\nstatistical data; and PDFc is the population delegate function,\\nappropriate to Conceptc, used for calculating a population delegate\\nvalues at the T-Granaggreg granularity (as described in Section 4.2).\\nThe GDM of the time-oriented population statistics has the\\nfollowing data structure:\\n< Conce ptc; po pulation delegate values\\x05; T-Startexplor; T-Endexplor;\\nT-Granexplor; ½RefP\\x07 > ; 1 \\x06 c \\x06 C;\\nwhere C is the number of concepts in the knowledge base; the\\npopulation_delegate_values* structure stores the values of the\\npopulation\\nstatistics\\ncalculated\\nfor\\nall\\ntime\\ngranules\\nby\\nthe appropriate PDFc function; T-Startexplor and T-Endexplor deﬁne\\nthe temporal range of the current display time window, which\\nincludes 0 or more population delegate values; T-Granexplor denotes\\nthe current temporal granularity level of the exploration; and RefP\\nis an optional parameter that deﬁnes the reference point (e.g., bone-\\nmarrow transplantation) when a relative time line is being used.\\nT-Granaggreg and T-Granexplor can be different or they can be the\\nsame. For example (see Fig. 4), the individual aggregation temporal\\ngranularity level might be seconds, while the exploration temporal\\ngranularity might be months. If both temporal granularities are the\\nsame, each patient will be represented by only one delegate-value\\npoint for the displayed concept for each time granule.\\nT-Granexplor deﬁnes the exploration granularity of the graphical\\npanel for all patient data displayed in the panel. It might be\\ndifferent for each panel, even in a case of the exploration of the data\\nof an equal concept twice. The default is to use the same\\nexploration granularity for all panels, which enables the synchro-\\nnization of the explored data among different concepts.\\nIf both the individual patient data and population statistics are\\ndisplayed in the panel, then T-Startexplor, T-Endexplor, T-Granexplor\\nparameters are same in each of the GDMs.\\n4.4.2. Visualization of the data of abstract concepts\\nTemporal abstractions for multiple patients are displayed as a\\ndistribution of the delegate values of the abstract concept of the\\npatients at the desired (interactively modiﬁed) time granularity.\\nFor the visualization, we use a modiﬁed version of the bar chart\\nvisualization technique. The modiﬁcation includes providing\\nseparate [0.100%] scales for each of the possible values of the\\ntemporal abstractions (e.g., for each toxicity grade), which is useful\\nfor discovering trends in the distribution. We assume a ﬁnite\\nnumber of [symbolic] values for each abstract concept. The\\nFig. 5. Visualization of the values of the Platelet-state abstract concept for a group of 58 patients (retrieved earlier by using a Select Patients expression) during 1995. The user\\nsees the monthly distribution of the patients’ values during this period (e.g., the dotted rectangle denotes the distribution of the concept’s values during May). Note that the\\nvertical axis has two scales: an external ordinal scale of possible values and an internal percentage scale, from 0 to 100%, within each concept value. The tooltip provides\\ndetailed information about the contents of the displayed area: the value (low) and its proportion (43.75%), the start and end time points and the actual number of patients\\nwith the selected value (7) out of all patients (16 patients). Note that only 16 patients in the group of 58 patients had the necessary raw data (in this example, the Platelet\\ncount) during May 1995.\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n20\\n\\nvisualization of the values of an abstract concept for multiple\\npatients is shown in Fig. 5.\\nThe GCM of a temporal abstraction graph has the following data\\nstructure:\\n< Conce ptc; in put data\\x05; T-Granaggreg; DFc > ; 1 \\x06 c \\x06 C;\\nwhere C is the number of concepts in the knowledge base;\\ninput_data* is the original source data accessed by the temporal\\nmediator and retrieved by the Get Patient Data expression; T-\\nGranaggreg deﬁnes the temporal granularity level of the patient’s\\naggregated data; and DFc is the delegate function, appropriate to\\nConceptc, used for calculating the delegate values for each patient\\nin the T-Granaggreg granularity.\\nSince the data of an abstract concept for multiple patients are\\ndisplayed as a set of distributions of the abstract-concept values, one\\nforeachofthe temporal granules ofthe T-Granaggreg level,the GDMof\\nthe temporal abstraction differs from the GDM of the raw data:\\n< Conce ptc; distribution\\x05; T-Startexplor; T-Endexplor; T-Granexplor;\\n½RefP\\x07 > ; 1 \\x06 c \\x06 C;\\nwhere C is the number of concepts in the knowledge base;\\ndistribution* is the data structure {[vall\\nc, Prevl\\nc]1 . . . [vall\\nc, Prevl\\nc]J},\\nwhere vall\\nc is the 1st value of Conceptc (usually measured on ordinal\\nsymbolic scale) and Prevl\\nc is the prevalence of patients having that\\nvalue; and J is the number of temporal granules at the aggregation\\ngranularity level T-Granaggreg, during the exploration time interval\\n[T-Startexplor, T-Endexplor]. The delegate values are calculated by the\\ndelegate function appropriate to the T-Granaggreg as stored by\\nthe GCM. T-Startexplor and T-Endexplor deﬁne the temporal range of the\\ncurrent display time window, which includes 0 or more delegate\\nvalues of the Conceptc for each relevant patient. T-Granexplor denotes\\nthe current temporal granularity level of the exploration. RefP is an\\noptional parameter that deﬁnes the reference point (e.g., bone-\\nmarrow transplantation) when a relative time line is being used. In\\nthe case of temporal abstractions, T-Granaggreg and T-Granexplor are\\nalways the same; for example (see Fig. 5), the aggregation\\ngranularity and exploration granularity could both be months.\\n4.5. Exploration operators of time-oriented data of multiple patients\\nIn this section, we describe the formal deﬁnition and semantics\\nof the general exploration operators for the exploration of time-\\noriented data for multiple patients, i.e., the input, output, and\\nnecessary calculations performed on the patients’ data. Each of the\\noperators can be applied on three different possible types of data\\ndisplayed in the graph:\\n\\x03 Individual raw data, such as Hemoglobin values, White blood cell\\ncount, and other laboratory tests, interventions and medications.\\n\\x03 Time-oriented\\npopulation\\nstatistical\\nvalues\\n(e.g.,\\nmaximal\\nmonthly value of Hemoglobin), calculated from the data of\\nraw concepts for multiple patients.\\n\\x03 Temporal abstractions, such as the values of the Hemoglobin-\\nstate abstract concept, derived from the data of raw concepts by\\nusing the domain-speciﬁc context-sensitive knowledge base.\\nThe application of the exploration operators to different data\\ntypes differs with the performed calculation and the output.\\nSeveral of the operators act in a ‘‘syntactic’’ fashion, i.e., perform\\nsimple magniﬁcation or miniﬁcation of the displayed data without\\nadditional calculation; thus the input and output data are same.\\nOther operators return output data that are different from the\\ninput data (e.g., by computing the mean values), but that are still in\\nthe input concept’s domain of values and units.\\nFor conciseness, we introduce the following annotation, which\\nwe use to formally deﬁne the semantics of all three operators:\\n\\x03 current_values*: denotes the current data that are displayed by\\nthe GDM before the application of the operator (i.e., delegate_-\\nvalues* in the case of individual raw data, population_delegate_-\\nvalues* in the case of time-oriented population statistics, and\\ndistribution* in the case of temporal abstractions).\\n\\x03 D-function: denotes the delegate function DFc in the case of\\nindividual raw data and temporal abstractions or the population\\ndelegate function PDFc in the case of time-oriented population\\nstatistics.\\n\\x03 new_values*: denotes the new actual data to be displayed by the\\nGDM after the application of the operator. The new_values are\\ncalculated by the function accordingly to the aggregation time\\ngranularity from the original data accessed by the temporal\\nmediator.\\n4.5.1. The Temporal Exploration Operator\\nThe VISITORS system enables the user to manipulate the data\\ninteractively, e.g., to pan the patient data within various time\\nintervals or to zoom-in and zoom-out of the patient data within\\ndifferent time granularities ranging from seconds to decades.\\nFor individual values of raw concepts for multiple patients the\\nTime Exploration Operator operates as a syntactic zoom. It\\nperforms a magniﬁcation or miniﬁcation of the displayed data\\nduring the speciﬁc predeﬁned period of time (i.e., a particular time\\ngranule or a speciﬁc time period). This zoom function facilitates the\\ndisplay of large amounts of patient data (i.e., an overview mode) or\\na reduction of the time granularity to support in-depth exploration\\nof the patients’ data for a particular time granule of an arbitrary\\ntime range. The individual delegate and population delegate values\\nof a raw data concept are not recalculated during the application of\\nthe Time Exploration Operator; thus, this operator is indeed purely\\nFig. 6. Application of the Time Exploration Operator (in this case a zoom-in) to the data for the White blood cell (WBC) raw concept for a group of 58 patients selected earlier\\nby the user. Clicking on the ‘‘Mar’’ (March) month widget in panel A results in a display of the data throughout the month of March, as shown in panel B. Note, clicking on the\\n‘‘Mar 95’’ granule in the panel B would again provide panel A.\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n21\\n\\nsyntactic, as opposed to the semantic operators, which signiﬁ-\\ncantly modify the displayed data.\\nFor an individual patient’s data and for population statistics of\\nraw concepts the Temporal Exploration Operator is applied\\nthrough the GDM and may be formally deﬁned as follows:\\nTEOðGDM : < Conce ptc; current values\\x05; T-Startexplor; T-Endexplor;\\nT-Granexplor; ½RefP\\x07 > ; T-Start0\\nexplor; T-End0\\nexplor; T-Gran0\\nexplorÞ\\n) GDM0 : < Conce ptc; new values\\x05; T-Start0\\nexplor; T-End0\\nexplor;\\nT-Gran0\\nexplor; ½RefP\\x07 > ;\\nwhere T-Start0explor, T-End0explor, and T-Gran0explor are user-deter-\\nmined parameters of the new earliest and latest time points for\\ndisplay, and the new exploration temporal granularity, respec-\\ntively. Fig. 6 shows the application of the Temporal Exploration\\nOperator to the data of a raw concept for a group of patients.\\nNote that the default delegate function is the identity function.\\nThus, the output from the mediator will be recalculated according\\nto the input temporal granularity of the data and represented using\\nan identical temporal granularity.\\nApplication of the Temporal Exploration Operator to the values\\nof abstract concepts calculated for multiple patients is more\\ncomplex and quite different. To apply the Temporal Exploration\\nOperator in this case, we must recalculate the distributions\\naccording to the new time granularity level; thus, the application\\nof the operator is, in fact, semi-semantic and is applied through\\nboth the GCM and the GDM:\\nTEOðGCM : < Conce ptc; in put data\\x05; T-Granaggreg; DFc > ;\\nGDM : < Conce ptc; distribution\\x05; T-Startexplor; T-Endexplor;\\nT-Granexplor; ½RefP\\x07 > T-Gran0\\naggreg; T-Start0\\nexplor; T-End0\\nexplor;\\nT-Gran0\\nexplorÞ ) ðGCM0 : < Conce ptc; in put data\\x05;\\nT-Gran0\\naggreg; DFc > GDM0 : < Conce ptc; distribution0\\x05;\\nT-Start0\\nexplor; T-End0\\nexplor; T-Gran0\\nexplor; ½RefP\\x07 > Þ;\\nwhere T-Start0explor, T-End0explor, and T-Gran0explor are user-deter-\\nmined parameters of the new earliest and latest time points for\\ndisplay, and the new exploration temporal granularity, respec-\\ntively; and distribution0* is the new distributions of delegate values\\nthat were calculated by the delegate function appropriate to the T-\\nGranaggreg as stored by the GCM. Fig. 7 shows the application of the\\nTemporal Exploration Operator to the data of abstract concept for a\\ngroup of patients.\\nThe user-determined parameters T-Start0explor, T-End0explor, and\\nT-Gran0explor can be derived in several ways: panning (i.e., shifting\\nthe displayed data to the right or left) changes the T-Start0explor and\\nT-End0explor parameters; using a calendaric-range zoom (i.e.,\\nstandard calendar function) enables the user to specify the T-\\nStart0explor and the T-End0explor time points to zoom-in to a speciﬁc\\ntime range; and clicking the speciﬁc temporal granule at the\\nbottom part of the graphs (see Figs. 6 and 7) performs a zoom-in or\\nzoom-out operation according to the selected time granule; thus,\\nall three parameters may be changed. A detailed exposition of all of\\nthe ways for manipulating the temporal granularity can be found\\nin the description of the KNAVE-II system, which focuses on\\nexploration of individual patient data [2].\\n4.5.2. The Change Delegate Value Operator\\nAs explained earlier, the VISITORS system supports aggregation\\nof patient data according to a given time granularity. Such\\naggregate values are designated the ‘‘delegate values’’ for that\\ngranule (for example, a representative blood-glucose value for the\\nwhole day or even the whole month). To apply another delegate\\nfunction or to change the granularity level of the aggregation, the\\nuser can use the Change Delegate Value Operator (CDVO). Note\\nthat the data values are changed, but the data still maintain the\\nsame type and domain of values.\\nFor all three types of displayed data, the operator is applied\\nthrough the GCM. Thus, the GDM will display the new values within\\nthe same display conﬁguration, i.e., the earliest and latest times for\\ndisplay and exploration granularity are the same before and after\\napplication of the Change Delegate Value Operator. The formal\\ndeﬁnition of the Change Delegate Value Operator is as follows:\\nCDVOðGCM : < Conce ptc; in put data\\x05; T-Granaggreg; D- function > ;\\nGDM : < Conce ptc; current values\\x05; T-Startexplor; T-Endexplor;\\nT-Granexplor; ½RefP\\x07 > ; T-Gran0\\naggreg; D- function0Þ ) GCM0 :\\n< Conce ptc; in put data\\x05; T-Gran0\\naggreg; D- function0 > ;\\nGDM0 : < Conce ptc; new values\\x05; T-Startexplor; T-Endexplor;\\nT-Granexplor; ½RefP\\x07 > ;\\nwhere T-Gran0aggreg, and D-function0 are the user-determined\\nparameters of the new temporal aggregation granularity and D-\\nfunction, respectively. The new_values* are derived from the input\\ndata as explained in Section 4.2.\\nFigs. 8 and 9 show the application of the Change Delegate Value\\nOperator to the values of raw and abstract concepts for a group of\\npatients.\\nFor both cases of the visualizations, i.e., for the raw and abstract\\nconcepts, we provide a Graph Manager interface (not shown here),\\nin which the user can change the delegate function and the\\naggregation temporal granularity for each graph in the panel; e.g.,\\nthe maximal and mean population statistics might be represented\\nFig. 7. Application of the Temporal Exploration Operator (in this case a zoom-out) to the data for the White blood cell (WBC) state abstract concept for a group of 58 patients\\nselected earlier by the user. Clicking on the ‘‘May 95’’ month widget in panel A produces a display of the distributions of data throughout the year of 1995 according to the\\nmonths time granularity, as shown in panel B. Note that the required raw data (in this example, the WBC count) might not necessarily have been available for all 58 patients\\nduring May 1995. Clicking on the ‘‘May’’ granule in the panel B would again provide panel A.\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n22\\n\\nat a monthly granularity level, and the individual raw data might be\\nrepresented at a resolution level of seconds, as is shown in Fig. 4.\\n4.5.3. The Set Relative Time Operator\\nChanging dynamically the point of view from an absolute\\n(calendar-based) time line to a relative time line is a key operator\\nin the VISITORS system. The relative time line is set by identifying\\nclinically signiﬁcant events, or another clinically signiﬁcant time\\npoint in the domain’s temporal-abstraction ontology (e.g., start of\\ntherapy, birth of the child, start of high fever), which serve as a date\\nof reference (time zero) for all patient data.\\nSeveral patients in the group might not have experienced the\\nreference event (e.g., a particular type of intervention). Thus, only\\nthe data points of patients who have experienced the reference\\nevent are displayed. Since each patient may have experienced the\\nintervention at a different time from the other patients, we must\\nalign the data of all of the patients according to the reference time\\npoint, which is set as the new (relative) zero-time point. Moreover,\\nin the case of a temporal abstraction, we must calculate the\\ndistribution of abstract concept values by including only patients\\nwho had experienced the selected event or intervention.\\nFor uniformity, each ‘‘month’’ in the relative time line includes\\nby deﬁnition 30 days and each ‘‘year’’ comprises 360 days (or 12\\nmonths) (see Section 2.2.3). These are, of course, only labels for\\ndurations, and no longer bear any relation to absolute calendrical\\nmonths.\\nNote that the Set Relative Time Operator is not purely a\\nsyntactic operator, because, although the data type and domain of\\nvalues remain the same, the output data values are typically quite\\ndifferent, all values and statistics being recalculated according to\\nthe new zero point.\\nThe Set Relative Time Operator (SRTO) is formally deﬁned as\\nfollows:\\nSRTOðGCM : < Conce ptc; in put data\\x05; T-Granaggreg; function > ;\\nGDM : < Conce ptc; current values\\x05; T-Startexplor; T-Endexplor;\\nT-Granexplor; ½RefP\\x07 > ; RefP0Þ ) GDM0 : < Conce ptc; new values0\\x05;\\nT-Startexplor; T-Endexplor; T-Granexplor; RefP0 > ;\\nwhere RefP is the user-determined parameter of the zero-point of\\nthe displayed data. Note that the GCM supports the computation of\\nthe\\nnew\\nvalues;\\nthe\\naggregation\\n(T-Granaggreg)\\nand\\ndisplay\\nparameters (T-Startexplor, T-Endexplor, T-Granexplor) are not changed.\\nOnly the visualized data are changed.\\nBefore application of the Set Relative Time Operator, the T-\\nStartexplor and T-Endexplor time points have an absolute time value\\n(e.g., 01.07.1995), but after application they have a relative time\\nformat (e.g., +1 month following the RefP, for example, a bone-\\nmarrow transplantation intervention). Figs. 10 and 11 show the\\napplication of the Set Relative Time Operator to the values of a raw\\nand abstract concepts for a group of patients.\\n4.5.4. Other general exploration and documentation operators\\nOther visualization and exploration operators that are well\\ndescribed in our previous work regarding exploration of individual\\npatient data [2] are also implemented in the VISITORS system.\\nThese operators include: (1) export of data to a spreadsheet, (2)\\nperformance of dynamic sensitivity analysis of possible changes to\\nthe values (‘What-if’’ dynamic simulation) of the raw data, and\\ntheir effect on the derived temporal abstraction for the individual\\npatient (i.e., the user is able to simulate the effect of changing the\\nderived temporal abstractions by modifying, deleting or adding\\nFig. 9. Visualization of the data for the White blood cell (WBC) state abstract concept for group of patients before (panel A) and after (panel B) application the Change Delegate\\nValue Operator. Panel A displays the distribution of the values by representing the value of the abstraction that has the maximal cumulative duration. By using such a function,\\nthe state of the WBC count for all patients was abstracted as ‘‘normal’’ during April 1995 (see tooltip in the panel A). The resultant visualization (panel B) displays the new\\ndistribution of the values, this time using a delegate function that selects the value associated with the longest-duration interval. Thus, during April 1995, the state of the WBC\\ncount for half of the patients was abstracted as ‘‘normal’’, and for half of the patients was abstracted as ‘‘very-low’’.\\nFig. 10. Application of the Set Relative Time Operator to the data for the White blood cell (WBC) count raw concept for a group of 58 patients. Panel A displays the data for the\\nyear 1995 (the absolute time-line navigation bottom bars), and the panel B displays the data after application of the Set Relative Time Operator, showing the ﬁrst year\\nfollowing the allogenic bone-marrow transplantation (selected by user), as shown in the new relative time-line navigation bottom bars. Note that absolute calendrical labels\\nsuch as May 1995 no longer appear; instead, only relative temporal labels such as 5m (+5 months) can be seen.\\nFig. 8. Visualization of the data for the Red blood cell (RBC) count raw concept for the 58 patients (selected earlier by the user) before (panel A) and after (panel B) application\\nof the Change Delegate Value Operator. Panel A displays the RBC data at a granularity of seconds and minimal- and maximal-value population delegate values (population\\nstatistics) at a granularity of months (the mean is not displayed). The resultant visualization (panel B) displays the monthly mean RBC values for each patient. The green graph\\nrepresents the yearly mean value (one value during year 1995) for all patients for whom data had been collected during the year 1995.\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n23\\n\\nraw data), (3) the exploration of the knowledge-based ontology\\n(i.e., exploration of the properties of both the raw concepts and\\ntheir abstractions, using meaningful domain-speciﬁc semantic\\nrelations such as derived-from, part-of), (4) support for the use of\\nclinical proﬁles (e.g., a diabetic proﬁle includes a set of diabetes-\\nrelated raw and abstract concepts such as Blood-glucose value,\\nGlucose-state level, HbA1C value, etc.), (5) support of intra-group\\ncollaboration capabilities, and (6) documentation operators.\\n4.6. Interactive visual exploration of temporal associations among\\ntime-oriented data of multiple patients\\nA temporal association chart (TAC) [57] is a new, user-driven,\\ninteractive knowledge-based visualization technique that supports\\nthe investigation of temporal and statistical associations within\\nmultiple patient records among both raw and abstract temporal\\nconcepts. A complete exposition of the computational semantics\\nunderlyingTACS,aswellasadetailedevaluationoftheirfunctionality\\nandusability,appearselsewhere[57].Here,weprovideasummaryof\\nthe core functionality that is relevant to the current study.\\nThe core of the TAC is an ordered list of raw and/or abstract\\ndomain concepts (e.g., Platelet-state, Hemoglobin value, White\\nblood cell count), designated a temporal association template (TAT),\\nin a particular order determined by the user. Each concept is\\nmeasured (or computed, in the case of an abstract concept) for a\\nparticular patient group during a particular concept-speciﬁc time\\nperiod. The period can be different for each concept. Given the data\\nfor a group of patients, between each consecutive pair of concepts\\nin the list, a relationship will be computed based on the delegate\\nvalues of the concepts for each patient. If one of the concepts is raw,\\nthe result will be a set of relations, each relation being between a\\nvalue of the ﬁrst concept and a value of the second concept for a\\nparticular patient. If both concepts are abstract, the result will be\\naggregated into a set of extended relations—temporal association\\nrules, one rule per each combination of values from both concepts,\\neach rule representing the set of patients who have this particular\\ncombination of values for the two abstract concepts.\\nTACs are created by the user in two steps. First, the user creates\\nthe TAT, by the selection of two or more concepts, using an\\nappropriate interface (not shown here), possibly changing the order\\nas necessary; and second, the user selects the group of patients (e.g.,\\nfrom a list of groups retrieved earlier by Select Patients expressions).\\nAlthough full exposition of TACs is beyond the scope of the current\\npaper, we will brieﬂy explain their core semantics.\\n4.6.1. Temporal association templates\\nA TAT is an ordered list of time-oriented concepts (TOCs)\\n(jTOCsj \\x08 2), in which each TOC denotes a combination of a raw or\\nabstract domain concept (such as a Hemoglobin value or a bone-\\nmarrow toxicity grade) and a time interval <T-Start, T-End>. A\\nspeciﬁc concept can appear more than once in the TAT, but only\\nwithin different time intervals. An example of a TAT listing the\\nHemoglobin-state and White blood cell-state abstract concepts\\nand the Platelet count raw-data concepts, and their respective time\\nperiods, would be <(Hemoglobin-state, 1/1/95, 31/1/95), (WBC-\\nstate, 1/1/95, 31/1/95), (Platelet count, 1/1/95, 31/1/95), (WBC-\\nstate, 1/2/95, 28/2/95)>. Note that once a TAT is deﬁned, it can be\\napplied to different patient groups.\\n4.6.2. Application of a TAT to a set of patient records\\nWhen applying a TAT to a set P of patient records that includes N\\npatients, we get a TAC. A TAC is a list of instantiated TOCs and of\\nassociation relations (ARs), in which each instantiated TOC is\\ncomposed of the original TOC of the TAT upon which it is based and\\nthe patient-speciﬁc delegate values for that TOC within its\\nrespective time interval, based on the actual values of the records\\nin P. To be included in a TAC, a patient Pn (1 \\x06 n \\x06 N) must have at\\nleast one value from each TOC of the TAT deﬁning the TAC. The\\ngroup of such patients is the relevant group (or relevant patients). In\\nthe resulting TAC, each instantiated TOCi includes the original TAT\\nTOCi and the set of delegate values (one delegate value for each\\npatient) of the concept Ci, computed using the delegate function\\nappropriate to Ci from the set of patient data included within the\\nrespective time interval [T-Start i, T-Endi], as deﬁned in the TAT.\\n4.6.3. Association relations\\nThe relationship between the values of consecutive instantiated\\nTOCs <TOCi, TOCi+1>, 1 \\x06 i < I (I is the number of concepts in the\\nTAT) are denoted by ARs.\\nWhen at least one of the consecutive concepts is raw, the\\nnumber of ARs between each pair of TOCs is equal to the number of\\nrelevant patients. Each AR connects the delegate values vali\\nn and\\nvaliþ1\\nn\\nof the pair of concepts Ci and Ci+1, during the relevant period\\nof each concept, for one speciﬁc patient Pn.\\nIn the case of an abstract-abstract concept pair, we aggregate\\nthe ARs between two consecutive TOCs into groups, where each\\ngroup includes a set of identical pairs of delegate values (one pair\\nfor each concept). Each such group denotes a temporal association\\nrule (TAR) and includes:\\n\\x03 Support: the proportion of relevant patients who have the\\ncombination of delegate values < vali; j\\nn , valiþ1;k\\nn\\n>, 1 \\x06 j \\x06 J,\\n1 \\x06 k \\x06 K, where vali; j\\nn , valiþ1;k\\nn\\nare the jth and kth allowed values of\\nCi and Ci+1, respectively; and J and K are the numbers of different\\nvalues of the concepts Ci and Ci+1, respectively. (We assume a\\nﬁnite number of [symbolic] values for each abstract concept.)\\n\\x03 Conﬁdence: the fraction of the relevant patients who, given a\\ndelegatevaluevali; j\\nn ofconceptCiforpatientPn,haveadelegatevalue\\nof concept Ci+1 that is valiþ1;k\\nn\\n; i.e., the probability P½valiþ1;k\\nn\\njvali; j\\nn \\x07.\\n\\x03 Actual number of patients: the number of patients who have this\\ncombination of values.\\nThe number of possible TARs between two consecutive TOCs is\\nthus J*K.\\nNote, that the deﬁnitions of the support and conﬁdence\\nproperties are equal to support and conﬁdence in the context of\\nassociation rules, where performing general data mining.\\n4.7. Display of TACs and interactive data mining using TACs\\nFig. 12 presents an example of a TAC computed by applying a\\nTAT [user-deﬁned on the ﬂy, using another interface (not shown\\nFig. 11. Application of the Set Relative Time Operator to the data for the Platelet-state abstract concept for a group of 58 patients. Panel A displays the data for May 1995 (the\\nabsolute time-line navigation bottom bars), and panel B displays the data after application of the Set Relative Time Operator, showing the ﬁrst month following the allogenic\\nbone-marrow transplantation (selected by user), as shown in the new relative time-line navigation bottom bars. Note that absolute calendrical labels such as March 15th,\\n1995 no longer appear; instead, only relative temporal labels such as 8d (+8 days) can be seen.\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n24\\n\\nhere) that enables the user to select TAT concepts]. The TAT\\nincludes three hematological concepts (Platelet-state, White blood\\ncells (WBC)-state abstract concepts, and the Hemoglobin (HGB)\\nraw concept) and two hepatic concepts (Total bilirubin (T-Bili) and\\nAlkaline-phosphatase (Alk-Phos)-state abstract concepts) applied\\nto a group of 58 patients selected earlier by the user. For the\\nabstract concepts, WBC-, Platelet-, and Alkaline-phosphatase\\nstates, the visualization in Fig. 12 shows the relative proportion\\n(i.e., distribution) of all the values of the speciﬁc abstract concept\\nfor the relevant patients, within the speciﬁc time interval. It also\\nshows each patient’s mean values for Hemoglobin and Total\\nbilirubin during the year 1995. Delegate values of all adjacent\\nconcept pairs for each patient are connected by lines, denoting the\\nARs. Only 44 patients in this particular group had data for all\\nconcepts during 1995.\\nAs described above, ARs among values of abstract concepts\\nprovide additional statistical information. For example, the AR’s\\nwidth indicates to the user the support for each combination of\\nvalues,\\nwhile\\nthe\\ncolor\\nsaturation\\nrepresents\\nthe\\nlevel\\nof\\nconﬁdence: a deep shade of red signiﬁes high conﬁdence, while\\npink denotes lower conﬁdence. The support, conﬁdence, and the\\nnumber of patients in each association are displayed numerically\\non the edge. For example, the widest edge among two distributions\\non the left hand side in Fig. 12 represents the relation between the\\n‘‘low’’ value of the Platelet-state concept and the ‘‘normal’’ value of\\nthe WBC-state concept during 1995. The edge shows that 50.0% of\\nall of the patients in the relevant patient group had this, particular,\\ncombination of values during 1995 (i.e., support = 0.500). 75.9% of\\nthe patients who had a ‘‘low’’ Platelet-state value have also had a\\n‘‘normal’’ WBC-state value during 1995 (i.e., conﬁdence = 0.759).\\nThis association was valid for 22 patients. Note that in this\\nparticular case, the two periods were identical (the year 1995).\\nHowever, the user could have asked to visualize ARs that link\\ndifferent time periods (e.g., different years or different months\\nwithin the same or another year).\\nUsing this visualization interface, the user can dynamically\\napply a value and time lens to interactively analyze the time and\\nvalue associations among multiple patients’ data:\\n\\x03 Dynamic application of a value lens enables the user to answer\\nthe question ‘‘how does constraining the value of one concept\\nduring a particular time period affect the association between\\nmultiple concepts during that and/or during additional time\\nperiods’’. The user can either select another range of values for\\nthe data of the raw concepts, using trackbars, or select a subset\\nof the relevant values in the case of an abstract concept. In the\\nfuture versions of VISITORS, we are planning to allow the user\\nto vary\\nalso\\nthe\\ndelegate function\\nto\\nenable\\nadditional\\nanalyses.\\n\\x03 The system also supports the application of a time lens by\\nchanging the range of the time interval for each instantiated TOC,\\nincluding ranges on the relative time line. The time lens can be\\nespecially useful for clinical research involving longitudinal\\nmonitoring (e.g., clinical trials). For example, the researcher can\\ninvestigate the relation between the values among several\\nconcepts before and after treatment.\\nIn addition, the user can change the order of the displayed\\nconcepts, export all the visualized data and associations to an\\nelectronic spreadsheet, and add or remove displayed concepts.\\nThe main limitation of TACs in the current version of the\\nVISITORS system is that the system does not recommend which\\nconcepts to select, nor the time periods in which to examine the\\nedges (ARs) among them. However, we intend to combine the\\nVISITORS system with temporal data mining tools (that we have\\nbeen developing [58]) for automated detection of sufﬁciently\\nfrequent temporal associations.\\n5. Example of a clinical scenario\\nIn this section, we present an example of an exploration clinical\\nscenario for application of the VISITORS system and an analysis\\nusing a TAC. The example, which relates to a retrospective database\\nof bone-marrow transplantation (BMT) patients (see evaluation\\nSection 6), comprises an investigation of the bone-marrow recovery\\ncharacteristics of patients, who are either young [<20 years] or old\\n[>70 years], following an autologous BMT procedure.\\nFig. 12. Visualization of associations among three hematological and two hepatic concepts for 44 patients during the year 1995. Association rules are displayed between the\\nPlatelet-state and White blood cells (WBC)-state abstract concepts. The conﬁdence and support scales are represented on the left.\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n25\\n\\n(1) In a previous study [4] we introduced the graphical interfaces\\nfor construction of the Select Patients expression ‘‘Select all\\nmale patients, either younger than 20 years or older than 70\\nyears, whose value of the Hemoglobin (HGB) state abstract\\nconcept was at least ‘‘moderately-low’’ or higher, for at least\\nten days, during the ﬁrst month following an autologous BMT\\nand whose White blood cell (WBC) counts were abstracted as\\n‘‘increasing’’ during those ten days’’. As a result of applying this\\nSelect Patients expression, a group of patients who recovered\\nfrom the autologous BMT (designated as BMT_Au Recovering by\\nthe clinician) was returned and saved. Our current scenario\\ncontinues from that point.\\n(2) In the second stage, the clinician explores the data of 124\\npatients in the ‘‘BMT_Au Recovering’’ group of Myelotoxicity-,\\nWBC-, Platelet-, and HGB-state abstract concepts (Fig. 13)\\nduring the ﬁrst month following the autologous BMT.\\nIt is clear from the ﬁrst (top) panel, i.e., Myelotoxicity-state\\nconcept panel, that until the sixth day after the BMT, the\\nportion of patients with a high value of the myelotoxicity-state\\nconcept (grade_3) was relatively high and increased from day 1\\nto day 6. During these ﬁrst six days, the portion of patients with\\nthe ‘‘very-low’’ value of the WBC-state concept also increased\\neach day (up to 87.78% of patients in the group by the sixth\\nday), as shown in the tooltip of the second panel from the top.\\nStarting from the sixth day following the transplantation,\\nthe portion of patients in the group with values of the WBC-\\nstate concept higher than ‘‘very-low’’ began to increase, and by\\nthe twelfth day, most patients had ‘‘normal’’ or higher values of\\nthe WBC-state, as shown in panel 2.\\nFrom the third panel, i.e., the Platelet-state concept panel,\\nwe can visually conclude that during the ﬁrst month following\\nthe transplantation at least 50% of the patients had a daily\\n‘‘low’’ or ‘‘moderately-low’’ value of the Platelet-state concept.\\nFrom the tenth day onwards, the portion of patients with\\nvalues of Platelet-state higher than ‘‘low’’ appeared to increase.\\nExploration of the fourth panel, i.e., the HGB-state concept\\npanel, shows that during the ﬁrst month following the\\ntransplantation most of the patients had a ‘‘moderately-low’’\\nHGB-state concept.\\nThus, given the data for the selected 124 patients, by\\nfollowing a simple exploration process, we can conclude that:\\n(1) the post-BMT recovery process for most patients seemed to\\nhave started on the tenth day following the BMT; and (2) the\\nHGB-state concept did not seem to be affected by the BMT\\nprocedure or by the recovery of the WBC and Platelet counts.\\n(3) Finally, the clinician creates a TAC to examine the relationships\\namong the delegate values of the 124 patients for the above four\\nabstract concepts during the ﬁrst two weeks following the BMT\\n(Fig. 14). Note that for only 91 patients in the group were there\\nvalues for all four concepts during the desired time period.\\nDuring the period of the ﬁrst two weeks following the BMT,\\nmost patients had a high-grade Myelotoxicity level, a very-low\\nWBC-state, and a low Platelet-state. Moreover, for a patient with a\\n‘‘grade_3’’ value of the Myelotoxicity-state concept, there was a\\n92.3% probability of a ‘‘very-low’’ value of the WBC-state abstract\\nconcept. Similarly, given a ‘‘very-low’’ value of the WBC-state\\nconcept, 86.8% of the patients had a ‘‘low’’ value of the Platelet-\\nstate concept.\\nFrom the TAC in Fig. 14, we can conclude that the characteristic\\n‘‘proﬁle’’ of most of the 124 patients was a combination of a\\n‘‘grade_3’’ value of the Myelotoxicity-state, a ‘‘very-low’’ value of\\nthe\\nWBC-state, a\\n‘‘low’’ value of the Platelet-state, and a\\n‘‘moderately-low’’ value of the HGB-state. If each concept were\\nFig. 13. Exploration of the data of 124 patients for the Myelotoxicity-, WBC-, Platelet-, and HGB-state abstract concepts (denoted by 1–4 from the top to the bottom panel). See\\nthe text for details.\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n26\\n\\nmeasured at a different period, we would get a characteristic\\ntemporal path.\\n6. Evaluation of the functionality and usability of the VISITORS\\nsystem\\n6.1. Research questions\\nWe designed and developed the VISITORS system according to\\nthe desiderata listed in Section 3, and envision it as potentially\\nuseful for two types of users: clinicians and medical informati-\\ncians. We also envision that the system can be used to answer\\ndifferent\\nclinically\\nmotivated\\nquestions.\\nWe\\nconducted\\nan\\nevaluation of the system with the aim of answering the following\\nfour research questions:\\n1. Overall functionality and usability: are the interactive explora-\\ntion operators of the VISITORS system feasible, functional, and\\nusable?\\n2. Effect of the interaction mode: it there a signiﬁcant difference in\\nthe accuracy or in the time to answer between the answers\\nobtained using the operators of the general exploration mode\\nand those obtained using the operators of the TACs?\\n3. Effect of the time line: is there a signiﬁcant difference in the\\naccuracy of the clinical scenarios or in the time to answer using\\nthe two types of time line (an absolute, i.e., calendar time line,\\nversus a relative time line, i.e., a time line referenced to an\\nexternal signiﬁcant clinical time point, such as a therapeutic\\nintervention), and if so, which time line leads to a lower or a\\nhigher level of accuracy and time to answer?\\n4. Effect of the user group: can both medical informaticians\\n(e.g.,\\ninformation\\nsystem\\nengineers\\nwho\\nwork\\nin\\nthe\\nmedical informatics domain) and clinicians use the system\\neffectively?\\n6.2. Measurement methods and data collection\\nTo the best of our knowledge, there is no known method that is\\nfunctionally equivalent to the VISITORS system. Furthermore, with\\ncurrent methods users can answer the complex questions for\\nwhich VISITORS was designed only through the performing the\\nlaborious computations. Thus, we have chosen an objective-based\\napproach [59] for evaluation of the VISITORS system. In such an\\napproach, certain reasonable objectives are deﬁned for a new\\nsystem, and the evaluation strives to demonstrate that these\\nobjectives have been achieved. In this case, we set out to prove\\ncertain functionality and usability objectives of the VISITORS\\nsystem.\\nThe evaluation of the VISITORS system was performed in the\\noncology domain. In all the tests, we used a retrospective database\\nof more than 1000 oncology patients who had received bone-\\nmarrow transplants and who had been followed-up for 2–4 years.\\nThe knowledge source used for the evaluation was an oncology\\nknowledge base speciﬁc to the bone-marrow transplantation\\ndomain; the source was acquired with the help of a domain expert,\\nas reported in a previous study [60].\\nTen participants, ﬁve medical informaticians, i.e., the informa-\\ntion system engineers who work in the medical informatics\\ndomain, and ﬁve clinicians with different medical training levels,\\nwere asked to answer ten clinical questions: ﬁve questions\\nFig. 14. Visualization of associations among the myelotoxicity-, WBC-, Platelet-, and HGB-state abstract concepts (denoted by 1–4 from the left to the right) for 124 patients\\nduring the ﬁrst two weeks following BMT. See the text for details.\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n27\\n\\nrequired the use of the general exploration operators of VISITORS,\\nand ﬁve questions required the use of TACs. None of the study\\nparticipants was a member of the VISITORS development team.\\nThe ten questions were selected in consultation with oncology\\ndomain experts. They represented typical questions relevant to the\\nmonitoring of a group of oncology patients or to the analysis of an\\nexperimental protocol in oncology. Examples of the questions are\\npresented in Table 1. The order of the questions (in each of the two\\ncategories according to the interaction mode) was permuted\\nrandomly across participants. Each evaluation session with a\\nparticipant started with a 20-min tutorial, which included a brief\\ndescription of the KBTA methodology [1] and of the general\\nexploration and TAC operators. A demonstration of the general and\\nTAC operators was given, showing how several typical clinical\\nquestions can be answered. The scope of the instruction was\\npredetermined and included (after the demo) testing each\\nparticipant by asking him/her to answer three clinical questions,\\none of which included the use of TACs. When the participant could\\nanswer the questions correctly, he/she was considered ready to\\nperform for the evaluation.\\nThe functionality was assessed using two parameters: the\\ntime in minutes needed to answer the question, and the\\naccuracy of the answer. The scale determining the accuracy of\\neach answer was pre-deﬁned before the start of the study with\\nthe help of a medical expert. The accuracy was measured on a\\nscale from 0 (completely wrong value) to 100 (completely\\ncorrect value).\\nTo test the usability of VISITORS system, we used the system\\nusability scale (SUS) [61], a common validated method to evaluate\\ninterface usability. The SUS is a questionnaire that includes ten\\npredeﬁned questions regarding the effectiveness, efﬁciency, and\\nsatisfaction from an interface. SUS scores have a range of 0–100.\\nInformally, a score higher than 50 is considered to indicate a usable\\nsystem.\\n7. Results\\nThis section summarizes the evaluation results in terms of the\\nfour research questions deﬁned in Section 6.1.\\n7.1. Overall functionality and usability\\n7.1.1. Method of measurement\\nThe effectiveness of the users in answering clinical questions\\nusing the general exploration operators and TACs was assessed by\\ncalculating the overall means and standard deviations of the\\nanswer accuracy and of the answer response time. To test the\\nusability of the system, the SUS questionnaire was used. To\\ncompare the usability scores of the two groups of participants, a t-\\ntest was performed.\\n7.1.2. Results\\nTable 2 summarizes the response times and answer accuracy\\nlevels for all participants.\\nMost of the participants (9 of 10) successfully answered the\\nclinical questions with a mean accuracy of more than 96 (out of\\n100); 6 of them had a mean accuracy of 100. One participant had a\\nmean accuracy 90.5. All participants answered the clinical\\nquestions in a mean time of less than 3 min.\\nThe mean SUS score for all operators, across all participants,\\nwas 69.3 (over 50 is usable). The results of a t-test analysis showed\\nthat the mean SUS score of the medical informaticians (80.5) was\\nsigniﬁcantly higher than that of the clinicians (58): [t(8) = 3.88,\\np < 0.01].\\n7.1.3. Conclusion\\nBased on the results of the VISITORS evaluation, we may\\nconclude that, after a short training period, the participants were\\nable to answer the clinical questions with high accuracy and within\\nshort period of time. The SUS scores showed that VISITORS system\\nis usable but still needs to be improved.\\n7.2. Effects of the interaction mode, time line and user group\\nTo answer research questions 2, 3 and 4, i.e., questions\\nregarding the interaction mode effect, effect of the time line,\\nand the user group effect, a joint analysis was performed, as\\nexplained below.\\nTable 1\\nExamples of clinical questions used in the evaluation, ordered informally by level of\\ncomplexity.\\nCategory\\nExamples of questions\\nGeneral\\nexploration\\noperators\\n\\x03 What was the mean (i.e., average) annual value of the\\nHemoglobin (HGB) raw data concept during each of\\nthe years 1995, 1996, and 1997?\\n\\x03 What was the distribution of the values of the\\nPlatelet-state abstract concept during the ﬁrst and\\nsecond months (i.e., relative time) following the\\nbone-marrow transplantation (BMT) procedure?\\n\\x03 What was the distribution of the aggregate\\nvalues of the Platelet-state abstract concept\\nduring the ﬁrst month after BMT? Who (i.e., which\\npatient) had the maximal delegate value of the\\nWhite blood cell (WBC) count raw concept and\\nwhich patient had the minimal delegate value\\nof Red blood cells (RBC) count?\\nTemporal\\nassociation\\ncharts\\n\\x03 What delegate value of the HGB-state abstract\\nconcept was the most frequent among the\\npatients who previously had a ‘‘low’’ aggregate\\nvalue of the Platelet-state abstract concept?\\n\\x03 What was the distribution of the delegate values\\nof the Platelet-state in patients whose minimal\\ndelegate value of the WBC count raw concept was\\n5000 cells/ml (instead of the previous minimal value)?\\nWhat were the new maximal and minimal\\ndelegate values of the RBC?\\n\\x03 What percentage of the patients had previously had a\\n‘‘low’’ delegate value of the Platelet-state abstract\\nconcept during both the ﬁrst and second month\\nfollowing the BMT?\\nTable 2\\nAccuracy and response times for all participants.\\nGeneral exploration\\noperators\\nTemporal association\\ncharts\\nOverall\\nAccuracy score (0–100)\\nMean accuracy\\n99.5 \\x02 1.6\\n97.9 \\x02 3.4\\n98.7 \\x02 2.4\\nRange of mean accuracy per question across all participants\\n[97.5. . .100]\\n[95.0. . .100]\\n[95.0. . .100]\\nRange of mean accuracy per participant across all questions\\n[95.0. . .100]\\n[90.5. . .100]\\n[93. . .100]\\nResponse time (min)\\nMean time\\n2.2 \\x02 0.2\\n2.7 \\x02 0.4\\n2.5 \\x02 0.2\\nRange of mean time per question across all participants\\n[2.2. . .2.4]\\n[2.4. . .3.0]\\n[2.2. . .3.0]\\nRange of mean time per participant across all questions\\n[2.0. . .2.6]\\n[2.2. . .3.6]\\n[2.2. . .3.0]\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n28\\n\\n7.2.1. Method of measurements\\nThe effects of the interaction mode (i.e., general exploration\\noperators and TACs), time line (i.e., absolute and relative times),\\nand the group of participants (i.e., medical informaticians and\\nclinicians) on the dependent variables of response time and\\naccuracy of answering were examined using two different three-\\nway ANOVA tests with repeated measures (one for each dependent\\nvariable). The interaction mode and the time-line type were\\nwithin-subject independent variables, and the group of subjects\\nwas a between-subjects independent variable. Since we did not\\nﬁnd statistically signiﬁcant differences among the response times\\nto different clinical questions (and among resultant accuracy levels\\nof answering these questions) of the same time line using the same\\ninteraction mode, the mean value of the response time (and of the\\naccuracy) of the clinical questions of the same time line was used\\nas the dependent variable.\\n7.2.2. Results\\nThe results of the analysis of the accuracy showed no signiﬁcant\\neffects (p > 0.05) of the interaction mode, time line, or of the group\\nof participants. With respect to the response time, the results of the\\nanalysis showed that the only signiﬁcant effect was the main effect\\nof the type of interaction mode [F(1, 8) = 11.08, p < 0.05], i.e., a\\nmean of 2.2 \\x02 0.2 min for answering the clinical questions when\\nusing the general exploration operators of VISITORS, and a mean of\\n2.7 \\x02 0.4 min for answering the clinical questions using the TACs.\\nThere was no signiﬁcant difference in the response time either of the\\ntime line or of the group of participants.\\n7.2.3. Conclusion\\nInteraction mode, time line and user type did not affect the\\naccuracy of the answers to clinical questions. The mean time\\nneeded to answer the clinical scenarios using the TACs was\\nsigniﬁcantly higher than that for the general exploration operators\\nof VISITORS, but it was still less than 3 min. The time line and the\\nuser type did not affect the response time of the answers to the\\nclinical questions.\\n8. Discussion\\n8.1. Contributions and advantages\\nThe major contribution of the VISITORS system is the provision\\nof a comprehensive environment for intelligent, i.e., knowledge-\\nbased, investigation of time-oriented data for multiple patients,\\nincluding the speciﬁcation and retrieval, visualization, exploration,\\nand analysis of the time and value associations among both raw\\nand abstract clinical concepts. Based on the results of the current\\nstudy, the VISITORS system might be described as an ‘‘intelligent\\nequalizer’’ for data interpretation and exploration, which results in\\na uniform performance level, regardless of the patient-speciﬁca-\\ntion complexity, the exploration question or mode, or the user\\ntype. A similar insight has emerged from our previous study,\\nregarding assessment of the usability and functionality of the\\nOBTAIN\\nlanguage\\nexpression-speciﬁcation\\ntool\\n[4].\\nIn\\nthat\\nexperiment, both medical informaticians and clinicians have\\nconstructed complex expressions quickly and efﬁciently, al-\\nthough the clinicians found the interface less usable than the\\ninformaticians.\\nThe VISITORS system can access diverse types of temporal\\nabstraction knowledge and clinical data. Moreover, the VISITORS\\nsystem is quite generic and can support exploration within non-\\nmedical domains, for example, in the information security domain\\n[62].\\nSomewhat\\nsurprisingly,\\nexploration\\nof\\ndata\\nof\\nmultiple\\npatients using a relative time line (i.e., relative to a meaningful\\nreference event, or to another clinically signiﬁcant time point) did\\nnot seem to be more difﬁcult to either clinicians and informa-\\nticians than exploration using an absolute (i.e., calendrical) time\\nline. A possible explanation might be that there is no semantic\\ndifference between ‘‘the ﬁrst day of December’’ and ‘‘the ﬁrst day\\nafter bone-marrow transplantation’’; thus, it is equally easy for\\npeople to explore data within absolute and relative time lines.\\nFurthermore, clinicians are trained to think along relative time\\nlines, such as ‘‘the fourth month of pregnancy’’ or the ‘‘second day\\nof therapy’’. However, using TACs to answer questions, although\\nresulting in equal accuracy levels, required signiﬁcantly higher\\nresponse times. These results probably reﬂect the highly unusual\\nTACs interface and data mining parameters (i.e., support and\\nconﬁdence), which unlike the relative time line, are qualitatively\\ndifferent from the general exploration operators and exploration\\ninterfaces. A possible reason for the lack of signiﬁcant differences\\nin the accuracy scores when using different interaction modes was\\nthat\\nthe\\nevaluation\\nincluded\\na\\nrelatively\\nsmall\\ngroup\\nof\\nparticipants and questions. However, it should be noted that\\nthe variance among accuracy scores was quite low for both\\ninteraction modes, all of the participants achieving scores above\\n90. Thus, the absence of a signiﬁcant effect could not be attributed\\nto random differences and high variability in each interaction\\nmode.\\n8.2. Limitations and future work\\nAlthough the user interfaces of the VISITORS system were\\ninitially\\nconsidered\\nby\\nthe\\nusers\\nas\\nquite\\ncomplex,\\nmost\\nparticipants, during the population speciﬁcation and exploration\\nevaluations successfully ﬁnished all the evaluation tasks. After the\\ntraining session and the learning of the principles of the system,\\nthe system was considered less complex than before. However, the\\nparticipants noted that the system provides more functionality\\nthan will usually be exploited by typical clinicians or even clinical\\nresearchers. Thus, in the future, the main user interface should be\\nsimpliﬁed by providing only the basic functions, while more\\ncomplex features (such as time-range constraints regarding the\\nstart or end of time intervals) would be enabled only in an\\nadvanced mode.\\nThe VISITORS system can also be improved by adding\\ncapabilities for interactive speciﬁcation of the patient popula-\\ntion during the exploration process (in the current state,\\nselection and retrieval of patients is a separate process). The\\nvisual exploration operators can be enhanced by adding more\\nintelligence to the semantic zoom, e.g., increasing the temporal\\ngranularity level of the display (e.g., from the day to month) will\\ncause a display of higher level abstractions when appropriate.\\nFurthermore, ‘‘typical day’’ (modal day) or other granularity\\nvisualizations can be added, which is very important for periodic\\ndomains such as diabetes. Finally, a visual comparison of several\\npopulation groups, based on one or more speciﬁc concepts (for\\nclinical trials or quality assessment tasks), would be quite\\nuseful. We intend to explore all of these options in our future\\nwork.\\nTo summarize, we conclude that intelligent retrieval and\\nexploration of longitudinal data for multiple patients using the\\nVISITORS system is feasible, functional, and usable. Future work is\\nneeded to extend the visual exploration operators and to assess the\\nvalue of such extensions to the users.\\nAcknowledgments\\nThis research was supported by Deutsche Telekom Labs at Ben-\\nGurion University of the Negev and the Israel Ministry of Defense,\\nBGU award No. #89357628-01. We thank all the clinicians and\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n29\\n\\nmedical\\ninformaticians\\nwho\\ncontributed\\ntheir\\ntime\\nto\\nthe\\nevaluation. We thank Ms. Efrat German for her work on the\\nTempura system, and Mr. Ido Hacham and Mr. Shahar Albia for\\ntheir work on the Multi-TOQ system.\\nAppendix A. BNF syntax for a Select Patients expression\\nselect-patients-expression: <data-base> <knowledge-base>\\n<patient-constraints>\\npatient-constraints:\\n<demographic-constraints>*\\n<time-\\nand-value-constraints>+\\nj\\ndemographic-constraints>+\\n<time-\\nand-value-constraints>*\\ndemographic-constraints: <selection-condition>+\\ntime-and-value-constraints: <local-constraints>+ [<global-\\npairwise-constraints>+]\\nselection-condition: <attribute-name> <min value> <max\\nvalue>\\nlocal-constraints:\\n<concept-name>\\n<value-constraints\\n>\\n<time point-constraints> [<duration-constraints>] [<relative-\\ntime-constraints>] [<proportion-constraints>] [<statistical-con-\\nstraints>]\\nglobal-pairwise-constraints:\\n<value-pairwise-constraints>*\\n[<temporal-pairwise-constraints>*]\\nvalue-constraints: <min-value> <max-value>\\nnecessary-context: <context-name>\\ntime point-constraints: <start-point, end-point> [<earliest-\\nstart-point> <latest-start-point>] [<earliest-end-point> <latest-\\nend-point>]\\nduration-constraints: <min-duration> <max-duration>\\nrelative-time-constraints:\\n<relative-start-point>\\n<relative\\nend\\npoint>\\n[<relative-earliest-start-point>\\n<relative-latest-\\nstart-point>]\\n[<relative-earliest-end-point>\\n<relative-latest-\\nend-point>]\\nproportion constraints: <min-threshold> <max-threshold>\\nstatistical\\nconstraints:\\n<individual-patient-delegate-\\nfunction>\\n<population-delegate-function>\\n<value-relation>\\n[<delta>]\\nvalue-pairwise-constraints: <ﬁrst-concept name> <second-\\nconcept name> <time-relation> < individual-patient-delegate-\\nfunction>\\ntemporal-pairwise-constraints:\\n<ﬁrst-concept-name>\\n<second-concept-name>\\n<ﬁrst-boundary-time\\npoint>\\n<second-boundary-time point> <value-relation> [<difference>]\\nindividual-patient-delegate-function:\\n<maximal>\\nj\\n<minimal> j <mean>\\npopulation-delegate-function: <maximal> j <minimal> j\\n<mean>\\nvalue-relation: <great-than-equal> j <great-than> j <less-\\nthan> j <less-than-equal>\\ntime-relation: <before> j <after> j <starts> j <ends> j\\n<within> j <meets> j <overlaps> j <equal>\\ndifference: <min-difference> <max-difference>\\nAppendix B. BNF syntax for a Select Time Intervals expression\\nselect-time-intervals-expression: <data-base> <knowledge-\\nbase> <interval-constraints>\\ninterval-constraints: <granularity> <concept-constraints>*\\n[<time-constraints> j <relative-time-constraints>]\\ngranularity: <seconds> j <minutes> j <hours> j <days> j\\n<months> j <years>\\nconcept-constraints:\\n<concept-name><population-thresh-\\nolds>\\n<value-constraints>\\n<individual-patient-delegate-\\nfunction>\\ntime point-constraints: <start-point, end-point> [<earliest-\\nstart-point> <latest-start-point>] [<earliest-end-point> <latest-\\nend-point>]\\nrelative-time-constraints:\\n<relative-start-point>\\n<relative\\nend\\npoint>\\n[<relative-earliest-start-point>\\n<relative-latest-\\nstart-point>]\\n[<relative-earliest-end-point>\\n<relative-latest-\\nend-point>]\\npopulation-thresholds: <min-threshold> <max-threshold>\\nvalue-constraints: <min-value> <max-value>\\nindividual-patient-delegate-function:\\n<maximal>\\nj\\n<minimal> j <mean>\\nAppendix C. BNF syntax for a Get Patient Data expression\\nget-patients-data-expression:\\n<data-base>\\n<knowledge-\\nbase> < concept-name > <patient-list> [<interval-list>]\\nReferences\\n[1] Shahar Y. A framework for knowledge-based temporal abstraction. Artif Intell\\nMed 1997;90(1–2):79–133.\\n[2] Shahar Y, Goren-Bar D, Boaz D, Tahan G. Distributed, intelligent, interactive\\nvisualization and exploration of time-oriented clinical data and their abstrac-\\ntions. Artif Intell Med 2006;38(2):115–35.\\n[3] Martins S, Shahar Y, Goren-Bar D, Galperin M, Kaizer H, Basso L, et al.\\nEvaluation of an architecture for intelligent query and exploration of time-\\noriented clinical data. Artif Intell Med 2008;43(1):17–34.\\n[4] Klimov D, Shahar Y, Taieb-Maimon M. Intelligent selection and retrieval of\\nmultiple time-oriented records. Journal of Intelligent Information Systems; in\\npress. doi:10.1007/s10844-009r-r0100-0.\\n[5] Hearst M. User interfaces and visualization. In: Baeza Yates R, Ribeiro-Neto B,\\neditors. Modern information retrieval. NY: ACM Press; 2000 . p. 257–324\\n[Chapter 1].\\n[6] Goldstein M, Hoffman B. Graphical displays to improve guideline-based\\ntherapy of hypertension. In: Izzo Jr JL, Black HR, editors. Hypertension primer.\\n3rd ed., Baltimore: Lippincot, Williams & Wikkins; 2003.\\n[7] Silvent A-S, Dojat M, Garbay C. Multi-level temporal abstraction for medical\\nscenario construction. Int J Adapt Control 2005;19:377–94.\\n[8] Miksch S, Horn W, Popow C, Paky F. Utilizing temporal data abstraction for\\ndata validation and therapy planning for artiﬁcially ventilated newborn\\ninfants. Artif Intell Med 1996;8:543–76.\\n[9] Cooley J, Tukey J. An algorithm for the machine calculation of complex Fourier\\nseries. Math Comput 1965;19(90):297–301.\\n[10] Miksch S, Seyfang A, Popow C. Abstraction and representation of repeated\\npatterns in high-frequency data. In: The ﬁfth workshop on intelligent data\\nanalysis in medicine and pharmacology (IDAMAP-2000), workshop notes of\\nthe 14th European conference on artiﬁcial intelligence (ECAI-2000);\\n2000.p. 32–9.\\n[11] Combi C, Chittaro L. Abstraction on clinical data sequences: an object-oriented\\ndata model and a query language based on the event calculus. Artif Intell Med\\n1999;17:271–301.\\n[12] Kowalaski R, Sergot M. A logic-based calculus of events. New Gener Comput\\n1986;4:67–95.\\n[13] Chittaro L. Information visualization and its application to medicine. Artif\\nIntell Med 2001;22(2):81–8.\\n[14] Plaisant C, Milash B, Rose A, Widoff S, Shneiderman B. LifeLines: visualizing\\npersonal histories. In: Proceedings of ACM CHI ‘96 Conference. New York, NY,\\nUSA: ACM Publisher; 1996. p. 221–7. ISBN:0-89791r-r777-4.\\n[15] Plaisant C., Mushlin R., Snyder A., Li J., Heller D., Shneiderman B. LifeLines:\\nusing visualization to enhance navigation and analysis of patient records.\\nRevised version appeared in American Medical Informatics Association Annual\\nFall Symposium, 1998: 76–80.\\n[16] Wang T, Plaisant C, Quinn A, Stanchak R, Shneiderman B, Murphy S. Aligning\\ntemporal data by sentinel events: discovering patterns in electronic health\\nrecords. In: Proc. ACM Conf. on human factors in computing systems\\n(CHI2008), ACM. New York, NY, USA: ACM Publisher; 2008 . p. 457–66.\\nISBN:978-1-60558r-r011-1.\\n[17] Spenke M. Visualization and interactive analysis of blood parameters with\\nInfoZoom. Artif Intell Med 2001;22(2):159–72.\\n[18] Falkman G. Information visualisation in clinical Odontology: multidimen-\\nsional\\nanalysis\\nand\\ninteractive\\ndata\\nexploration.\\nArtif\\nIntell\\nMed\\n2001;22(2):133–58.\\n[19] Chittaro L, Combi C, Trapasso G. Data mining on temporal data: a visual\\napproach and its clinical application to hemodialysis. J Vis Lang Comput\\n2003;14(6):591–620.\\n[20] Aigner W, Miksch S, Mu¨ ller W, Schumann H, Tominski C. Visual methods for\\nanalyzing time-oriented data. IEEE Trans Visual Comput Graph 2008;14(1):\\n47–60.\\n[21] Navathe S, Ahmed R. Temporal extensions to the relational model and SQL.\\nIn: Tansel AU, Clifford J, Gadia S, Segev A, Snodgrass R, editors. Temporal\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n30\\n\\nDatabases: Theory, Design, and Implementation. Redwood City, CA: Benja-\\nmin/Cummings; 1993 . p. 6–27.\\n[22] Sarda N. HSQL: A historical query language. In: Tansel A, Clifford J, Gadia S,\\nJajodia S, Segev A, Snodgrass R, editors. Temporal Databases: Theory, Design,\\nand Implementation. Redwood City, CA: Benjamin/Cummings; 1993. p. 110–\\n40.\\n[23] Snodgrass R. The TSQL2 temporal query language. Hingham, MA: Kluwer;\\n1995, ISBN 0-7923-9614-6.\\n[24] Combi C, Pinciroli F, Cavallaro M, Cucchi G. Querying temporal clinical\\ndatabases with different time granularities: the GCH-OSQL language.\\nIn:\\nGardner RM, editor. 19 Annual symposium on computer applications in\\nmedical care. Philadelphia: Hanley & Belfus; 1995. p. 326–30.\\n[25] Combi C, Cucchi G. GCH-OSQL: a temporally-oriented object-oriented query\\nlanguage based on a three-valued logic. In: Proceedings of the Fourth Inter-\\nnational Workshop on Temporal Representation and Reasoning, 1997\\n(TIME’97), 119-126, 1997. ISBN:0-8186-7937-9.\\n[26] Combi C, Cucchi G, Pinciroli F. Applying object-oriented technologies in\\nmodeling and querying temporally-oriented clinical databases dealing with\\ntemporal granularity and indeterminacy. IEEE Trans Inform Technol Biomed\\n1997;1(2):100–27.\\n[27] Das A, Musen M. A temporal query system for protocol directed decision\\nsupport. Methods Inform Med 1994;33(4):358–70.\\n[28] Combi C, Montanari A, Pozzi G. The t4sql temporal query language. In: Mario J,\\nSilva, Alberto HF, Laender, Ricardo A, Baeza-Yates, Deborah L, McGuinness,\\nBjorn Olstad, Oystein Haug Olsen, Andre O, Falcao, editors. Proceedings of the\\nsixteenth ACM conference on information and knowledge management, CIKM\\n2007. 2007. p. 193–202.\\n[29] Combi C, Montanari A. Data models with multiple temporal dimensions:\\ncompleting the picture. In: Klaus R i, Andreas Geppert, Moira C o, editors.\\nCAiSE, volume 2068 of LNCS:187-202, 2001.\\n[30] Hibino S, Rudensteiner E. A visual multimedia query language for temporal\\nanalysis of video data. Multimedia database systems: design and implemen-\\ntation strategies. Kluwer Academic Publishers; 1995, 123–59.\\n[31] Silva S, Catarci T, Schiel U. Formalizing visual interaction with historical\\ndatabases. Inform Systs; 2002;27(7):487–521.\\n[32] Dionisio J, Cardenas A. MQuery: a visual query language for multimedia,\\ntimeline and simulation data. J Visual Lang Comput 1996;7:377–401.\\n[33] HochheiserH,Shneiderman B.Visualspeciﬁcation ofqueries forﬁnding patterns\\nin time-series data. In: Proceedings of discovery science; 2001. p. 441–6.\\n[34] Hochheiser H, Shneiderman B. Dynamic query tools for time series data sets:\\ntimebox widgets for interactive exploration. Inform Visual Spring 2004;3(1):\\n1–18.\\n[35] Chittaro L, Combi C. Visualizing queries on databases of temporal histories:\\nnew metaphors and their evaluation. Data Knowl Eng 2003;44(2):239–64.\\n[36] Chu W, Chih-Cheng H, Cardenas A, Taira R. Knowledge-based image retrieval\\nwith\\nspatial\\nand\\ntemporal\\nconstructs.\\nIEEE\\nTrans\\nKnowl\\nData\\nEng\\n1998;10(6):872–88.\\n[37] Bresciani P, Nori M, Pedot N. QueloDB: a knowledge based visual query system.\\nIn: Proceedings of the 2000 international conference on artiﬁcial intelligence\\nIC-AI 2000, vol. III; 2000.\\n[38] Dumas M, Fauvet M-C, Scholl P-C. Handling temporal grouping and pattern-\\nmatching queries in a temporal object model. In: Georges Gardarin, James C,\\nFrench, Niki Pissinou, Kia Makki, Luc Bouganim, editors. Proceedings of the\\n1998 ACM CIKM international conference on information and knowledge\\nmanagement. 1998. p. 424–31.\\n[39] Clifford J, Croker A, Grandi F, Tuzhilin A. On Temporal Grouping. Temporal\\nDatabases.\\nIn: Clifford J, Tuzhilin A, editors. Recent Advances in Temporal\\nDatabases, Proceedings of the International Workshop on Temporal Data-\\nbases, Zurich, Switzerland, 17-18 September 1995. Springer, Workshops in\\nComputing; 1995. p. 194–213.\\n[40] Moon B, Lupez F, Vijaykumar I. Efﬁcient algorithms for large-scale temporal\\naggregation. IEEE Trans Knowl Data Eng 2003;15(3):744–59.\\n[41] Goralwalla I, Leontiev Y, O¨ zsu T, Szafron D, Combi C. Temporal granularity:\\ncompleting the puzzle. J Intell Inf Syst 2001;16(1):41–63.\\n[42] Goralwalla I, Leontiev Y, O¨ zsu T, Szafron D, Combi C. Temporal granularity for\\nunanchored temporal data. Proceedings of the seventh international confer-\\nence on Information and knowledge management, November 02–07, 1998,\\nBethesda, Maryland, United States. New York, NY, USA: ACM Publisher, 414–\\n23, ISBN:1-58113r-r061-9.\\n[43] Clifford J, Rao A. A simple, general structure for temporal domains. Temporal\\nAspects Inform Systs 1987;17–28.\\n[44] Dal Lago U, Montanari A. Calendars. Time granularities, and automata.\\nProceedings of the 7th international symposium on advances in spatial\\nand temporal databases, 12–15, 2001, Lecture notes in computer science,\\nvolume 2121/2001. Springer Berlin/Heidelberg, 279–98, ISBN:978-3-540-\\n42301-0.\\n[45] Combi C, Pinciroli F, Pozzi G. Managing different time granularities of clinical\\ninformation by an interval-based temporal data model. Methods Inform Med\\n1995;34:458–74.\\n[46] Bettini C, Mascetti S, Pupillo V. A system prototype for solving multi-granu-\\nlarity temporal CSP. In: Recent advances in constraints, revised selected papers\\nfrom the workshop on constraint solving and constraint logic programming\\n(CSCLP) volume 3419 of lecture notes in computer science. Springer; 2005. p.\\n142–56.\\n[47] Bettini C, Wang X, Jajodia S. Solving multi-granularity temporal constraint\\nnetworks. Artif Intell 2002;140(1/2):107–52.\\n[48] Bettini C, Mascetti S, Wang X. Mapping calendar expressions into periodical\\ngranularities. Proc. of the 11th international symposium on temporal repre-\\nsentation and reasoning (TIME). IEEE Computer Society; 2004, p. 96–102,\\nISBN:0-8186-7937-9.\\n[49] Nigrin D, Kohane IS. Temporal expressiveness in querying a time-stamp–based\\nclinical database. J Am Med Inform Assoc 2000;7:152–63.\\n[50] Narayanan A, Shaman T. Iconic SQL: practical issues in the querying of\\ndatabase through structured iconic expressions. J Visual Lang Comput\\n2002;13(6):623–47.\\n[51] Sacchi L, Larizza C, Combi C, Bellazzi R. Data mining with temporal abstrac-\\ntions: learning rules from time series. Data Min Knowl Discov 2007;15(2):\\n217–47.\\n[52] Boaz D, Shahar Y. A distributed temporal-abstraction mediation architecture\\nfor medical databases. Artif Intell Med 2005;34(1):3–24.\\n[53] Shahar Y, Musen M. Knowledge-based temporal abstraction in clinical\\ndomains. Artif Intell Med 1996;8(3):267–98.\\n[54] Spokoiny A, Shahar Y. Active database architecture for knowledge-based\\nincremental abstraction of complex concepts from continuously arriving\\ntime-oriented raw data. J Intell Inform Syst 2007;28(3):199–231.\\n[55] Spokoiny A, Shahar Y. Incremental application of knowledge to continuously\\narriving time-oriented data. J Intell Inform Syst 2008;31(1):1–33.\\n[56] Shahar Y. Knowledge-based temporal interpolation. J Exp Theor Artif Intell\\n1996;11(1):123–44.\\n[57] Klimov D, Shahar Y, Taieb-Maimon M. Intelligent interactive visual explora-\\ntion of temporal associations among multiple time-oriented patient records.\\nMethods Inform Med 2009;48(3):254–62.\\n[58] Moskovitch R, Shahar Y. Temporal data mining based on temporal abstrac-\\ntions. In: ICDM-05 workshop on temporal data mining; 2005.\\n[59] Friedman C, Wyatt J. Evaluation methods in medical informatics. New York:\\nSpringer; 1997.\\n[60] Chakravarty S, Shahar Y. Speciﬁcation and detection of periodicity in clinical\\ndata. Methods Inform Med 2001, 40(5):410–20. [Reprinted in: Haux R, Kuli-\\nkowski C., editors. Yearbook of Medical Informatics 2003, Stuttgart: F.K.\\nSchattauer and The International Medical Informatics Association, forthcom-\\ning].\\n[61] Brooke J. SUS: a ‘‘quick and dirty’’ usability scale. In: Jordan PW, Thomas B,\\nWeerdmeester BA, McClelland AL, editors. Usability evaluation in industry.\\nLondon: Taylor and Francis; 1996.\\n[62] Shabtai A, Klimov D, Shahar Y, Elovici Y. An intelligent, interactive tool for\\nexploration and visualization of time-oriented security data. In: Conference on\\ncomputer and communications security. Proceedings of the 3rd international\\nworkshop on visualization for computer security; 2006. p. 15–22.\\nD. Klimov et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 11–31\\n31\\n'}, {'source': '/Users/sir/Downloads/Data/PDF/Cookiecutter Data Science.pdf', 'text': \"Cookiecutter Data Science\\nA logical, ﬂexible, and reasonably standardized project structure for doing and\\nsharing data science work.\\nCCDS\\nCCDS Project template\\nProject template\\nVersion 2 of Cookiecutter Data Science has launched recently. To learn more about what's different and\\nwhat's in progress, see the announcement blog post for more information.\\nQuickstart\\nCookiecutter Data Science v2 requires Python 3.8+. Since this is a cross-project\\nutility application, we recommend installing it with pipx. Installation command\\noptions:\\nCookiecutter Data Science v2 now requires installing the new cookiecutter-data-science  Python package,\\nwhich extends the functionality of the cookiecutter  templating utility. Use the provided ccds  command-\\nline program instead of cookiecutter .\\nStarting a new project\\nStarting a new project is as easy as running this command at the command line.\\nNo need to create a directory ﬁrst, the cookiecutter will do it for you.\\nThe ccds  commandline tool defaults to the Cookiecutter Data Science template,\\nbut you can pass your own template as the ﬁrst argument if you want.\\nExample\\nNow that you've got your project, you're ready to go! You should do the following:\\nCheck out the directory structure below so you know what's in the project and\\nhow to use it.\\nRead the opinions that are baked into the project so you understand best\\npractices and the philosophy behind the project structure.\\nRead the using the template guide to understand how to get started on a project\\nthat uses the template.\\nEnjoy!\\nDirectory structure\\nThe directory structure of your new project will look something like this (depending\\non the settings that you choose):\\nProject maintained by the friendly folks at DrivenData.\\nMade with Material for MkDocs\\nCookiecutter Data Science\\nis a DrivenData project.\\nHome\\nQuickstart\\nStarting a new project\\nExample\\nDirectory structure\\nWhy ccds?\\nOpinions\\nUsing the template\\nAll options\\nContributing\\nRelated projects\\nv1 Template\\nCookiecutter Data Science\\nCCDS V2 Announcement\\nWith pipx (recommended)\\nWith pip\\nWith conda (coming soon!)\\nUse the v1 template\\npipx install cookiecutter-data-science\\n# From the parent directory where you want your project\\nccds\\nUse the ccds command-line tool\\nccds\\nfast →\\n$  ccds https://github.com/drivendata/cookiecutter-data-science\\nproject_name (project_name): My Analysis\\nrepo_name (my_analysis): my_analysis\\nmodule_name (my_analysis):\\nauthor_name (Your name (or your organization/company/team)): Dat A. Scientist\\ndescription (A short description of the project.): This is my analysis of the data.\\npython_version_number (3.10): 3.12\\nSelect dataset_storage\\n1 - none\\n2 - azure\\n3 - s3\\n4 - gcs\\nChoose from [1/2/3/4] (1): 3\\nbucket (bucket-name): s3://my-aws-bucket\\naws_proﬁle (default):\\nSelect environment_manager\\n1 - virtualenv\\n2 - conda\\n3 - pipenv\\n4 - none\\nChoose from [1/2/3/4] (1): 2\\nSelect dependency_ﬁle\\n1 - requirements.txt\\n2 - environment.yml\\n3 - Pipﬁle\\nChoose from [1/2/3] (1): 1\\nSelect pydata_packages\\n1 - none\\n2 - basic\\nChoose from [1/2] (1): 2 ▋\\nbash\\n├── LICENSE            <- Open-source license if one is chosen\\n├── Makefile           <- Makefile with convenience commands like `make data` or `make train`\\n├── README.md          <- The top-level README for developers using this project.\\n├── data\\n│   ├── external       <- Data from third party sources.\\n│   ├── interim        <- Intermediate data that has been transformed.\\n│   ├── processed      <- The final, canonical data sets for modeling.\\n│   └── raw            <- The original, immutable data dump.\\n│\\n├── docs               <- A default mkdocs project; see www.mkdocs.org for details\\n│\\n├── models             <- Trained and serialized models, model predictions, or model summaries\\n│\\n├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\\n│                         the creator's initials, and a short `-` delimited description, e.g.\\n│                         `1.0-jqp-initial-data-exploration`.\\n│\\n├── pyproject.toml     <- Project configuration file with package metadata for \\n│                         {{ cookiecutter.module_name }} and configuration for tools like black\\n│\\n├── references         <- Data dictionaries, manuals, and all other explanatory materials.\\n│\\n├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\\n│   └── figures        <- Generated graphics and figures to be used in reporting\\n│\\n├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\\n│                         generated with `pip freeze > requirements.txt`\\n│\\n├── setup.cfg          <- Configuration file for flake8\\n│\\n└── {{ cookiecutter.module_name }}   <- Source code for use in this project.\\n    │\\n    ├── __init__.py             <- Makes {{ cookiecutter.module_name }} a Python module\\n    │\\n    ├── config.py               <- Store useful variables and configuration\\n    │\\n    ├── dataset.py              <- Scripts to download or generate data\\n    │\\n    ├── features.py             <- Code to create features for modeling\\n    │\\n    ├── modeling                \\n    │\\xa0\\xa0 ├── __init__.py \\n    │\\xa0\\xa0 ├── predict.py          <- Code to run model inference with trained models          \\n    │\\xa0\\xa0 └── train.py            <- Code to train models\\n    │\\n    └── plots.py                <- Code to create visualizations   \\nHome\\n GitHub\\nv2.0.0\\n7.8k\\n2.4k\\n\"}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/Cookiecutter Data Science.pdf', 'text': \"Cookiecutter Data Science\\nA logical, ﬂexible, and reasonably standardized project structure for doing and\\nsharing data science work.\\nCCDS\\nCCDS Project template\\nProject template\\nVersion 2 of Cookiecutter Data Science has launched recently. To learn more about what's different and\\nwhat's in progress, see the announcement blog post for more information.\\nQuickstart\\nCookiecutter Data Science v2 requires Python 3.8+. Since this is a cross-project\\nutility application, we recommend installing it with pipx. Installation command\\noptions:\\nCookiecutter Data Science v2 now requires installing the new cookiecutter-data-science  Python package,\\nwhich extends the functionality of the cookiecutter  templating utility. Use the provided ccds  command-\\nline program instead of cookiecutter .\\nStarting a new project\\nStarting a new project is as easy as running this command at the command line.\\nNo need to create a directory ﬁrst, the cookiecutter will do it for you.\\nThe ccds  commandline tool defaults to the Cookiecutter Data Science template,\\nbut you can pass your own template as the ﬁrst argument if you want.\\nExample\\nNow that you've got your project, you're ready to go! You should do the following:\\nCheck out the directory structure below so you know what's in the project and\\nhow to use it.\\nRead the opinions that are baked into the project so you understand best\\npractices and the philosophy behind the project structure.\\nRead the using the template guide to understand how to get started on a project\\nthat uses the template.\\nEnjoy!\\nDirectory structure\\nThe directory structure of your new project will look something like this (depending\\non the settings that you choose):\\nProject maintained by the friendly folks at DrivenData.\\nMade with Material for MkDocs\\nCookiecutter Data Science\\nis a DrivenData project.\\nHome\\nQuickstart\\nStarting a new project\\nExample\\nDirectory structure\\nWhy ccds?\\nOpinions\\nUsing the template\\nAll options\\nContributing\\nRelated projects\\nv1 Template\\nCookiecutter Data Science\\nCCDS V2 Announcement\\nWith pipx (recommended)\\nWith pip\\nWith conda (coming soon!)\\nUse the v1 template\\npipx install cookiecutter-data-science\\n# From the parent directory where you want your project\\nccds\\nUse the ccds command-line tool\\nccds\\nfast →\\n$  ccds https://github.com/drivendata/cookiecutter-data-science\\nproject_name (project_name): My Analysis\\nrepo_name (my_analysis): my_analysis\\nmodule_name (my_analysis):\\nauthor_name (Your name (or your organization/company/team)): Dat A. Scientist\\ndescription (A short description of the project.): This is my analysis of the data.\\npython_version_number (3.10): 3.12\\nSelect dataset_storage\\n1 - none\\n2 - azure\\n3 - s3\\n4 - gcs\\nChoose from [1/2/3/4] (1): 3\\nbucket (bucket-name): s3://my-aws-bucket\\naws_proﬁle (default):\\nSelect environment_manager\\n1 - virtualenv\\n2 - conda\\n3 - pipenv\\n4 - none\\nChoose from [1/2/3/4] (1): 2\\nSelect dependency_ﬁle\\n1 - requirements.txt\\n2 - environment.yml\\n3 - Pipﬁle\\nChoose from [1/2/3] (1): 1\\nSelect pydata_packages\\n1 - none\\n2 - basic\\nChoose from [1/2] (1): 2 ▋\\nbash\\n├── LICENSE            <- Open-source license if one is chosen\\n├── Makefile           <- Makefile with convenience commands like `make data` or `make train`\\n├── README.md          <- The top-level README for developers using this project.\\n├── data\\n│   ├── external       <- Data from third party sources.\\n│   ├── interim        <- Intermediate data that has been transformed.\\n│   ├── processed      <- The final, canonical data sets for modeling.\\n│   └── raw            <- The original, immutable data dump.\\n│\\n├── docs               <- A default mkdocs project; see www.mkdocs.org for details\\n│\\n├── models             <- Trained and serialized models, model predictions, or model summaries\\n│\\n├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\\n│                         the creator's initials, and a short `-` delimited description, e.g.\\n│                         `1.0-jqp-initial-data-exploration`.\\n│\\n├── pyproject.toml     <- Project configuration file with package metadata for \\n│                         {{ cookiecutter.module_name }} and configuration for tools like black\\n│\\n├── references         <- Data dictionaries, manuals, and all other explanatory materials.\\n│\\n├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\\n│   └── figures        <- Generated graphics and figures to be used in reporting\\n│\\n├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\\n│                         generated with `pip freeze > requirements.txt`\\n│\\n├── setup.cfg          <- Configuration file for flake8\\n│\\n└── {{ cookiecutter.module_name }}   <- Source code for use in this project.\\n    │\\n    ├── __init__.py             <- Makes {{ cookiecutter.module_name }} a Python module\\n    │\\n    ├── config.py               <- Store useful variables and configuration\\n    │\\n    ├── dataset.py              <- Scripts to download or generate data\\n    │\\n    ├── features.py             <- Code to create features for modeling\\n    │\\n    ├── modeling                \\n    │\\xa0\\xa0 ├── __init__.py \\n    │\\xa0\\xa0 ├── predict.py          <- Code to run model inference with trained models          \\n    │\\xa0\\xa0 └── train.py            <- Code to train models\\n    │\\n    └── plots.py                <- Code to create visualizations   \\nHome\\n GitHub\\nv2.0.0\\n7.8k\\n2.4k\\n\"}, {'source': '/Users/sir/Downloads/Data/PDF/Insights From the UNOS Database .pdf', 'text': 'Brief Report\\nPredictive Abilities of Machine Learning Techniques May Be\\nLimited by Dataset Characteristics: Insights From the UNOS\\nDatabase\\nP. ELLIOTT MILLER, MD,1 SUMEET PAWAR, MD,1 BENJAMIN VACCARO, MD,1 MEGAN MCCULLOUGH, MD,1\\nPOOJA RAO, MBBS, PhD,3 ROHIT GHOSH, MSc,3 PRASHANT WARIER, PhD,3 NIHAR R. DESAI, MD, MPH,1,2 AND\\nTARIQ AHMAD, MD, MPH1\\nNew Haven, Connecticut; and Mumbai, India\\nABSTRACT\\nBackground: Traditional statistical approaches to prediction of outcomes have drawbacks when applied to\\nlarge clinical databases. It is hypothesized that machine learning methodologies might overcome these lim-\\nitations by considering higher-dimensional and nonlinear relationships among patient variables.\\nMethods and Results: The Uniﬁed Network for Organ Sharing (UNOS) database was queried from 1987\\nto 2014 for adult patients undergoing cardiac transplantation. The dataset was divided into 3 time periods\\ncorresponding to major allocation adjustments and based on geographic regions. For our outcome of 1-year\\nsurvival, we used the standard statistical methods logistic regression, ridge regression, and regressions with\\nLASSO (least absolute shrinkage and selection operator) and compared them with the machine learning\\nmethodologies neural networks, na€ıve-Bayes, tree-augmented na€ıve-Bayes, support vector machines, ran-\\ndom forest, and stochastic gradient boosting. Receiver operating characteristic curves and C-statistics were\\ncalculated for each model. C-Statistics were used for comparison of discriminatory capacity across models\\nin the validation sample. After identifying 56,477 patients, the major univariate predictors of 1-year sur-\\nvival after heart transplantation were consistent with earlier reports and included age, renal function, body\\nmass index, liver function tests, and hemodynamics. Advanced analytic models demonstrated similarly\\nmodest discrimination capabilities compared with traditional models (C-statistic \\x010.66, all). The neural\\nnetwork model demonstrated the highest C-statistic (0.66) but this was only slightly superior to the simple\\nlogistic regression, ridge regression, and regression with LASSO models (C-statistic = 0.65, all). Discrimi-\\nnation did not vary signiﬁcantly across the 3 historically important time periods.\\nConclusions: The use of advanced analytic algorithms did not improve prediction of 1-year survival from\\nheart transplant compared with more traditional prediction models. The prognostic abilities of machine\\nlearning techniques may be limited by quality of the clinical dataset. (J Cardiac Fail 2019;25:479\\x01483)\\nKey Words: Advanced analytics, heart transplantation, prediction algorithms.\\nA key expectation of the “big data” revolution in medi-\\ncine is that advanced analytic methods will unearth nonlin-\\near\\nrelationships\\nand\\nhigher-dimensional\\nassociations\\nbetween clinical variables, leading to signiﬁcant improve-\\nments in the usefulness of information collected on\\npatients.1,2 The clinical potential of this approach has been\\naggressively promoted under the umbrella of personalized\\nor precision medicine.3 With widespread use of electronic\\nhealth records (EHRs), it is anticipated that advanced ana-\\nlytics will allow for clinician decision making based on dis-\\ntilled interpretations of large amounts of patient data.4 So\\nfar, however, there is no tangible evidence to suggest that\\nFrom the 1Section of Cardiovascular Medicine, Yale School of Medicine,\\nNew Haven, Connecticut; 2Center for Outcomes Research and Evaluation,\\nNew Haven, Connecticut and 3Qure.ai, Mumbai, India.\\nManuscript received July 31, 2018; revised manuscript received January\\n15, 2019; revised manuscript accepted January 23, 2019.\\nReprint requests: Tariq Ahmad MD, MPH, Section of Cardiovascular\\nMedicine, Center for Outcomes Research and Evaluation (CORE), Yale\\nUniversity School of Medicine, New Haven, CT 06520 Tel: 203-785-\\n7191; Fax: 203-785-2917. E-mail: tariq.ahmad@yale.edu\\nSee page 483 for disclosure information.\\n1071-9164/$ - see front matter\\n© 2019 Elsevier Inc. All rights reserved.\\nhttps://doi.org/10.1016/j.cardfail.2019.01.018\\n479\\nJournal of Cardiac Failure Vol. 25 No. 6 2019\\n\\nsimply applying more complex algorithms to patient data\\ncan remove its shortcomings and provide novel clinically\\nimportant information.5 Furthermore, it appears that suc-\\ncess in these efforts has largely involved their application\\nto imaging data and very highly curated databases.6 A rea-\\nson for this might be that even the most robust of statistical\\nmethodologies can not circumvent systemic issues with\\ndata granularity and quality.7,8 To explore this question, we\\nexamined whether a host of machine learning methodolo-\\ngies could outperform traditional statistical approaches for\\nprediction of 1-year mortality in a large national database\\nof patients undergoing cardiac transplantation.9\\nMethods\\nStudy Sample\\nThe United Network of Organ Sharing (UNOS) is a pri-\\nvate, nonproﬁt organization that manages the United States\\norgan transplant system under contract with the federal gov-\\nernment and maintains a robust clinical database of patients\\nwaitlisted and who underwent solid organ transplantation.10\\nWe included patients in the database since its inception in\\n1987 through 2014, limiting our analysis to adult patients\\n(\\x0318 years old) undergoing their ﬁrst single-organ heart\\ntransplantation. We excluded patients with >20% missing\\ndata, yielding 50,453/62,621 patients for our analyses. The\\nEthics Committee for Clinical Research at Yale University\\napproved the study protocol. The data were anonymized\\nand deidentiﬁed before analysis, and the Institutional\\nReview Board waived the need for written informed con-\\nsents from the participants; the data are publicly available\\non request, without patient or center identiﬁers.\\nOutcome and Variables\\nOur primary outcome of interest was 1-year survival. Con-\\nsistent with previous analyses of UNOS predictors, variables\\nincluded in our traditional methods included age (both donor\\nand recipient), creatinine, body mass index, liver function\\ntests, aspartate transaminase, and hemodynamics.\\nStatistical Analysis\\nWe applied several traditional and advanced machine-learn-\\ning methods to the data set for 2006\\x012014 as well as across\\nUNOS regions and 3 historically important times correspond-\\ning to major UNOS allocation adjustments, which included\\ninception to 1996 (n = 14,770), 1996 to 2006 (n = 18,587), and\\n2006 to 2014 (n = 17,096). These time periods were chosen to\\naccount for changes in national allocation algorithms, which\\nchanged the severity of illness of patients being transplanted.\\nBefore 1996, there were only 2 tiers (status 1 and status 2) of\\nmedical urgency. In 1996, the Organ Procurement and Trans-\\nplant Organization expanded to 3 tiers (status 1A, 1B, and 2).\\nIn 2006, changes were made to the heart allocation sequence\\nso that patients with a higher status could be offered an organ\\nin another zone before lower-status patients in the same zone\\nas the donor.11\\nThe traditional methodologies tested were: logistic\\nregression (LR), ridge regression, and regressions with\\nLASSO (least absolute shrinkage and selection operator).\\nThe advanced analytical methodologies tested were: neural\\nnetworks, na€ıve-Bayes, tree-augmented na€ıve-Bayes, sup-\\nport vector machines (SVMs), random forest, and stochastic\\ngradient\\nboosting\\n(see\\ndetailed\\nexplanations\\nbelow).\\nReceiver operating characteristic (ROC) curves for sensitiv-\\nity and speciﬁcity of each model were generated and a\\nC-statistic calculated by calculating the area under the ROC\\ncurve (AUC) to estimate ﬁt. To test C-statistic stability and\\nvalidity of the model, serial bootstrapping was applied\\n10 times. Models were trained on 80% of the data and then\\ntested on the remaining 20%. Calibration curves were gen-\\nerated for each model to measure the agreement between\\npredicted and observed risk. Patients were grouped into\\ndeciles according to the model’s predicted risk and plotted\\nagainst observed risk with the use of the Hosmer and Leme-\\nshow goodness of ﬁt test. Calibration was further conﬁrmed\\nby calculating Brier scores for each model, which encapsu-\\nlate the model’s uncertainty, resolution, and reliability into\\none value by measuring the average gap (mean squared dif-\\nference) or alignment between forecasted probabilities and\\nactual outcomes. Analyses were performed with the use of\\nR 3.2.2 (R Development Core Team, Vienna, Austria).\\nTwo\\x01sided P \\x01 .05 was considered to be statistically\\nsigniﬁcant for all analyses.\\nRationale for Choice of Various Statistical Methods\\nLogistic Regression\\nLR is a relatively simple method that aims to model the\\nsurvival probability with the multiple variables as linear a\\nfunction with learnable weights. Like other regression-\\nbased techniques, LR iteratively learns the weights to mini-\\nmize the cross-entropy loss for predictions. Because LR\\nassumes a linear model, this prevents the iterative learning\\nto get stuck at local minima. Because of its inherent\\nassumption of linear relationship between input variables\\nand output, LR fails to capture nonlinear relationships\\nbetween output and input variables without explicitly\\nincluding nonlinear or interaction terms.\\nSupport Vector Machines\\nSVMs are another set of machine learning algorithm that\\naim to ﬁnd the most separating hyperplane between the sur-\\nvival groups in the n-dimensional space of inputs. SVMs can\\nhave linear or nonlinear kernels that are used to measure dis-\\ntance across input data points. SVMs are convex and therefore\\nguarantee global minima assumptions of convex optimization.\\nA major drawback with SVMs is the manual kernel selection,\\nthereby choosing the apt nonlinearity to model the internal\\nrepresentations of data points and the choice of loss functions\\nthat can be used for SVM. Finally, SVMs, in their default\\nform, are not able to predict probabilistically, thus rendering\\nthemselves inapplicable for comparison metrics such as AUC.\\n480\\nJournal of Cardiac Failure Vol. 25 No. 6 June 2019\\n\\nDecision Trees\\nDecision trees aim to predict survival of patients by\\nmodeling outcome as a sequence of decisions based on\\ninput variables. Decision trees aim to segregate patients\\ninto distinct survival clusters conditioned on decision\\nsequence. Interpretability in the form of visualizing the\\ndecision sequence is the most important advantage in deci-\\nsion trees. By modeling classiﬁcations as decision steps,\\ndecision trees can capture nonlinear dependencies. How-\\never, in case of high-dimensional input, such as for survival\\nprediction, decision trees are extremely computationally\\nexpensive. Also, decision trees tend to be unstable, eg, par-\\ntially duplicating inputs can lead to completely different\\ndecision tree.\\nRandom Forest\\nRandom forest is an ensemble of multiple decision trees.\\nRandom trees in ensembles can rectify the instability of\\nresults because there are multiple trees contributing to the\\nresults, thus making the predictions robust to data. How-\\never, the other problems of decision trees persist with the\\nuse of random forest models.\\nNeural Networks\\nMulti-layer perceptron (MLP) is the most basic form of\\nneural networks. MLP can be viewed as a logistic regres-\\nsion classiﬁer where the input is ﬁrst transformed succes-\\nsively with the use of a sequence of learnt nonlinear\\ntransformations. This sequence of transformations projects\\nthe input data into a space where it becomes linearly separa-\\nble. This virtually makes MLP a universal approximator,\\nimplying that any function can ideally be approximated by\\nMLP if trained appropriately. The biggest downside of\\nMLP is that, in its default form, it does not lend itself to\\ninterpretability, eg, to draw on the relative importance of\\nfeatures and other attributes. MLPs are more prone to over-\\nﬁtting because the number of parameters tend to be lot\\nmore than in a simple logistic regression. Also, training of\\nsuch algorithms is generally computationally expensive and\\ntime consuming.\\nResults\\nAs shown in Fig. 1, advanced analytical models demon-\\nstrated similar discrimination compared with traditional\\nanalytical models. The neural network model demonstrated\\nthe highest C-statistic (0.66) but this was only slightly supe-\\nrior to the simple LR, ridge regression, and regressions with\\nLASSO models (C-statistic =0.65, all). All other advanced\\nmodels produced inferior C-statistics, ranging from SVMs\\n(0.52) to tree-augmented na€ıve-Bayes (0.62) and random\\nforest (0.63). The performance of these prediction algo-\\nrithms was similar to the most commonly cited risk score\\nfor mortality after heart transplantation—the IMPACT\\n(Index for Mortality Prediction After Cardiac Transplanta-\\ntion) score, which has a C-statistic of 0.65.9\\nWe applied the neural network models to each UNOS\\nregion and across the 3 time historically important periods\\n(Table 1). All regions across all 3 time periods demon-\\nstrated only modest discrimination. Cross-validation to\\nassess the stability of the models demonstrated the tightest\\ncorrelation with the neural network model (variation 0.004,\\nC-statistic range 0.66\\x010.67). All other models demon-\\nstrated an acceptable level of correlation (all <0.02 varia-\\ntion across 10 validation tests) except for the SVMs model\\n(variation 0.12, C-statistic range 0.47\\x010.59). Hosmer-\\nLemeshow testing depicted good calibration with the 3 tra-\\nditional models as well as random forest and stochastic\\ngradient boosting models. Inferior calibration was observed\\nwith the remaining advanced models. Quantitative assess-\\nment of calibration with the use of Brier scores conﬁrmed\\nthese patterns (data not presented).\\nThe major predictors of 1-year survival after heart trans-\\nplantation are shown in Fig. 2, which also depicts the\\nchange in AUC as the number of variables in the model\\nincreases. Key predictors included donor and recipient age,\\nbilirubin, creatinine clearance, hemodynamics, donor blood\\npH, and candidate diagnosis, closely reﬂecting previously\\npublished reports.\\nDiscussion\\nWe found that the prediction of 1-year outcomes after\\ncardiac transplantation was similar between machine learn-\\ning and traditional statistical methods in the central reposi-\\ntory of patients undergoing heart transplantation in the\\nUnited States. All of the models developed in this study\\nshowed similar and very modest discrimination, with C-\\nstatistics consistently »0.65 regardless of their complexity.\\nA traditional statistical approach consisting of multivariate\\nlogistic regression has been previously used on the UNOS\\nFig. 1. Model discrimination according to traditional and novel\\nanalytical models. LASSO, least absolute shrinkage and selection\\noperator; SVM, support vector machines; TAN, tree-augmented\\nna€ıve.\\nApplication of Advanced Analytics to the UNOS Database\\n\\x03\\nMiller et al\\n481\\n\\ndatabase for predicting 1-year mortality with a C-statistic of\\n0.65, consistent with our results. Speciﬁcally, our ﬁndings\\nreplicate the predictive capabilities of the IMPACT score.9\\nAlthough machine learning has been touted as a path to\\nunearthing nonlinear relationships and higher-dimensional\\nassociations between variables in medicine, it remains to\\nlive up to its expectations.12 Our results raise the notion\\nthat large clinical datasets might lack the accuracy and\\ngranularity needed for machine learning methodologies to\\nuncover unique associations.\\nSimilar limitations in the application of machine learning\\nmethods to large datasets of patient information have been\\ndemonstrated for prediction of heart failure readmis-\\nsions.2,13\\nHowever,\\nthese\\nmethods\\nhave\\nperformed\\nextremely well when applied to imaging information, as\\nnoted in recent reports involving head computerized tomog-\\nraphy and echocardiography, with C-statistics >0.90.8,14\\nWe think that our results, when considered in conjunction\\nwith these previous ﬁndings, provide an insight into the\\ninability of machine learning methods to overcome key sys-\\ntemic limitations of clinical datasets. In this case, whereas\\nUNOS is a robust clinical registry, it is based on administra-\\ntive data, which can blunt phenotyping of complex patients\\nand attenuate the predictive ability of both traditional meth-\\nods as well as machine learning techniques; indeed, this has\\nbeen noted several times in efforts aimed at extracting\\nmeaningful information from the EHR.15 Our analysis illus-\\ntrates this limitation as we see the AUC for neural networks\\nplateau after inclusion of »25 variables in the model.\\nSeveral limitations of this conclusion must be considered.\\nThe patient journey after heart transplantation is very com-\\nplex, and 1-year outcomes are likely to be determined by\\nevents after transplantation. It is very likely that detailed\\npatient information after the surgery would signiﬁcantly\\nimprove our ability to predict outcomes. However, pre-\\ntransplantation prognostication is given an inordinate\\namount of attention during decision making for listing and\\ntransplanting patients.16\\nTo our knowledge, the present study is the ﬁrst to com-\\npare different predictive models in patients undergoing car-\\ndiac transplantation. We demonstrated that the neural\\nnetworks model demonstrated the highest discrimination\\nand the most reliable C-statistic when validated. However,\\nits calibration was inferior to all of the traditional models.\\nThis is a common problem with advanced methodologies,\\nwherein algorithms can become unstable due to multicollin-\\near predictors or overﬁtting due to random correlations.\\nFuture work is required to understand the best use of spe-\\nciﬁc statistical methodologies to apply to clinical datasets\\nof different characteristics, and consensus is needed on how\\nto validate the resultant ﬁndings.17\\nConclusion\\nWe found that machine learning performed similarly to tra-\\nditional statistic methods for prediction of 1-year survival after\\ncardiac transplantation. All statistical methodologies tested\\nTable 1. C-Statistics for Neural Networks Across United Network of Organ Sharing Regions and Time Periods*\\nRegion 1\\nRegion 2\\nRegion 3\\nRegion 4\\nRegion 5\\nRegion 6\\nRegion 7\\nRegion 8\\nRegion 9\\nRegion 10\\nRegion 11\\nAverage or Total\\nAUC\\nPeriod 1\\n0.548\\n0.589\\n0.575\\n0.604\\n0.582\\n0.71\\n0.589\\n0.628\\n0.619\\n0.607\\n0.66\\n0.613\\nPeriod 2\\n0.702\\n0.671\\n0.639\\n0.68\\n0.658\\n0.698\\n0.647\\n0.512\\n0.715\\n0.62\\n0.663\\n0.659\\nPeriod 3\\n0.605\\n0.692\\n0.671\\n0.645\\n0.673\\n0.658\\n0.678\\n0.62\\n0.695\\n0.626\\n0.676\\n0.663\\nPatient numbers\\nPeriod 1\\n611\\n1656\\n1726\\n1649\\n2398\\n462\\n1391\\n1085\\n723\\n1435\\n1634\\n14,770\\nPeriod 2\\n686\\n2392\\n2383\\n1897\\n2700\\n583\\n1779\\n933\\n1179\\n1893\\n2182\\n18,587\\nPeriod 3\\n704\\n2398\\n1999\\n1845\\n2626\\n474\\n1626\\n1045\\n1091\\n1402\\n1886\\n17,096\\nAUC, area under the receiver operating characteristic curve.\\n*Period 1: inception to 1996; period 2: 1996 to 2006; period 3: 2006 to 2014.\\n482\\nJournal of Cardiac Failure Vol. 25 No. 6 June 2019\\n\\noffered only modest predictive power, with C-statistics consis-\\ntently \\x010.66. Before widespread application of machine learn-\\ning methodologies to clinical datasets, a focus on improving\\nthe quality of available data might be required.\\nDisclosures\\nDr Desai reports being a recipient of a research agreement\\nfrom Johnson & Johnson, through Yale University, to develop\\nmethods of clinical trial data sharing. Dr Ahmad is supported\\nby grant K12 HS023000-04 from the Agency for Healthcare\\nResearch and Quality. All of the other authors report no poten-\\ntial conﬂicts of interest or ﬁnancial relationships.\\nReferences\\n1. Krumholz HM. Big data and new knowledge in medicine: the\\nthinking, training, and tools needed for a learning health sys-\\ntem. Health Aff (Millwood) 2014;33:1163–70.\\n2. Mortazavi BJ, Downing NS, Bucholz EM, Dharmarajan K,\\nManhapra A, Li SX, Negahban SN, Krumholz HM. Analysis\\nof machine learning techniques for heart failure readmissions.\\nCirc Cardiovasc Qual Outcomes 2016;9:629–40.\\n3. Parikh RB, Schwartz JS, Navathe AS. Beyond genes and\\nmolecules—a precision delivery initiative for precision\\nmedicine. N Engl J Med 2017;376:1609–12.\\n4. Srinivas TR, Taber DJ, Su Z, Zhang J, Mour G, Northrup D,\\net al. Big data, predictive analytics, and quality improvement\\nin kidney transplantation: a proof of concept. Am J Transplant\\n2017;17:671–81.\\n5. Joyner MJ, Paneth N, Ioannidis JP. What happens when\\nunderperforming big ideas in research become entrenched?\\nJAMA 2016;316:1355–6.\\n6. Ahmad T, Lund LH, Rao P, Ghosh R, Warier P, Vaccaro B,\\net al. Machine learning methods improve prognostication,\\nidentify clinically distinct phenotypes, and detect heterogene-\\nity in response to therapy in a large cohort of heart failure\\npatients. J Am Heart Assoc 2018 Apr 12;7(8). pii: e008081.\\n7. Ahmad T, Testani JM, Desai NR. Can big data simplify the\\ncomplexity of modern medicine?: prediction of right ventricu-\\nlar failure after left ventricular assist device support as a test\\ncase. JACC Heart Fail 2016;4:722–5.\\n8. Chilamkurthy S, Ghosh R, Tanamala S, Biviji M, Campeau\\nNG, Venugopal VK, et al. Deep learning algorithms for detec-\\ntion of critical ﬁndings in head CT scans: a retrospective\\nstudy. Lancet 2018.\\n9. Weiss ES, Allen JG, Arnaoutakis GJ, George TJ, Russell SD,\\nShah AS, Conte JV. Creation of a quantitative recipient risk\\nindex for mortality prediction after cardiac transplantation\\n(IMPACT). Ann Thorac Surg 2011;92:914–21. discussion\\n921\\x012.\\n10. Ravi Y, Lella SK, Copeland LA, Zolfaghari K, Grady K,\\nEmani S, Sai-Sudhakar CB. Does recipient work status pre-\\ntransplant affect post-heart transplant survival? A United Net-\\nwork for Organ Sharing database review. J Heart Lung Trans-\\nplant 2018;37:604–10.\\n11. Colvin-Adams M, Valapour M, Hertz M, Heubner B, Paulson\\nK, Dhungel V, et al. Lung and heart allocation in the United\\nStates. Am J Transplant 2012;12:3213–34.\\n12. Shortliffe EH, Sepulveda MJ. Clinical decision support in the era\\nof artiﬁcial intelligence. JAMA 2018 Dec 4;320(21):2199–200.\\n13. Frizzell JD, Liang L, Schulte PJ, Yancy CW, Heidenreich PA,\\nHernandez AF, et al. Prediction of 30-day all-cause readmis-\\nsions in patients hospitalized for heart failure: comparison of\\nmachine learning and other statistical approaches. JAMA Car-\\ndiol 2017;2:204–9.\\n14. Zhang J, Gajjala S, Agrawal P, Tison GH, Hallock LA,\\nBeussink-Nelson L, et al. Fully Automated Echocardio-\\ngram Interpretation in Clinical Practice. Circulation 2018;\\n138:1623–35.\\n15. Taggart J, Liaw ST, Yu H. Structured data quality reports to\\nimprove EHR data quality. Int J Med Inform 2015;84:1094–8.\\n16. Colvin-Adams M, Smith JM, Heubner BM, Skeans MA,\\nEdwards LB, Waller CD, et al. OPTN/SRTR 2013 annual\\ndata report: heart. Am J Transplant 2015;15(Suppl 2):\\n1–28.\\n17. Medved D, Ohlsson M, Hoglund P, Andersson B, Nugues P,\\nNilsson J. Improving prediction of heart transplantation outcome\\nusing deep learning techniques. Sci Rep 2018 Feb 26;8(1):3613.\\nFig. 2. Predictors of 1-year mortality after cardiac transplantation with the use of logistic regression.\\nApplication of Advanced Analytics to the UNOS Database\\n\\x03\\nMiller et al\\n483\\n'}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/Insights From the UNOS Database .pdf', 'text': 'Brief Report\\nPredictive Abilities of Machine Learning Techniques May Be\\nLimited by Dataset Characteristics: Insights From the UNOS\\nDatabase\\nP. ELLIOTT MILLER, MD,1 SUMEET PAWAR, MD,1 BENJAMIN VACCARO, MD,1 MEGAN MCCULLOUGH, MD,1\\nPOOJA RAO, MBBS, PhD,3 ROHIT GHOSH, MSc,3 PRASHANT WARIER, PhD,3 NIHAR R. DESAI, MD, MPH,1,2 AND\\nTARIQ AHMAD, MD, MPH1\\nNew Haven, Connecticut; and Mumbai, India\\nABSTRACT\\nBackground: Traditional statistical approaches to prediction of outcomes have drawbacks when applied to\\nlarge clinical databases. It is hypothesized that machine learning methodologies might overcome these lim-\\nitations by considering higher-dimensional and nonlinear relationships among patient variables.\\nMethods and Results: The Uniﬁed Network for Organ Sharing (UNOS) database was queried from 1987\\nto 2014 for adult patients undergoing cardiac transplantation. The dataset was divided into 3 time periods\\ncorresponding to major allocation adjustments and based on geographic regions. For our outcome of 1-year\\nsurvival, we used the standard statistical methods logistic regression, ridge regression, and regressions with\\nLASSO (least absolute shrinkage and selection operator) and compared them with the machine learning\\nmethodologies neural networks, na€ıve-Bayes, tree-augmented na€ıve-Bayes, support vector machines, ran-\\ndom forest, and stochastic gradient boosting. Receiver operating characteristic curves and C-statistics were\\ncalculated for each model. C-Statistics were used for comparison of discriminatory capacity across models\\nin the validation sample. After identifying 56,477 patients, the major univariate predictors of 1-year sur-\\nvival after heart transplantation were consistent with earlier reports and included age, renal function, body\\nmass index, liver function tests, and hemodynamics. Advanced analytic models demonstrated similarly\\nmodest discrimination capabilities compared with traditional models (C-statistic \\x010.66, all). The neural\\nnetwork model demonstrated the highest C-statistic (0.66) but this was only slightly superior to the simple\\nlogistic regression, ridge regression, and regression with LASSO models (C-statistic = 0.65, all). Discrimi-\\nnation did not vary signiﬁcantly across the 3 historically important time periods.\\nConclusions: The use of advanced analytic algorithms did not improve prediction of 1-year survival from\\nheart transplant compared with more traditional prediction models. The prognostic abilities of machine\\nlearning techniques may be limited by quality of the clinical dataset. (J Cardiac Fail 2019;25:479\\x01483)\\nKey Words: Advanced analytics, heart transplantation, prediction algorithms.\\nA key expectation of the “big data” revolution in medi-\\ncine is that advanced analytic methods will unearth nonlin-\\near\\nrelationships\\nand\\nhigher-dimensional\\nassociations\\nbetween clinical variables, leading to signiﬁcant improve-\\nments in the usefulness of information collected on\\npatients.1,2 The clinical potential of this approach has been\\naggressively promoted under the umbrella of personalized\\nor precision medicine.3 With widespread use of electronic\\nhealth records (EHRs), it is anticipated that advanced ana-\\nlytics will allow for clinician decision making based on dis-\\ntilled interpretations of large amounts of patient data.4 So\\nfar, however, there is no tangible evidence to suggest that\\nFrom the 1Section of Cardiovascular Medicine, Yale School of Medicine,\\nNew Haven, Connecticut; 2Center for Outcomes Research and Evaluation,\\nNew Haven, Connecticut and 3Qure.ai, Mumbai, India.\\nManuscript received July 31, 2018; revised manuscript received January\\n15, 2019; revised manuscript accepted January 23, 2019.\\nReprint requests: Tariq Ahmad MD, MPH, Section of Cardiovascular\\nMedicine, Center for Outcomes Research and Evaluation (CORE), Yale\\nUniversity School of Medicine, New Haven, CT 06520 Tel: 203-785-\\n7191; Fax: 203-785-2917. E-mail: tariq.ahmad@yale.edu\\nSee page 483 for disclosure information.\\n1071-9164/$ - see front matter\\n© 2019 Elsevier Inc. All rights reserved.\\nhttps://doi.org/10.1016/j.cardfail.2019.01.018\\n479\\nJournal of Cardiac Failure Vol. 25 No. 6 2019\\n\\nsimply applying more complex algorithms to patient data\\ncan remove its shortcomings and provide novel clinically\\nimportant information.5 Furthermore, it appears that suc-\\ncess in these efforts has largely involved their application\\nto imaging data and very highly curated databases.6 A rea-\\nson for this might be that even the most robust of statistical\\nmethodologies can not circumvent systemic issues with\\ndata granularity and quality.7,8 To explore this question, we\\nexamined whether a host of machine learning methodolo-\\ngies could outperform traditional statistical approaches for\\nprediction of 1-year mortality in a large national database\\nof patients undergoing cardiac transplantation.9\\nMethods\\nStudy Sample\\nThe United Network of Organ Sharing (UNOS) is a pri-\\nvate, nonproﬁt organization that manages the United States\\norgan transplant system under contract with the federal gov-\\nernment and maintains a robust clinical database of patients\\nwaitlisted and who underwent solid organ transplantation.10\\nWe included patients in the database since its inception in\\n1987 through 2014, limiting our analysis to adult patients\\n(\\x0318 years old) undergoing their ﬁrst single-organ heart\\ntransplantation. We excluded patients with >20% missing\\ndata, yielding 50,453/62,621 patients for our analyses. The\\nEthics Committee for Clinical Research at Yale University\\napproved the study protocol. The data were anonymized\\nand deidentiﬁed before analysis, and the Institutional\\nReview Board waived the need for written informed con-\\nsents from the participants; the data are publicly available\\non request, without patient or center identiﬁers.\\nOutcome and Variables\\nOur primary outcome of interest was 1-year survival. Con-\\nsistent with previous analyses of UNOS predictors, variables\\nincluded in our traditional methods included age (both donor\\nand recipient), creatinine, body mass index, liver function\\ntests, aspartate transaminase, and hemodynamics.\\nStatistical Analysis\\nWe applied several traditional and advanced machine-learn-\\ning methods to the data set for 2006\\x012014 as well as across\\nUNOS regions and 3 historically important times correspond-\\ning to major UNOS allocation adjustments, which included\\ninception to 1996 (n = 14,770), 1996 to 2006 (n = 18,587), and\\n2006 to 2014 (n = 17,096). These time periods were chosen to\\naccount for changes in national allocation algorithms, which\\nchanged the severity of illness of patients being transplanted.\\nBefore 1996, there were only 2 tiers (status 1 and status 2) of\\nmedical urgency. In 1996, the Organ Procurement and Trans-\\nplant Organization expanded to 3 tiers (status 1A, 1B, and 2).\\nIn 2006, changes were made to the heart allocation sequence\\nso that patients with a higher status could be offered an organ\\nin another zone before lower-status patients in the same zone\\nas the donor.11\\nThe traditional methodologies tested were: logistic\\nregression (LR), ridge regression, and regressions with\\nLASSO (least absolute shrinkage and selection operator).\\nThe advanced analytical methodologies tested were: neural\\nnetworks, na€ıve-Bayes, tree-augmented na€ıve-Bayes, sup-\\nport vector machines (SVMs), random forest, and stochastic\\ngradient\\nboosting\\n(see\\ndetailed\\nexplanations\\nbelow).\\nReceiver operating characteristic (ROC) curves for sensitiv-\\nity and speciﬁcity of each model were generated and a\\nC-statistic calculated by calculating the area under the ROC\\ncurve (AUC) to estimate ﬁt. To test C-statistic stability and\\nvalidity of the model, serial bootstrapping was applied\\n10 times. Models were trained on 80% of the data and then\\ntested on the remaining 20%. Calibration curves were gen-\\nerated for each model to measure the agreement between\\npredicted and observed risk. Patients were grouped into\\ndeciles according to the model’s predicted risk and plotted\\nagainst observed risk with the use of the Hosmer and Leme-\\nshow goodness of ﬁt test. Calibration was further conﬁrmed\\nby calculating Brier scores for each model, which encapsu-\\nlate the model’s uncertainty, resolution, and reliability into\\none value by measuring the average gap (mean squared dif-\\nference) or alignment between forecasted probabilities and\\nactual outcomes. Analyses were performed with the use of\\nR 3.2.2 (R Development Core Team, Vienna, Austria).\\nTwo\\x01sided P \\x01 .05 was considered to be statistically\\nsigniﬁcant for all analyses.\\nRationale for Choice of Various Statistical Methods\\nLogistic Regression\\nLR is a relatively simple method that aims to model the\\nsurvival probability with the multiple variables as linear a\\nfunction with learnable weights. Like other regression-\\nbased techniques, LR iteratively learns the weights to mini-\\nmize the cross-entropy loss for predictions. Because LR\\nassumes a linear model, this prevents the iterative learning\\nto get stuck at local minima. Because of its inherent\\nassumption of linear relationship between input variables\\nand output, LR fails to capture nonlinear relationships\\nbetween output and input variables without explicitly\\nincluding nonlinear or interaction terms.\\nSupport Vector Machines\\nSVMs are another set of machine learning algorithm that\\naim to ﬁnd the most separating hyperplane between the sur-\\nvival groups in the n-dimensional space of inputs. SVMs can\\nhave linear or nonlinear kernels that are used to measure dis-\\ntance across input data points. SVMs are convex and therefore\\nguarantee global minima assumptions of convex optimization.\\nA major drawback with SVMs is the manual kernel selection,\\nthereby choosing the apt nonlinearity to model the internal\\nrepresentations of data points and the choice of loss functions\\nthat can be used for SVM. Finally, SVMs, in their default\\nform, are not able to predict probabilistically, thus rendering\\nthemselves inapplicable for comparison metrics such as AUC.\\n480\\nJournal of Cardiac Failure Vol. 25 No. 6 June 2019\\n\\nDecision Trees\\nDecision trees aim to predict survival of patients by\\nmodeling outcome as a sequence of decisions based on\\ninput variables. Decision trees aim to segregate patients\\ninto distinct survival clusters conditioned on decision\\nsequence. Interpretability in the form of visualizing the\\ndecision sequence is the most important advantage in deci-\\nsion trees. By modeling classiﬁcations as decision steps,\\ndecision trees can capture nonlinear dependencies. How-\\never, in case of high-dimensional input, such as for survival\\nprediction, decision trees are extremely computationally\\nexpensive. Also, decision trees tend to be unstable, eg, par-\\ntially duplicating inputs can lead to completely different\\ndecision tree.\\nRandom Forest\\nRandom forest is an ensemble of multiple decision trees.\\nRandom trees in ensembles can rectify the instability of\\nresults because there are multiple trees contributing to the\\nresults, thus making the predictions robust to data. How-\\never, the other problems of decision trees persist with the\\nuse of random forest models.\\nNeural Networks\\nMulti-layer perceptron (MLP) is the most basic form of\\nneural networks. MLP can be viewed as a logistic regres-\\nsion classiﬁer where the input is ﬁrst transformed succes-\\nsively with the use of a sequence of learnt nonlinear\\ntransformations. This sequence of transformations projects\\nthe input data into a space where it becomes linearly separa-\\nble. This virtually makes MLP a universal approximator,\\nimplying that any function can ideally be approximated by\\nMLP if trained appropriately. The biggest downside of\\nMLP is that, in its default form, it does not lend itself to\\ninterpretability, eg, to draw on the relative importance of\\nfeatures and other attributes. MLPs are more prone to over-\\nﬁtting because the number of parameters tend to be lot\\nmore than in a simple logistic regression. Also, training of\\nsuch algorithms is generally computationally expensive and\\ntime consuming.\\nResults\\nAs shown in Fig. 1, advanced analytical models demon-\\nstrated similar discrimination compared with traditional\\nanalytical models. The neural network model demonstrated\\nthe highest C-statistic (0.66) but this was only slightly supe-\\nrior to the simple LR, ridge regression, and regressions with\\nLASSO models (C-statistic =0.65, all). All other advanced\\nmodels produced inferior C-statistics, ranging from SVMs\\n(0.52) to tree-augmented na€ıve-Bayes (0.62) and random\\nforest (0.63). The performance of these prediction algo-\\nrithms was similar to the most commonly cited risk score\\nfor mortality after heart transplantation—the IMPACT\\n(Index for Mortality Prediction After Cardiac Transplanta-\\ntion) score, which has a C-statistic of 0.65.9\\nWe applied the neural network models to each UNOS\\nregion and across the 3 time historically important periods\\n(Table 1). All regions across all 3 time periods demon-\\nstrated only modest discrimination. Cross-validation to\\nassess the stability of the models demonstrated the tightest\\ncorrelation with the neural network model (variation 0.004,\\nC-statistic range 0.66\\x010.67). All other models demon-\\nstrated an acceptable level of correlation (all <0.02 varia-\\ntion across 10 validation tests) except for the SVMs model\\n(variation 0.12, C-statistic range 0.47\\x010.59). Hosmer-\\nLemeshow testing depicted good calibration with the 3 tra-\\nditional models as well as random forest and stochastic\\ngradient boosting models. Inferior calibration was observed\\nwith the remaining advanced models. Quantitative assess-\\nment of calibration with the use of Brier scores conﬁrmed\\nthese patterns (data not presented).\\nThe major predictors of 1-year survival after heart trans-\\nplantation are shown in Fig. 2, which also depicts the\\nchange in AUC as the number of variables in the model\\nincreases. Key predictors included donor and recipient age,\\nbilirubin, creatinine clearance, hemodynamics, donor blood\\npH, and candidate diagnosis, closely reﬂecting previously\\npublished reports.\\nDiscussion\\nWe found that the prediction of 1-year outcomes after\\ncardiac transplantation was similar between machine learn-\\ning and traditional statistical methods in the central reposi-\\ntory of patients undergoing heart transplantation in the\\nUnited States. All of the models developed in this study\\nshowed similar and very modest discrimination, with C-\\nstatistics consistently »0.65 regardless of their complexity.\\nA traditional statistical approach consisting of multivariate\\nlogistic regression has been previously used on the UNOS\\nFig. 1. Model discrimination according to traditional and novel\\nanalytical models. LASSO, least absolute shrinkage and selection\\noperator; SVM, support vector machines; TAN, tree-augmented\\nna€ıve.\\nApplication of Advanced Analytics to the UNOS Database\\n\\x03\\nMiller et al\\n481\\n\\ndatabase for predicting 1-year mortality with a C-statistic of\\n0.65, consistent with our results. Speciﬁcally, our ﬁndings\\nreplicate the predictive capabilities of the IMPACT score.9\\nAlthough machine learning has been touted as a path to\\nunearthing nonlinear relationships and higher-dimensional\\nassociations between variables in medicine, it remains to\\nlive up to its expectations.12 Our results raise the notion\\nthat large clinical datasets might lack the accuracy and\\ngranularity needed for machine learning methodologies to\\nuncover unique associations.\\nSimilar limitations in the application of machine learning\\nmethods to large datasets of patient information have been\\ndemonstrated for prediction of heart failure readmis-\\nsions.2,13\\nHowever,\\nthese\\nmethods\\nhave\\nperformed\\nextremely well when applied to imaging information, as\\nnoted in recent reports involving head computerized tomog-\\nraphy and echocardiography, with C-statistics >0.90.8,14\\nWe think that our results, when considered in conjunction\\nwith these previous ﬁndings, provide an insight into the\\ninability of machine learning methods to overcome key sys-\\ntemic limitations of clinical datasets. In this case, whereas\\nUNOS is a robust clinical registry, it is based on administra-\\ntive data, which can blunt phenotyping of complex patients\\nand attenuate the predictive ability of both traditional meth-\\nods as well as machine learning techniques; indeed, this has\\nbeen noted several times in efforts aimed at extracting\\nmeaningful information from the EHR.15 Our analysis illus-\\ntrates this limitation as we see the AUC for neural networks\\nplateau after inclusion of »25 variables in the model.\\nSeveral limitations of this conclusion must be considered.\\nThe patient journey after heart transplantation is very com-\\nplex, and 1-year outcomes are likely to be determined by\\nevents after transplantation. It is very likely that detailed\\npatient information after the surgery would signiﬁcantly\\nimprove our ability to predict outcomes. However, pre-\\ntransplantation prognostication is given an inordinate\\namount of attention during decision making for listing and\\ntransplanting patients.16\\nTo our knowledge, the present study is the ﬁrst to com-\\npare different predictive models in patients undergoing car-\\ndiac transplantation. We demonstrated that the neural\\nnetworks model demonstrated the highest discrimination\\nand the most reliable C-statistic when validated. However,\\nits calibration was inferior to all of the traditional models.\\nThis is a common problem with advanced methodologies,\\nwherein algorithms can become unstable due to multicollin-\\near predictors or overﬁtting due to random correlations.\\nFuture work is required to understand the best use of spe-\\nciﬁc statistical methodologies to apply to clinical datasets\\nof different characteristics, and consensus is needed on how\\nto validate the resultant ﬁndings.17\\nConclusion\\nWe found that machine learning performed similarly to tra-\\nditional statistic methods for prediction of 1-year survival after\\ncardiac transplantation. All statistical methodologies tested\\nTable 1. C-Statistics for Neural Networks Across United Network of Organ Sharing Regions and Time Periods*\\nRegion 1\\nRegion 2\\nRegion 3\\nRegion 4\\nRegion 5\\nRegion 6\\nRegion 7\\nRegion 8\\nRegion 9\\nRegion 10\\nRegion 11\\nAverage or Total\\nAUC\\nPeriod 1\\n0.548\\n0.589\\n0.575\\n0.604\\n0.582\\n0.71\\n0.589\\n0.628\\n0.619\\n0.607\\n0.66\\n0.613\\nPeriod 2\\n0.702\\n0.671\\n0.639\\n0.68\\n0.658\\n0.698\\n0.647\\n0.512\\n0.715\\n0.62\\n0.663\\n0.659\\nPeriod 3\\n0.605\\n0.692\\n0.671\\n0.645\\n0.673\\n0.658\\n0.678\\n0.62\\n0.695\\n0.626\\n0.676\\n0.663\\nPatient numbers\\nPeriod 1\\n611\\n1656\\n1726\\n1649\\n2398\\n462\\n1391\\n1085\\n723\\n1435\\n1634\\n14,770\\nPeriod 2\\n686\\n2392\\n2383\\n1897\\n2700\\n583\\n1779\\n933\\n1179\\n1893\\n2182\\n18,587\\nPeriod 3\\n704\\n2398\\n1999\\n1845\\n2626\\n474\\n1626\\n1045\\n1091\\n1402\\n1886\\n17,096\\nAUC, area under the receiver operating characteristic curve.\\n*Period 1: inception to 1996; period 2: 1996 to 2006; period 3: 2006 to 2014.\\n482\\nJournal of Cardiac Failure Vol. 25 No. 6 June 2019\\n\\noffered only modest predictive power, with C-statistics consis-\\ntently \\x010.66. Before widespread application of machine learn-\\ning methodologies to clinical datasets, a focus on improving\\nthe quality of available data might be required.\\nDisclosures\\nDr Desai reports being a recipient of a research agreement\\nfrom Johnson & Johnson, through Yale University, to develop\\nmethods of clinical trial data sharing. Dr Ahmad is supported\\nby grant K12 HS023000-04 from the Agency for Healthcare\\nResearch and Quality. All of the other authors report no poten-\\ntial conﬂicts of interest or ﬁnancial relationships.\\nReferences\\n1. Krumholz HM. Big data and new knowledge in medicine: the\\nthinking, training, and tools needed for a learning health sys-\\ntem. Health Aff (Millwood) 2014;33:1163–70.\\n2. Mortazavi BJ, Downing NS, Bucholz EM, Dharmarajan K,\\nManhapra A, Li SX, Negahban SN, Krumholz HM. Analysis\\nof machine learning techniques for heart failure readmissions.\\nCirc Cardiovasc Qual Outcomes 2016;9:629–40.\\n3. Parikh RB, Schwartz JS, Navathe AS. Beyond genes and\\nmolecules—a precision delivery initiative for precision\\nmedicine. N Engl J Med 2017;376:1609–12.\\n4. Srinivas TR, Taber DJ, Su Z, Zhang J, Mour G, Northrup D,\\net al. Big data, predictive analytics, and quality improvement\\nin kidney transplantation: a proof of concept. Am J Transplant\\n2017;17:671–81.\\n5. Joyner MJ, Paneth N, Ioannidis JP. What happens when\\nunderperforming big ideas in research become entrenched?\\nJAMA 2016;316:1355–6.\\n6. Ahmad T, Lund LH, Rao P, Ghosh R, Warier P, Vaccaro B,\\net al. Machine learning methods improve prognostication,\\nidentify clinically distinct phenotypes, and detect heterogene-\\nity in response to therapy in a large cohort of heart failure\\npatients. J Am Heart Assoc 2018 Apr 12;7(8). pii: e008081.\\n7. Ahmad T, Testani JM, Desai NR. Can big data simplify the\\ncomplexity of modern medicine?: prediction of right ventricu-\\nlar failure after left ventricular assist device support as a test\\ncase. JACC Heart Fail 2016;4:722–5.\\n8. Chilamkurthy S, Ghosh R, Tanamala S, Biviji M, Campeau\\nNG, Venugopal VK, et al. Deep learning algorithms for detec-\\ntion of critical ﬁndings in head CT scans: a retrospective\\nstudy. Lancet 2018.\\n9. Weiss ES, Allen JG, Arnaoutakis GJ, George TJ, Russell SD,\\nShah AS, Conte JV. Creation of a quantitative recipient risk\\nindex for mortality prediction after cardiac transplantation\\n(IMPACT). Ann Thorac Surg 2011;92:914–21. discussion\\n921\\x012.\\n10. Ravi Y, Lella SK, Copeland LA, Zolfaghari K, Grady K,\\nEmani S, Sai-Sudhakar CB. Does recipient work status pre-\\ntransplant affect post-heart transplant survival? A United Net-\\nwork for Organ Sharing database review. J Heart Lung Trans-\\nplant 2018;37:604–10.\\n11. Colvin-Adams M, Valapour M, Hertz M, Heubner B, Paulson\\nK, Dhungel V, et al. Lung and heart allocation in the United\\nStates. Am J Transplant 2012;12:3213–34.\\n12. Shortliffe EH, Sepulveda MJ. Clinical decision support in the era\\nof artiﬁcial intelligence. JAMA 2018 Dec 4;320(21):2199–200.\\n13. Frizzell JD, Liang L, Schulte PJ, Yancy CW, Heidenreich PA,\\nHernandez AF, et al. Prediction of 30-day all-cause readmis-\\nsions in patients hospitalized for heart failure: comparison of\\nmachine learning and other statistical approaches. JAMA Car-\\ndiol 2017;2:204–9.\\n14. Zhang J, Gajjala S, Agrawal P, Tison GH, Hallock LA,\\nBeussink-Nelson L, et al. Fully Automated Echocardio-\\ngram Interpretation in Clinical Practice. Circulation 2018;\\n138:1623–35.\\n15. Taggart J, Liaw ST, Yu H. Structured data quality reports to\\nimprove EHR data quality. Int J Med Inform 2015;84:1094–8.\\n16. Colvin-Adams M, Smith JM, Heubner BM, Skeans MA,\\nEdwards LB, Waller CD, et al. OPTN/SRTR 2013 annual\\ndata report: heart. Am J Transplant 2015;15(Suppl 2):\\n1–28.\\n17. Medved D, Ohlsson M, Hoglund P, Andersson B, Nugues P,\\nNilsson J. Improving prediction of heart transplantation outcome\\nusing deep learning techniques. Sci Rep 2018 Feb 26;8(1):3613.\\nFig. 2. Predictors of 1-year mortality after cardiac transplantation with the use of logistic regression.\\nApplication of Advanced Analytics to the UNOS Database\\n\\x03\\nMiller et al\\n483\\n'}, {'source': '/Users/sir/Downloads/Data/PDF/Artificial Intelligence in Medicine.pdf', 'text': 'A machine learning-based approach to prognostic analysis of thoracic\\ntransplantations\\nDursun Delen a,*, Asil Oztekin b,c, Zhenyu (James) Kong b\\na Spears School of Business, Oklahoma State University, T-NCB 378, 700 North Greenwood Avenue, Tulsa, OK, 74106, USA\\nb School of Industrial Engineering and Management, Oklahoma State University, 322 Engineering North, Stillwater, OK 74078, USA\\nc Department of Industrial Engineering, Gediz University, 35230 Cankaya-Izmir, Turkey\\n1. Introduction\\n1.1. Motivation\\nThoracic (heart and lung) transplantation has been accepted as a\\nviable treatment for end-stage cardiac and pulmonary failure. The\\nArtiﬁcial Intelligence in Medicine 49 (2010) 33–42\\nA R T I C L E\\nI N F O\\nArticle history:\\nReceived 25 February 2009\\nReceived in revised form 15 December 2009\\nAccepted 10 January 2010\\nKeywords:\\nData mining\\nMachine learning\\nUNOS\\nThoracic Transplantation\\nSurvival analysis\\nPrognostic index\\nA B S T R A C T\\nObjective: The prediction of survival time after organ transplantations and prognosis analysis of different\\nrisk groups of transplant patients are not only clinically important but also technically challenging. The\\ncurrent studies, which are mostly linear modeling-based statistical analyses, have focused on small sets\\nof disparate predictive factors where many potentially important variables are neglected in their\\nanalyses. Data mining methods, such as machine learning-based approaches, are capable of providing an\\neffective way of overcoming these limitations by utilizing sufﬁciently large data sets with many\\npredictive factors to identify not only linear associations but also highly complex, non-linear\\nrelationships. Therefore, this study is aimed at exploring risk groups of thoracic recipients through\\nmachine learning-based methods.\\nMethods and material: A large, feature-rich, nation-wide thoracic transplantation dataset (obtained from\\nthe United Network for Organ Sharing—UNOS) is used to develop predictive models for the survival time\\nestimation. The predictive factors that are most relevant to the survival time identiﬁed via, (1) conducting\\nsensitivity analysis on models developed by the machine learning methods, (2) extraction of variables\\nfrom the published literature, and (3) eliciting variables from the medical experts and other domain\\nspeciﬁc knowledge bases. A uniﬁed set of predictors is then used to develop a Cox regression model and\\nthe related prognosis indices. A comparison of clustering algorithm-based and conventional risk\\ngrouping techniques is conducted based on the outcome of the Cox regression model in order to identify\\noptimal number of risk groups of thoracic recipients. Finally, the Kaplan–Meier survival analysis is\\nperformed to validate the discrimination among the identiﬁed various risk groups.\\nResults: The machine learning models performed very effectively in predicting the survival time: the\\nsupport vector machine model with a radial basis Kernel function produced the best ﬁt with an R2 value of\\n0.879, the artiﬁcial neural network (multilayer perceptron-MLP-model) came the second with an R2\\nvalue of 0.847, and the M5 algorithm-based regression tree model came last with an R2 value of 0.785.\\nFollowing the proposed method, a consolidated set of predictive variables are determined and used to\\nbuild the Cox survival model. Using the prognosis indices revealed by the Cox survival model along with a\\nk-means clustering algorithm, an optimal number of ‘‘three’’ risk groups is identiﬁed. The signiﬁcance of\\ndifferences among these risk groups are also validated using the Kaplan–Meier survival analysis.\\nConclusions: This study demonstrated that the integrated machine learning method to select the predictor\\nvariables is more effective in developing the Cox survival models than the traditional methods commonly\\nfound in the literature. The signiﬁcant distinction among the risk groups of thoracic patients also validates\\nthe effectiveness of the methodology proposed herein. We anticipate that this study (and other AI based\\nanalytic studies like this one) will lead to more effective analyses of thoracic transplant procedures to better\\nunderstand the prognosis of thoracic organ recipients. It would potentially lead to new medical and\\nbiological advances and more effective allocation policies in the ﬁeld of organ transplantation.\\n\\x02 2010 Elsevier B.V. All rights reserved.\\n* Corresponding author. Tel.: +1 918 594 8283; fax: +1 918 594 8283.\\nE-mail address: dursun.delen@okstate.edu (D. Delen).\\nContents lists available at ScienceDirect\\nArtificial Intelligence in Medicine\\njournal homepage: www.elsevier.com/locate/aiim\\n0933-3657/$ – see front matter \\x02 2010 Elsevier B.V. All rights reserved.\\ndoi:10.1016/j.artmed.2010.01.002\\n\\nincreased experience in cardiac and pulmonary transplantation,\\nimprovements in patient selection, organ preservation, and preop-\\nerativesupporthavesigniﬁcantlyreducedtheearlythreatstopatient\\nsurvival [1]. Over the past decade, the thoracic transplant waiting\\ntime for a listed patient has markedly increased, but the number of\\ntransplants performed has declined. In addition, the research also\\nfound thatthere is a perceived inequity inaccesstoorgans. Theorgan\\nallocation system needs to be improved since it may become a major\\nfactor negatively inﬂuencing the survivability of thoracic transplant\\n[2].\\nThe survivability prediction is becoming increasingly more\\nimportant in medicine. When a resource is scarce, the need for\\naccurate prediction becomes acute [3]. Especially prediction of\\nsurvival time and prognosis prediction of medical treatments are\\nclinically important and challenging problems [4]. Scarceness of\\norgans necessitates the development of effective and efﬁcient\\nprocedures to select the most optimal organ receiver since demand\\nfor organs of all patients might not be satisﬁed. To achieve this, one\\ncritical step is to reveal the knowledge underlying huge amount of\\ndata collected and stored from organ transplantation procedures\\nperformedinthepast.Theobjectivesare(1)tomaximizethepatients’\\nsurvival time after the organ transplantation surgery, and (2) to\\noptimize the prognosis for the organ recipients. These can be\\npotentially achieved by discovering the knowledge that may be\\ncontained in large dataset consisting of more than hundreds of\\ndeterminative variables regarding the donors, the potential recipi-\\nents, and transplantation procedures. Therefore, in this study a data\\nmining method is proposed to process large amount of transplanta-\\ntion data obtained from UNOS to identify the important factors as\\nwell as their relationships to the survival of the graft and the patient.\\nThereafter, a prognostic index [5,6] is developed to classify the\\npatients into different risk groups for better understanding of the\\ntransplantation phenomenon. In short, this study will address the\\nfollowing questions: (1) what are the most important variables to be\\nincluded in an effective prognostic index related to thoracic organ\\ntransplantations?(2)whatarethemostcoherentriskgroupsthatcan\\nbe formed based on the prognostic index? Predicting the thoracic\\nsurvivability and classifying the patients (potential thoracic organ\\nreceivers)intodifferentclassesofriskswouldhelpdecisionmakersin\\ndeterminingpatients’priorityfortransplantationsourceassignment.\\n1.2. Literature review\\n1.2.1. Related research in survival analysis for organ transplantation\\nIn the recent past, a number of studies were conducted using\\ndata-driven analytics on various organ transplantation datasets.\\nClosely related to the study reported herein, Hariharan et al. [7]\\nfocused on the analysis of improved graft survival rate using\\ncyclosporine after renal transplantation in both short-term (less\\nthan 1 year) and long-term (more than 1 year). A regression analysis\\nwas used to predict the probability of the graft failure after kidney\\ntransplantation in both short-term and long-term period in the light\\nof demographic characteristics, transplant-related variables, and\\npost-transplantation variables. The study performed by Herrero\\net al. [8] included 116 patients who received a liver transplant\\nbetween the years 1994 and 2000. Statistical tests are used to\\ncompare the demographic and characteristic variables, pretrans-\\nplant, andintra-operativevariables betweenthetwo groups, namely\\nyounger and older than 60. The results indicate that there is a clear\\ntrend showing that older patients have lower survival after liver\\ntransplantation. Hong et al. [9] presented a survival analysis of liver\\ntransplant patients in Canada by considering some factors such as\\nage, blood type, donor type (cadaveric or alive), race, and gender of\\nrecipient and donors. However, having limited the variables with\\nthis scope, they also admitted that the clinical information lacks of\\nmany potential details.\\nTaking a data mining approach, Kusiak et al. [10] compared two\\nrule-based data mining techniques, i.e. decision trees and rough\\nsets, to predict survival time of kidney dialysis patients. This study\\nachieved satisfactorily high prediction accuracy. The main limita-\\ntion of the study was the utilization of a small dataset with only\\n188 patients in total and also many patient-related parameters\\nwere neglected in the problem formulation. Using more traditional\\nmethods, and speciﬁcally having focused on thoracic transplanta-\\ntion, Jenkins et al. [11] and Fernandez-Yanez et al. [12] had a rich\\npool of independent variables for survivability prediction. Their\\nstudies used popular statistical techniques such as Kaplan–Meier\\nmethod of survival analysis with Mantel–Haenszel log-rank test.\\nHowever, both of these techniques have been criticized with two\\nmajor limitations: (1) linear relationships are assumed, which\\nhence cannot capture the nonlinearity among the variables, and (2)\\nthe independent variables were selected solely based on the\\nexperiences and intuitions of the analysts who conducted these\\nstudies. Thus, many potentially signiﬁcant variables might be left\\noutside the scope of this study. Tjang et al. [13] added more\\nexplanatory variables to determine the survivability in heart\\ntransplantation, such as body mass index, waiting time on the list,\\nand previous cardiac surgery, their study also ignored the non-\\nlinear relationships among the pool of survivability-related\\nvariables. Similar limitations exist in some other studies focused\\ndirectly or indirectly on thoracic transplantation [14–16].\\nThe existing studies implicitly assume that the relationships\\namong the predictive variables and output variable are linear and\\nthe predictor variables are independent of each other, which may\\nnot be valid in reality. Moreover, the abovementioned studies focus\\non\\nsmall\\ndatasets\\nwith\\nlimited\\nnumber\\nof\\npredictors\\nfor\\nsurvivability of patients after transplantation. This limitation\\nmay cause incomprehensive modeling due to the insufﬁcient\\ninformation contents (i.e., omission of a number of potentially\\nimportant predictor variables).\\n1.2.2. Related research in devising a prognostic index\\nPrognostic index (PI) provides compact prognosis information\\nregarding a speciﬁc patient based on the results of a Cox\\nproportional hazards model [5]. Cox proportional hazards model\\nhelps identify variables of prognostic importance and hence\\nprognostic index can be used to deﬁne groups of individuals at\\ndifferent risk categories. Even though prognostic index is a\\nconvenient tool to measure how well the patients are doing after\\nthe transplantation, its use in the organ transplantation area has\\nbeen limited mostly due to the lack of follow-up data. Some\\nexisting studies related to devising a PI in transplant area are\\nsummarized as follows.\\nIn the study conducted by Christensen et al. [17], it is\\nmentioned that primary biliary cirrhosis requires a liver trans-\\nplantation operation at the end stage. Based on the prognosis\\nanalysis with as well as without transplantation, it is decided\\nwhether or not the transplantation is required, if so when. To\\nachieve this goal, corresponding PIs and probabilities of surviving\\nare computed for transplantation and non-transplantation cases.\\nYoo et al. [18] developed a similar index and revealed that\\nsocioeconomic status does not inﬂuence patient or graft survival\\nthat undergoes liver transplantation at the institute where they\\nperformed their study. Deng et al. [19] conducted a study with a\\nnational dataset in Germany, which discovers the effect of\\nreceiving a heart transplant for the patients in a waiting list.\\nThe results indicate that cardiac transplant is associated with\\nsurvival beneﬁt only for patients with a predicted high risk of dying\\non the waiting list. Ghobrial et al. [20] performed a study to\\ndetermine prognostic factors for overall survival in 107 adult\\npatients with post-transplantation lymphoproliferative disorders\\n(PTLDs). It is validated that in discriminating the low and high\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n34\\n\\nscored patients the proposed prognostic scoring signiﬁcantly\\nperforms better than the International Prognostic Index for the\\nsubset of the patients (56 out of 107) with lactate dehydrogenase.\\nThe common limitation in all of these studies is similar to the\\nlimitations of the studies summarized in Section 1.2.1. Namely,\\nthey directly devise a prognostic index without determining if the\\nvariables used in prognostic index devising phase are necessary\\nand sufﬁcient. This motivates a machine learning-based initial step\\nof variable selection procedure. Because, if the critical predictive\\nfactors are not captured effectively due to the intuition- and\\nexperience-based\\nselection,\\nthe\\nresulting\\nprognostic\\nindices\\ndeveloped based on the selected variables would be inaccurate\\nand, in turn, related risk groups of patients would be deviated from\\nthe real classes. This may cause mistakes for decision maker in\\nmaking organ transplantation policies.\\n2. Proposed method\\nSection 1.2 shows that the most of the existing studies for organ\\ntransplantation\\nprocedures\\nutilize\\nconventional\\nstatistical\\napproaches such as Kaplan–Meier function and log-rank test\\nalong with expert-selected variables to predict the survivability.\\nHowever, organ transplantation procedures consist of a large\\nnumber of variables (several hundred) that may have nontrivial\\nimpact on modeling the prognosis of the grafts/patients. Using a\\nsomewhat comprehensive variable list may help discriminate\\npatients from each other by placing them into proper risk groups.\\nUnintentional omission of the important variables may lead to\\ninaccurate classiﬁcation of patient risk groups, which may, in turn,\\nlead to suboptimal organ allocation policies and ineffective\\ntreatments.\\nThis study is aimed at overcoming the abovementioned\\nshortcomings by employing both machine learning techniques\\nas well as statistical methods to identify the most critical factors\\naffecting the survivability of thoracic transplant patients. To\\nachieve this goal, this study proposes adopting a 5-step approach\\nillustrated in Fig. 1. Step 1 involves data understanding and\\npreparation, which is arguably the most time demanding step in\\nthe process. Step 2 employs various predictive modeling techni-\\nques such as support vector machines, artiﬁcial neural networks,\\nand regression trees to develop survival time prediction models\\nand to extract the most important variables by means of sensitivity\\nanalysis through the best performing model. Step 3 determines the\\nconsolidated candidate set of critical predictor variables. Step 4\\ndevelops a Cox regression model using the consolidated set of\\npredictor variables and also devises a prognostic index. The last\\nstep, Step 5, classiﬁes the patients into various risk categories by\\ncomparing and contrasting the clustering performance of algo-\\nrithm-based and manually calculated groups. Then the resulting\\nrisk categories are validated by using the Kaplan–Meier survival\\ncurves. These steps will be further explained in details in Sections\\n2.1–2.5, respectively.\\n2.1. Step one: data source and data preparation\\nIn this study, the data source that was used to validate the\\nproposed method was thoracic organ transplant dataset provided by\\nUNOS, which is a tax-exempt, medical, scientiﬁc, and educational\\norganization that operates the national Organ Procurement and\\nTransplantation Network underthe contract to the Division of Organ\\nTransplantation of the Department of Health and Human Services\\n[21]. The data ﬁles were obtained from UNOS using a formal data\\nrequisition procedure (which includes submission of speciﬁc data\\nneeds, purpose of the study, and a data use agreement). These data\\nﬁles are named as UNOS Standard Transplant Analysis and Research\\n(STAR)\\nﬁles\\nfor\\nheart,\\nlung,\\nand\\nsimultaneous\\nheart–lung\\ntransplants, namely thoracic transplants. Each transplant STAR ﬁle\\nconsists of information on all thoracic transplants that had been\\nperformed in the US and reported to UNOS since October 1, 1987. It\\nincludes both deceased- and living-donor transplants. None of the\\nﬁles include any speciﬁc patient or transplant hospital identiﬁers\\ndue to the privacy and security issues. However, there is a patient\\nidentiﬁcation number, unique to each patient, which allows linking\\nmultiple ﬁles and tracking the patient. Considering these features,\\nUNOS’s data ﬁles are recognized as the most comprehensive source\\nof information available in any single ﬁeld of medicine and for organ\\ntransplantation in US [22].\\nThere are two datasets involved in our study, which are regular\\ndataset and follow-up dataset. The regular dataset contains all\\ninformation of donors and recipients before transplantation\\noccurred, and the follow-up dataset provides all information of\\ndonors and recipients after the transplantation. The TRR_ID\\nvariable (transplant identiﬁer) is the common variable between\\nthese two datasets and the one which is proposed by UNOS to\\nmerge and integrate these two datasets. Therefore, these two\\ndatasets were combined in a relational database environment\\nusing the link (a.k.a. primary key) of TRR_ID.\\nOverall, the complete dataset consists of 310,773 records and\\n565 variables. These variables include the socio-demographic and\\nFig. 1. A ﬂowchart representation of the proposed method.\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n35\\n\\nhealth-related factors with regard to both the donor and the\\nrecipients. There are also procedure-related factors among the\\ndataset. To assign as an output (dependent variable), there are four\\npossible variables which are called pstatus, ptime, gstatus, and\\ngtime. These variables have the following meanings: whether or\\nnot the patient died after transplantation occurred (referring to\\npstatus, with dead = 1 and alive = 0). A very similar variable was\\ngstatus, referring to whether or not graft has failed (1 denoting\\n‘‘failed’’ and 0 denoting ‘‘succeeded’’). The variable ptime denoted\\npatient follow-up time (in days) from transplant to death/last\\nfollow up time. Similarly, gtime is explained as graft lifespan from\\ntransplant to death/last follow up time. Since the goal of this study\\nis to develop models to predict the survivability solely based on\\nthoracic transplant, the dependent variable was assigned as gtime.\\nThis assignment was done to discriminate the patients who died\\nsolely due to the thoracic graft incompatibility from the ones who\\ndied from any other reasons. Therefore, the rest of the potential\\ndependent variables (pstatus and ptime) were eliminated from the\\ndataset. Besides, gstatus was kept inactive up to the stage where\\nCox regression model was implemented (Step 4 in Fig. 1).\\nConsidering the gtime as the continuous dependent variable,\\nthe records for the patients whose gtime information were missing\\nwere removed from the dataset. The data set also includes some\\nidentiﬁcation variables (e.g., Donor ID) which help track the\\nrecipient patient anonymously, track the thoracic transplant\\nprocedure, or link records from multiple data ﬁles to each other.\\nSince these types of identiﬁcation variables do not have any\\ninformation content to enhance the prediction capability of the\\nmodels, after linking and integrating the ﬁles they were also\\nexcluded from the analysis dataset. Moreover, the name of\\ntransplantation type was recorded in the dataset as a variable\\nnamed Dataset which had one value (TH referring to thoracic) and\\nthe date of data processing is recorded as a variable named Date of\\nRun which are useful for data integration purposes but has no\\ninformation for contributing to the prediction of survivability and\\nhence are also excluded from the analysis dataset. Similarly, other\\nvariables having only one possible value for all records in the\\ndataset, which have no discriminating information, are also\\neliminated from the predictive modeling.\\nThis dataset had excessive number of missing values which\\nrender most of the records and variables seemingly insigniﬁcant.\\nHowever, in data mining studies one should be very reluctant to\\nremove the candidate predictor variables while at the same time\\ntrying to avoid artiﬁcial data imputation procedures. There is an\\nobvious trade-off here. As a rule of thumb, for column (variable)\\ndeletion, we were cautious to remove any variable from the\\nanalysis and assumed that if a variable has more than 95% missing\\nvalues, only then it should be regarded as not having signiﬁcant\\ninformation content and hence should be deleted. Next step was to\\nhandle the missing values by following the general convention: for\\nthe categorical variables we ﬁlled the missing values with some\\nheuristic values such as E (referring to empty) or NR (referring to\\nnot reported), and for the continuous variables we imputed the\\nmissing values with the average of the existing records. After\\nadopting these data preparation strategies, the ﬁnal dataset was\\nreduced to 372 cleansed independent variables and one dependent\\nvariable (gtime) with the total record count of 106,398.\\n2.2. Step two: predictive modeling\\nSince the dependent variable herein was a continuous variable\\n(graft survival time, which is the number of days from transplant to\\ndeath or last follow-up), the problem refers to a prediction (or\\nregression) problem (as opposed to a classiﬁcation problem). Since\\nthe\\nrelationships between the\\ndependent\\nvariable\\nand\\nthe\\nindependent variables were not known in advance, this step\\nwas to develop various predictive models for graft survival time\\nusing all of the available independent variables. It is also required\\nto check whether the models have passed the pre-speciﬁed\\nthreshold values of performance measures, speciﬁcally the R2 and\\nmean square error (MSE), to determine the best model that\\nexplains these unknown relationships between dependent and\\nindependent variables by ranking them according to these\\nmeasures. The model which is deemed to be the most successful\\none would be kept for further modeling steps to determine the\\nimportance of the independent variables.\\nSupport vector machines (SVMs) are supervised learning\\nmethods that generate input–output mapping functions from a\\nset of training data. They belong to a family of generalized linear\\nmodels which achieve a classiﬁcation or regression decision based\\non the value of the linear combination of features. They are also\\nsaid to belong to the kernel methods [23]. The mapping function in\\nSVMs can be either a classiﬁcation function (used to categorize the\\ndata) or a regression function (used to estimate the numerical\\nvalue of the desired output, as is the case in this study). Nonlinear\\nkernel functions are often used to transform the input data\\n(inherently representing highly complex nonlinear relationships)\\nto a high dimensional feature space in which the input data\\nbecome more separable (i.e. linearly separable) compared to the\\noriginal input space. Then, maximum-margin hyperplanes are\\nconstructed to optimally separate the classes in the training data.\\nTwo parallel hyperplanes are constructed on each side of the\\nhyperplane that separates the data by maximizing the distance\\nbetween the two parallel hyperplanes. An assumption is made that\\nthe\\nlarger\\nthe\\nmargin\\nor\\ndistance\\nbetween\\nthese\\nparallel\\nhyperplanes, the better the generalization error of the prediction\\nwould be.\\nArtiﬁcial neural networks (ANNs) have been utilized to model\\ncomplex relationships (such as nonlinear functions and multi-\\ncollinearity) among the predictor variables and the dependent\\nvariable [24]. ANNs are highly sophisticated analytic techniques\\ncapable of predicting new observations (on speciﬁc variables)\\nfrom other observations (on the same or other variables) after\\nexecuting a process of so-called ‘‘learning’’ from existing data\\n[25]. ANNs have been one of the most popular artiﬁcial\\nintelligence-based data modeling algorithms used in recent\\nmedical informatics studies due to their satisfactory predictive\\nperformance [26]. On the other hand, compared to other\\nmachine learning methods (such as ANNs), decision trees have\\nthe advantage of not being a black box model, namely having the\\ncapability to explain the inner structure of the model in the form\\nof a graphically represented inverse tree or a collection of\\ncondition-action rules. This advantage has made them a viable\\nand desirable alternative method in medical informatics [27]. If\\nthe dependent variable is continuous (as in the case in this\\nstudy) the resulting decision tree is called a regression tree.\\nRegression trees are known to be among the highly adaptable,\\nrelatively ﬂexible, yet computationally intensive data mining\\ntechniques [28]. Popular regression tree algorithms are CART (or\\nC&RT) [29], CHAID [30], and M5 [31] which can be used for both\\nclassiﬁcation and regression type prediction problems. ID3 and\\nits successors, C4.5 and C5 are also among the popular decision\\ntree algorithms, but they can only work for classiﬁcation type\\nprediction problems.\\n2.2.1. Performance criteria\\nTo compare the abovementioned prediction models, two\\nperformance criteria are considered: mean squared error (MSE)\\nof the model on testing dataset and R2 value between the actual\\nobservation for the target variable (Yt) and the predicted value by\\nthe model (Ft). MSE which is given by Eq. (1) does not have a rule-\\nof-thumb threshold cut-off value for acceptable models. It is a\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n36\\n\\nrelative criterion to select the best model, namely the smaller the\\nvalue the better the model has performed [32].\\nMSE ¼ 1\\nn\\nX\\nn\\nt¼1\\nðYt \\x02 FtÞ2\\n(1)\\nOn the other hand, R2 (R2\\nFt;Yt or shortly R2) which is given by\\nEq. (2) can be considered as both an absolute measure and a\\nrelative measure to determine and rank the satisfactory models\\n[33]. Unlike the MSE, the higher the R2, the better the performance\\nfor the compared models.\\nR2 ¼ 1 \\x02\\nPn\\nt¼1 ðFt \\x02 YtÞ2\\nPn\\nt¼1 ðYt \\x02 ¯YtÞ\\n2\\n(2)\\n2.2.2. k-Fold cross-validation\\nIn order to minimize the bias associated with the random\\nsampling of the training and holdout data samples in comparing\\nthe predictive accuracy of two or more methods, researchers tend\\nto use k-fold cross-validation [34]. In k-fold cross-validation, also\\ncalled rotation estimation, the complete dataset (D) is randomly\\nsplit into k mutually exclusive subsets (the folds: D1, D2, . . ., Dk) of\\napproximately equal size. The prediction model is trained and\\ntested k times. Each time (t 2 {1, 2, . . ., k}), it is trained on all but one\\nfold (Dt) and tested on the remaining single fold (Dt). The cross-\\nvalidation estimate of the overall performance criteria is calculated\\nas simply the average of the k individual performance measures as\\nin Eq. (3),\\nCV ¼ 1\\nk\\nX\\nk\\ni¼1\\nPMi\\n(3)\\nwhere CV stands for cross-validation, k is the number of folds used,\\nand PM is the performance measure for each fold [35].\\nIn this study, to estimate the performance of the prediction\\nmodels a 10-fold cross-validation approach was used. Empirical\\nstudies showed that 10 seems to be an optimal number of folds\\n(that optimizes the time it takes to complete the test while\\nminimizing the bias and variance associated with the validation\\nprocess) [34]. In 10-fold cross-validation the entire dataset is\\ndivided into 10 mutually exclusive subsets (or folds). Each fold is\\nused once to test the performance of the prediction model that is\\ngenerated from the combined data of the remaining nine folds,\\nleading to 10 independent performance estimates.\\n2.2.3. Sensitivity analysis\\nAfter selecting the best prediction model based on the\\nperformance criteria as explained in Section 2.2.1, it is required\\nto determine the importance of the independent variables. In\\nmachine learning algorithms, sensitivity analysis is a method for\\nextracting the cause and effect relationship between the inputs\\nand outputs of a trained model [36]. In the process of performing\\nsensitivity analysis, after the model is trained the learning is\\ndisabled so that the network weights are not affected. The\\nfundamental idea is that the sensitivity analysis measures the\\npredictor variables based on the change in modeling performance\\nthat occurs if a predictor variable is not included in the model.\\nHence, the measure of sensitivity of a speciﬁc predictor variable is\\nthe ratio of the error of the trained model without the predictor\\nvariable to the error of the model that includes this predictor\\nvariable [37]. The more sensitive the network is to a particular\\nvariable, the greater the performance decrease would be in the\\nabsence of that variable, and therefore the greater the ratio of\\nimportance. This method is followed in support vector machines\\nand artiﬁcial neural networks to rank the variables in terms of their\\nimportance according to the sensitivity measure deﬁned in Eq. (4)\\n[38].\\nSi ¼\\nV\\nCðFtÞ ¼ VðEðFtjXiÞÞ\\nVðFtÞ\\n(4)\\nwhere V(Ft) is the unconditional output variance. In the numerator,\\nthe expectation operator E calls for an integral over X\\x02i; that is, over\\nall input variables but Xi, then the variance operator V implies a\\nfurther integral over Xi. Variable importance is then computed as\\nthe normalized sensitivity. Saltelli et al. [39] show that Eq. (4) is the\\nproper measure of sensitivity to rank the predictors in order of\\nimportance for any combination of interaction and non-orthogo-\\nnality among predictors. As for the decision trees, variable\\nimportance measures were used to judge the relative importance\\nof each predictor variable. Variable importance ranking uses\\nsurrogate splitting to produce a scale which is a relative\\nimportance measure for each predictor variable included in the\\nanalysis. Further details on this procedure can be seen in Breiman\\net al. [29].\\n2.3. Step three: determining the candidate sets of predictor variables\\nStep 3 is to determine which predictor variables to be used in\\ndevising a prognostic index in Step 4. This step helps eliminate\\nthe insigniﬁcant variables and improves the accuracy of the\\nmodel by optimizing the predictor variables list. The potential\\ninput variables to this step consist of three candidate sets of\\npredictor variables. The ﬁrst set is composed of variables\\nselected\\nby\\nthe\\npredictive\\nmodels.\\nThe\\npredictive\\nmodels\\nexplained in Section 2.2 rank the predictor variables based on\\ntheir importance level in predicting the graft survival time. The\\npredictive variables selected by the sensitivity analysis of the\\nbest-performing model (ranked in terms of R2 and MSE) are\\nchosen as the ﬁrst set of predictive variables. The second set of\\npredictive variables is obtained by considering the expert\\ndomain\\nknowledge.\\nThis\\nset\\nincludes\\nvariables\\nwhich\\nare\\nlogically related to heart and lung transplantation such as\\ndonor’s history of cigarette usage. The third set of predictive\\nvariables is selected from the related literature. This set consists\\nof the variables which have been commonly and repeatedly used\\nin previous studies in the organ transplantation area. The second\\nand third sets of predictive variables provide more comprehen-\\nsive information for the next step, the Cox regression model, by\\nincluding the variables that might have importance in the\\nsurvival analysis but were determined to be insigniﬁcant by the\\npredictive models in Step 2.\\n2.4. Step four: survival analysis and prognostic index devising\\nStep 4 takes all the three sets of predictive variables identiﬁed\\nin Step 3, and then applies Cox regression to model the graft\\nsurvivability and ﬁlter out the candidate predictive variables\\nwhich do not have signiﬁcant survival effect. Hence, in Step 4 the\\nﬁnal critical predictive variables are determined by the Cox\\nregression model. Cox regression model also enables devising a\\nprognostic index to categorize the patients into various groups\\nwith different levels of risks.\\nCox regression model is a semi-parametric model extensively\\nused in survival analysis [6]. The survival time of each patient is\\nassumed to follow the hazard function (hi) given by Eq. (5) as\\nfollows:\\nhi ¼ h0 expðxibÞ\\n(5)\\nwhere h0 is the baseline hazard function and xi is the vector of\\npredictor variables for the ith patient. b is the vector of regression\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n37\\n\\ncoefﬁcients for the predictor variables and is assumed to be the\\nsame for all patients [40,41].\\nOne important application of Cox regression model is to identify\\nvariables which may be of prognostic importance [5]. Once\\nidentiﬁed, knowledge from these variables will be combined and\\nused to deﬁne a prognostic index, which in turn deﬁnes groups of\\norgan recipients with different levels of risk. To use the prognostic\\nindex, key patient characteristics are recorded, from which a score\\nis derived. This score gives an indication of whether a particular\\npatient has high, intermediate, or low levels of prognosis for the\\ndisease [5,42]. Recalling Eq. (5), the prognostic index (PI) for each\\npatient can be calculated by Eq. (6):\\nPI ¼ x1b1 þ x2b2 þ . . . þ xnbn\\n(6)\\nwhere x1 to xn are the patient’s values for the variables in the Cox\\nmodel, and b1 to bn are the corresponding regression coefﬁcients\\ndetermined by Cox regression model [42].\\nNote that PI in Eq. (6) represents the exponent portion in Eq. (5).\\nTherefore, the smaller the PI, the smaller the hazard function value,\\nand hence the smaller the risk associated with a particular\\nrecipient.\\n2.5. Step ﬁve: determining risk groups of thoracic recipients\\nAnimportantquestionfollowingStep4is‘‘Howmanyriskgroups\\nshould the patients be classiﬁed into?’’ In Step 5, k-means clustering\\nalgorithm, two-step cluster analysis, and conventional heuristics-\\nbased approaches are used to answer to this question. As a statistical\\nand/or pictorial veriﬁcation mechanism for the number of groups\\ndetermined by the best performing abovementioned clustering\\napproaches, ﬁnally the Kaplan–Meier survival analysis [43] is\\nadopted and corresponding survival curves are generated.\\nk-Means method is an extensively used, arguably the most\\npopular clustering algorithm that searches for a nearly optimal\\npartition with ﬁxed number of clusters represented by the\\nparameter k [44]. It proceeds by assigning k initial centroids to\\nthe multidimensional datasets. Each record in the dataset is\\nallocated to the centroid which is nearest and hence forming a\\ncluster. Each cluster centroid is then updated to be the center of its\\nmembers, followed by a new assignment of records to the nearest\\ncentroids to re-construct the clusters. The algorithm converges\\nwhen there is no further change in allocation of members to clusters\\nor some predeﬁned time-based stopping criteria is satisﬁed [45].\\nAnother popular clustering algorithm istwo-stepcluster analysis\\n(TSCA) [46,47]. It has two steps: (1) to pre-cluster the cases (or\\nrecords) into many small sub-clusters, and (2) to cluster the sub-\\nclusters resulting from pre-cluster step into the desired number of\\nclusters. The pre-cluster step uses a sequential clustering approach. It\\nscans the data records one by one and decides if the current record\\nshould be merged with the previously formed clusters or starts a\\nnew cluster based on the distance criterion. Then the cluster step\\ntakes sub-clusters resulting from the pre-cluster step as input, and\\ngroups them into the desired number of clusters. Since the number\\nof sub-clusters is much less than the number of original records, the\\ntraditional clustering methods can be used effectively. This step uses\\nthe agglomerative hierarchical clustering method [46,47]. Although\\nthere are several other clustering algorithms (e.g. Kohonen net-\\nworks) they do not allow the modeler to specify a desired number of\\nclusters at the beginning of the clustering algorithm. k-Means and\\nTSCA algorithms overcome this issue. The modeler can predeﬁne a\\nspeciﬁcnumber of clusters to group the variables and compare them\\naccording to their clustering performances. Since this is the main\\nfocus of our study, we utilized k-means and TSCA algorithms for\\nclustering the PIs and thus identfying the risk groups of thoracic\\npatients.\\nThe Kaplan–Meier analysis is a non-parametric technique used\\nto test the statistical signiﬁcance of differences between the\\nsurvival curves associated with two different circumstances [43].\\nThe analysis expresses the distribution of patient survival times in\\nterms of the proportion of patients still alive up to a given time. On\\nthe other hand, the Kaplan–Meier survival curves plot the\\nproportion of patients surviving against time which has a\\ncharacteristic decline. In biostatistics, a typical application of\\nKaplan–Meier survival curves involves grouping patients into risk\\ngroups such as low, medium, and high risks.\\n3. The case study and discussion\\nIn order to demonstrate and validate the proposed methodolo-\\ngy in Section 2, two most popular data mining toolkit are used,\\nnamely SPSS PASW Modeler1 [48] and SAS 9.1.31 [49] statistical\\nsoftware package. Using the UNOS data set, Sections 3.1–3.5\\ndiscuss the results obtained by following the above mentioned\\nmodeling procedures presented in Section 2. The prediction\\nperformance results reported herein are all based on the test (or\\nholdout) dataset.\\n3.1. Predictive model results\\nTo reveal the initially unknown relationship between the\\nthoracic input/independent variables and the continuous output/\\ndependent variable (gtime), due to the high computational time\\nrequired for 10-fold cross-validation of each model we only used\\ntwo most popular models from each family of machine learning\\ntechniques. Radial basis function (RBF) and polynomial functions\\nas Kernel methods in support vector machine were deployed. We\\nused multilayer perceptron (MLP) and RBF type of network\\nstructures for ANNs. The most recent algorithms C&RT and M5\\nwere utilized for prediction with the decision trees. The 10-fold\\naveraged prediction results in terms of MSE and R2 for each model\\nare tabulated in Table 1. The acceptance of predictive models is\\nﬁrst evaluated based on their coefﬁcient of determination (R2)\\nvalues. It is widely accepted that if R2 is higher than 0.6, the\\npredictive model has performed fairly well [50,51]. Therefore, we\\nset this as a threshold value for the model sufﬁciency. Since all the\\nmodels have passed this threshold, we kept the one with the\\nhighest R2 and the smallest MSE for further analyses, which came\\nout to be the support vector machine model with radial basis\\nKernel function in this case study.\\n3.2. Determination of the candidate covariates for Cox regression\\nmodel\\nStep 3 in the proposed method provides three different sets of\\ncandidate covariates to be used in the Cox model. Since the best\\nperforming model to explain the relationships of independent and\\nTable 1\\nComparison of machine learning prediction model results.\\nPerformance measures\\nPrediction models\\nMSE\\nR2\\nSupport vector machine\\nRBF\\n0.023\\n0.879\\nPolynomial\\n0.793\\n0.643\\nArtiﬁcial neural network\\nMLP\\n0.031\\n0.847\\nRBF\\n0.146\\n0.835\\nDecision trees\\nM5\\n0.324\\n0.785\\nC&RT\\n0.578\\n0.766\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n38\\n\\ndependent variables was found to be RBF-SVM, the sensitivity\\nanalysis as explained in Section 2.2.3 by Eq. (4) was conducted on\\nthe predictor variables to rank them in terms of their importance in\\npredicting the gtime output variable. This ﬁrst set consists of the\\npredictor variables which are presented in Table 2.\\nThe secondsetofpredictorvariableswereselectedbythe authors\\nthrough brainstorming sessions with medical professionals. The\\nsecond set of candidate covariates are tabulated in Table 3.\\nThe third set of candidate covariates was determined through\\nthe recent literature [52]. This set includes the variables commonly\\nused in the\\npreviously published studies related to organ\\ntransplantation. The third set of candidate covariates are shown\\nin Table 4.\\nThe second and third set of candidate covariates can be\\nperceived as the expert component of the method. If the predictive\\nmodels in Step 3 do not reveal some very critical predictor\\nvariables (such as the age of the recipient in our case study), the\\nmethod proposes to force the Cox model once more to review the\\nsigniﬁcance of this kind of predictor variables.\\n3.3. Deployment of Cox regression model and devising the prognostic\\nindices\\nAll the candidate covariates as determined in Section 3.2 were\\nassigned to Cox regression model at this step. The stepwise\\nvariable selection procedure was applied with 0.05 for entry and\\nTable 3\\nThe 2nd set of candidate covariates.\\nVariables\\nExplanation\\nAntiarry\\nHeart medical factors: antiarrythmics\\nat registration\\nContin_Alcohol_Old_Don\\nDeceased donor-history of alcohol\\ndependency + recent 6 months use\\nContin_Cig_Don\\nDeceased donor-history of cigarettes in past\\nand >20 pack years + recent 6 months use\\nContin_IV_Drug_Old_Don\\nDeceased donor-history of iv drug use\\n+ recent 6 months use\\nContin_Oth_Drug_Don\\nDeceased donor-history of other drugs in\\npast + recent 6 months use\\nEint\\nEthnicity interaction between recipient and\\ndonor (in the same ethnic group, y/n)\\nGint\\nGender interaction between recipient and\\ndonor (having the same sex, y/n)\\nHist_Alcohol_Old_Don\\nDeceased donor-history of alcohol dependency\\nHist_Cancer_Don\\nDeceased donor-history of cancer (y/n)\\nHist_Cig_Don\\nDeceased donor-history of cigarettes in past\\nand >20pack yrs\\nHist_Cocaine_Don\\nDeceased donor-history of cocaine use in past\\nHist_Diabetes_Don\\nDeceased donor-history of diabetes, incl.\\nDuration of disease\\nHist_Hypertens_Don\\nDeceased donor-history of hypertension\\nLOS\\nRecipient length of stay post-transplant\\nOth_Tobacco\\nOther tobacco use\\nPack_Yrs\\nIf history of cigarette use, number of\\npack years\\nTable 2\\nThe 1st set of candidate covariates generated from RBF-SVM.\\nVariables\\nExplanation\\nCitizenship\\nRecipient citizenship @ registration\\nContin_alcohol_old_don\\nDeceased donor-history of alcohol\\ndependency + recent 6 months use\\nContin_iv_drug_old_don\\nDeceased donor-history of iv drug use\\n+ recent 6 months use\\nCreat2_old\\nMost recent creatinine >2.0 mg/dl y/n\\nDa2\\nDonor a2 antigen\\nDantiarr_old\\nDeceased donor given antiarrythmics 24 h\\nprior to cross-clamp\\nDayswait_chron\\nActive days on waiting list\\nDobut_don_old\\nDeceased donor-dobutamine w/in 24 h\\npre-cross-clamp\\nEducation\\nRecipient highest educational level @\\nregistration\\nEthcat_don\\nDonor ethnicity category\\nFluvaccine\\nAnti-viral treatment—ﬂuvaccine\\nFunc_stat_tcr\\nRecipient functional status @ registration\\nFunc_stat_trr\\nRecipient functional status @transplant\\nGender\\nRecipient gender\\nHbsab_don\\nDeceased donor hbsab test result\\nHemo_pa_dia_tcr\\nMost recent hemodynamics pa (dia) mm/hg\\n@ registration\\nHemo_pa_mn_tcr\\nMost recent hemodynamics pa (mean) mm/hg\\n@ registration\\nHeparin_don\\nDeceased donor management—heparin\\nHgt_cm_tcr\\nRecipient height @ registration\\nHist_alcohol_old_don\\nDeceased donor-history of alcohol\\ndependency\\nHtlv2_old_don\\nDeceased donor-antibody to htlv ii result\\nImpl_deﬁbril_after_list\\nImplantable deﬁbrillator inserted between\\nlisting and transplant\\nInotrop_agents\\nDeceased donor—three or more inotropic\\nagents at time of incision\\nInotrop_support_don\\nDeceased donor inotropic medication at\\nprocurement (y/n)\\nIschtime\\nIschemic time in hours\\nMed_cond_tcr\\nRecipient medical condition @ registration\\nMed_cond_trr\\nRecipient medical condition pretransplant\\n@ transplant\\nPhysical_capacity_tcr\\nPhysical capacity at listing\\nPretreat_med_don_old\\nDeceased donor medication(s) from brain\\ndeath to 24 h prior to procurement\\nPrior_lung_surg_tcr\\nRecipient prior lung surgery (non-transplant)\\nat listing\\nPst_airway\\nEvents prior to discharge: airway dehiscence\\nPst_cardiac\\nEvents prior to discharge: cardiac re-operation\\nPst_dial\\nEvents prior to discharge: dialysis\\nPst_drug_trt_infect\\nEvents prior to discharge: any drug\\ntreated infection\\nPst_surgical\\nEvents prior to discharge: other surgical\\nprocedures\\nPt_t4_don\\nDeceased donor-thyroxine-t4 b/n brain\\ndeath w/in 24 h of procurement\\nSternotomy_tcr\\nEvents occurring prior to listing: sternotomy\\nSternotomy_trr\\nEvents occurring between listing and\\ntransplant: sternotomy\\nSteroid\\nChronic steroid use y/n/u @ transplant\\nTrtrej1y\\nTreated for rejection within 1 year\\nTrt_pulm_sepsis\\nIV treated pulmonary sepsis y/n/u @\\nregistration\\nVad_tah_tcr\\nRecipient on life support—ventilator @\\nregistration (1 = yes, 0 = no)\\nVad_tah_trr\\nRecipient on life support—ventilator @\\ntransplant\\nTable 4\\nThe 3rd set of candidate covariates.\\nVariables\\nExplanation\\nABO\\nRecipient blood group at registration\\nABO_Don\\nDonor blood type\\nABO_Mat\\nDonor-recipient ABO match level\\nAge\\nRecipient age (years)\\nAge_Don\\nDonor age (years)\\nDayswait_Chron\\nActive days on waiting list\\nDon_TY\\nDonor type—deceased/living\\nEthcat\\nRecipient ethnicity category\\nEthcat_Don\\nDonor ethnicity category\\nGender\\nRecipient gender\\nGender_Don\\nDonor gender\\nHbsab_Don\\nDeceased donor hbsab test result\\nIschtime\\nIschemic time in hours\\nMed_Cond_Tcr\\nRecipient medical condition at registration\\nMed_Cond_Trr\\nRecipient medical condition pretransplant\\nat transplant\\nWgt_kg_Don\\nDonor weight (kg)\\nWgt_kg_Tcr\\nRecipient weight (kg) at registration\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n39\\n\\n0.1 for removal as signiﬁcance threshold criteria. The predictor\\nvariables determined to be signiﬁcant by Cox regression model are\\nlisted along with their corresponding statistics in Table 5. The rest\\nof the variables (which were in Tables 2, 3, or 4 but not in Table 5)\\nwere eliminated since they were found to be insigniﬁcant by Cox\\nregression model.\\nAs listed in Table 5, 14 of the variables had prognostic value\\nwhich are determined by the Cox model as signiﬁcant and kept in\\nthe Cox equation. Therefore, they were used to calculate the PIs by\\nmeans of Eq. (6). The PI values received here were ranging between\\n0 and 3.\\n3.4. Clustering the prognostic indices\\nOnce the prognostic indices (PIs) for each recipient calculated,\\nthe next step was to cluster the recipients through these PIs.\\nHowever, the problem of deﬁning these clusters and deciding\\nwhich value to cut off and categorize the recipients should be\\nsolved\\nﬁrst.\\nTwo\\ncommonly\\nused\\nclustering\\nalgorithms\\nas\\ndescribed in Section 2.5, namely k-means and TSCA were used\\nto determine these clusters. We also compared these algorithm-\\nbased clusters to conventional PI devising methods in medicine.\\nTwo potential ways to do the clustering are constructing equal-\\nwidth PIs and equal-percentile PIs in this research domain. In the\\nformer one, the PIs are separated in groups so that the increments\\nof PI in each group are equal whereas the latter method focuses on\\nallocating the patients equally to each group. The algorithms k-\\nmeans and TSCA were run by changing the value for k (number of\\nclusters to be formed). The value of k with 2, 3, 4, and 5 were tried\\nbecause it was considered that having clusters more than 5 would\\nnot provide logical risk groups to categorize and would probably\\nnot be easy to name and interpret medically afterwards. The\\nresults for each run are represented in Table 6.\\nThe performances of these entire four approaches with different\\nnumber of clusters (k = 2–5) were compared using intraclass inertia\\nas the performance measure to decide which one to adopt. It is a\\nmeasure which shows how compact each cluster is. Intraclass\\ninertia is the average of the distances between the means and the\\nobservations in each cluster. Eq. (7) indicates this value for given k\\nnumber of clusters [53].\\nFðkÞ ¼ 1\\nn\\nX\\nk\\nX\\ni 2 Ck\\nX\\nm\\nP¼1\\nðXiP \\x02 mkPÞ2\\n(7)\\nwhere n is the number of total observations, CK is the set of kth\\ncluster, XiP is the value of the attribute P for observation i and mkP is\\nthe mean of the attribute P in the kth cluster. Note that in our case\\nthere is only one attribute which is PI, and hence m = 1.\\nThe intraclass inertia values for each possible cluster are also\\nsummarized in Table 6. Prognostic indices were clustered best with\\nk = 3 with k-means clustering algorithm in our case as seen in\\nTable 6 considering its low intraclass inertia value. As seen in\\nTable 6, this classiﬁcation not only gives the lowest intraclass\\ninertia value but also provides an even distribution of the thoracic\\npatients for our nation-wide dataset (38%, 16%, and 46% for low,\\nmedium, and high risk groups of patients, respectively). Although 5\\nclusters with k-means algorithm and 3 clusters in two-step cluster\\nanalysis perform very close to k-means algorithm with 3 clusters,\\nneither of them provides such an even distribution of patients.\\nNote that in addition to considerably higher inertia scores,\\nheuristic calculation with equal-width PIs distribute the nation-\\nwide patients highly skewed to lower tails of risk groups for all ﬁve\\npotential cluster formations. Therefore, we conclude that the k-\\nmeans algorithm based clustering performs better than the other\\npotential groupings in terms of both objective and subjective\\naspects.\\n3.5. Validation of risk groups by Kaplan–Meier survival analysis\\nTo validate the established prognostic indices with 3 clusters\\nand hence the various risk groups in Section 3.4, Kaplan–Meier\\nsurvival analysis [43] was conducted. The corresponding PI\\nclusters were matched with the patients and their predictor\\nvariables from Table 5. In Kaplan–Meier survival analysis the\\npredictor variables were used as explanatory variables and the PI-\\nbased clusters were used as the strata variable to label the patients\\nwith different risks. The main objective here was to compare\\nsurvivor functions for different risk groups of thoracic recipients.\\nIf the survivor function for one risk group is always higher than the\\nsurvivor function for another risk group, than the ﬁrst group\\nclearly lives longer than the second one. The less the survivor\\nfunctions cross, the better the discrimination of the patients\\nwould be. Fig. 2 shows this clear distinction for k-means\\nalgorithm-based PIs.\\nIn order to show that there is a statistically signiﬁcant\\ndifference among these three risk groups, the test of equality\\nover strata was also conducted. Test of equality over strata\\ncontains rank and likelihood-based statistics for testing homoge-\\nneity of survivor functions across strata. The rank tests with the\\nlog-rank test and Wilcoxon test indicate a signiﬁcant difference\\nbetween the risk groups. These results are also supported by\\nlikelihood-based\\nstatistics.\\nThese\\nstatistical\\ntest\\nresults\\nare\\nsummarized in Table 7.\\nTable 5\\nThe variables kept in the Cox regression model.\\nVariable\\nSE\\nChi_square test\\nDF\\nSigniﬁcance\\nexp(b)\\n95% CI for exp(b)\\nLower\\nUpper\\nLOS\\n0.0002\\n385.8701\\n1\\n<.0001\\n1.004\\n1.004\\n1.005\\nEint\\n0.0178\\n56.9447\\n1\\n<.0001\\n0.844\\n0.844\\n0.905\\nGint\\n0.0183\\n11.8644\\n1\\n0.0006\\n0.906\\n0.906\\n0.973\\nAge_Don\\n0.0006\\n247.3162\\n1\\n<.0001\\n1.009\\n1.009\\n1.011\\nWgt_kg_Tcr\\n0.0004\\n5.5091\\n1\\n0.0189\\n0.998\\n0.998\\n1.000\\nWgt_kg_Don\\n0.0005\\n21.3483\\n1\\n<.0001\\n0.997\\n0.997\\n0.999\\nAcyclovir\\n0.0300\\n14.6651\\n1\\n0.0001\\n0.840\\n0.840\\n0.945\\nCitizenship\\n0.0554\\n5.5538\\n1\\n0.0184\\n0.787\\n0.787\\n0.978\\nDayswait_Chron\\n0.0002\\n7.5318\\n1\\n0.0061\\n1.000\\n1.000\\n1.000\\nFluvaccine\\n0.0189\\n15.9915\\n1\\n<.0001\\n1.039\\n1.039\\n1.119\\nIschtime\\n0.0058\\n239.5080\\n1\\n<.0001\\n1.081\\n1.081\\n1.105\\nMed_Cond_Tcr\\n0.0109\\n75.6231\\n1\\n<.0001\\n1.076\\n1.076\\n1.123\\nVad_Tah_Trr\\n0.0002\\n5.7861\\n1\\n0.0162\\n1.000\\n1.000\\n1.001\\nVad_Tah_Tcr\\n0.0077\\n48.9955\\n1\\n<.0001\\n1.040\\n1.040\\n1.072\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n40\\n\\n4. Conclusions and future research directions\\nThis study demonstrates that machine learning-based meth-\\nodology for selecting predictor variables in survivability and\\nprognostic modeling of thoracic organ transplantation is superior\\nto the approaches adopting only expert-selected variables. The\\nstudy showed that of the comprehensive list of predictors, some\\nhave been included in the previous studies (such as gender and age\\nof the recipient, his/her medical condition at registration) while\\nsome others (which are found to be critical) have been absent from\\nthe related literature. These variables (e.g. such as recipient length\\nof stay post-transplant and the interaction of gender and ethnicity\\nbetween the recipient and the donor) should be combined with the\\nfactors identiﬁed in previous studies to better understand and\\nimprove the organ transplantation process.\\nThe study revealed that based on k-means clustering algorithm\\nthe thoracic organ recipients should be allocated into an optimal\\nnumber of ‘‘three’’ risk groups, namely low, medium, and high. This\\nﬁnding conﬁrms the conventional medical discrimination com-\\nmonly used in this ﬁeld of study. However, it also proves that this\\ngrouping should be better performed through a data mining\\nperspective rather than a heuristics-based approach because the\\nlatter one gives more skewed distribution of patients for our US\\nnation-wide\\ndataset.\\nThis\\nis\\nthe\\npoint\\nwhere\\nthe\\nmedical\\nprofessionals should be advised to handle the problem in the\\nfuture.\\nSome of the research extensions to the study reported in this\\narticle includes analysis of other organ types as well as the analysis\\nFig. 2. Kaplan–Meier survival curves for three PIs.\\nTable 7\\nTests of equality over risk groups for k-means based three PI cluster.\\nTest\\nChi-square\\nDF\\nPr > Chi-square\\nLog-rank\\n1002.6135\\n2\\n<.0001\\nWilcoxon\\n939.7492\\n2\\n<.0001\\n\\x022 log(LR)\\n1013.3153\\n2\\n<.0001\\nTable 6\\nThe comparative results for clustering and heuristic-based algorithms.\\nBy clustering algorithms\\nk-Means algorithm\\nTwo-step cluster analysis\\nNumber of clusters\\nRisk group\\nPrognostic index\\nNumber of patients\\nIntraclass inertia\\nPrognostic index\\nNumber of patients\\nIntraclass inertia\\nCluster 1\\nLow\\n0–0.69\\n21163 (58%)\\n12.4 \\x03 10\\x028\\n0–1.09\\n34199 (94%)\\n866.30 \\x03 10\\x028\\nCluster 2\\nHigh\\n0.70–3\\n15262 (41%)\\n1.1–3\\n2226 (6%)\\nCluster 1\\nLow\\n0–0.56\\n13766 (38%)\\n1.68 \\x03 10\\x028\\n0–1.04\\n33529 (92%)\\n2.20 \\x03 10\\x028\\nCluster 2\\nMedium\\n0.57–0.91\\n5834 (16%)\\n1.05–1.83\\n2807 (7.7%)\\nCluster 3\\nHigh\\n0.92–3\\n16825 (46%)\\n1.84–3\\n89(0.3%)\\nCluster 1\\nLow\\n0–0.49\\n15227 (42%)\\n11.2 \\x03 10\\x028\\n0–0.41\\n6410 (17%)\\n445.39 \\x03 10\\x028\\nCluster 2\\nLow–medium\\n0.50–0.77\\n1764 (5%)\\n0.42–0.70\\n15163 (42%)\\nCluster 3\\nMedium–high\\n0.78–1.12\\n9542 (26%)\\n0.71–1.04\\n11892(33%)\\nCluster 4\\nHigh\\n1.13–3\\n9892 (27%)\\n1.05–3\\n2960 (8%)\\nCluster 1\\nVery low\\n0–0.44\\n13266 (36%)\\n3.02 \\x03 10\\x028\\n0–0.36\\n2960 (8%)\\n720.71 \\x03 10\\x028\\nCluster 2\\nLow\\n0.45–0.69\\n451(1%)\\n0.37–0.53\\n10475 (29%)\\nCluster 3\\nMedium\\n0.70–0.95\\n4449 (12%)\\n0.54–0.73\\n10815 (29%)\\nCluster 4\\nHigh\\n0.96–1.39\\n7814 (22%)\\n0.74–1.04\\n4674(13%)\\nCluster 5\\nVery high\\n1.40–3\\n10445 (29%)\\n1.05–3\\n7501 (21%)\\nBy heuristics-based calculation\\nWith equal PI widths\\nWith equal percentiles\\nNumber of clusters\\nRisk group\\nPrognostic index\\nNumber of patients\\nIntraclass inertia\\nPrognostic index\\nNumber of patients\\nIntraclas s inertia\\nCluster 1\\nLow\\n0–1.5\\n36154(99%)\\n713.68 \\x03 10\\x028\\n0–0.64\\n18212(50%)\\n7.02 \\x03 10\\x026\\nCluster 2\\nHigh\\n1.6–3\\n271 (1%)\\n0.65–3\\n18213 (50%)\\nCluster 1\\nLow\\n0–0.9\\n32571 (89%)\\n1678.65 \\x03 10\\x028\\n0–0.53\\n12142 (33.5%;\\n2.01 \\x03 10\\x026\\nCluster 2\\nMedium\\n1–1.9\\n3794 (10%)\\n0.54–0.76\\n12141 (33%)\\nCluster 3\\nHigh\\n2–3.0\\n60 (1%)\\n0.77–3\\n12142 (33.5%;\\nCluster 1\\nLow\\n0–0.7\\n26087 (72%)\\n12961.43 \\x03 10\\x028\\n0–0.47\\n9106 (25%)\\n2755.48 \\x03 10\\x026\\nCluster 2\\nLow–medium\\n0.8–1.5\\n10153 (28%)\\n0.48–0.64\\n9106(25%)\\nCluster 3\\nMedium–high\\n1.6–2.3\\n162(0.4%)\\n0.65–0.82\\n9106 (25%)\\nCluster 4\\nHigh\\n2.4–3\\n23 (0.06%)\\n0.83–3\\n9107 (25%)\\nCluster 1\\nVery low\\n0–0.5\\n15605 (43%)\\n457.67 \\x03 10\\x028\\n0–0.43\\n7285 (20%)\\n3.16 \\x03 10\\x026\\nCluster 2\\nLow\\n0.6–1.1\\n19608 (54%)\\n0.44–0.58\\n7285 (20%)\\nCluster 3\\nMedium\\n1.2–1.7\\n1109(3%)\\n0.59–0.71\\n7285 (20%)\\nCluster 4\\nHigh\\n1.8–2.3\\n80 (0.2%)\\n0.72–0.87\\n7285 (20%)\\nCluster 5\\nVery high\\n2.4–3\\n23 (0.06%)\\n0.88–3\\n7285 (20%)\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n41\\n\\nof multiorgan scenarios where the correlations among the organs\\ncoming from the same donor are also included in the formulation\\nof the problem. Another potential further research direction of this\\nstudy is to validate the patterns obtained from the data mining\\nmodels with a comprehensive simulation model of the organ\\ntransplantation process. Using actual cases, a comprehensive\\ndiscrete-event simulation model can be developed and used as a\\ntest-bed where the potential beneﬁts and limitations of these\\nnovel patterns are tested and validated for a sufﬁciently long\\nperiod of time in the computer simulation environment.\\nReferences\\n[1] Trigt PV, Davis D, Shaeffer GS, Gaynor JW, Landolfo KP, Higginbotham MB, et al.\\nSurvival beneﬁts of heart and lung transplantation. Annals of Surgery\\n1996;223:576–84.\\n[2] Pierson RN, Barr ML, McCullough KP, Egan T, Garrity E, Jessup M, et al. Thoracic\\norgan transplantation. American Journal of Transplantation 2004;4:93–105.\\n[3] Sheppard D, McPhee D, Darke C, Shretha B, Moore R, Jurewitz A, et al. Predicting\\ncytomegalovirus disease after renal transplantation: an artiﬁcial neural network\\napproach. International Journal of Medical Informatics 1999;54:\\n55–76.\\n[4] Lin RS, Horn SD, Hurdle JF, Goldfarb-Rumyantzev S. Single and multiple time-\\npoint prediction models in kidney transplant outcomes. Journal of Biomedical\\nInformatics 2008;41:944–52.\\n[5] Parmar MKB, Machin D. Survival analysis: a practical approach. Cambridge,\\nUK: John Wiley & Sons; 1996.\\n[6] Cox DR. Analysis of survival data. London: Chapman&Hall; 1984.\\n[7] Hariharan S, Johnson CP, Bresnahan BA, Taranto SE, McIntosh MJ, Stablein D.\\nImproved graft survival after renal transplantation in the United States, 1988\\nto 1996. The New England Journal of Medicine 2000;342:605–12.\\n[8] Herrero JI, Lucena JF, Quiroga J, Sangro B, Pardo F, Rotellar F, et al. Liver transplant\\nrecipients older than 60 years have lower survival and higher incidence of\\nmalignancy. American Journal of Transplantation 2003;3:1407–12.\\n[9] Hong Z, Wu J, Smart G, Kaita K, Wen SW, Paton S, et al. Survival analysis of liver\\ntransplant patients in Canada. Transplantation Proceedings 2006;38:2951–6.\\n[10] Kusiak A, Dixon B, Shah S. Predicting survival time for kidney dialysis patients:\\na data mining approach. Computers in Biology and Medicine 2005;35:311–27.\\n[11] Jenkins PC, Flanagan MF, Jenkins KJ, Sargent JD, Canter CE, Chinnock RE, et al.\\nSurvival analysis and risk factors for mortality in transplantation and staged\\nsurgery for hypoplastic left heart syndrome. Journal of the American College of\\nCardiology 2000;36:1178–85.\\n[12] Fernandez-Yanez J, Palomo J, Torrecilla EG, Pascual D, Garrido G, de Diego JJG,\\net al. Prognosis of heart transplant candidates stabilized on medical therapy.\\nRevista Espanola de Cardiologia 2005;58:1162–70.\\n[13] Tjang YS, Heijdan GJMG, Tenderich G, Grobbee D, Korfer R. Survival analysis in\\nheart transplantation: results from an analysis of 1290 Cases in a single center.\\nEuropean Journal of Cardio-Thoracic Surgery 2008;33:856–61.\\n[14] Lin HM, Kaufmann HM, McBride MA, Davies DB, Rosendale JD, Smith CM, et al.\\nCenter-speciﬁc graf and patient survival rates: 1997 UNOS report. JAMA\\n1998;280:1153–60.\\n[15] Cope JT, Kaza AK, Reade CC, Shockey KS, Kern JA, Tribble CG, et al. A cost\\ncomparison of heart transplantation versus alternative operations for cardio-\\nmyopathy. Annual thoracic Surgery 2001;72:1298–305.\\n[16] Aguero J, Almenar L, Martinez-Dolz L, Moro J, Izquierdo MT, Cano O, et al.\\nDifferences in clinical proﬁle and survival after heart transplantation accord-\\ning to prior heart disease. Transplantation Proceedings 2007;39:2350–2.\\n[17] Christensen E, Gunson B, Neuberger J. Optimal timing of liver transplantation\\nfor patients with primary biliary cirrhosis: use of prognostic modeling. Journal\\nof Hepatology 1999;30:285–92.\\n[18] Yoo HY, Galabova V, Edwin D, Thuluvath PJ. Socioeconomicstatus does not affect\\nthe outcome of liver transplantation. Liver Transplantation 2002;8:1133–7.\\n[19] Deng MC, DeMeester MJ, Smiths JMA, Heinecke J, Scheld HH. Effect of receiving\\na heart transplant: analysis of a national cohort entered on to waiting list,\\nstratiﬁed by heart failure severity. British Medical Journal 2000;321:540–5.\\n[20] Ghobrial IM, Habermann TM, Maurer MJ, Geyer SM, Ristow KM, Larson TS,\\net al. Prognostic analysis for survival in adult solid organ transplant recipients\\nwith posy-transplantation lymphoproliferative disorders. Journal of Clinical\\nOncology 2005;23:7574–82.\\n[21] Harper AM, Taranto SE, Edwards EB, Daily OP. An update on a successful\\nsimulation project: the UNOS liver allocation model. In: Joines JA, Barton RR,\\nKang K, Fishwick PA, editors. Proceedings of the winter simulation conference.\\nNew York, NY: ACM Publications; 2000. p. 1955–62.\\n[22] Cupples SA, Ohler L. Transplantation nursing secrets. St. Louis, MO: Hanley &\\nBelfus Publication; 2002.\\n[23] Cristianini N, Shawe-Taylor J. An introduction to support vector machines and\\nother Kernel-based learning methods. London: Cambridge University Press;\\n2000.\\n[24] Mitchell T. Machine learning. New York, NY: McGraw-Hill; 1997.\\n[25] Haykin S. Neural networks: a comprehensive foundation. Upper Saddle River,\\nNJ: Prentice Hall; 1998.\\n[26] Bellazzi R, Zupan B. Predictive data mining in clinical medicine: current issues\\nand guidelines. International Journal of Medical Informatics 2008;77:81–97.\\n[27] Dreiseitl S, Ohno-Machado L. Logistic regression and artiﬁcial neural network\\nclassiﬁcation models: a methodology review. Journal of Biomedical Informat-\\nics 2002;35:352–9.\\n[28] Efron B, Tibshirani R. Statistical data analysis in the computer age. Science\\n1991;253:390–5.\\n[29] Breiman L, Friedman JH, Olshen RA, Stone CJ. Classiﬁcation and regression\\ntrees. Monterey, CA: Wadsworth & Brooks/Cole Advanced Books & Software;\\n1984.\\n[30] Kass GV. An exploratory technique for investigating large quantities of cate-\\ngorical data. Applied Statistics 1980;29:119–27.\\n[31] Quinlan JR. Learning with continuous classes.\\nIn: Adams, Sterling, editors.\\nProceedings of 5th Australian joint conference on artiﬁcial intelligence. Sin-\\ngapore: World Scientiﬁc; 1992. p. 343–8.\\n[32] Makridakis S, Wheelwright SC, Hyndman RJ. Forecasting: methods and appli-\\ncations. New York, NY: John Wiley and Sons; 1998.\\n[33] Everitt BS. Cambridge dictionary of statistics. Cambridge, UK: Cambridge\\nUniversity Press; 2002.\\n[34] Kohavi R. A study of cross-validation and bootstrap for accuracy estimation\\nand model selection. In: Boutilier C, editor. Proceedings of the 14th interna-\\ntional conference on AI (IJCAI). San Mateo, CA: Morgan Kaufmann; 1995. p.\\n1137–45.\\n[35] Olson DL, Delen D. Advanced data mining techniques. New York, NY: Springer;\\n2008.\\n[36] Davis G. Sensitivity analysis in neural net solutions. IEEE Transactions on\\nSystems Man and Cybernetics 1989;19:1078–82.\\n[37] Principe JC, Euliano NR, Lefebvre WC. Neural and adaptive systems. New York,\\nNY: John Wiley and Sons; 2001.\\n[38] Saltelli A. Making best use of model evaluations to compute sensitivity indices.\\nComputer Physics Communications 2002;145:280–97.\\n[39] Saltelli A, Tarantola S, Campolongo F, Ratto M. Sensitivity analysis in practice—\\na guide to assessing scientiﬁc models. New York, NY: John Wiley and Sons;\\n2004.\\n[40] Ohno-Machado L. Modeling medical prognosis: survival analysis techniques.\\nJournal of Biomedical Informatics 2001;34:428–39.\\n[41] Grambsch P, Therneau T. Proportional hazards rates and diagnostics based on\\nweighted residuals. Biometrika 1994;81:515–26.\\n[42] Christensen E. Multivariate survival analysis using Cox’s regression model.\\nHepatology 1987;7:1346–58.\\n[43] Kaplan E, Meier P. Nonparametric estimation from incomplete observations.\\nJournal of the American Statistical Association 1958;53:187–220.\\n[44] MacQueen JB. Some methods for classiﬁcation and analysis of multivariate\\nobservations.\\nIn: Le Cam LM, Neyman J, editors. Proceedings of the ﬁfth\\nsymposium on math, statistics, and probability. Berkeley, CA, USA: University\\nof California Press; 1967. p. 281–97.\\n[45] Krishna K, Murty MN. Genetic k-means algorithm. IEEE Transactions on\\nSystems Man and Cybernetics-Part B Cybernetics 1999;29:433–9.\\n[46] Chiu T, Fang D, Chen J, Wang Y, Jeris C. A robust and scalable clustering\\nalgorithm for mixed type attributes in large database environment. In: Lee D,\\neditor. Proceedings of the seventh ACM SIGKDD international conference on\\nknowledge discovery and data mining. New York, NY: ACM Publications; 2001.\\np. 263.\\n[47] Li ZH, Luo P. Statistical analysis lectures of SPSS for windows. Beijing, China:\\nBeijing Publishing House of Electronics Industry; 2004.\\n[48] SPSS\\nInc.\\nPASW\\nModeler\\nData\\nMining\\nToolkit,\\nVersion\\n13.0,\\nhttp://\\nwww.spss.com/software/modeling/modeler/ 2009 (accessed: June 5, 2009).\\n[49] SAS Institute Inc. Statistical Analysis Systems, Version 9.1.3, http://www.sas.\\ncom/technologies/analytics/statistics/stat/ 2008 (accessed: May 11, 2009).\\n[50] Hair JF, Anderson RE, Tatham RL, Black W. Multivariate data analysis. Upper\\nSaddle River, NJ: Prentice Hall; 1998.\\n[51] Johnson DE. Applied multivariate methods for data analysts. Paciﬁc Grove, CA:\\nDuxbury Press; 1998.\\n[52] Oztekin A, Delen D, Kong ZJ. Predicting the graft survival for heart–lung\\ntransplantation patients: An integrated data mining methodology. Interna-\\ntional Journal of Medical Informatics 2009;78:84–96.\\n[53] Michaud P. Clustering techniques. Future Generation Computer Systems\\n1997;13:135–47.\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n42\\n'}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/Artificial Intelligence in Medicine.pdf', 'text': 'A machine learning-based approach to prognostic analysis of thoracic\\ntransplantations\\nDursun Delen a,*, Asil Oztekin b,c, Zhenyu (James) Kong b\\na Spears School of Business, Oklahoma State University, T-NCB 378, 700 North Greenwood Avenue, Tulsa, OK, 74106, USA\\nb School of Industrial Engineering and Management, Oklahoma State University, 322 Engineering North, Stillwater, OK 74078, USA\\nc Department of Industrial Engineering, Gediz University, 35230 Cankaya-Izmir, Turkey\\n1. Introduction\\n1.1. Motivation\\nThoracic (heart and lung) transplantation has been accepted as a\\nviable treatment for end-stage cardiac and pulmonary failure. The\\nArtiﬁcial Intelligence in Medicine 49 (2010) 33–42\\nA R T I C L E\\nI N F O\\nArticle history:\\nReceived 25 February 2009\\nReceived in revised form 15 December 2009\\nAccepted 10 January 2010\\nKeywords:\\nData mining\\nMachine learning\\nUNOS\\nThoracic Transplantation\\nSurvival analysis\\nPrognostic index\\nA B S T R A C T\\nObjective: The prediction of survival time after organ transplantations and prognosis analysis of different\\nrisk groups of transplant patients are not only clinically important but also technically challenging. The\\ncurrent studies, which are mostly linear modeling-based statistical analyses, have focused on small sets\\nof disparate predictive factors where many potentially important variables are neglected in their\\nanalyses. Data mining methods, such as machine learning-based approaches, are capable of providing an\\neffective way of overcoming these limitations by utilizing sufﬁciently large data sets with many\\npredictive factors to identify not only linear associations but also highly complex, non-linear\\nrelationships. Therefore, this study is aimed at exploring risk groups of thoracic recipients through\\nmachine learning-based methods.\\nMethods and material: A large, feature-rich, nation-wide thoracic transplantation dataset (obtained from\\nthe United Network for Organ Sharing—UNOS) is used to develop predictive models for the survival time\\nestimation. The predictive factors that are most relevant to the survival time identiﬁed via, (1) conducting\\nsensitivity analysis on models developed by the machine learning methods, (2) extraction of variables\\nfrom the published literature, and (3) eliciting variables from the medical experts and other domain\\nspeciﬁc knowledge bases. A uniﬁed set of predictors is then used to develop a Cox regression model and\\nthe related prognosis indices. A comparison of clustering algorithm-based and conventional risk\\ngrouping techniques is conducted based on the outcome of the Cox regression model in order to identify\\noptimal number of risk groups of thoracic recipients. Finally, the Kaplan–Meier survival analysis is\\nperformed to validate the discrimination among the identiﬁed various risk groups.\\nResults: The machine learning models performed very effectively in predicting the survival time: the\\nsupport vector machine model with a radial basis Kernel function produced the best ﬁt with an R2 value of\\n0.879, the artiﬁcial neural network (multilayer perceptron-MLP-model) came the second with an R2\\nvalue of 0.847, and the M5 algorithm-based regression tree model came last with an R2 value of 0.785.\\nFollowing the proposed method, a consolidated set of predictive variables are determined and used to\\nbuild the Cox survival model. Using the prognosis indices revealed by the Cox survival model along with a\\nk-means clustering algorithm, an optimal number of ‘‘three’’ risk groups is identiﬁed. The signiﬁcance of\\ndifferences among these risk groups are also validated using the Kaplan–Meier survival analysis.\\nConclusions: This study demonstrated that the integrated machine learning method to select the predictor\\nvariables is more effective in developing the Cox survival models than the traditional methods commonly\\nfound in the literature. The signiﬁcant distinction among the risk groups of thoracic patients also validates\\nthe effectiveness of the methodology proposed herein. We anticipate that this study (and other AI based\\nanalytic studies like this one) will lead to more effective analyses of thoracic transplant procedures to better\\nunderstand the prognosis of thoracic organ recipients. It would potentially lead to new medical and\\nbiological advances and more effective allocation policies in the ﬁeld of organ transplantation.\\n\\x02 2010 Elsevier B.V. All rights reserved.\\n* Corresponding author. Tel.: +1 918 594 8283; fax: +1 918 594 8283.\\nE-mail address: dursun.delen@okstate.edu (D. Delen).\\nContents lists available at ScienceDirect\\nArtificial Intelligence in Medicine\\njournal homepage: www.elsevier.com/locate/aiim\\n0933-3657/$ – see front matter \\x02 2010 Elsevier B.V. All rights reserved.\\ndoi:10.1016/j.artmed.2010.01.002\\n\\nincreased experience in cardiac and pulmonary transplantation,\\nimprovements in patient selection, organ preservation, and preop-\\nerativesupporthavesigniﬁcantlyreducedtheearlythreatstopatient\\nsurvival [1]. Over the past decade, the thoracic transplant waiting\\ntime for a listed patient has markedly increased, but the number of\\ntransplants performed has declined. In addition, the research also\\nfound thatthere is a perceived inequity inaccesstoorgans. Theorgan\\nallocation system needs to be improved since it may become a major\\nfactor negatively inﬂuencing the survivability of thoracic transplant\\n[2].\\nThe survivability prediction is becoming increasingly more\\nimportant in medicine. When a resource is scarce, the need for\\naccurate prediction becomes acute [3]. Especially prediction of\\nsurvival time and prognosis prediction of medical treatments are\\nclinically important and challenging problems [4]. Scarceness of\\norgans necessitates the development of effective and efﬁcient\\nprocedures to select the most optimal organ receiver since demand\\nfor organs of all patients might not be satisﬁed. To achieve this, one\\ncritical step is to reveal the knowledge underlying huge amount of\\ndata collected and stored from organ transplantation procedures\\nperformedinthepast.Theobjectivesare(1)tomaximizethepatients’\\nsurvival time after the organ transplantation surgery, and (2) to\\noptimize the prognosis for the organ recipients. These can be\\npotentially achieved by discovering the knowledge that may be\\ncontained in large dataset consisting of more than hundreds of\\ndeterminative variables regarding the donors, the potential recipi-\\nents, and transplantation procedures. Therefore, in this study a data\\nmining method is proposed to process large amount of transplanta-\\ntion data obtained from UNOS to identify the important factors as\\nwell as their relationships to the survival of the graft and the patient.\\nThereafter, a prognostic index [5,6] is developed to classify the\\npatients into different risk groups for better understanding of the\\ntransplantation phenomenon. In short, this study will address the\\nfollowing questions: (1) what are the most important variables to be\\nincluded in an effective prognostic index related to thoracic organ\\ntransplantations?(2)whatarethemostcoherentriskgroupsthatcan\\nbe formed based on the prognostic index? Predicting the thoracic\\nsurvivability and classifying the patients (potential thoracic organ\\nreceivers)intodifferentclassesofriskswouldhelpdecisionmakersin\\ndeterminingpatients’priorityfortransplantationsourceassignment.\\n1.2. Literature review\\n1.2.1. Related research in survival analysis for organ transplantation\\nIn the recent past, a number of studies were conducted using\\ndata-driven analytics on various organ transplantation datasets.\\nClosely related to the study reported herein, Hariharan et al. [7]\\nfocused on the analysis of improved graft survival rate using\\ncyclosporine after renal transplantation in both short-term (less\\nthan 1 year) and long-term (more than 1 year). A regression analysis\\nwas used to predict the probability of the graft failure after kidney\\ntransplantation in both short-term and long-term period in the light\\nof demographic characteristics, transplant-related variables, and\\npost-transplantation variables. The study performed by Herrero\\net al. [8] included 116 patients who received a liver transplant\\nbetween the years 1994 and 2000. Statistical tests are used to\\ncompare the demographic and characteristic variables, pretrans-\\nplant, andintra-operativevariables betweenthetwo groups, namely\\nyounger and older than 60. The results indicate that there is a clear\\ntrend showing that older patients have lower survival after liver\\ntransplantation. Hong et al. [9] presented a survival analysis of liver\\ntransplant patients in Canada by considering some factors such as\\nage, blood type, donor type (cadaveric or alive), race, and gender of\\nrecipient and donors. However, having limited the variables with\\nthis scope, they also admitted that the clinical information lacks of\\nmany potential details.\\nTaking a data mining approach, Kusiak et al. [10] compared two\\nrule-based data mining techniques, i.e. decision trees and rough\\nsets, to predict survival time of kidney dialysis patients. This study\\nachieved satisfactorily high prediction accuracy. The main limita-\\ntion of the study was the utilization of a small dataset with only\\n188 patients in total and also many patient-related parameters\\nwere neglected in the problem formulation. Using more traditional\\nmethods, and speciﬁcally having focused on thoracic transplanta-\\ntion, Jenkins et al. [11] and Fernandez-Yanez et al. [12] had a rich\\npool of independent variables for survivability prediction. Their\\nstudies used popular statistical techniques such as Kaplan–Meier\\nmethod of survival analysis with Mantel–Haenszel log-rank test.\\nHowever, both of these techniques have been criticized with two\\nmajor limitations: (1) linear relationships are assumed, which\\nhence cannot capture the nonlinearity among the variables, and (2)\\nthe independent variables were selected solely based on the\\nexperiences and intuitions of the analysts who conducted these\\nstudies. Thus, many potentially signiﬁcant variables might be left\\noutside the scope of this study. Tjang et al. [13] added more\\nexplanatory variables to determine the survivability in heart\\ntransplantation, such as body mass index, waiting time on the list,\\nand previous cardiac surgery, their study also ignored the non-\\nlinear relationships among the pool of survivability-related\\nvariables. Similar limitations exist in some other studies focused\\ndirectly or indirectly on thoracic transplantation [14–16].\\nThe existing studies implicitly assume that the relationships\\namong the predictive variables and output variable are linear and\\nthe predictor variables are independent of each other, which may\\nnot be valid in reality. Moreover, the abovementioned studies focus\\non\\nsmall\\ndatasets\\nwith\\nlimited\\nnumber\\nof\\npredictors\\nfor\\nsurvivability of patients after transplantation. This limitation\\nmay cause incomprehensive modeling due to the insufﬁcient\\ninformation contents (i.e., omission of a number of potentially\\nimportant predictor variables).\\n1.2.2. Related research in devising a prognostic index\\nPrognostic index (PI) provides compact prognosis information\\nregarding a speciﬁc patient based on the results of a Cox\\nproportional hazards model [5]. Cox proportional hazards model\\nhelps identify variables of prognostic importance and hence\\nprognostic index can be used to deﬁne groups of individuals at\\ndifferent risk categories. Even though prognostic index is a\\nconvenient tool to measure how well the patients are doing after\\nthe transplantation, its use in the organ transplantation area has\\nbeen limited mostly due to the lack of follow-up data. Some\\nexisting studies related to devising a PI in transplant area are\\nsummarized as follows.\\nIn the study conducted by Christensen et al. [17], it is\\nmentioned that primary biliary cirrhosis requires a liver trans-\\nplantation operation at the end stage. Based on the prognosis\\nanalysis with as well as without transplantation, it is decided\\nwhether or not the transplantation is required, if so when. To\\nachieve this goal, corresponding PIs and probabilities of surviving\\nare computed for transplantation and non-transplantation cases.\\nYoo et al. [18] developed a similar index and revealed that\\nsocioeconomic status does not inﬂuence patient or graft survival\\nthat undergoes liver transplantation at the institute where they\\nperformed their study. Deng et al. [19] conducted a study with a\\nnational dataset in Germany, which discovers the effect of\\nreceiving a heart transplant for the patients in a waiting list.\\nThe results indicate that cardiac transplant is associated with\\nsurvival beneﬁt only for patients with a predicted high risk of dying\\non the waiting list. Ghobrial et al. [20] performed a study to\\ndetermine prognostic factors for overall survival in 107 adult\\npatients with post-transplantation lymphoproliferative disorders\\n(PTLDs). It is validated that in discriminating the low and high\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n34\\n\\nscored patients the proposed prognostic scoring signiﬁcantly\\nperforms better than the International Prognostic Index for the\\nsubset of the patients (56 out of 107) with lactate dehydrogenase.\\nThe common limitation in all of these studies is similar to the\\nlimitations of the studies summarized in Section 1.2.1. Namely,\\nthey directly devise a prognostic index without determining if the\\nvariables used in prognostic index devising phase are necessary\\nand sufﬁcient. This motivates a machine learning-based initial step\\nof variable selection procedure. Because, if the critical predictive\\nfactors are not captured effectively due to the intuition- and\\nexperience-based\\nselection,\\nthe\\nresulting\\nprognostic\\nindices\\ndeveloped based on the selected variables would be inaccurate\\nand, in turn, related risk groups of patients would be deviated from\\nthe real classes. This may cause mistakes for decision maker in\\nmaking organ transplantation policies.\\n2. Proposed method\\nSection 1.2 shows that the most of the existing studies for organ\\ntransplantation\\nprocedures\\nutilize\\nconventional\\nstatistical\\napproaches such as Kaplan–Meier function and log-rank test\\nalong with expert-selected variables to predict the survivability.\\nHowever, organ transplantation procedures consist of a large\\nnumber of variables (several hundred) that may have nontrivial\\nimpact on modeling the prognosis of the grafts/patients. Using a\\nsomewhat comprehensive variable list may help discriminate\\npatients from each other by placing them into proper risk groups.\\nUnintentional omission of the important variables may lead to\\ninaccurate classiﬁcation of patient risk groups, which may, in turn,\\nlead to suboptimal organ allocation policies and ineffective\\ntreatments.\\nThis study is aimed at overcoming the abovementioned\\nshortcomings by employing both machine learning techniques\\nas well as statistical methods to identify the most critical factors\\naffecting the survivability of thoracic transplant patients. To\\nachieve this goal, this study proposes adopting a 5-step approach\\nillustrated in Fig. 1. Step 1 involves data understanding and\\npreparation, which is arguably the most time demanding step in\\nthe process. Step 2 employs various predictive modeling techni-\\nques such as support vector machines, artiﬁcial neural networks,\\nand regression trees to develop survival time prediction models\\nand to extract the most important variables by means of sensitivity\\nanalysis through the best performing model. Step 3 determines the\\nconsolidated candidate set of critical predictor variables. Step 4\\ndevelops a Cox regression model using the consolidated set of\\npredictor variables and also devises a prognostic index. The last\\nstep, Step 5, classiﬁes the patients into various risk categories by\\ncomparing and contrasting the clustering performance of algo-\\nrithm-based and manually calculated groups. Then the resulting\\nrisk categories are validated by using the Kaplan–Meier survival\\ncurves. These steps will be further explained in details in Sections\\n2.1–2.5, respectively.\\n2.1. Step one: data source and data preparation\\nIn this study, the data source that was used to validate the\\nproposed method was thoracic organ transplant dataset provided by\\nUNOS, which is a tax-exempt, medical, scientiﬁc, and educational\\norganization that operates the national Organ Procurement and\\nTransplantation Network underthe contract to the Division of Organ\\nTransplantation of the Department of Health and Human Services\\n[21]. The data ﬁles were obtained from UNOS using a formal data\\nrequisition procedure (which includes submission of speciﬁc data\\nneeds, purpose of the study, and a data use agreement). These data\\nﬁles are named as UNOS Standard Transplant Analysis and Research\\n(STAR)\\nﬁles\\nfor\\nheart,\\nlung,\\nand\\nsimultaneous\\nheart–lung\\ntransplants, namely thoracic transplants. Each transplant STAR ﬁle\\nconsists of information on all thoracic transplants that had been\\nperformed in the US and reported to UNOS since October 1, 1987. It\\nincludes both deceased- and living-donor transplants. None of the\\nﬁles include any speciﬁc patient or transplant hospital identiﬁers\\ndue to the privacy and security issues. However, there is a patient\\nidentiﬁcation number, unique to each patient, which allows linking\\nmultiple ﬁles and tracking the patient. Considering these features,\\nUNOS’s data ﬁles are recognized as the most comprehensive source\\nof information available in any single ﬁeld of medicine and for organ\\ntransplantation in US [22].\\nThere are two datasets involved in our study, which are regular\\ndataset and follow-up dataset. The regular dataset contains all\\ninformation of donors and recipients before transplantation\\noccurred, and the follow-up dataset provides all information of\\ndonors and recipients after the transplantation. The TRR_ID\\nvariable (transplant identiﬁer) is the common variable between\\nthese two datasets and the one which is proposed by UNOS to\\nmerge and integrate these two datasets. Therefore, these two\\ndatasets were combined in a relational database environment\\nusing the link (a.k.a. primary key) of TRR_ID.\\nOverall, the complete dataset consists of 310,773 records and\\n565 variables. These variables include the socio-demographic and\\nFig. 1. A ﬂowchart representation of the proposed method.\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n35\\n\\nhealth-related factors with regard to both the donor and the\\nrecipients. There are also procedure-related factors among the\\ndataset. To assign as an output (dependent variable), there are four\\npossible variables which are called pstatus, ptime, gstatus, and\\ngtime. These variables have the following meanings: whether or\\nnot the patient died after transplantation occurred (referring to\\npstatus, with dead = 1 and alive = 0). A very similar variable was\\ngstatus, referring to whether or not graft has failed (1 denoting\\n‘‘failed’’ and 0 denoting ‘‘succeeded’’). The variable ptime denoted\\npatient follow-up time (in days) from transplant to death/last\\nfollow up time. Similarly, gtime is explained as graft lifespan from\\ntransplant to death/last follow up time. Since the goal of this study\\nis to develop models to predict the survivability solely based on\\nthoracic transplant, the dependent variable was assigned as gtime.\\nThis assignment was done to discriminate the patients who died\\nsolely due to the thoracic graft incompatibility from the ones who\\ndied from any other reasons. Therefore, the rest of the potential\\ndependent variables (pstatus and ptime) were eliminated from the\\ndataset. Besides, gstatus was kept inactive up to the stage where\\nCox regression model was implemented (Step 4 in Fig. 1).\\nConsidering the gtime as the continuous dependent variable,\\nthe records for the patients whose gtime information were missing\\nwere removed from the dataset. The data set also includes some\\nidentiﬁcation variables (e.g., Donor ID) which help track the\\nrecipient patient anonymously, track the thoracic transplant\\nprocedure, or link records from multiple data ﬁles to each other.\\nSince these types of identiﬁcation variables do not have any\\ninformation content to enhance the prediction capability of the\\nmodels, after linking and integrating the ﬁles they were also\\nexcluded from the analysis dataset. Moreover, the name of\\ntransplantation type was recorded in the dataset as a variable\\nnamed Dataset which had one value (TH referring to thoracic) and\\nthe date of data processing is recorded as a variable named Date of\\nRun which are useful for data integration purposes but has no\\ninformation for contributing to the prediction of survivability and\\nhence are also excluded from the analysis dataset. Similarly, other\\nvariables having only one possible value for all records in the\\ndataset, which have no discriminating information, are also\\neliminated from the predictive modeling.\\nThis dataset had excessive number of missing values which\\nrender most of the records and variables seemingly insigniﬁcant.\\nHowever, in data mining studies one should be very reluctant to\\nremove the candidate predictor variables while at the same time\\ntrying to avoid artiﬁcial data imputation procedures. There is an\\nobvious trade-off here. As a rule of thumb, for column (variable)\\ndeletion, we were cautious to remove any variable from the\\nanalysis and assumed that if a variable has more than 95% missing\\nvalues, only then it should be regarded as not having signiﬁcant\\ninformation content and hence should be deleted. Next step was to\\nhandle the missing values by following the general convention: for\\nthe categorical variables we ﬁlled the missing values with some\\nheuristic values such as E (referring to empty) or NR (referring to\\nnot reported), and for the continuous variables we imputed the\\nmissing values with the average of the existing records. After\\nadopting these data preparation strategies, the ﬁnal dataset was\\nreduced to 372 cleansed independent variables and one dependent\\nvariable (gtime) with the total record count of 106,398.\\n2.2. Step two: predictive modeling\\nSince the dependent variable herein was a continuous variable\\n(graft survival time, which is the number of days from transplant to\\ndeath or last follow-up), the problem refers to a prediction (or\\nregression) problem (as opposed to a classiﬁcation problem). Since\\nthe\\nrelationships between the\\ndependent\\nvariable\\nand\\nthe\\nindependent variables were not known in advance, this step\\nwas to develop various predictive models for graft survival time\\nusing all of the available independent variables. It is also required\\nto check whether the models have passed the pre-speciﬁed\\nthreshold values of performance measures, speciﬁcally the R2 and\\nmean square error (MSE), to determine the best model that\\nexplains these unknown relationships between dependent and\\nindependent variables by ranking them according to these\\nmeasures. The model which is deemed to be the most successful\\none would be kept for further modeling steps to determine the\\nimportance of the independent variables.\\nSupport vector machines (SVMs) are supervised learning\\nmethods that generate input–output mapping functions from a\\nset of training data. They belong to a family of generalized linear\\nmodels which achieve a classiﬁcation or regression decision based\\non the value of the linear combination of features. They are also\\nsaid to belong to the kernel methods [23]. The mapping function in\\nSVMs can be either a classiﬁcation function (used to categorize the\\ndata) or a regression function (used to estimate the numerical\\nvalue of the desired output, as is the case in this study). Nonlinear\\nkernel functions are often used to transform the input data\\n(inherently representing highly complex nonlinear relationships)\\nto a high dimensional feature space in which the input data\\nbecome more separable (i.e. linearly separable) compared to the\\noriginal input space. Then, maximum-margin hyperplanes are\\nconstructed to optimally separate the classes in the training data.\\nTwo parallel hyperplanes are constructed on each side of the\\nhyperplane that separates the data by maximizing the distance\\nbetween the two parallel hyperplanes. An assumption is made that\\nthe\\nlarger\\nthe\\nmargin\\nor\\ndistance\\nbetween\\nthese\\nparallel\\nhyperplanes, the better the generalization error of the prediction\\nwould be.\\nArtiﬁcial neural networks (ANNs) have been utilized to model\\ncomplex relationships (such as nonlinear functions and multi-\\ncollinearity) among the predictor variables and the dependent\\nvariable [24]. ANNs are highly sophisticated analytic techniques\\ncapable of predicting new observations (on speciﬁc variables)\\nfrom other observations (on the same or other variables) after\\nexecuting a process of so-called ‘‘learning’’ from existing data\\n[25]. ANNs have been one of the most popular artiﬁcial\\nintelligence-based data modeling algorithms used in recent\\nmedical informatics studies due to their satisfactory predictive\\nperformance [26]. On the other hand, compared to other\\nmachine learning methods (such as ANNs), decision trees have\\nthe advantage of not being a black box model, namely having the\\ncapability to explain the inner structure of the model in the form\\nof a graphically represented inverse tree or a collection of\\ncondition-action rules. This advantage has made them a viable\\nand desirable alternative method in medical informatics [27]. If\\nthe dependent variable is continuous (as in the case in this\\nstudy) the resulting decision tree is called a regression tree.\\nRegression trees are known to be among the highly adaptable,\\nrelatively ﬂexible, yet computationally intensive data mining\\ntechniques [28]. Popular regression tree algorithms are CART (or\\nC&RT) [29], CHAID [30], and M5 [31] which can be used for both\\nclassiﬁcation and regression type prediction problems. ID3 and\\nits successors, C4.5 and C5 are also among the popular decision\\ntree algorithms, but they can only work for classiﬁcation type\\nprediction problems.\\n2.2.1. Performance criteria\\nTo compare the abovementioned prediction models, two\\nperformance criteria are considered: mean squared error (MSE)\\nof the model on testing dataset and R2 value between the actual\\nobservation for the target variable (Yt) and the predicted value by\\nthe model (Ft). MSE which is given by Eq. (1) does not have a rule-\\nof-thumb threshold cut-off value for acceptable models. It is a\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n36\\n\\nrelative criterion to select the best model, namely the smaller the\\nvalue the better the model has performed [32].\\nMSE ¼ 1\\nn\\nX\\nn\\nt¼1\\nðYt \\x02 FtÞ2\\n(1)\\nOn the other hand, R2 (R2\\nFt;Yt or shortly R2) which is given by\\nEq. (2) can be considered as both an absolute measure and a\\nrelative measure to determine and rank the satisfactory models\\n[33]. Unlike the MSE, the higher the R2, the better the performance\\nfor the compared models.\\nR2 ¼ 1 \\x02\\nPn\\nt¼1 ðFt \\x02 YtÞ2\\nPn\\nt¼1 ðYt \\x02 ¯YtÞ\\n2\\n(2)\\n2.2.2. k-Fold cross-validation\\nIn order to minimize the bias associated with the random\\nsampling of the training and holdout data samples in comparing\\nthe predictive accuracy of two or more methods, researchers tend\\nto use k-fold cross-validation [34]. In k-fold cross-validation, also\\ncalled rotation estimation, the complete dataset (D) is randomly\\nsplit into k mutually exclusive subsets (the folds: D1, D2, . . ., Dk) of\\napproximately equal size. The prediction model is trained and\\ntested k times. Each time (t 2 {1, 2, . . ., k}), it is trained on all but one\\nfold (Dt) and tested on the remaining single fold (Dt). The cross-\\nvalidation estimate of the overall performance criteria is calculated\\nas simply the average of the k individual performance measures as\\nin Eq. (3),\\nCV ¼ 1\\nk\\nX\\nk\\ni¼1\\nPMi\\n(3)\\nwhere CV stands for cross-validation, k is the number of folds used,\\nand PM is the performance measure for each fold [35].\\nIn this study, to estimate the performance of the prediction\\nmodels a 10-fold cross-validation approach was used. Empirical\\nstudies showed that 10 seems to be an optimal number of folds\\n(that optimizes the time it takes to complete the test while\\nminimizing the bias and variance associated with the validation\\nprocess) [34]. In 10-fold cross-validation the entire dataset is\\ndivided into 10 mutually exclusive subsets (or folds). Each fold is\\nused once to test the performance of the prediction model that is\\ngenerated from the combined data of the remaining nine folds,\\nleading to 10 independent performance estimates.\\n2.2.3. Sensitivity analysis\\nAfter selecting the best prediction model based on the\\nperformance criteria as explained in Section 2.2.1, it is required\\nto determine the importance of the independent variables. In\\nmachine learning algorithms, sensitivity analysis is a method for\\nextracting the cause and effect relationship between the inputs\\nand outputs of a trained model [36]. In the process of performing\\nsensitivity analysis, after the model is trained the learning is\\ndisabled so that the network weights are not affected. The\\nfundamental idea is that the sensitivity analysis measures the\\npredictor variables based on the change in modeling performance\\nthat occurs if a predictor variable is not included in the model.\\nHence, the measure of sensitivity of a speciﬁc predictor variable is\\nthe ratio of the error of the trained model without the predictor\\nvariable to the error of the model that includes this predictor\\nvariable [37]. The more sensitive the network is to a particular\\nvariable, the greater the performance decrease would be in the\\nabsence of that variable, and therefore the greater the ratio of\\nimportance. This method is followed in support vector machines\\nand artiﬁcial neural networks to rank the variables in terms of their\\nimportance according to the sensitivity measure deﬁned in Eq. (4)\\n[38].\\nSi ¼\\nV\\nCðFtÞ ¼ VðEðFtjXiÞÞ\\nVðFtÞ\\n(4)\\nwhere V(Ft) is the unconditional output variance. In the numerator,\\nthe expectation operator E calls for an integral over X\\x02i; that is, over\\nall input variables but Xi, then the variance operator V implies a\\nfurther integral over Xi. Variable importance is then computed as\\nthe normalized sensitivity. Saltelli et al. [39] show that Eq. (4) is the\\nproper measure of sensitivity to rank the predictors in order of\\nimportance for any combination of interaction and non-orthogo-\\nnality among predictors. As for the decision trees, variable\\nimportance measures were used to judge the relative importance\\nof each predictor variable. Variable importance ranking uses\\nsurrogate splitting to produce a scale which is a relative\\nimportance measure for each predictor variable included in the\\nanalysis. Further details on this procedure can be seen in Breiman\\net al. [29].\\n2.3. Step three: determining the candidate sets of predictor variables\\nStep 3 is to determine which predictor variables to be used in\\ndevising a prognostic index in Step 4. This step helps eliminate\\nthe insigniﬁcant variables and improves the accuracy of the\\nmodel by optimizing the predictor variables list. The potential\\ninput variables to this step consist of three candidate sets of\\npredictor variables. The ﬁrst set is composed of variables\\nselected\\nby\\nthe\\npredictive\\nmodels.\\nThe\\npredictive\\nmodels\\nexplained in Section 2.2 rank the predictor variables based on\\ntheir importance level in predicting the graft survival time. The\\npredictive variables selected by the sensitivity analysis of the\\nbest-performing model (ranked in terms of R2 and MSE) are\\nchosen as the ﬁrst set of predictive variables. The second set of\\npredictive variables is obtained by considering the expert\\ndomain\\nknowledge.\\nThis\\nset\\nincludes\\nvariables\\nwhich\\nare\\nlogically related to heart and lung transplantation such as\\ndonor’s history of cigarette usage. The third set of predictive\\nvariables is selected from the related literature. This set consists\\nof the variables which have been commonly and repeatedly used\\nin previous studies in the organ transplantation area. The second\\nand third sets of predictive variables provide more comprehen-\\nsive information for the next step, the Cox regression model, by\\nincluding the variables that might have importance in the\\nsurvival analysis but were determined to be insigniﬁcant by the\\npredictive models in Step 2.\\n2.4. Step four: survival analysis and prognostic index devising\\nStep 4 takes all the three sets of predictive variables identiﬁed\\nin Step 3, and then applies Cox regression to model the graft\\nsurvivability and ﬁlter out the candidate predictive variables\\nwhich do not have signiﬁcant survival effect. Hence, in Step 4 the\\nﬁnal critical predictive variables are determined by the Cox\\nregression model. Cox regression model also enables devising a\\nprognostic index to categorize the patients into various groups\\nwith different levels of risks.\\nCox regression model is a semi-parametric model extensively\\nused in survival analysis [6]. The survival time of each patient is\\nassumed to follow the hazard function (hi) given by Eq. (5) as\\nfollows:\\nhi ¼ h0 expðxibÞ\\n(5)\\nwhere h0 is the baseline hazard function and xi is the vector of\\npredictor variables for the ith patient. b is the vector of regression\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n37\\n\\ncoefﬁcients for the predictor variables and is assumed to be the\\nsame for all patients [40,41].\\nOne important application of Cox regression model is to identify\\nvariables which may be of prognostic importance [5]. Once\\nidentiﬁed, knowledge from these variables will be combined and\\nused to deﬁne a prognostic index, which in turn deﬁnes groups of\\norgan recipients with different levels of risk. To use the prognostic\\nindex, key patient characteristics are recorded, from which a score\\nis derived. This score gives an indication of whether a particular\\npatient has high, intermediate, or low levels of prognosis for the\\ndisease [5,42]. Recalling Eq. (5), the prognostic index (PI) for each\\npatient can be calculated by Eq. (6):\\nPI ¼ x1b1 þ x2b2 þ . . . þ xnbn\\n(6)\\nwhere x1 to xn are the patient’s values for the variables in the Cox\\nmodel, and b1 to bn are the corresponding regression coefﬁcients\\ndetermined by Cox regression model [42].\\nNote that PI in Eq. (6) represents the exponent portion in Eq. (5).\\nTherefore, the smaller the PI, the smaller the hazard function value,\\nand hence the smaller the risk associated with a particular\\nrecipient.\\n2.5. Step ﬁve: determining risk groups of thoracic recipients\\nAnimportantquestionfollowingStep4is‘‘Howmanyriskgroups\\nshould the patients be classiﬁed into?’’ In Step 5, k-means clustering\\nalgorithm, two-step cluster analysis, and conventional heuristics-\\nbased approaches are used to answer to this question. As a statistical\\nand/or pictorial veriﬁcation mechanism for the number of groups\\ndetermined by the best performing abovementioned clustering\\napproaches, ﬁnally the Kaplan–Meier survival analysis [43] is\\nadopted and corresponding survival curves are generated.\\nk-Means method is an extensively used, arguably the most\\npopular clustering algorithm that searches for a nearly optimal\\npartition with ﬁxed number of clusters represented by the\\nparameter k [44]. It proceeds by assigning k initial centroids to\\nthe multidimensional datasets. Each record in the dataset is\\nallocated to the centroid which is nearest and hence forming a\\ncluster. Each cluster centroid is then updated to be the center of its\\nmembers, followed by a new assignment of records to the nearest\\ncentroids to re-construct the clusters. The algorithm converges\\nwhen there is no further change in allocation of members to clusters\\nor some predeﬁned time-based stopping criteria is satisﬁed [45].\\nAnother popular clustering algorithm istwo-stepcluster analysis\\n(TSCA) [46,47]. It has two steps: (1) to pre-cluster the cases (or\\nrecords) into many small sub-clusters, and (2) to cluster the sub-\\nclusters resulting from pre-cluster step into the desired number of\\nclusters. The pre-cluster step uses a sequential clustering approach. It\\nscans the data records one by one and decides if the current record\\nshould be merged with the previously formed clusters or starts a\\nnew cluster based on the distance criterion. Then the cluster step\\ntakes sub-clusters resulting from the pre-cluster step as input, and\\ngroups them into the desired number of clusters. Since the number\\nof sub-clusters is much less than the number of original records, the\\ntraditional clustering methods can be used effectively. This step uses\\nthe agglomerative hierarchical clustering method [46,47]. Although\\nthere are several other clustering algorithms (e.g. Kohonen net-\\nworks) they do not allow the modeler to specify a desired number of\\nclusters at the beginning of the clustering algorithm. k-Means and\\nTSCA algorithms overcome this issue. The modeler can predeﬁne a\\nspeciﬁcnumber of clusters to group the variables and compare them\\naccording to their clustering performances. Since this is the main\\nfocus of our study, we utilized k-means and TSCA algorithms for\\nclustering the PIs and thus identfying the risk groups of thoracic\\npatients.\\nThe Kaplan–Meier analysis is a non-parametric technique used\\nto test the statistical signiﬁcance of differences between the\\nsurvival curves associated with two different circumstances [43].\\nThe analysis expresses the distribution of patient survival times in\\nterms of the proportion of patients still alive up to a given time. On\\nthe other hand, the Kaplan–Meier survival curves plot the\\nproportion of patients surviving against time which has a\\ncharacteristic decline. In biostatistics, a typical application of\\nKaplan–Meier survival curves involves grouping patients into risk\\ngroups such as low, medium, and high risks.\\n3. The case study and discussion\\nIn order to demonstrate and validate the proposed methodolo-\\ngy in Section 2, two most popular data mining toolkit are used,\\nnamely SPSS PASW Modeler1 [48] and SAS 9.1.31 [49] statistical\\nsoftware package. Using the UNOS data set, Sections 3.1–3.5\\ndiscuss the results obtained by following the above mentioned\\nmodeling procedures presented in Section 2. The prediction\\nperformance results reported herein are all based on the test (or\\nholdout) dataset.\\n3.1. Predictive model results\\nTo reveal the initially unknown relationship between the\\nthoracic input/independent variables and the continuous output/\\ndependent variable (gtime), due to the high computational time\\nrequired for 10-fold cross-validation of each model we only used\\ntwo most popular models from each family of machine learning\\ntechniques. Radial basis function (RBF) and polynomial functions\\nas Kernel methods in support vector machine were deployed. We\\nused multilayer perceptron (MLP) and RBF type of network\\nstructures for ANNs. The most recent algorithms C&RT and M5\\nwere utilized for prediction with the decision trees. The 10-fold\\naveraged prediction results in terms of MSE and R2 for each model\\nare tabulated in Table 1. The acceptance of predictive models is\\nﬁrst evaluated based on their coefﬁcient of determination (R2)\\nvalues. It is widely accepted that if R2 is higher than 0.6, the\\npredictive model has performed fairly well [50,51]. Therefore, we\\nset this as a threshold value for the model sufﬁciency. Since all the\\nmodels have passed this threshold, we kept the one with the\\nhighest R2 and the smallest MSE for further analyses, which came\\nout to be the support vector machine model with radial basis\\nKernel function in this case study.\\n3.2. Determination of the candidate covariates for Cox regression\\nmodel\\nStep 3 in the proposed method provides three different sets of\\ncandidate covariates to be used in the Cox model. Since the best\\nperforming model to explain the relationships of independent and\\nTable 1\\nComparison of machine learning prediction model results.\\nPerformance measures\\nPrediction models\\nMSE\\nR2\\nSupport vector machine\\nRBF\\n0.023\\n0.879\\nPolynomial\\n0.793\\n0.643\\nArtiﬁcial neural network\\nMLP\\n0.031\\n0.847\\nRBF\\n0.146\\n0.835\\nDecision trees\\nM5\\n0.324\\n0.785\\nC&RT\\n0.578\\n0.766\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n38\\n\\ndependent variables was found to be RBF-SVM, the sensitivity\\nanalysis as explained in Section 2.2.3 by Eq. (4) was conducted on\\nthe predictor variables to rank them in terms of their importance in\\npredicting the gtime output variable. This ﬁrst set consists of the\\npredictor variables which are presented in Table 2.\\nThe secondsetofpredictorvariableswereselectedbythe authors\\nthrough brainstorming sessions with medical professionals. The\\nsecond set of candidate covariates are tabulated in Table 3.\\nThe third set of candidate covariates was determined through\\nthe recent literature [52]. This set includes the variables commonly\\nused in the\\npreviously published studies related to organ\\ntransplantation. The third set of candidate covariates are shown\\nin Table 4.\\nThe second and third set of candidate covariates can be\\nperceived as the expert component of the method. If the predictive\\nmodels in Step 3 do not reveal some very critical predictor\\nvariables (such as the age of the recipient in our case study), the\\nmethod proposes to force the Cox model once more to review the\\nsigniﬁcance of this kind of predictor variables.\\n3.3. Deployment of Cox regression model and devising the prognostic\\nindices\\nAll the candidate covariates as determined in Section 3.2 were\\nassigned to Cox regression model at this step. The stepwise\\nvariable selection procedure was applied with 0.05 for entry and\\nTable 3\\nThe 2nd set of candidate covariates.\\nVariables\\nExplanation\\nAntiarry\\nHeart medical factors: antiarrythmics\\nat registration\\nContin_Alcohol_Old_Don\\nDeceased donor-history of alcohol\\ndependency + recent 6 months use\\nContin_Cig_Don\\nDeceased donor-history of cigarettes in past\\nand >20 pack years + recent 6 months use\\nContin_IV_Drug_Old_Don\\nDeceased donor-history of iv drug use\\n+ recent 6 months use\\nContin_Oth_Drug_Don\\nDeceased donor-history of other drugs in\\npast + recent 6 months use\\nEint\\nEthnicity interaction between recipient and\\ndonor (in the same ethnic group, y/n)\\nGint\\nGender interaction between recipient and\\ndonor (having the same sex, y/n)\\nHist_Alcohol_Old_Don\\nDeceased donor-history of alcohol dependency\\nHist_Cancer_Don\\nDeceased donor-history of cancer (y/n)\\nHist_Cig_Don\\nDeceased donor-history of cigarettes in past\\nand >20pack yrs\\nHist_Cocaine_Don\\nDeceased donor-history of cocaine use in past\\nHist_Diabetes_Don\\nDeceased donor-history of diabetes, incl.\\nDuration of disease\\nHist_Hypertens_Don\\nDeceased donor-history of hypertension\\nLOS\\nRecipient length of stay post-transplant\\nOth_Tobacco\\nOther tobacco use\\nPack_Yrs\\nIf history of cigarette use, number of\\npack years\\nTable 2\\nThe 1st set of candidate covariates generated from RBF-SVM.\\nVariables\\nExplanation\\nCitizenship\\nRecipient citizenship @ registration\\nContin_alcohol_old_don\\nDeceased donor-history of alcohol\\ndependency + recent 6 months use\\nContin_iv_drug_old_don\\nDeceased donor-history of iv drug use\\n+ recent 6 months use\\nCreat2_old\\nMost recent creatinine >2.0 mg/dl y/n\\nDa2\\nDonor a2 antigen\\nDantiarr_old\\nDeceased donor given antiarrythmics 24 h\\nprior to cross-clamp\\nDayswait_chron\\nActive days on waiting list\\nDobut_don_old\\nDeceased donor-dobutamine w/in 24 h\\npre-cross-clamp\\nEducation\\nRecipient highest educational level @\\nregistration\\nEthcat_don\\nDonor ethnicity category\\nFluvaccine\\nAnti-viral treatment—ﬂuvaccine\\nFunc_stat_tcr\\nRecipient functional status @ registration\\nFunc_stat_trr\\nRecipient functional status @transplant\\nGender\\nRecipient gender\\nHbsab_don\\nDeceased donor hbsab test result\\nHemo_pa_dia_tcr\\nMost recent hemodynamics pa (dia) mm/hg\\n@ registration\\nHemo_pa_mn_tcr\\nMost recent hemodynamics pa (mean) mm/hg\\n@ registration\\nHeparin_don\\nDeceased donor management—heparin\\nHgt_cm_tcr\\nRecipient height @ registration\\nHist_alcohol_old_don\\nDeceased donor-history of alcohol\\ndependency\\nHtlv2_old_don\\nDeceased donor-antibody to htlv ii result\\nImpl_deﬁbril_after_list\\nImplantable deﬁbrillator inserted between\\nlisting and transplant\\nInotrop_agents\\nDeceased donor—three or more inotropic\\nagents at time of incision\\nInotrop_support_don\\nDeceased donor inotropic medication at\\nprocurement (y/n)\\nIschtime\\nIschemic time in hours\\nMed_cond_tcr\\nRecipient medical condition @ registration\\nMed_cond_trr\\nRecipient medical condition pretransplant\\n@ transplant\\nPhysical_capacity_tcr\\nPhysical capacity at listing\\nPretreat_med_don_old\\nDeceased donor medication(s) from brain\\ndeath to 24 h prior to procurement\\nPrior_lung_surg_tcr\\nRecipient prior lung surgery (non-transplant)\\nat listing\\nPst_airway\\nEvents prior to discharge: airway dehiscence\\nPst_cardiac\\nEvents prior to discharge: cardiac re-operation\\nPst_dial\\nEvents prior to discharge: dialysis\\nPst_drug_trt_infect\\nEvents prior to discharge: any drug\\ntreated infection\\nPst_surgical\\nEvents prior to discharge: other surgical\\nprocedures\\nPt_t4_don\\nDeceased donor-thyroxine-t4 b/n brain\\ndeath w/in 24 h of procurement\\nSternotomy_tcr\\nEvents occurring prior to listing: sternotomy\\nSternotomy_trr\\nEvents occurring between listing and\\ntransplant: sternotomy\\nSteroid\\nChronic steroid use y/n/u @ transplant\\nTrtrej1y\\nTreated for rejection within 1 year\\nTrt_pulm_sepsis\\nIV treated pulmonary sepsis y/n/u @\\nregistration\\nVad_tah_tcr\\nRecipient on life support—ventilator @\\nregistration (1 = yes, 0 = no)\\nVad_tah_trr\\nRecipient on life support—ventilator @\\ntransplant\\nTable 4\\nThe 3rd set of candidate covariates.\\nVariables\\nExplanation\\nABO\\nRecipient blood group at registration\\nABO_Don\\nDonor blood type\\nABO_Mat\\nDonor-recipient ABO match level\\nAge\\nRecipient age (years)\\nAge_Don\\nDonor age (years)\\nDayswait_Chron\\nActive days on waiting list\\nDon_TY\\nDonor type—deceased/living\\nEthcat\\nRecipient ethnicity category\\nEthcat_Don\\nDonor ethnicity category\\nGender\\nRecipient gender\\nGender_Don\\nDonor gender\\nHbsab_Don\\nDeceased donor hbsab test result\\nIschtime\\nIschemic time in hours\\nMed_Cond_Tcr\\nRecipient medical condition at registration\\nMed_Cond_Trr\\nRecipient medical condition pretransplant\\nat transplant\\nWgt_kg_Don\\nDonor weight (kg)\\nWgt_kg_Tcr\\nRecipient weight (kg) at registration\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n39\\n\\n0.1 for removal as signiﬁcance threshold criteria. The predictor\\nvariables determined to be signiﬁcant by Cox regression model are\\nlisted along with their corresponding statistics in Table 5. The rest\\nof the variables (which were in Tables 2, 3, or 4 but not in Table 5)\\nwere eliminated since they were found to be insigniﬁcant by Cox\\nregression model.\\nAs listed in Table 5, 14 of the variables had prognostic value\\nwhich are determined by the Cox model as signiﬁcant and kept in\\nthe Cox equation. Therefore, they were used to calculate the PIs by\\nmeans of Eq. (6). The PI values received here were ranging between\\n0 and 3.\\n3.4. Clustering the prognostic indices\\nOnce the prognostic indices (PIs) for each recipient calculated,\\nthe next step was to cluster the recipients through these PIs.\\nHowever, the problem of deﬁning these clusters and deciding\\nwhich value to cut off and categorize the recipients should be\\nsolved\\nﬁrst.\\nTwo\\ncommonly\\nused\\nclustering\\nalgorithms\\nas\\ndescribed in Section 2.5, namely k-means and TSCA were used\\nto determine these clusters. We also compared these algorithm-\\nbased clusters to conventional PI devising methods in medicine.\\nTwo potential ways to do the clustering are constructing equal-\\nwidth PIs and equal-percentile PIs in this research domain. In the\\nformer one, the PIs are separated in groups so that the increments\\nof PI in each group are equal whereas the latter method focuses on\\nallocating the patients equally to each group. The algorithms k-\\nmeans and TSCA were run by changing the value for k (number of\\nclusters to be formed). The value of k with 2, 3, 4, and 5 were tried\\nbecause it was considered that having clusters more than 5 would\\nnot provide logical risk groups to categorize and would probably\\nnot be easy to name and interpret medically afterwards. The\\nresults for each run are represented in Table 6.\\nThe performances of these entire four approaches with different\\nnumber of clusters (k = 2–5) were compared using intraclass inertia\\nas the performance measure to decide which one to adopt. It is a\\nmeasure which shows how compact each cluster is. Intraclass\\ninertia is the average of the distances between the means and the\\nobservations in each cluster. Eq. (7) indicates this value for given k\\nnumber of clusters [53].\\nFðkÞ ¼ 1\\nn\\nX\\nk\\nX\\ni 2 Ck\\nX\\nm\\nP¼1\\nðXiP \\x02 mkPÞ2\\n(7)\\nwhere n is the number of total observations, CK is the set of kth\\ncluster, XiP is the value of the attribute P for observation i and mkP is\\nthe mean of the attribute P in the kth cluster. Note that in our case\\nthere is only one attribute which is PI, and hence m = 1.\\nThe intraclass inertia values for each possible cluster are also\\nsummarized in Table 6. Prognostic indices were clustered best with\\nk = 3 with k-means clustering algorithm in our case as seen in\\nTable 6 considering its low intraclass inertia value. As seen in\\nTable 6, this classiﬁcation not only gives the lowest intraclass\\ninertia value but also provides an even distribution of the thoracic\\npatients for our nation-wide dataset (38%, 16%, and 46% for low,\\nmedium, and high risk groups of patients, respectively). Although 5\\nclusters with k-means algorithm and 3 clusters in two-step cluster\\nanalysis perform very close to k-means algorithm with 3 clusters,\\nneither of them provides such an even distribution of patients.\\nNote that in addition to considerably higher inertia scores,\\nheuristic calculation with equal-width PIs distribute the nation-\\nwide patients highly skewed to lower tails of risk groups for all ﬁve\\npotential cluster formations. Therefore, we conclude that the k-\\nmeans algorithm based clustering performs better than the other\\npotential groupings in terms of both objective and subjective\\naspects.\\n3.5. Validation of risk groups by Kaplan–Meier survival analysis\\nTo validate the established prognostic indices with 3 clusters\\nand hence the various risk groups in Section 3.4, Kaplan–Meier\\nsurvival analysis [43] was conducted. The corresponding PI\\nclusters were matched with the patients and their predictor\\nvariables from Table 5. In Kaplan–Meier survival analysis the\\npredictor variables were used as explanatory variables and the PI-\\nbased clusters were used as the strata variable to label the patients\\nwith different risks. The main objective here was to compare\\nsurvivor functions for different risk groups of thoracic recipients.\\nIf the survivor function for one risk group is always higher than the\\nsurvivor function for another risk group, than the ﬁrst group\\nclearly lives longer than the second one. The less the survivor\\nfunctions cross, the better the discrimination of the patients\\nwould be. Fig. 2 shows this clear distinction for k-means\\nalgorithm-based PIs.\\nIn order to show that there is a statistically signiﬁcant\\ndifference among these three risk groups, the test of equality\\nover strata was also conducted. Test of equality over strata\\ncontains rank and likelihood-based statistics for testing homoge-\\nneity of survivor functions across strata. The rank tests with the\\nlog-rank test and Wilcoxon test indicate a signiﬁcant difference\\nbetween the risk groups. These results are also supported by\\nlikelihood-based\\nstatistics.\\nThese\\nstatistical\\ntest\\nresults\\nare\\nsummarized in Table 7.\\nTable 5\\nThe variables kept in the Cox regression model.\\nVariable\\nSE\\nChi_square test\\nDF\\nSigniﬁcance\\nexp(b)\\n95% CI for exp(b)\\nLower\\nUpper\\nLOS\\n0.0002\\n385.8701\\n1\\n<.0001\\n1.004\\n1.004\\n1.005\\nEint\\n0.0178\\n56.9447\\n1\\n<.0001\\n0.844\\n0.844\\n0.905\\nGint\\n0.0183\\n11.8644\\n1\\n0.0006\\n0.906\\n0.906\\n0.973\\nAge_Don\\n0.0006\\n247.3162\\n1\\n<.0001\\n1.009\\n1.009\\n1.011\\nWgt_kg_Tcr\\n0.0004\\n5.5091\\n1\\n0.0189\\n0.998\\n0.998\\n1.000\\nWgt_kg_Don\\n0.0005\\n21.3483\\n1\\n<.0001\\n0.997\\n0.997\\n0.999\\nAcyclovir\\n0.0300\\n14.6651\\n1\\n0.0001\\n0.840\\n0.840\\n0.945\\nCitizenship\\n0.0554\\n5.5538\\n1\\n0.0184\\n0.787\\n0.787\\n0.978\\nDayswait_Chron\\n0.0002\\n7.5318\\n1\\n0.0061\\n1.000\\n1.000\\n1.000\\nFluvaccine\\n0.0189\\n15.9915\\n1\\n<.0001\\n1.039\\n1.039\\n1.119\\nIschtime\\n0.0058\\n239.5080\\n1\\n<.0001\\n1.081\\n1.081\\n1.105\\nMed_Cond_Tcr\\n0.0109\\n75.6231\\n1\\n<.0001\\n1.076\\n1.076\\n1.123\\nVad_Tah_Trr\\n0.0002\\n5.7861\\n1\\n0.0162\\n1.000\\n1.000\\n1.001\\nVad_Tah_Tcr\\n0.0077\\n48.9955\\n1\\n<.0001\\n1.040\\n1.040\\n1.072\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n40\\n\\n4. Conclusions and future research directions\\nThis study demonstrates that machine learning-based meth-\\nodology for selecting predictor variables in survivability and\\nprognostic modeling of thoracic organ transplantation is superior\\nto the approaches adopting only expert-selected variables. The\\nstudy showed that of the comprehensive list of predictors, some\\nhave been included in the previous studies (such as gender and age\\nof the recipient, his/her medical condition at registration) while\\nsome others (which are found to be critical) have been absent from\\nthe related literature. These variables (e.g. such as recipient length\\nof stay post-transplant and the interaction of gender and ethnicity\\nbetween the recipient and the donor) should be combined with the\\nfactors identiﬁed in previous studies to better understand and\\nimprove the organ transplantation process.\\nThe study revealed that based on k-means clustering algorithm\\nthe thoracic organ recipients should be allocated into an optimal\\nnumber of ‘‘three’’ risk groups, namely low, medium, and high. This\\nﬁnding conﬁrms the conventional medical discrimination com-\\nmonly used in this ﬁeld of study. However, it also proves that this\\ngrouping should be better performed through a data mining\\nperspective rather than a heuristics-based approach because the\\nlatter one gives more skewed distribution of patients for our US\\nnation-wide\\ndataset.\\nThis\\nis\\nthe\\npoint\\nwhere\\nthe\\nmedical\\nprofessionals should be advised to handle the problem in the\\nfuture.\\nSome of the research extensions to the study reported in this\\narticle includes analysis of other organ types as well as the analysis\\nFig. 2. Kaplan–Meier survival curves for three PIs.\\nTable 7\\nTests of equality over risk groups for k-means based three PI cluster.\\nTest\\nChi-square\\nDF\\nPr > Chi-square\\nLog-rank\\n1002.6135\\n2\\n<.0001\\nWilcoxon\\n939.7492\\n2\\n<.0001\\n\\x022 log(LR)\\n1013.3153\\n2\\n<.0001\\nTable 6\\nThe comparative results for clustering and heuristic-based algorithms.\\nBy clustering algorithms\\nk-Means algorithm\\nTwo-step cluster analysis\\nNumber of clusters\\nRisk group\\nPrognostic index\\nNumber of patients\\nIntraclass inertia\\nPrognostic index\\nNumber of patients\\nIntraclass inertia\\nCluster 1\\nLow\\n0–0.69\\n21163 (58%)\\n12.4 \\x03 10\\x028\\n0–1.09\\n34199 (94%)\\n866.30 \\x03 10\\x028\\nCluster 2\\nHigh\\n0.70–3\\n15262 (41%)\\n1.1–3\\n2226 (6%)\\nCluster 1\\nLow\\n0–0.56\\n13766 (38%)\\n1.68 \\x03 10\\x028\\n0–1.04\\n33529 (92%)\\n2.20 \\x03 10\\x028\\nCluster 2\\nMedium\\n0.57–0.91\\n5834 (16%)\\n1.05–1.83\\n2807 (7.7%)\\nCluster 3\\nHigh\\n0.92–3\\n16825 (46%)\\n1.84–3\\n89(0.3%)\\nCluster 1\\nLow\\n0–0.49\\n15227 (42%)\\n11.2 \\x03 10\\x028\\n0–0.41\\n6410 (17%)\\n445.39 \\x03 10\\x028\\nCluster 2\\nLow–medium\\n0.50–0.77\\n1764 (5%)\\n0.42–0.70\\n15163 (42%)\\nCluster 3\\nMedium–high\\n0.78–1.12\\n9542 (26%)\\n0.71–1.04\\n11892(33%)\\nCluster 4\\nHigh\\n1.13–3\\n9892 (27%)\\n1.05–3\\n2960 (8%)\\nCluster 1\\nVery low\\n0–0.44\\n13266 (36%)\\n3.02 \\x03 10\\x028\\n0–0.36\\n2960 (8%)\\n720.71 \\x03 10\\x028\\nCluster 2\\nLow\\n0.45–0.69\\n451(1%)\\n0.37–0.53\\n10475 (29%)\\nCluster 3\\nMedium\\n0.70–0.95\\n4449 (12%)\\n0.54–0.73\\n10815 (29%)\\nCluster 4\\nHigh\\n0.96–1.39\\n7814 (22%)\\n0.74–1.04\\n4674(13%)\\nCluster 5\\nVery high\\n1.40–3\\n10445 (29%)\\n1.05–3\\n7501 (21%)\\nBy heuristics-based calculation\\nWith equal PI widths\\nWith equal percentiles\\nNumber of clusters\\nRisk group\\nPrognostic index\\nNumber of patients\\nIntraclass inertia\\nPrognostic index\\nNumber of patients\\nIntraclas s inertia\\nCluster 1\\nLow\\n0–1.5\\n36154(99%)\\n713.68 \\x03 10\\x028\\n0–0.64\\n18212(50%)\\n7.02 \\x03 10\\x026\\nCluster 2\\nHigh\\n1.6–3\\n271 (1%)\\n0.65–3\\n18213 (50%)\\nCluster 1\\nLow\\n0–0.9\\n32571 (89%)\\n1678.65 \\x03 10\\x028\\n0–0.53\\n12142 (33.5%;\\n2.01 \\x03 10\\x026\\nCluster 2\\nMedium\\n1–1.9\\n3794 (10%)\\n0.54–0.76\\n12141 (33%)\\nCluster 3\\nHigh\\n2–3.0\\n60 (1%)\\n0.77–3\\n12142 (33.5%;\\nCluster 1\\nLow\\n0–0.7\\n26087 (72%)\\n12961.43 \\x03 10\\x028\\n0–0.47\\n9106 (25%)\\n2755.48 \\x03 10\\x026\\nCluster 2\\nLow–medium\\n0.8–1.5\\n10153 (28%)\\n0.48–0.64\\n9106(25%)\\nCluster 3\\nMedium–high\\n1.6–2.3\\n162(0.4%)\\n0.65–0.82\\n9106 (25%)\\nCluster 4\\nHigh\\n2.4–3\\n23 (0.06%)\\n0.83–3\\n9107 (25%)\\nCluster 1\\nVery low\\n0–0.5\\n15605 (43%)\\n457.67 \\x03 10\\x028\\n0–0.43\\n7285 (20%)\\n3.16 \\x03 10\\x026\\nCluster 2\\nLow\\n0.6–1.1\\n19608 (54%)\\n0.44–0.58\\n7285 (20%)\\nCluster 3\\nMedium\\n1.2–1.7\\n1109(3%)\\n0.59–0.71\\n7285 (20%)\\nCluster 4\\nHigh\\n1.8–2.3\\n80 (0.2%)\\n0.72–0.87\\n7285 (20%)\\nCluster 5\\nVery high\\n2.4–3\\n23 (0.06%)\\n0.88–3\\n7285 (20%)\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n41\\n\\nof multiorgan scenarios where the correlations among the organs\\ncoming from the same donor are also included in the formulation\\nof the problem. Another potential further research direction of this\\nstudy is to validate the patterns obtained from the data mining\\nmodels with a comprehensive simulation model of the organ\\ntransplantation process. Using actual cases, a comprehensive\\ndiscrete-event simulation model can be developed and used as a\\ntest-bed where the potential beneﬁts and limitations of these\\nnovel patterns are tested and validated for a sufﬁciently long\\nperiod of time in the computer simulation environment.\\nReferences\\n[1] Trigt PV, Davis D, Shaeffer GS, Gaynor JW, Landolfo KP, Higginbotham MB, et al.\\nSurvival beneﬁts of heart and lung transplantation. Annals of Surgery\\n1996;223:576–84.\\n[2] Pierson RN, Barr ML, McCullough KP, Egan T, Garrity E, Jessup M, et al. Thoracic\\norgan transplantation. American Journal of Transplantation 2004;4:93–105.\\n[3] Sheppard D, McPhee D, Darke C, Shretha B, Moore R, Jurewitz A, et al. Predicting\\ncytomegalovirus disease after renal transplantation: an artiﬁcial neural network\\napproach. International Journal of Medical Informatics 1999;54:\\n55–76.\\n[4] Lin RS, Horn SD, Hurdle JF, Goldfarb-Rumyantzev S. Single and multiple time-\\npoint prediction models in kidney transplant outcomes. Journal of Biomedical\\nInformatics 2008;41:944–52.\\n[5] Parmar MKB, Machin D. Survival analysis: a practical approach. Cambridge,\\nUK: John Wiley & Sons; 1996.\\n[6] Cox DR. Analysis of survival data. London: Chapman&Hall; 1984.\\n[7] Hariharan S, Johnson CP, Bresnahan BA, Taranto SE, McIntosh MJ, Stablein D.\\nImproved graft survival after renal transplantation in the United States, 1988\\nto 1996. The New England Journal of Medicine 2000;342:605–12.\\n[8] Herrero JI, Lucena JF, Quiroga J, Sangro B, Pardo F, Rotellar F, et al. Liver transplant\\nrecipients older than 60 years have lower survival and higher incidence of\\nmalignancy. American Journal of Transplantation 2003;3:1407–12.\\n[9] Hong Z, Wu J, Smart G, Kaita K, Wen SW, Paton S, et al. Survival analysis of liver\\ntransplant patients in Canada. Transplantation Proceedings 2006;38:2951–6.\\n[10] Kusiak A, Dixon B, Shah S. Predicting survival time for kidney dialysis patients:\\na data mining approach. Computers in Biology and Medicine 2005;35:311–27.\\n[11] Jenkins PC, Flanagan MF, Jenkins KJ, Sargent JD, Canter CE, Chinnock RE, et al.\\nSurvival analysis and risk factors for mortality in transplantation and staged\\nsurgery for hypoplastic left heart syndrome. Journal of the American College of\\nCardiology 2000;36:1178–85.\\n[12] Fernandez-Yanez J, Palomo J, Torrecilla EG, Pascual D, Garrido G, de Diego JJG,\\net al. Prognosis of heart transplant candidates stabilized on medical therapy.\\nRevista Espanola de Cardiologia 2005;58:1162–70.\\n[13] Tjang YS, Heijdan GJMG, Tenderich G, Grobbee D, Korfer R. Survival analysis in\\nheart transplantation: results from an analysis of 1290 Cases in a single center.\\nEuropean Journal of Cardio-Thoracic Surgery 2008;33:856–61.\\n[14] Lin HM, Kaufmann HM, McBride MA, Davies DB, Rosendale JD, Smith CM, et al.\\nCenter-speciﬁc graf and patient survival rates: 1997 UNOS report. JAMA\\n1998;280:1153–60.\\n[15] Cope JT, Kaza AK, Reade CC, Shockey KS, Kern JA, Tribble CG, et al. A cost\\ncomparison of heart transplantation versus alternative operations for cardio-\\nmyopathy. Annual thoracic Surgery 2001;72:1298–305.\\n[16] Aguero J, Almenar L, Martinez-Dolz L, Moro J, Izquierdo MT, Cano O, et al.\\nDifferences in clinical proﬁle and survival after heart transplantation accord-\\ning to prior heart disease. Transplantation Proceedings 2007;39:2350–2.\\n[17] Christensen E, Gunson B, Neuberger J. Optimal timing of liver transplantation\\nfor patients with primary biliary cirrhosis: use of prognostic modeling. Journal\\nof Hepatology 1999;30:285–92.\\n[18] Yoo HY, Galabova V, Edwin D, Thuluvath PJ. Socioeconomicstatus does not affect\\nthe outcome of liver transplantation. Liver Transplantation 2002;8:1133–7.\\n[19] Deng MC, DeMeester MJ, Smiths JMA, Heinecke J, Scheld HH. Effect of receiving\\na heart transplant: analysis of a national cohort entered on to waiting list,\\nstratiﬁed by heart failure severity. British Medical Journal 2000;321:540–5.\\n[20] Ghobrial IM, Habermann TM, Maurer MJ, Geyer SM, Ristow KM, Larson TS,\\net al. Prognostic analysis for survival in adult solid organ transplant recipients\\nwith posy-transplantation lymphoproliferative disorders. Journal of Clinical\\nOncology 2005;23:7574–82.\\n[21] Harper AM, Taranto SE, Edwards EB, Daily OP. An update on a successful\\nsimulation project: the UNOS liver allocation model. In: Joines JA, Barton RR,\\nKang K, Fishwick PA, editors. Proceedings of the winter simulation conference.\\nNew York, NY: ACM Publications; 2000. p. 1955–62.\\n[22] Cupples SA, Ohler L. Transplantation nursing secrets. St. Louis, MO: Hanley &\\nBelfus Publication; 2002.\\n[23] Cristianini N, Shawe-Taylor J. An introduction to support vector machines and\\nother Kernel-based learning methods. London: Cambridge University Press;\\n2000.\\n[24] Mitchell T. Machine learning. New York, NY: McGraw-Hill; 1997.\\n[25] Haykin S. Neural networks: a comprehensive foundation. Upper Saddle River,\\nNJ: Prentice Hall; 1998.\\n[26] Bellazzi R, Zupan B. Predictive data mining in clinical medicine: current issues\\nand guidelines. International Journal of Medical Informatics 2008;77:81–97.\\n[27] Dreiseitl S, Ohno-Machado L. Logistic regression and artiﬁcial neural network\\nclassiﬁcation models: a methodology review. Journal of Biomedical Informat-\\nics 2002;35:352–9.\\n[28] Efron B, Tibshirani R. Statistical data analysis in the computer age. Science\\n1991;253:390–5.\\n[29] Breiman L, Friedman JH, Olshen RA, Stone CJ. Classiﬁcation and regression\\ntrees. Monterey, CA: Wadsworth & Brooks/Cole Advanced Books & Software;\\n1984.\\n[30] Kass GV. An exploratory technique for investigating large quantities of cate-\\ngorical data. Applied Statistics 1980;29:119–27.\\n[31] Quinlan JR. Learning with continuous classes.\\nIn: Adams, Sterling, editors.\\nProceedings of 5th Australian joint conference on artiﬁcial intelligence. Sin-\\ngapore: World Scientiﬁc; 1992. p. 343–8.\\n[32] Makridakis S, Wheelwright SC, Hyndman RJ. Forecasting: methods and appli-\\ncations. New York, NY: John Wiley and Sons; 1998.\\n[33] Everitt BS. Cambridge dictionary of statistics. Cambridge, UK: Cambridge\\nUniversity Press; 2002.\\n[34] Kohavi R. A study of cross-validation and bootstrap for accuracy estimation\\nand model selection. In: Boutilier C, editor. Proceedings of the 14th interna-\\ntional conference on AI (IJCAI). San Mateo, CA: Morgan Kaufmann; 1995. p.\\n1137–45.\\n[35] Olson DL, Delen D. Advanced data mining techniques. New York, NY: Springer;\\n2008.\\n[36] Davis G. Sensitivity analysis in neural net solutions. IEEE Transactions on\\nSystems Man and Cybernetics 1989;19:1078–82.\\n[37] Principe JC, Euliano NR, Lefebvre WC. Neural and adaptive systems. New York,\\nNY: John Wiley and Sons; 2001.\\n[38] Saltelli A. Making best use of model evaluations to compute sensitivity indices.\\nComputer Physics Communications 2002;145:280–97.\\n[39] Saltelli A, Tarantola S, Campolongo F, Ratto M. Sensitivity analysis in practice—\\na guide to assessing scientiﬁc models. New York, NY: John Wiley and Sons;\\n2004.\\n[40] Ohno-Machado L. Modeling medical prognosis: survival analysis techniques.\\nJournal of Biomedical Informatics 2001;34:428–39.\\n[41] Grambsch P, Therneau T. Proportional hazards rates and diagnostics based on\\nweighted residuals. Biometrika 1994;81:515–26.\\n[42] Christensen E. Multivariate survival analysis using Cox’s regression model.\\nHepatology 1987;7:1346–58.\\n[43] Kaplan E, Meier P. Nonparametric estimation from incomplete observations.\\nJournal of the American Statistical Association 1958;53:187–220.\\n[44] MacQueen JB. Some methods for classiﬁcation and analysis of multivariate\\nobservations.\\nIn: Le Cam LM, Neyman J, editors. Proceedings of the ﬁfth\\nsymposium on math, statistics, and probability. Berkeley, CA, USA: University\\nof California Press; 1967. p. 281–97.\\n[45] Krishna K, Murty MN. Genetic k-means algorithm. IEEE Transactions on\\nSystems Man and Cybernetics-Part B Cybernetics 1999;29:433–9.\\n[46] Chiu T, Fang D, Chen J, Wang Y, Jeris C. A robust and scalable clustering\\nalgorithm for mixed type attributes in large database environment. In: Lee D,\\neditor. Proceedings of the seventh ACM SIGKDD international conference on\\nknowledge discovery and data mining. New York, NY: ACM Publications; 2001.\\np. 263.\\n[47] Li ZH, Luo P. Statistical analysis lectures of SPSS for windows. Beijing, China:\\nBeijing Publishing House of Electronics Industry; 2004.\\n[48] SPSS\\nInc.\\nPASW\\nModeler\\nData\\nMining\\nToolkit,\\nVersion\\n13.0,\\nhttp://\\nwww.spss.com/software/modeling/modeler/ 2009 (accessed: June 5, 2009).\\n[49] SAS Institute Inc. Statistical Analysis Systems, Version 9.1.3, http://www.sas.\\ncom/technologies/analytics/statistics/stat/ 2008 (accessed: May 11, 2009).\\n[50] Hair JF, Anderson RE, Tatham RL, Black W. Multivariate data analysis. Upper\\nSaddle River, NJ: Prentice Hall; 1998.\\n[51] Johnson DE. Applied multivariate methods for data analysts. Paciﬁc Grove, CA:\\nDuxbury Press; 1998.\\n[52] Oztekin A, Delen D, Kong ZJ. Predicting the graft survival for heart–lung\\ntransplantation patients: An integrated data mining methodology. Interna-\\ntional Journal of Medical Informatics 2009;78:84–96.\\n[53] Michaud P. Clustering techniques. Future Generation Computer Systems\\n1997;13:135–47.\\nD. Delen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 33–42\\n42\\n'}, {'source': '/Users/sir/Downloads/Data/PDF/Modified-tabu-search-approach-for-variable-selection-in-q_2010_Artificial-In.pdf', 'text': 'Modiﬁed tabu search approach for variable selection in quantitative\\nstructure–activity relationship studies of toxicity of aromatic compounds\\nQi Shen, Wei-Min Shi *, Wei Kong\\nDepartment of Chemistry, Zhengzhou University, Zhengzhou 450052, China\\n1. Introduction\\nQuantitative structure–activity relationships (QSAR) represent\\nan attempt to search for quantitative relationship between\\nchemical structural or property descriptors and activities by\\ndeveloping a QSAR model. Activities used in QSAR include\\nchemical measurements, biological activity, toxicity or bioavail-\\nability, which are taken as dependent variables in building a model.\\nChemical structure is represented by a variety of descriptors,\\nwhich include parameters to account for hydrophobicity, topology,\\nelectronic properties, and steric effects. QSAR data are character-\\nized by hundreds even thousands of structural descriptors on only\\na few compounds with the biological activity values available. This\\nleads either to possible overﬁtting and curse-of-dimensionality or\\neven to a complete failure in building a meaningful regression\\nmodel. Most descriptors may not be relevant to the given activity\\nand these descriptors may potentially degrade the predictive\\nperformance of QSAR analysis by masking the contribution of the\\nrelevant descriptors. The selection of descriptors that are really\\nindicative of activity concerned is one of the key steps in QSAR\\nstudies. The beneﬁt gained from variable selection in QSAR data\\nanalysis is not only the improved predictive performance of the\\nanalysis model, but also the biological interpretability of relation-\\nship between the descriptors and biological activity. A large\\nnumbers of descriptors also increases computational complexity\\nand is time-consuming. Therefore, variables selection is a key step\\nin developing a successful QSAR analysis system.\\nThe goal of variable selection is to ﬁnd an ‘‘optimal’’ subset of all\\ndescriptors that maximizes information contents. To avoid the\\nexponential explosion of an exhaustive search, several methods\\nhave been designed to determine the variable space in a more\\nefﬁcient way [1,2]. For the variable selections, the classical\\nstepwise regression procedure can be used, as well as some more\\nsophisticated techniques such as simulated annealing (SA) [3],\\ngenetic algorithms (GAs) [4,5] and evolution algorithm (EAs)[6].\\nAmong these, GAs and EAs are randomized search algorithms that\\nattempt to overcome the computational costs of exponential\\nmethods and are classiﬁed as a category of the research of so-called\\nartiﬁcial life. Tabu search (TS), a relatively new optimization\\ntechnique in this category, can also be used as a powerful optimizer\\nwhich has been successfully applied to a number of combinatorial\\noptimization problems [7–15]. It employs a ﬂexible memory\\nsystem to avoid convergence to local minima. But the convergence\\nspeed of TS depends on the initial solution and is slow [16]. It\\nusually reaches local minima since a single candidate solution is\\nused to generate offspring [17]. In the present paper, the TS\\nArtiﬁcial Intelligence in Medicine 49 (2010) 61–66\\nA R T I C L E\\nI N F O\\nArticle history:\\nReceived 3 December 2007\\nReceived in revised form 2 November 2009\\nAccepted 17 January 2010\\nKeywords:\\nTabu search\\nVariable selection\\nQuantitative structure–activity relationship\\nAromatic compound\\nA B S T R A C T\\nObjective: Variable selection is a key step in developing a successful quantitative structure–activity\\nrelationships (QSAR) analysis system. Tabu search (TS) can be used for variable selection which employs\\na ﬂexible memory system to avoid convergence to local minima. But the convergence speed of TS\\ndepends on the initial solution and is slow. It usually reaches local minima since a single candidate\\nsolution is used to generate offspring. In the present paper, the TS algorithm was modiﬁed to assist TS to\\nﬁnd the promising regions of the search space rapidly.\\nMethods and materials: A version of modiﬁed TS algorithm is proposed to select variables in QSAR\\nmodeling and to predict toxicity of some aromatic compounds. In the modiﬁed TS, the information which\\nshares mechanism among the best position of all iteration and the personal position is introduced in the\\nstep of generating neighbors of the given solution. The move function which directs the moving of the\\nsolution is recorded as tabu. The modiﬁed Cp statistic is employed as ﬁtness function.\\nResults and conclusions: For comparison, the conventional TS and stepwise regression were also\\nexamined. Experimental results demonstrate that the modiﬁed TS is a useful tool for variable selection\\nwhich converges quickly towards the optimal position.\\n\\x02 2010 Published by Elsevier B.V.\\n* Corresponding author. Tel.: +86 371 67767957; fax: +86 371 67763220.\\nE-mail address: shiweimin@zzu.edu.cn (W.-M. Shi).\\nContents lists available at ScienceDirect\\nArtificial Intelligence in Medicine\\njournal homepage: www.elsevier.com/locate/aiim\\n0933-3657/$ – see front matter \\x02 2010 Published by Elsevier B.V.\\ndoi:10.1016/j.artmed.2010.01.004\\n\\nalgorithm was modiﬁed to assist the TS to ﬁnd the promising\\nregions of the search space rapidly. A modiﬁed TS algorithm was\\nproposed to select variables in multiple linear regression (MLR)\\nmodeling and used to predict toxicity of aromatic compounds. The\\nresults were compared to those obtained by stepwise regression.\\nThe results demonstrate that the modiﬁed TS is a useful tool for\\nvariable selection which converges quickly towards the optimal\\nposition.\\nAromatic compounds are widely used in papermaking, leather,\\ntextile and other industries. Benzene derivatives comprise a\\nsigniﬁcant component of the pollutant burden on the environment\\nand have known harmful effects on human health and the\\nenvironment. Experimental assessment of toxicity of aromatic\\ncompounds can be expensive, time-consuming and hazardous.\\nQSAR can be used to predict toxicity of aromatic compounds when\\nexperimental data are not available.\\n2. Methods\\n2.1. Tabu search\\nTS which is a metaheuristic strategy was initially proposed by\\nFred Glover [7]. Lots of applications of TS can be found in tutorials\\n[7–9]. TS is an iterative procedure designed for the solution of\\noptimization problems. TS starts with a random solution or a\\nsolution obtained by a constructive and deterministic method and\\nevaluates the ﬁtness function. Then all possible neighbors of the\\ngiven solution are generated and evaluated. A neighbor is a\\nsolution which can be reached from the current solution by a\\nsimple, basic transformation or move. New solution is generated\\nfrom the neighbors of the current one. To avoid retracing the used\\nsteps, the method records recent moves in a tabu list. The tabu list\\nkeeps track of previously explored solutions and forbids the search\\nfrom returning to a previously visited solution. If the best of these\\nneighbors is not in the tabu list, pick it to be the new current\\nsolution. One of the most important features of TS is that a new\\nsolution may be accepted even if the best neighbor solution is\\nworse than the current one. In this way it is possible to overcome\\ntrapping in local minima. If a neighbor solution is selected as new\\nsolution, this solution or moves is classiﬁed as tabu. Some\\naspiration criteria which allow overriding of tabu status can be\\nintroduced if that moves is found to lead to a better ﬁtness with\\nrespect to the ﬁtness of the current optimum. The aspiration\\ncriterion selected here will avoid missing good solutions. If the best\\nobject function of the generation fulﬁlls the end condition or the\\nnumber of iteration reaches a user-deﬁned limit, the algorithm\\nstops. Otherwise, the algorithm continues the TS procedures. TS\\nmethod usually is completed with diversiﬁcation and intensiﬁca-\\ntion procedures.\\nFor variable selection problem, TS was implemented as follows:\\n(1) variable selection solution is represented by a 0/1 bit string.\\nInitial solution is randomly generated. (2) The neighbor of a\\nsolution vector x is a set of solutions, which are generated through\\nadding or deleting a variable on x. (3) Twenty neighbors solutions\\nare randomly selected as candidate solutions. (4) If a neighbor\\nsolution is selected as the new current solution, this move is\\nrecorded in tabu list. The tabu list size is selected as 5. (5) If a\\nneighbor solution results in a best ﬁtness for all previous iterations,\\npick it to be the new current solution even if it is in the tabu list and\\nrecord this move in tabu list. (6) Termination condition is a pre-\\ndeﬁned number of iterations.\\n2.2. Modiﬁed TS\\nIn TS algorithm, if the neighbor solution is not in tabu list, pick it\\nto be the new current solution. However, this solution is often\\nworse than the current best solution. On the other hand, it usually\\nreaches local minima and the best solution in the TS is unchanged\\nfor a lot of iterations. It will take much time to reach the near-\\nglobal minimum and the convergence speed of TS sometimes is\\nslow [16,17]. To improve the performance, the information sharing\\nmechanism among the best previous solution of all iteration and\\nthe current solution is introduced in the step of generating\\nneighbors of a given solution. The neighbors are generated by\\nmoving the given solution following the best solution of all\\niteration and the move function directs the moving of it.\\nFor D-variable selection in QSAR, the solution is expressed a\\nstring of binary bits in a D-dimensional space and is represented as\\nX = (x1, x2, . . ., xD). The binary-bit-coded string stands for a set of\\nvariable, which are used for evaluating the ﬁtness function (please\\nrefer to the next section). A bit ‘‘0’’ in a solution represents the\\nuselessness of corresponding variable. A move in a search space is\\nrestricted to 0 or 1 on each dimension. The best previous solution\\nthat gives the best ﬁtness value is represented as P = (p1, p2, . . ., pD).\\nIn binary problem, updating a solution represents changes of a bit\\nwhich should be in either state 1 or 0. The move function (F) which\\ndirects the moving of the solution is represented as Fi = (f1, f2, . . .,\\nfD). The move function fd of every neighbor (N = (n1, n2, . . ., nD)) is a\\nrandom number in the range of (0, 1). The resulting move in\\nsolution is then deﬁned by the following rule:\\nIf ð0 < f d \\x02 aÞ; then nd ¼ xd\\n(1)\\nIf ða < f dÞ; then nd ¼ pd\\n(2)\\nwhere a is a random value in the range of (0, 1) named static\\nprobability. The static probability a plays the role of balancing the\\nglobal and local search. That means the larger the value of the\\nparameter a is, the greater the probability for the neighbors to\\noverleap local optima. On the other hand, a small value of\\nparameter a is favorable for the neighbors to follow the best past\\npositions and for the algorithm to converge more quickly.\\nTherefore, we deﬁne the parameter descending along with the\\ngeneration. Static probability a starts with a value 0.7 and\\ndecreases to 0.3 when the iteration terminates. As introduction\\nof the information sharing mechanism, the random solution tends\\nto converge to the best solution quickly. The modiﬁed TS is\\ndescribed as follows:\\n\\x03 Step 1. Randomly generate an initial solution (X, initial binary\\nstring) and evaluate the ﬁtness function of this individual. The\\ninitial solution is a string of binary bits corresponding to each\\nvariable.\\n\\x03 Step 2. Generate the neighbors of the solution according to the\\ninformation sharing mechanism (Eqs. (1) and (2)). Then the\\nperformance of each neighbor or solution is measured according\\nto a pre-deﬁned ﬁtness function. When setting the number of\\nneighbor solution as a large value, the algorithm would make a\\ndeep search in a local region. But this may increase the\\ncalculation burden. Hence, the number of neighbors solution\\nis set as 20 for variable selection by experience to keep balance\\nbetween the search depth and calculation burden.\\n\\x03 Step 3. Pick new individual from the examined neighbor\\naccording to the aspiration criteria and tabu conditions and\\nthen update the solution. The move function which directs the\\nmoving of the solution was recorded in tabu list. If the neighbor\\nsolution is not in tabu list, pick it to be the new current solution. If\\nthe best of these neighbors is found to lead to a better ﬁtness\\nwith respect to the ﬁtness of the current optimum, override the\\ntabu status and pick it to be the new current solution according\\nto aspiration criteria. A small value of the tabu list size is\\nfavorable to reduce the calculation burden. However setting the\\ntabu list size as a small value may cause the algorithm to\\nQ. Shen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 61–66\\n62\\n\\nconverge to local optima. The tabu list size was selected as 5 to\\njump out from local minima.\\n\\x03 Step 4. If all neighbors are tabu solutions, a new solution is\\ngenerated randomly to further improve the ability of modiﬁed TS\\nto overleap local optima. The ﬁtness function of the new solution\\nis then evaluated.\\n\\x03 Step 5. If the number of iteration reaches a pre-deﬁned number of\\niterations,\\nthe\\ntraining\\nstopped\\nwith\\nthe\\nresults\\noutput,\\notherwise, go to the second step to renew solution. The modiﬁed\\nTS scheme is presented in Fig. 1.\\n2.3. Fitness function\\nIn modiﬁed TS, the performance of each neighbor or solution is\\nmeasured according to a pre-deﬁned ﬁtness function. The modiﬁed\\nCp statistic as objective function is applied to variable selection in\\nthe modiﬁed TS. The modiﬁed Cp in MLR is expressed as follows:\\nCpð pÞ ¼ RSS p\\nˆs2\\nPLS\\n\\x04 ðn \\x04 2 pÞ\\n(3)\\nwhere n is the number of dependent variables and p is the number\\nof independent variables. RSSp is the residual sum of the squares of\\np-variable MLR model,\\nˆs2\\nPLS is deﬁned as the value of RSS\\ncorresponding to the minimum number of principal components\\nin conventional partial least squares (PLS) analysis of the original\\ndata set when further increase of the number of principal\\ncomponents does not cause a signiﬁcant reduction in RSS. PLS\\nanalysis is a factor analytical technique that is useful when there\\nare more independent variables in data matrix (X) than in the\\ntarget matrix (Y). The underlying assumption of PLS is that the\\nobserved data is generated by a process which is driven by a small\\nnumber of latent variables or principal components. In its general\\nform PLS creates orthogonal score vectors or components by\\nmaximizing the covariance between different sets of variables. The\\ndetails of modiﬁed Cp have been described elsewhere [18].\\n3. Aromatic compounds toxicity data\\nA total of 65 aromatic chemicals with the observed toxicity to\\nchlorella vulgaris in a novel short-term assay taken from the study\\nby Netzeva et al. [19] were used to assess the performance of the\\nmodiﬁed TS in variable selection of QSAR. The data set is\\nchemically heterogeneous (includes phenols, anilines, nitroben-\\nzenes, benzaldehydes, etc.) and represents several mechanisms of\\ntoxic action.\\nA series of descriptors were calculated, which encoded different\\naspects of the molecular structure and consist of spatial, thermody-\\nnamic, structural, electronic and information-content descriptors.\\nThe spatial descriptors [20,21] used involve radius of gyration\\n(RadOfGyration), density, molecular surface area, principal moment\\nof inertia (PMI), molecular volume, and shadow indices. The\\nthermodynamic descriptors [22] were taken to describe the\\nhydrophobic character, refractivity (MolRef: molar refractivity),\\nheat of formation (Hf) and the dissolution free energy for water and\\noctanol (Fh2o: desolvation free energy for H2O; Foct: desolvation\\nfreeenergyforoctanol).Structural descriptorsinclude the molecular\\nweight (MW), the number of rotatable bonds (Rotbonds) and the\\nnumber of hydrogen bond (Hbond acceptor). The electronic\\ndescriptors [23] taken were concerning surperdelocalizability (Sr),\\natomic polarizabilities (Apol), and the dipole moment (Dipole).\\nElectrotopological-state indices (E-State indices) [24,25] involved S-\\naasC, S-aaN, S-aNH2, etc. Some so-called information-content\\ndescriptors [26] such as atomic composition indices and multigraph\\ninformation-content indices were also included in the candidate list.\\nAll these molecular descriptors were generated using Cerius23.5\\nsoftware on Silicon Graphics R3000 workstation. Besides the\\naforementioned molecular descriptors, 7 variables used by Netzeva\\netal. were also includedin the list of the candidatevariables (logKow:\\nlogarithm of the octanol–water partition coefﬁcient; Ehomo: MOPAC\\nenergy of the highest occupied molecular orbital; Elumo: energy of\\nthe lowest unoccupied molecular orbital; Amax: maximum acceptor\\nsuperdelocalizability; QHmax: maximum positive partial charge on a\\nhydrogen atom; Qmin: maximum negative partial charge;\\n0xv:\\nmolecular connectivity indices).\\nThe descriptor analysis involves the detection and removal of\\nthose structural descriptors which exhibit high pair-wise correla-\\ntions with other descriptors or which contain little discriminatory\\ninformation. Pairs of descriptors that are highly correlated\\n(r \\x05 0.90) encoded similar information, and one of them should\\nbe removed. Descriptors that contain a high percentage (\\x0590%) of\\nidentical values are also discarded. Table 1 summarizes all\\nmolecular descriptors used as the candidate variables for selection.\\nThe modiﬁed TS and MLR algorithms were written in Matlab 5.3\\nand run on a personal computer (Intel Pentium processor 4/1.5G\\nHz 256 MB RAM).\\n4. Results and discussion\\nThe modiﬁed TS was ﬁrst used for variable selection, in which\\nthe number of neighbors solution was set as 20 and the tabu list\\nsize was selected as 5. The number of iterations was set as 1000.\\nThe selected descriptors were taken as dependent variables to\\nbuild QSAR models with MLR method. The best model with\\nminimum ﬁtness value contains three variables as given by the\\nmodiﬁed TS. The three variables are AlogP, MolRef and Amax. The\\ncorrelation between the calculated and experimental values of lg1/\\nEC50 of three-descriptor model is shown in Fig. 2. The correlation\\nFig. 1. The chart of the modiﬁed TS scheme.\\nQ. Shen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 61–66\\n63\\n\\ncoefﬁcient (R2) and the standard deviation for the three-variable\\nmodel is 0.8664 and 0.3941, respectively. The two-variable model\\nobtained by modiﬁed TS is the same as those obtained by Netzeva\\nin the literature [19]. These models and the best model that contain\\nfour variables are shown in Table 2. In these equations, positive\\ncoefﬁcient of AlogP indicates that the large hydrophobic character\\nof the molecule would promote the toxicity. MolRef is a combined\\nmeasure\\nof\\nmolecular\\nsize\\nand\\npolarizability,\\nand\\npositive\\ncoefﬁcient of MolRef indicates that increasing the molar refractiv-\\nity of the molecule causes an increase in toxicity. These results\\nshows that Amax is one of the important variables and the larger\\nAmax can increase the toxicity. That is in accordance with the\\nconclusions by Netzeva et al. that Amax was found to be a superior\\ndescriptor for modeling the acute toxicity of aromatic compounds\\n[20].\\nVariable selection by conventional TS was also performed to\\ncompare with the modiﬁed TS. The best model with minimum\\nﬁtness value given by the conventional TS contains two variables.\\nThe two variables are logKow and Elumo. The correlation coefﬁcients\\n(R2) and the standard deviation were 0.8389 and 0.4291, respec-\\ntively. The minimum ﬁtness could be obtained in about 40 cycles\\nduring the modiﬁed TS algorithm, but about 630 cycles were needed\\nduring conventional TS algorithm. Searching speed of the conven-\\ntional TS algorithm is slow, but experimental results demonstrated\\nthat the modiﬁed TS converges to the global best solution rapidly.\\nTo compare with modiﬁed TS, variables selection by stepwise\\nregression was also performed. The obtained model by stepwise\\nregression contains four variables. The four variables are Shadow-\\nXYfrac, logKow, 0xv and Amax. The correlation coefﬁcients (R2) and\\nthe standard deviation were 0.8433 and 0.4303, respectively. A\\ncomparison with stepwise regression shows that better results\\nwere obtained from modiﬁed TS algorithm.\\nThe real goal of developing QSAR is to predict the activity. To\\ncheck the validity of the proposed methods, the data set of 65\\naromatic chemicals was stochastically divided into two groups.\\nTwo-third compounds were used as the training set for developing\\nregression models, while the remaining one-third compounds\\nwere used as the predicted dataset. The best MLR models selected\\nby the training set contain three variables (AlogP, MolRef and\\nAmax). The plot of observed toxicity against that calculated by the\\nthree-variable model is shown in Fig. 3. It was found that using\\nTable 1\\nList of molecular descriptors for aromatic compounds studied as candidate\\nvariables.\\nFunctional families\\nof descriptors\\nDescriptors\\nSpatial descriptors\\nShadow indices (surface area projections)\\n(Shadow-XY, Shadow-XZ, Shadow-YZ,\\nShadow-nu, Shadow-XYfrac, Shadow-XZfrac,\\nShadow-YZfrac)\\nVm (molecular volume)\\nDensity\\nArea (molecular surface area)\\nRadOfGyration (Radius of gyration)\\nJurs descriptors (Jurs Charged Partial Surface\\nArea descriptors)\\nPMI (Principal moment of inertia, including\\nPMI-mag, PMI-X, PMI-Y, PMI-Z)\\nStructural descriptors\\nMW (molecular weight)\\nHbond acceptor (number of hydrogen\\nbond acceptors)\\nHbond donor (number of hydrogen bond donors)\\nRotbonds (number of rotatable bonds)\\nElectronic descriptors\\nApol (sum of atomic polarizabilities)\\nDipole (Dipole-mag, Dipole-X, Dipole-Y, Dipole-Z)\\nSr (superdelocalizability)\\nQuantum mechanical\\ndescriptors\\nHOMO, Ehomo (highest occupied molecular\\norbital energy)\\nLUMO, Elumo (lowest unoccupied molecular\\norbital energy)\\nAmax (maximum acceptor superdelocalizability)\\nQmax (maximum positive partial charge on\\na hydrogen atom)\\nQmin (maximum negative partial charge)\\nThermodynamic\\ndescriptors\\nAlogP, logP, logKow (the octanol/water\\npartition coefﬁcient)\\nFh2o (desolvation free energy for water)\\nFoct (desolvation free energy for octanol)\\nMolRef (molar refractivity)\\nHeat of formation (Hf)\\nE-State index\\nS-sCH3, S-aaCH, S-aasC, S-aNH2, S-aasN,\\nS-ssO, S-sOH\\nTopological index\\n0xv, 1xv, 2xv, 3xv, 4xv,0x, 1x, 2x, 3x, 4x\\nFig. 2. Calculated versus observed lg1/EC50 of three-variable model by modiﬁed TS using multiple linear regression modeling.\\nQ. Shen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 61–66\\n64\\n\\nthree descriptors the correlation coefﬁcients (R2) for the training\\nand the test set were 0.8709 and 0.8597, respectively. The standard\\ndeviation for training set was 0.3813.\\nEven the data set was stochastically divided into two groups, it\\nshould be noted that the predicted accuracy of a model at each\\niteration is not necessarily the same because of the various\\npartition of training and tests sets. The reliability of a model is an\\nessential issue in QSAR analysis. To evaluate the predictive ability\\nand reliability of models by the modiﬁed TS accurately, the total\\nsamples were randomly partitioned into training and tests sets 200\\nTable 2\\nResults of variable selection by the modiﬁed TS and MLR modeling.\\nNo.\\nEquation\\nR2a\\nSa\\nFa\\n1\\nlg1/IE50 = 0.731 \\x06 logKow \\x04 0.5906 \\x06 Elumo \\x04 1.9142\\n0.8389\\n0.4291\\n161.4795\\n2\\nlg1/IE50 = 0.4880 \\x06 AlogP + 0.0314 \\x06 MolRef + 19.4433 \\x06 Amax \\x04 8.6175\\n0.8664\\n0.3941\\n131.7711\\n3\\nlg1/IE50 = 0.4340 \\x06 AlogP + 0.0375 \\x06 MolRef + 18.0856 \\x06 Amax \\x04 0.0551S-ssO \\x04 8.2768\\n0.8729\\n0.3874\\n103.0543\\na R: correlation coefﬁcient; S: standard deviation; F: F-statistics.\\nFig. 3. Plot of lg1/EC50 calculated from three-variable model versus the observed lg1/EC50 values for training and test set.\\nFig. 4. Distribution of correlation coefﬁcient (R2) over 200 runs of partition samples using the best three-variable model.\\nQ. Shen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 61–66\\n65\\n\\ntimes and then averaged the model accuracy for each partition of\\ntraining and tests sets. By resampling a large number of learning\\nsamples, the correlation coefﬁcient (R2) for training set and test set\\nwere 0.8682 and 0.8515, respectively by the modiﬁed TS. The root\\nmean square errors for training set and test set were 0.3582 and\\n0.4646, respectively. Fig. 4 shows the distribution of correlation\\ncoefﬁcient (R2) over 200 runs of partition samples using the best\\nthree-variable model. As shown in Fig. 4, correlation coefﬁcient\\n(R2) larger than 0.8515 is about 107 times in 200 runs for test set\\nand the highest R2 achieve 0.9650. The results show that the model\\nby the modiﬁed TS is stable and reliable.\\nThe modiﬁed TS was run for about 1000 times, and the number\\nof times which a particular molecular descriptor appears in 1000\\ncycles was counted. When the descriptors by the order of\\ndecreasing numbers of times of appearance were listed, the top\\ndescriptors or the most frequently appeared feathers were\\nobtained. Amax is an index of reactivity in aromatic compounds\\nthat was used by Netzeva et al. [19], and it turned to be one of the\\nmost important variables. MolRef is a combined measure of its\\nsize and polarizability which seems essential with respect to\\ntoxicity of aromatic compounds. AlogP which is a factor relating to\\nthe hydrophobic character of the molecule are factors is usually\\nmuch considered in the development of QSAR in biochemistry.\\nThey are shown to be important in QSAR of aromatic compounds\\ntoxicity. ‘‘Elumo’’ is also a preferred descriptors and the negative\\ncoefﬁcient of descriptor Elumo implied that molecules with low-\\nenergy\\nElumo\\nwould\\npromote\\nthe\\ntoxicity.\\nThere\\nare\\ntwo\\nelectrotopological-state descriptors (S-sCH3, S-aNH2) among\\nthe top descriptors and these indices seem to be information-\\nrich in describing molecular structure. The subscript ‘–sCH3’\\nrefers to methyl and ‘aNH2’ refers to the amine in phenyl group.\\n‘NH2’ in phenyl group can increase toxicity and ‘CH3’ in aromatic\\ncompounds would decrease toxicity. Besides these variables,\\ndescriptors Shadow-nu, Vm, Apol, Dipole-Z, PMI-mag, PMI-Z and\\nQHmax also have advantage for toxicity of aromatic compounds.\\nThe toxicity of aromatic compounds is a complex one, which\\ninvolves\\nspatial,\\nthermodynamic,\\nelectronic,\\nand\\nstructural\\neffects.\\n5. Conclusion\\nIn the present study, the TS algorithm was modiﬁed to be used\\nin variable selection in QSAR modeling for predicting toxicity of\\naromatic compounds. The modiﬁed Cp was employed as ﬁtness\\nfunction. It has been demonstrated that the modiﬁed TS is a useful\\ntool for variable selection with nice performance and the ability to\\nselect preferred variables with satisfactory convergence rates. In\\nthe selected descriptors, Amax, MolRef and AlogP are the most\\nimportant descriptors in predicting toxicity of aromatic com-\\npounds.\\nAcknowledgement\\nThe work was ﬁnancially supported by the National Natural\\nScience Foundation of China (Grant no. 20505015).\\nReferences\\n[1] Gualdro´n O, Llobet E, Brezmes J, Vilanova X, Correi X. Fast variable selection for\\ngas sensing applications, vol. 2. In: Sensors, Proceedings of IEEE; 2004. p. 892–5.\\n[2] Liu H, Motoda H. Feature selection for knowledge discovery and data mining.\\nBoston: Kluwer Academic Publishers; 1998. p. 37–51.\\n[3] Shen M, Arnaud L, Xiao Y, Alexander G, Harold K, Alexander T. Quantitative\\nstructure–activity relationship analysis of functionalized amino acid anticon-\\nvulsant agents using k nearest neighbor and simulated annealing PLS methods.\\nJ Med Chem 2002;45:2811–23.\\n[4] Cho SJ, Hermsmeier MA. Genetic algorithm guided selection: variable selection\\nand subset selection. J Chem Inf Comput Sci 2002;42:927–36.\\n[5] Yasri A, Hartsough D. Toward an optimal procedure for variable selection and\\nQSAR model building. J Chem Inf Comput Sci 2001;41:1218–27.\\n[6] Brian TL. Evolutionary programming applied to the development of quantita-\\ntive structure–activity relationships and quantitative structure–property rela-\\ntionships. J Chem Inf Comput Sci 1994;34:1279–85.\\n[7] Glover, F. Tabu search. Part II. ORSA J Comput 1990;2(1):4–32.\\n[8] Glover F, Laguna M. Tabu search. Boston: Kluwer Academic Publishers; 1997.\\n[9] Glover F, Laguna M. Tabu search, handbook of applied optimization. In:\\nPardalos PM, Resende MGC, editors. Oxford: Oxford University Press; 2002.\\np. 194–208.\\n[10] Pacheco J, Casado S, Nu´ n˜ez L, Go´mez O. Analysis of new variable selection\\nmethods for discriminant analysis. Comput Stat Data Anal 2006;51(3):1463–78.\\n[11] Todeschini R, Consonni V, Pavan M. A distance measure between models: a\\ntool for similarity/diversity analysis of model populations. Chemometr Intell\\nLab Syst 2004;70:55–61.\\n[12] Chavali S, Lin B, Miller DC, Camarda KV. Environmentally-benign transition\\nmetal catalyst design using optimization techniques. Comput Chem Eng\\n2004;28:605–11.\\n[13] Vainio MJ, Johnson MS. McQSAR: a multiconformational quantitative struc-\\nture–activity relationship engine driven by genetic algorithms. J Chem Inf\\nModel 2005;45(6):1953–61.\\n[14] Gani R, Harper PM, Hostrup M. Automatic creation of missing groups through\\nconnectivity index for pure-component property prediction. Ind Eng Chem Res\\n2005;44(18):7262–9.\\n[15] Mills Jamie D, Olejnik Stephen F, Marcoulides George A. The tabu search\\nprocedure: an alternative to the variable selection methods. Multivariate\\nBehav Res 2005;40(3):351–71.\\n[16] Pad JS, Lut ZM, Chu SC, Sun SH. Non-redundant VQ channel coding using\\nmodiﬁed tabu search approach with simulated annealing. In: Third interna-\\ntional conference on knowledge-based intelligent information engineering\\nsystems. Adelaide, Australia: IEEE Press; 1999. p. 242–5.\\n[17] Kvasnicka V, Pospichal J. Fast evaluation of chemical distance by tabu search\\nalgorithm. J Chem Inf Comput Sci 1994;34(5):1109–12.\\n[18] Shen Q, Jing JH, Yu RQ. Variable selection by an evolution algorithm using\\nmodiﬁed Cp based on MLR and PLS modeling: QSAR studies of carcinogenicity\\nof aromatic amines. Anal Bioanal Chem 2003;375:248–54.\\n[19] Netzeva TI, Dearden JC, Edwards R, Worgan ADP, Cronin MTD. QSAR analysis of\\nthe toxicity of aromatic compounds to chlorella vulgaris in a novel short-term\\nassay. J Chem Inf Comput Sci 2004;44:258–65.\\n[20] Rohrbaugh RH, Jurs PC. Descriptions of molecular shape applied in studies of\\nstructure/activity and structure/property relationships. Anal Chim Acta\\n1987;199:99–109.\\n[21] Stanton DT, Jurs PC. Development and use of charged partial surface area\\nstructural descriptors in computer-assisted quantitative structure–property\\nrelationship studies. Anal Chem 1990;62:2323–9.\\n[22] Viswanadhan VN, Ghose AK, Revankar GR, Robins RK. Atomic physicochemical\\nparameters for three dimensional structure directed quantitative structure–\\nactivity relationships. 4’. Additional parameters for hydrophobic and disper-\\nsive interactions and their application for an automated superposition of\\ncertain naturally occurring nucleoside antibiotics. J Chem Inf Comput Sci\\n1989;29:163–72.\\n[23] Ciuprina G, Loan D, Munteanu I. Use of intelligent-particle swarm optimiza-\\ntion in electromagentics. IEEE Trans Magn 2002;38(2):1037–40.\\n[24] Hall LH, Kier LB. The electrotopological state: structure information at the\\natomic level for molecular graphs. J Chem Inf Comput Sci 1991;31:76–8.\\n[25] Hall LH, Kier LB. Electrotopological state indices for atom types: a novel\\ncombination of electronic, topological, and valence state information. J Chem\\nInf Comput Sci 1995;35:1039–45.\\n[26] Boncher. Danailinformation theoretic indices for characterization of chemical\\nstructures. Chichester, UK: Research Studies Press; 1983. p. 249.\\nQ. Shen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 61–66\\n66\\n'}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/Modified-tabu-search-approach-for-variable-selection-in-q_2010_Artificial-In.pdf', 'text': 'Modiﬁed tabu search approach for variable selection in quantitative\\nstructure–activity relationship studies of toxicity of aromatic compounds\\nQi Shen, Wei-Min Shi *, Wei Kong\\nDepartment of Chemistry, Zhengzhou University, Zhengzhou 450052, China\\n1. Introduction\\nQuantitative structure–activity relationships (QSAR) represent\\nan attempt to search for quantitative relationship between\\nchemical structural or property descriptors and activities by\\ndeveloping a QSAR model. Activities used in QSAR include\\nchemical measurements, biological activity, toxicity or bioavail-\\nability, which are taken as dependent variables in building a model.\\nChemical structure is represented by a variety of descriptors,\\nwhich include parameters to account for hydrophobicity, topology,\\nelectronic properties, and steric effects. QSAR data are character-\\nized by hundreds even thousands of structural descriptors on only\\na few compounds with the biological activity values available. This\\nleads either to possible overﬁtting and curse-of-dimensionality or\\neven to a complete failure in building a meaningful regression\\nmodel. Most descriptors may not be relevant to the given activity\\nand these descriptors may potentially degrade the predictive\\nperformance of QSAR analysis by masking the contribution of the\\nrelevant descriptors. The selection of descriptors that are really\\nindicative of activity concerned is one of the key steps in QSAR\\nstudies. The beneﬁt gained from variable selection in QSAR data\\nanalysis is not only the improved predictive performance of the\\nanalysis model, but also the biological interpretability of relation-\\nship between the descriptors and biological activity. A large\\nnumbers of descriptors also increases computational complexity\\nand is time-consuming. Therefore, variables selection is a key step\\nin developing a successful QSAR analysis system.\\nThe goal of variable selection is to ﬁnd an ‘‘optimal’’ subset of all\\ndescriptors that maximizes information contents. To avoid the\\nexponential explosion of an exhaustive search, several methods\\nhave been designed to determine the variable space in a more\\nefﬁcient way [1,2]. For the variable selections, the classical\\nstepwise regression procedure can be used, as well as some more\\nsophisticated techniques such as simulated annealing (SA) [3],\\ngenetic algorithms (GAs) [4,5] and evolution algorithm (EAs)[6].\\nAmong these, GAs and EAs are randomized search algorithms that\\nattempt to overcome the computational costs of exponential\\nmethods and are classiﬁed as a category of the research of so-called\\nartiﬁcial life. Tabu search (TS), a relatively new optimization\\ntechnique in this category, can also be used as a powerful optimizer\\nwhich has been successfully applied to a number of combinatorial\\noptimization problems [7–15]. It employs a ﬂexible memory\\nsystem to avoid convergence to local minima. But the convergence\\nspeed of TS depends on the initial solution and is slow [16]. It\\nusually reaches local minima since a single candidate solution is\\nused to generate offspring [17]. In the present paper, the TS\\nArtiﬁcial Intelligence in Medicine 49 (2010) 61–66\\nA R T I C L E\\nI N F O\\nArticle history:\\nReceived 3 December 2007\\nReceived in revised form 2 November 2009\\nAccepted 17 January 2010\\nKeywords:\\nTabu search\\nVariable selection\\nQuantitative structure–activity relationship\\nAromatic compound\\nA B S T R A C T\\nObjective: Variable selection is a key step in developing a successful quantitative structure–activity\\nrelationships (QSAR) analysis system. Tabu search (TS) can be used for variable selection which employs\\na ﬂexible memory system to avoid convergence to local minima. But the convergence speed of TS\\ndepends on the initial solution and is slow. It usually reaches local minima since a single candidate\\nsolution is used to generate offspring. In the present paper, the TS algorithm was modiﬁed to assist TS to\\nﬁnd the promising regions of the search space rapidly.\\nMethods and materials: A version of modiﬁed TS algorithm is proposed to select variables in QSAR\\nmodeling and to predict toxicity of some aromatic compounds. In the modiﬁed TS, the information which\\nshares mechanism among the best position of all iteration and the personal position is introduced in the\\nstep of generating neighbors of the given solution. The move function which directs the moving of the\\nsolution is recorded as tabu. The modiﬁed Cp statistic is employed as ﬁtness function.\\nResults and conclusions: For comparison, the conventional TS and stepwise regression were also\\nexamined. Experimental results demonstrate that the modiﬁed TS is a useful tool for variable selection\\nwhich converges quickly towards the optimal position.\\n\\x02 2010 Published by Elsevier B.V.\\n* Corresponding author. Tel.: +86 371 67767957; fax: +86 371 67763220.\\nE-mail address: shiweimin@zzu.edu.cn (W.-M. Shi).\\nContents lists available at ScienceDirect\\nArtificial Intelligence in Medicine\\njournal homepage: www.elsevier.com/locate/aiim\\n0933-3657/$ – see front matter \\x02 2010 Published by Elsevier B.V.\\ndoi:10.1016/j.artmed.2010.01.004\\n\\nalgorithm was modiﬁed to assist the TS to ﬁnd the promising\\nregions of the search space rapidly. A modiﬁed TS algorithm was\\nproposed to select variables in multiple linear regression (MLR)\\nmodeling and used to predict toxicity of aromatic compounds. The\\nresults were compared to those obtained by stepwise regression.\\nThe results demonstrate that the modiﬁed TS is a useful tool for\\nvariable selection which converges quickly towards the optimal\\nposition.\\nAromatic compounds are widely used in papermaking, leather,\\ntextile and other industries. Benzene derivatives comprise a\\nsigniﬁcant component of the pollutant burden on the environment\\nand have known harmful effects on human health and the\\nenvironment. Experimental assessment of toxicity of aromatic\\ncompounds can be expensive, time-consuming and hazardous.\\nQSAR can be used to predict toxicity of aromatic compounds when\\nexperimental data are not available.\\n2. Methods\\n2.1. Tabu search\\nTS which is a metaheuristic strategy was initially proposed by\\nFred Glover [7]. Lots of applications of TS can be found in tutorials\\n[7–9]. TS is an iterative procedure designed for the solution of\\noptimization problems. TS starts with a random solution or a\\nsolution obtained by a constructive and deterministic method and\\nevaluates the ﬁtness function. Then all possible neighbors of the\\ngiven solution are generated and evaluated. A neighbor is a\\nsolution which can be reached from the current solution by a\\nsimple, basic transformation or move. New solution is generated\\nfrom the neighbors of the current one. To avoid retracing the used\\nsteps, the method records recent moves in a tabu list. The tabu list\\nkeeps track of previously explored solutions and forbids the search\\nfrom returning to a previously visited solution. If the best of these\\nneighbors is not in the tabu list, pick it to be the new current\\nsolution. One of the most important features of TS is that a new\\nsolution may be accepted even if the best neighbor solution is\\nworse than the current one. In this way it is possible to overcome\\ntrapping in local minima. If a neighbor solution is selected as new\\nsolution, this solution or moves is classiﬁed as tabu. Some\\naspiration criteria which allow overriding of tabu status can be\\nintroduced if that moves is found to lead to a better ﬁtness with\\nrespect to the ﬁtness of the current optimum. The aspiration\\ncriterion selected here will avoid missing good solutions. If the best\\nobject function of the generation fulﬁlls the end condition or the\\nnumber of iteration reaches a user-deﬁned limit, the algorithm\\nstops. Otherwise, the algorithm continues the TS procedures. TS\\nmethod usually is completed with diversiﬁcation and intensiﬁca-\\ntion procedures.\\nFor variable selection problem, TS was implemented as follows:\\n(1) variable selection solution is represented by a 0/1 bit string.\\nInitial solution is randomly generated. (2) The neighbor of a\\nsolution vector x is a set of solutions, which are generated through\\nadding or deleting a variable on x. (3) Twenty neighbors solutions\\nare randomly selected as candidate solutions. (4) If a neighbor\\nsolution is selected as the new current solution, this move is\\nrecorded in tabu list. The tabu list size is selected as 5. (5) If a\\nneighbor solution results in a best ﬁtness for all previous iterations,\\npick it to be the new current solution even if it is in the tabu list and\\nrecord this move in tabu list. (6) Termination condition is a pre-\\ndeﬁned number of iterations.\\n2.2. Modiﬁed TS\\nIn TS algorithm, if the neighbor solution is not in tabu list, pick it\\nto be the new current solution. However, this solution is often\\nworse than the current best solution. On the other hand, it usually\\nreaches local minima and the best solution in the TS is unchanged\\nfor a lot of iterations. It will take much time to reach the near-\\nglobal minimum and the convergence speed of TS sometimes is\\nslow [16,17]. To improve the performance, the information sharing\\nmechanism among the best previous solution of all iteration and\\nthe current solution is introduced in the step of generating\\nneighbors of a given solution. The neighbors are generated by\\nmoving the given solution following the best solution of all\\niteration and the move function directs the moving of it.\\nFor D-variable selection in QSAR, the solution is expressed a\\nstring of binary bits in a D-dimensional space and is represented as\\nX = (x1, x2, . . ., xD). The binary-bit-coded string stands for a set of\\nvariable, which are used for evaluating the ﬁtness function (please\\nrefer to the next section). A bit ‘‘0’’ in a solution represents the\\nuselessness of corresponding variable. A move in a search space is\\nrestricted to 0 or 1 on each dimension. The best previous solution\\nthat gives the best ﬁtness value is represented as P = (p1, p2, . . ., pD).\\nIn binary problem, updating a solution represents changes of a bit\\nwhich should be in either state 1 or 0. The move function (F) which\\ndirects the moving of the solution is represented as Fi = (f1, f2, . . .,\\nfD). The move function fd of every neighbor (N = (n1, n2, . . ., nD)) is a\\nrandom number in the range of (0, 1). The resulting move in\\nsolution is then deﬁned by the following rule:\\nIf ð0 < f d \\x02 aÞ; then nd ¼ xd\\n(1)\\nIf ða < f dÞ; then nd ¼ pd\\n(2)\\nwhere a is a random value in the range of (0, 1) named static\\nprobability. The static probability a plays the role of balancing the\\nglobal and local search. That means the larger the value of the\\nparameter a is, the greater the probability for the neighbors to\\noverleap local optima. On the other hand, a small value of\\nparameter a is favorable for the neighbors to follow the best past\\npositions and for the algorithm to converge more quickly.\\nTherefore, we deﬁne the parameter descending along with the\\ngeneration. Static probability a starts with a value 0.7 and\\ndecreases to 0.3 when the iteration terminates. As introduction\\nof the information sharing mechanism, the random solution tends\\nto converge to the best solution quickly. The modiﬁed TS is\\ndescribed as follows:\\n\\x03 Step 1. Randomly generate an initial solution (X, initial binary\\nstring) and evaluate the ﬁtness function of this individual. The\\ninitial solution is a string of binary bits corresponding to each\\nvariable.\\n\\x03 Step 2. Generate the neighbors of the solution according to the\\ninformation sharing mechanism (Eqs. (1) and (2)). Then the\\nperformance of each neighbor or solution is measured according\\nto a pre-deﬁned ﬁtness function. When setting the number of\\nneighbor solution as a large value, the algorithm would make a\\ndeep search in a local region. But this may increase the\\ncalculation burden. Hence, the number of neighbors solution\\nis set as 20 for variable selection by experience to keep balance\\nbetween the search depth and calculation burden.\\n\\x03 Step 3. Pick new individual from the examined neighbor\\naccording to the aspiration criteria and tabu conditions and\\nthen update the solution. The move function which directs the\\nmoving of the solution was recorded in tabu list. If the neighbor\\nsolution is not in tabu list, pick it to be the new current solution. If\\nthe best of these neighbors is found to lead to a better ﬁtness\\nwith respect to the ﬁtness of the current optimum, override the\\ntabu status and pick it to be the new current solution according\\nto aspiration criteria. A small value of the tabu list size is\\nfavorable to reduce the calculation burden. However setting the\\ntabu list size as a small value may cause the algorithm to\\nQ. Shen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 61–66\\n62\\n\\nconverge to local optima. The tabu list size was selected as 5 to\\njump out from local minima.\\n\\x03 Step 4. If all neighbors are tabu solutions, a new solution is\\ngenerated randomly to further improve the ability of modiﬁed TS\\nto overleap local optima. The ﬁtness function of the new solution\\nis then evaluated.\\n\\x03 Step 5. If the number of iteration reaches a pre-deﬁned number of\\niterations,\\nthe\\ntraining\\nstopped\\nwith\\nthe\\nresults\\noutput,\\notherwise, go to the second step to renew solution. The modiﬁed\\nTS scheme is presented in Fig. 1.\\n2.3. Fitness function\\nIn modiﬁed TS, the performance of each neighbor or solution is\\nmeasured according to a pre-deﬁned ﬁtness function. The modiﬁed\\nCp statistic as objective function is applied to variable selection in\\nthe modiﬁed TS. The modiﬁed Cp in MLR is expressed as follows:\\nCpð pÞ ¼ RSS p\\nˆs2\\nPLS\\n\\x04 ðn \\x04 2 pÞ\\n(3)\\nwhere n is the number of dependent variables and p is the number\\nof independent variables. RSSp is the residual sum of the squares of\\np-variable MLR model,\\nˆs2\\nPLS is deﬁned as the value of RSS\\ncorresponding to the minimum number of principal components\\nin conventional partial least squares (PLS) analysis of the original\\ndata set when further increase of the number of principal\\ncomponents does not cause a signiﬁcant reduction in RSS. PLS\\nanalysis is a factor analytical technique that is useful when there\\nare more independent variables in data matrix (X) than in the\\ntarget matrix (Y). The underlying assumption of PLS is that the\\nobserved data is generated by a process which is driven by a small\\nnumber of latent variables or principal components. In its general\\nform PLS creates orthogonal score vectors or components by\\nmaximizing the covariance between different sets of variables. The\\ndetails of modiﬁed Cp have been described elsewhere [18].\\n3. Aromatic compounds toxicity data\\nA total of 65 aromatic chemicals with the observed toxicity to\\nchlorella vulgaris in a novel short-term assay taken from the study\\nby Netzeva et al. [19] were used to assess the performance of the\\nmodiﬁed TS in variable selection of QSAR. The data set is\\nchemically heterogeneous (includes phenols, anilines, nitroben-\\nzenes, benzaldehydes, etc.) and represents several mechanisms of\\ntoxic action.\\nA series of descriptors were calculated, which encoded different\\naspects of the molecular structure and consist of spatial, thermody-\\nnamic, structural, electronic and information-content descriptors.\\nThe spatial descriptors [20,21] used involve radius of gyration\\n(RadOfGyration), density, molecular surface area, principal moment\\nof inertia (PMI), molecular volume, and shadow indices. The\\nthermodynamic descriptors [22] were taken to describe the\\nhydrophobic character, refractivity (MolRef: molar refractivity),\\nheat of formation (Hf) and the dissolution free energy for water and\\noctanol (Fh2o: desolvation free energy for H2O; Foct: desolvation\\nfreeenergyforoctanol).Structural descriptorsinclude the molecular\\nweight (MW), the number of rotatable bonds (Rotbonds) and the\\nnumber of hydrogen bond (Hbond acceptor). The electronic\\ndescriptors [23] taken were concerning surperdelocalizability (Sr),\\natomic polarizabilities (Apol), and the dipole moment (Dipole).\\nElectrotopological-state indices (E-State indices) [24,25] involved S-\\naasC, S-aaN, S-aNH2, etc. Some so-called information-content\\ndescriptors [26] such as atomic composition indices and multigraph\\ninformation-content indices were also included in the candidate list.\\nAll these molecular descriptors were generated using Cerius23.5\\nsoftware on Silicon Graphics R3000 workstation. Besides the\\naforementioned molecular descriptors, 7 variables used by Netzeva\\netal. were also includedin the list of the candidatevariables (logKow:\\nlogarithm of the octanol–water partition coefﬁcient; Ehomo: MOPAC\\nenergy of the highest occupied molecular orbital; Elumo: energy of\\nthe lowest unoccupied molecular orbital; Amax: maximum acceptor\\nsuperdelocalizability; QHmax: maximum positive partial charge on a\\nhydrogen atom; Qmin: maximum negative partial charge;\\n0xv:\\nmolecular connectivity indices).\\nThe descriptor analysis involves the detection and removal of\\nthose structural descriptors which exhibit high pair-wise correla-\\ntions with other descriptors or which contain little discriminatory\\ninformation. Pairs of descriptors that are highly correlated\\n(r \\x05 0.90) encoded similar information, and one of them should\\nbe removed. Descriptors that contain a high percentage (\\x0590%) of\\nidentical values are also discarded. Table 1 summarizes all\\nmolecular descriptors used as the candidate variables for selection.\\nThe modiﬁed TS and MLR algorithms were written in Matlab 5.3\\nand run on a personal computer (Intel Pentium processor 4/1.5G\\nHz 256 MB RAM).\\n4. Results and discussion\\nThe modiﬁed TS was ﬁrst used for variable selection, in which\\nthe number of neighbors solution was set as 20 and the tabu list\\nsize was selected as 5. The number of iterations was set as 1000.\\nThe selected descriptors were taken as dependent variables to\\nbuild QSAR models with MLR method. The best model with\\nminimum ﬁtness value contains three variables as given by the\\nmodiﬁed TS. The three variables are AlogP, MolRef and Amax. The\\ncorrelation between the calculated and experimental values of lg1/\\nEC50 of three-descriptor model is shown in Fig. 2. The correlation\\nFig. 1. The chart of the modiﬁed TS scheme.\\nQ. Shen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 61–66\\n63\\n\\ncoefﬁcient (R2) and the standard deviation for the three-variable\\nmodel is 0.8664 and 0.3941, respectively. The two-variable model\\nobtained by modiﬁed TS is the same as those obtained by Netzeva\\nin the literature [19]. These models and the best model that contain\\nfour variables are shown in Table 2. In these equations, positive\\ncoefﬁcient of AlogP indicates that the large hydrophobic character\\nof the molecule would promote the toxicity. MolRef is a combined\\nmeasure\\nof\\nmolecular\\nsize\\nand\\npolarizability,\\nand\\npositive\\ncoefﬁcient of MolRef indicates that increasing the molar refractiv-\\nity of the molecule causes an increase in toxicity. These results\\nshows that Amax is one of the important variables and the larger\\nAmax can increase the toxicity. That is in accordance with the\\nconclusions by Netzeva et al. that Amax was found to be a superior\\ndescriptor for modeling the acute toxicity of aromatic compounds\\n[20].\\nVariable selection by conventional TS was also performed to\\ncompare with the modiﬁed TS. The best model with minimum\\nﬁtness value given by the conventional TS contains two variables.\\nThe two variables are logKow and Elumo. The correlation coefﬁcients\\n(R2) and the standard deviation were 0.8389 and 0.4291, respec-\\ntively. The minimum ﬁtness could be obtained in about 40 cycles\\nduring the modiﬁed TS algorithm, but about 630 cycles were needed\\nduring conventional TS algorithm. Searching speed of the conven-\\ntional TS algorithm is slow, but experimental results demonstrated\\nthat the modiﬁed TS converges to the global best solution rapidly.\\nTo compare with modiﬁed TS, variables selection by stepwise\\nregression was also performed. The obtained model by stepwise\\nregression contains four variables. The four variables are Shadow-\\nXYfrac, logKow, 0xv and Amax. The correlation coefﬁcients (R2) and\\nthe standard deviation were 0.8433 and 0.4303, respectively. A\\ncomparison with stepwise regression shows that better results\\nwere obtained from modiﬁed TS algorithm.\\nThe real goal of developing QSAR is to predict the activity. To\\ncheck the validity of the proposed methods, the data set of 65\\naromatic chemicals was stochastically divided into two groups.\\nTwo-third compounds were used as the training set for developing\\nregression models, while the remaining one-third compounds\\nwere used as the predicted dataset. The best MLR models selected\\nby the training set contain three variables (AlogP, MolRef and\\nAmax). The plot of observed toxicity against that calculated by the\\nthree-variable model is shown in Fig. 3. It was found that using\\nTable 1\\nList of molecular descriptors for aromatic compounds studied as candidate\\nvariables.\\nFunctional families\\nof descriptors\\nDescriptors\\nSpatial descriptors\\nShadow indices (surface area projections)\\n(Shadow-XY, Shadow-XZ, Shadow-YZ,\\nShadow-nu, Shadow-XYfrac, Shadow-XZfrac,\\nShadow-YZfrac)\\nVm (molecular volume)\\nDensity\\nArea (molecular surface area)\\nRadOfGyration (Radius of gyration)\\nJurs descriptors (Jurs Charged Partial Surface\\nArea descriptors)\\nPMI (Principal moment of inertia, including\\nPMI-mag, PMI-X, PMI-Y, PMI-Z)\\nStructural descriptors\\nMW (molecular weight)\\nHbond acceptor (number of hydrogen\\nbond acceptors)\\nHbond donor (number of hydrogen bond donors)\\nRotbonds (number of rotatable bonds)\\nElectronic descriptors\\nApol (sum of atomic polarizabilities)\\nDipole (Dipole-mag, Dipole-X, Dipole-Y, Dipole-Z)\\nSr (superdelocalizability)\\nQuantum mechanical\\ndescriptors\\nHOMO, Ehomo (highest occupied molecular\\norbital energy)\\nLUMO, Elumo (lowest unoccupied molecular\\norbital energy)\\nAmax (maximum acceptor superdelocalizability)\\nQmax (maximum positive partial charge on\\na hydrogen atom)\\nQmin (maximum negative partial charge)\\nThermodynamic\\ndescriptors\\nAlogP, logP, logKow (the octanol/water\\npartition coefﬁcient)\\nFh2o (desolvation free energy for water)\\nFoct (desolvation free energy for octanol)\\nMolRef (molar refractivity)\\nHeat of formation (Hf)\\nE-State index\\nS-sCH3, S-aaCH, S-aasC, S-aNH2, S-aasN,\\nS-ssO, S-sOH\\nTopological index\\n0xv, 1xv, 2xv, 3xv, 4xv,0x, 1x, 2x, 3x, 4x\\nFig. 2. Calculated versus observed lg1/EC50 of three-variable model by modiﬁed TS using multiple linear regression modeling.\\nQ. Shen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 61–66\\n64\\n\\nthree descriptors the correlation coefﬁcients (R2) for the training\\nand the test set were 0.8709 and 0.8597, respectively. The standard\\ndeviation for training set was 0.3813.\\nEven the data set was stochastically divided into two groups, it\\nshould be noted that the predicted accuracy of a model at each\\niteration is not necessarily the same because of the various\\npartition of training and tests sets. The reliability of a model is an\\nessential issue in QSAR analysis. To evaluate the predictive ability\\nand reliability of models by the modiﬁed TS accurately, the total\\nsamples were randomly partitioned into training and tests sets 200\\nTable 2\\nResults of variable selection by the modiﬁed TS and MLR modeling.\\nNo.\\nEquation\\nR2a\\nSa\\nFa\\n1\\nlg1/IE50 = 0.731 \\x06 logKow \\x04 0.5906 \\x06 Elumo \\x04 1.9142\\n0.8389\\n0.4291\\n161.4795\\n2\\nlg1/IE50 = 0.4880 \\x06 AlogP + 0.0314 \\x06 MolRef + 19.4433 \\x06 Amax \\x04 8.6175\\n0.8664\\n0.3941\\n131.7711\\n3\\nlg1/IE50 = 0.4340 \\x06 AlogP + 0.0375 \\x06 MolRef + 18.0856 \\x06 Amax \\x04 0.0551S-ssO \\x04 8.2768\\n0.8729\\n0.3874\\n103.0543\\na R: correlation coefﬁcient; S: standard deviation; F: F-statistics.\\nFig. 3. Plot of lg1/EC50 calculated from three-variable model versus the observed lg1/EC50 values for training and test set.\\nFig. 4. Distribution of correlation coefﬁcient (R2) over 200 runs of partition samples using the best three-variable model.\\nQ. Shen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 61–66\\n65\\n\\ntimes and then averaged the model accuracy for each partition of\\ntraining and tests sets. By resampling a large number of learning\\nsamples, the correlation coefﬁcient (R2) for training set and test set\\nwere 0.8682 and 0.8515, respectively by the modiﬁed TS. The root\\nmean square errors for training set and test set were 0.3582 and\\n0.4646, respectively. Fig. 4 shows the distribution of correlation\\ncoefﬁcient (R2) over 200 runs of partition samples using the best\\nthree-variable model. As shown in Fig. 4, correlation coefﬁcient\\n(R2) larger than 0.8515 is about 107 times in 200 runs for test set\\nand the highest R2 achieve 0.9650. The results show that the model\\nby the modiﬁed TS is stable and reliable.\\nThe modiﬁed TS was run for about 1000 times, and the number\\nof times which a particular molecular descriptor appears in 1000\\ncycles was counted. When the descriptors by the order of\\ndecreasing numbers of times of appearance were listed, the top\\ndescriptors or the most frequently appeared feathers were\\nobtained. Amax is an index of reactivity in aromatic compounds\\nthat was used by Netzeva et al. [19], and it turned to be one of the\\nmost important variables. MolRef is a combined measure of its\\nsize and polarizability which seems essential with respect to\\ntoxicity of aromatic compounds. AlogP which is a factor relating to\\nthe hydrophobic character of the molecule are factors is usually\\nmuch considered in the development of QSAR in biochemistry.\\nThey are shown to be important in QSAR of aromatic compounds\\ntoxicity. ‘‘Elumo’’ is also a preferred descriptors and the negative\\ncoefﬁcient of descriptor Elumo implied that molecules with low-\\nenergy\\nElumo\\nwould\\npromote\\nthe\\ntoxicity.\\nThere\\nare\\ntwo\\nelectrotopological-state descriptors (S-sCH3, S-aNH2) among\\nthe top descriptors and these indices seem to be information-\\nrich in describing molecular structure. The subscript ‘–sCH3’\\nrefers to methyl and ‘aNH2’ refers to the amine in phenyl group.\\n‘NH2’ in phenyl group can increase toxicity and ‘CH3’ in aromatic\\ncompounds would decrease toxicity. Besides these variables,\\ndescriptors Shadow-nu, Vm, Apol, Dipole-Z, PMI-mag, PMI-Z and\\nQHmax also have advantage for toxicity of aromatic compounds.\\nThe toxicity of aromatic compounds is a complex one, which\\ninvolves\\nspatial,\\nthermodynamic,\\nelectronic,\\nand\\nstructural\\neffects.\\n5. Conclusion\\nIn the present study, the TS algorithm was modiﬁed to be used\\nin variable selection in QSAR modeling for predicting toxicity of\\naromatic compounds. The modiﬁed Cp was employed as ﬁtness\\nfunction. It has been demonstrated that the modiﬁed TS is a useful\\ntool for variable selection with nice performance and the ability to\\nselect preferred variables with satisfactory convergence rates. In\\nthe selected descriptors, Amax, MolRef and AlogP are the most\\nimportant descriptors in predicting toxicity of aromatic com-\\npounds.\\nAcknowledgement\\nThe work was ﬁnancially supported by the National Natural\\nScience Foundation of China (Grant no. 20505015).\\nReferences\\n[1] Gualdro´n O, Llobet E, Brezmes J, Vilanova X, Correi X. Fast variable selection for\\ngas sensing applications, vol. 2. In: Sensors, Proceedings of IEEE; 2004. p. 892–5.\\n[2] Liu H, Motoda H. Feature selection for knowledge discovery and data mining.\\nBoston: Kluwer Academic Publishers; 1998. p. 37–51.\\n[3] Shen M, Arnaud L, Xiao Y, Alexander G, Harold K, Alexander T. Quantitative\\nstructure–activity relationship analysis of functionalized amino acid anticon-\\nvulsant agents using k nearest neighbor and simulated annealing PLS methods.\\nJ Med Chem 2002;45:2811–23.\\n[4] Cho SJ, Hermsmeier MA. Genetic algorithm guided selection: variable selection\\nand subset selection. J Chem Inf Comput Sci 2002;42:927–36.\\n[5] Yasri A, Hartsough D. Toward an optimal procedure for variable selection and\\nQSAR model building. J Chem Inf Comput Sci 2001;41:1218–27.\\n[6] Brian TL. Evolutionary programming applied to the development of quantita-\\ntive structure–activity relationships and quantitative structure–property rela-\\ntionships. J Chem Inf Comput Sci 1994;34:1279–85.\\n[7] Glover, F. Tabu search. Part II. ORSA J Comput 1990;2(1):4–32.\\n[8] Glover F, Laguna M. Tabu search. Boston: Kluwer Academic Publishers; 1997.\\n[9] Glover F, Laguna M. Tabu search, handbook of applied optimization. In:\\nPardalos PM, Resende MGC, editors. Oxford: Oxford University Press; 2002.\\np. 194–208.\\n[10] Pacheco J, Casado S, Nu´ n˜ez L, Go´mez O. Analysis of new variable selection\\nmethods for discriminant analysis. Comput Stat Data Anal 2006;51(3):1463–78.\\n[11] Todeschini R, Consonni V, Pavan M. A distance measure between models: a\\ntool for similarity/diversity analysis of model populations. Chemometr Intell\\nLab Syst 2004;70:55–61.\\n[12] Chavali S, Lin B, Miller DC, Camarda KV. Environmentally-benign transition\\nmetal catalyst design using optimization techniques. Comput Chem Eng\\n2004;28:605–11.\\n[13] Vainio MJ, Johnson MS. McQSAR: a multiconformational quantitative struc-\\nture–activity relationship engine driven by genetic algorithms. J Chem Inf\\nModel 2005;45(6):1953–61.\\n[14] Gani R, Harper PM, Hostrup M. Automatic creation of missing groups through\\nconnectivity index for pure-component property prediction. Ind Eng Chem Res\\n2005;44(18):7262–9.\\n[15] Mills Jamie D, Olejnik Stephen F, Marcoulides George A. The tabu search\\nprocedure: an alternative to the variable selection methods. Multivariate\\nBehav Res 2005;40(3):351–71.\\n[16] Pad JS, Lut ZM, Chu SC, Sun SH. Non-redundant VQ channel coding using\\nmodiﬁed tabu search approach with simulated annealing. In: Third interna-\\ntional conference on knowledge-based intelligent information engineering\\nsystems. Adelaide, Australia: IEEE Press; 1999. p. 242–5.\\n[17] Kvasnicka V, Pospichal J. Fast evaluation of chemical distance by tabu search\\nalgorithm. J Chem Inf Comput Sci 1994;34(5):1109–12.\\n[18] Shen Q, Jing JH, Yu RQ. Variable selection by an evolution algorithm using\\nmodiﬁed Cp based on MLR and PLS modeling: QSAR studies of carcinogenicity\\nof aromatic amines. Anal Bioanal Chem 2003;375:248–54.\\n[19] Netzeva TI, Dearden JC, Edwards R, Worgan ADP, Cronin MTD. QSAR analysis of\\nthe toxicity of aromatic compounds to chlorella vulgaris in a novel short-term\\nassay. J Chem Inf Comput Sci 2004;44:258–65.\\n[20] Rohrbaugh RH, Jurs PC. Descriptions of molecular shape applied in studies of\\nstructure/activity and structure/property relationships. Anal Chim Acta\\n1987;199:99–109.\\n[21] Stanton DT, Jurs PC. Development and use of charged partial surface area\\nstructural descriptors in computer-assisted quantitative structure–property\\nrelationship studies. Anal Chem 1990;62:2323–9.\\n[22] Viswanadhan VN, Ghose AK, Revankar GR, Robins RK. Atomic physicochemical\\nparameters for three dimensional structure directed quantitative structure–\\nactivity relationships. 4’. Additional parameters for hydrophobic and disper-\\nsive interactions and their application for an automated superposition of\\ncertain naturally occurring nucleoside antibiotics. J Chem Inf Comput Sci\\n1989;29:163–72.\\n[23] Ciuprina G, Loan D, Munteanu I. Use of intelligent-particle swarm optimiza-\\ntion in electromagentics. IEEE Trans Magn 2002;38(2):1037–40.\\n[24] Hall LH, Kier LB. The electrotopological state: structure information at the\\natomic level for molecular graphs. J Chem Inf Comput Sci 1991;31:76–8.\\n[25] Hall LH, Kier LB. Electrotopological state indices for atom types: a novel\\ncombination of electronic, topological, and valence state information. J Chem\\nInf Comput Sci 1995;35:1039–45.\\n[26] Boncher. Danailinformation theoretic indices for characterization of chemical\\nstructures. Chichester, UK: Research Studies Press; 1983. p. 249.\\nQ. Shen et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 61–66\\n66\\n'}, {'source': '/Users/sir/Downloads/Data/PDF/Predicting_the_outcome_for_patients_in_a_heart_transplantation_queue_using_deep_learning.pdf', 'text': 'Predicting the Outcome for Patients in a Heart Transplantation Queue\\nusing Deep Learning\\nDennis Medved1, Pierre Nugues1, and Johan Nilsson2\\nAbstract— Heart transplantations have made it possible to\\nextend the median survival time to 12 years for patients with\\nend-stage heart diseases. This operation is unfortunately limited\\nby the availability of donor organs and patients have to wait on\\naverage about 200 days in a waiting list before being operated.\\nThis waiting time varies considerably across the patients. In\\nthis paper, we studied the outcome for patients entering a\\ntransplantation waiting list using deep learning techniques. We\\nimplemented a model in the form of two-layer neural networks\\nand we predicted the outcome as still waiting, transplanted\\nor dead in the waiting list, at three different time points: 180\\ndays, 365 days, and 730 days. As data source, we used the\\nUnited Network for Organ Sharing (UNOS) registry, where\\nwe extracted adult patients (>17 years) from January 2000\\nto December 2011. We trained our model using the Keras\\nframework, and we report F1 macro scores of respectively\\n0.674, 0.680, and 0.680 compared to a baseline of 0.271. We\\nalso applied a backward elimination procedure, using our\\nneural network, to extract the 10 most signiﬁcant parameters\\npredicting the patient status for the three different time points.\\nI. INTRODUCTION\\nHeart transplantations have made it possible to extend the\\nmedian survival time to 12 years for patients with end-stage\\nheart diseases. Unfortunately, the need for donated hearts\\ngreatly exceeds supply and many candidates die awaiting\\ntransplantation. Estimating the probability of dying in the\\nwaiting list for a speciﬁc time period, could support the\\ndecision of surgeons on the priority of a transplantation.\\nIn addition, knowing the probability for a patient to be\\ntransplanted within a certain time frame would help plan\\noperation resources and inform the patient.\\nIn this study, we have used neural network models to\\npredict the outcome for patients entering a heart transplan-\\ntation waiting list. We carried out the prediction at three\\ndifferent time points: 180 days, 365 days and 730 days. We\\ncategorized the patient status with three possible outcomes:\\nstill waiting, transplanted, or dead in the waiting list.\\nII. PREVIOUS WORK\\nA few studies investigated waiting times of allografts.\\nThey include heart transplants [10, 4], liver [1], and kidney\\n[3], that all revealed increased waiting times for group O\\nrecipients. Other studies proposed models to predict the\\n*This research was supported by Heart Lung Fondation, The Swedish\\nResearch Council, and the eSSENCE program.\\n1Department of Computer Science, Lund University, Lund, Sweden\\n{dennis.medved, pierre.nugues}@cs.lth.se\\n2Department\\nof\\nClinical\\nSciences\\nLund,\\nCardiothoracic\\nSurgery,\\nLund\\nUniversity\\nand\\nSk˚ane\\nUniversity\\nHospital,\\nLund,\\nSweden\\njohan.nilsson@med.lu.se\\noutcome in heart failures and outlined lists of predictors. [9]\\nis a review of 64 such models, where the possible outcomes\\nwere death, hospitalization, and death or hospitalization,\\ndepending on the model. The authors could distill a list of 10\\nconsistently used predictors: age, renal function, blood pres-\\nsure, blood sodium level, etc. Other papers provide models to\\nestimate the survival time after a heart transplantation such\\nas [16, 6], while [5] describe a procedure to extract features\\npredicting the one, ﬁve, and ten year survival of patients.\\nIII. MATERIALS AND METHODS\\nA. Data Source\\nUNOS administers the only Organ Procurement and Trans-\\nplantation Network in the United States of America [7],\\nand is a non-proﬁt organization. The patient data that we\\nused was obtained from the UNOS database. The database\\ncontains data from October 1, 1987 and onwards. In the\\ndatabase, there is information that encompass recipient,\\ndonor and transplant data. It includes almost 500 variables\\nreﬂecting different attributes of the patients.\\nThe Ethics Committee for Clinical Research at Lund\\nUniversity, Sweden approved the study protocol. The data\\nwas de-identiﬁed prior to analyzing it and the institutional\\nreview board waived the need for written informed consent\\nfrom the participants.\\nB. Study population\\nWe included adult (> 17 years) heart transplantation (HT)\\npatients from January 2000 to December 2011, that either\\ndied in the queue, got transplanted, or were still waiting in\\nthe queue. We did not include patients, who were removed\\nfrom the list for other reasons, such as being too sick to\\nbe operated. We excluded these patients because they could\\npotentially confuse the model. We assumed that the features\\npredicting the death of a patient would probably be correlated\\nwith a removal from the queue.\\nWe used this data set to create three different temporal\\ncohorts, where we recorded the patients’ outcome after 180,\\n365, and 730 days. Table I shows the distribution of outcomes\\nin the different time periods.\\nThe total number of patients included in our data\\nset was of 27,444. We randomly divided the data in\\ntrain/validation/test in sets of 70%/15%/15% which translates\\nto 19210/4117/4117 patients, respectively.\\nAs features, we included 87 variables describing the\\npatients in the queue that were available at the time of listing\\nsuch as: age, sex, weight, and blood group.\\n978-1-5090-2809-2/17/$31.00 ©2017 IEEE\\n74\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:26:59 UTC from IEEE Xplore.  Restrictions apply. \\n\\nTABLE I\\nTHE THREE TEMPORAL COHORTS AND OUTCOME DISTRIBUTION\\nDays\\nDead (%)\\nTransplanted (%)\\nQueueing (%)\\n180\\n9.7\\n57.0\\n33.4\\n365\\n11.6\\n69.1\\n19.3\\n730\\n13.4\\n77.3\\n9.3\\nC. Imputation of Missing Data\\nAs with all large registries, there is missing patient data.\\nNo patient has a complete information record and excluding\\nthe patients with missing data ﬁelds from the cohorts would\\nhave reduced the data set to almost nothing. To mitigate this,\\nwe chose to impute the missing data, where we applied a\\nprobabilistic approach. For each variable, we replaced the\\nmissing values with a random value from a discrete uniform\\ndistribution of the non-missing values in this variable, fol-\\nlowing the method in [11].\\nD. Evaluation\\nTo evaluate the models, we used the F1 score, which is\\nthe harmonic mean of precision and recall [8]. These metrics\\nwere created for binary classes and to generalize them to\\nmore than two classes, we averaged the results using micro\\nand macro averages.\\nThe micro average method consists of summing up the\\nindividual true positives, false positives, and false negatives\\nof the system for the different classes and then calculating\\nthe average. The macro average takes the average of the\\nprecision and recall of the system on the different classes.\\nWhen the examples are unevenly distributed across the\\nclasses, the macro average method is less biased toward the\\nlargest class [15].\\nWe also computed a confusion matrix, where each column\\nof the matrix represents the instances of a predicted class,\\nwhile each row represents the actual class. The diagonal\\nthen represents the correctly classiﬁed outcomes. Confusion\\nmatrices make it easier to visualize the classiﬁcation errors\\nthat a model produces [13].\\nE. Implementation Details\\nWe used the Keras framework to train the model [2]. It\\nutilizes Python as a programming interface and enables the\\nuser to easily create and conﬁgure artiﬁcial neural networks\\n(ANN) of different architectures. It serves as a high level\\nabstraction, that utilizes Theano as the back-end [14].\\nWe created a network with two hidden layers and 128\\nnodes in each layer. The hidden layers used the rectiﬁed\\nlinear unit as activation function and the ﬁnal output layer\\nused a softmax activation. We selected categorical cross\\nentropy as the loss function and adamax as the optimizer\\nwith 30 epochs.\\nDropout is a regularization technique for reducing overﬁt-\\nting in neural networks [12]. The idea behind dropout is to\\nrandomly drop units, together with their connections, from\\nthe neural network during training. The dropout rate controls\\nthe probability of a neuron being removed. We chose to use\\na dropout rate of 0.5.\\nF. Feature Signiﬁcance\\nWe wanted to know which features contributed the most\\nto the result of the classiﬁcation. We utilized backward\\nelimination to ﬁnd these features.\\nBackward elimination starts with all the features and\\nremoves them one by one from the set. The resulting feature\\nset is then used to produce the classiﬁcation probabilities. We\\ncalculate the F1 macro metric for each of the new feature\\nsets and remove the feature that produced the best score when\\nexcluded. We repeat this process until the desired amount of\\nfeatures remain.\\nIV. RESULTS\\nWe optimized the hyperparameters on the validation set.\\nUsing these parameters, Table II shows the precision and\\nrecall values we obtained on the test set, while Table III\\nshows the F1 values for 180, 365, and 730 days, respectively.\\nWe included a baseline model in the table that always\\nclassiﬁes the most frequent class, in this case: the patient was\\ntransplanted. The best macro averaged F1 was achieved for\\n365 days: 0.680. Figure 1 shows the precision-recall curve\\nfor this time period.\\nTABLE II\\nTHE PRECISION AND RECALL VALUES FOR 180, 365, AND 730 DAYS\\nOBTAINED ON THE TEST SET\\nDays\\nClass\\nPrecision\\nRecall\\nF1\\n180\\nDead\\n0.680\\n0.644\\n0.664\\nTransplanted\\n0.764\\n0.887\\n0.820\\nQueuing\\n0.654\\n0.485\\n0.557\\n365\\nDead\\n0.782\\n0.684\\n0.705\\nTransplanted\\n0.842\\n0.967\\n0.900\\nQueuing\\n0.605\\n0.314\\n0.413\\n730\\nDead\\n0.770\\n0.747\\n0.759\\nTransplanted\\n0.918\\n0.992\\n0.954\\nQueuing\\n0.606\\n0.226\\n0.329\\nBaseline 180\\nDead\\n0.000\\n0.000\\n0.000\\nTransplanted\\n0.567\\n1.000\\n0.724\\nQueuing\\n0.000\\n0.000\\n0.000\\nBaseline 365\\nDead\\n0.000\\n0.000\\n0.000\\nTransplanted\\n0.77\\n1.000\\n0.869\\nQueuing\\n0.000\\n0.000\\n0.000\\nBaseline 730\\nDead\\n0.000\\n0.000\\n0.000\\nTransplanted\\n0.685\\n1.000\\n0.813\\nQueuing\\n0.000\\n0.000\\n0.000\\nUsing the neural network and backward elimination, we\\nextracted the ten most important features, shown in Table IV.\\nThe features are ranked within the sets, according to their\\nremoval order. We evaluated these feature sets and Table V\\nshows the results. Figure 2 shows the confusion matrix\\nfor the 365 days time period that reveals that the most\\nmisclassiﬁed outcome is queueing as transplanted.\\nWe wanted to look at the distributions of outcomes\\ndepending on the patient having blood group O or not,\\nmostly because previous studies had shown that it was a\\npredictor. In addition, it was also implicitly included in the\\nten most predictive features for 365 days. Table VI shows\\nthat there is a 17% absolute difference between the number\\nof transplanted.\\n75\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:26:59 UTC from IEEE Xplore.  Restrictions apply. \\n\\nFig. 1.\\nPrecision-recall curves for 365 days\\nTABLE III\\nTHE F1 VALUES FOR 180, 365, AND 730 DAYS OBTAINED ON THE TEST\\nSET\\nDays\\nF1\\nF1\\n(micro)\\n(macro)\\n180\\n0.750\\n0.675\\n365\\n0.760\\n0.680\\n730\\n0.888\\n0.680\\nBaseline 180\\n0.567\\n0.241\\nBaseline 365\\n0.685\\n0.271\\nBaseline 730\\n0.769\\n0.290\\nV. DISCUSSION\\nThe distribution of patient outcomes within the cohorts\\nis quite imbalanced, where transplanted is the outcome for\\n57-77% of the patients, during the chosen time periods.\\nWe tried a simple baseline, where we classiﬁed all the\\npatient outcomes as the most frequent, see Table III for the\\nresults. It produced quite good micro averaged values, mostly\\nbecause these metrics are biased towards the largest class, but\\ncomparatively bad macro values.\\nThe largest misclassiﬁcation error in Figure 2 corresponds\\nto queueing as transplanted. This is probably because it is\\nhard to differentiate between the patients that were trans-\\nplanted at a certain time point versus those that are still\\nwaiting in the queue, based on the available features.\\nWe carried out a backward elimination using our neural\\nnetwork and the ten most contributing features is shown\\nin Table IV. This results in a decrease of only about 2%\\n(absolute difference) from the F1 macro score with all the\\nfeatures, see Table V. This means that most of the predictive\\npower from the ANN comes from a few features. Neural\\nnetworks do a kind of feature selection naturally as part of the\\nmodel, weighing up more predictive features and weighing\\ndown the less predictive. Because of this, feature search\\nfor neural networks is usually not needed. But considering\\nit is hard to interpret the matrices produced by the ANN\\nmodel directly, we carried out a backward elimination to\\napproximate the features importance.\\nFig. 2.\\nConfusion matrix for 365 days time period.\\nThe features shared by all of the three sets are: urgency\\nstatus 2, weight, height and body mass index (BMI). BMI can\\nbe considered a feature transformation of weight and height\\nas BMI = weight × height2, but it provided extra predictive\\ninformation over the constituent variables. A sufﬁciently\\ncomplex neural network could probably approximate this\\ntransformation and therefore BMI would probably not be\\nneeded.\\nTable VI shows some discrepancy between the number\\nof transplanted patients depending on having blood group\\nO. This can probably be explained by the fact that only\\npatients that are blood-group compatible with the donor are\\ntransplanted. Even though type O is quite common, patients\\nof this group can only receive from donors from the same\\nblood group and can give to all other types.\\nA. Future Work\\nWe did not have time to fully optimize the hyperparam-\\neters of the neural network and there are some variables\\nthat are available that we did not include, both which could\\nproduce better results.\\nWe also plan to build a more advanced model based on\\nnetworks similar to those we described in this paper to be\\nable to estimate the probability the patient would die or\\nwould be transplanted depending on the time s/he spent in\\nthe waiting list.\\nACKNOWLEDGMENT\\nThis work is based on OPTN data as of October 1, 2013\\nand was supported in part by the Health Resources and\\nServices Administration contract 234-2005-370011C. The\\ncontent is the responsibility of the authors alone and does not\\nnecessarily reﬂect the views or policies of the Department\\nof Health and Human Services. This research was supported\\nby Heart Lung Foundation, The Swedish Research Council,\\nand the eSSENCE program.\\n76\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:26:59 UTC from IEEE Xplore.  Restrictions apply. \\n\\nTABLE IV\\nTHE TEN MOST CONTRIBUTING FEATURES FOR EACH TIME PERIOD USING BACKWARD ELIMINATION, IN ORDER OF IMPORTANCE.\\nRank\\n180 days\\n365 days\\n730 days\\n1\\nUrgency status 2\\nBMI\\nBMI\\n2\\nWeight\\nWeight\\nWeight\\n3\\nBMI\\nHeight\\nHeight\\n4\\nHeight\\nUrgency status 2\\nUrgency status 2\\n5\\nInotropes\\nCreatine clearance\\nCreatinine\\n6\\nBlood group: AB\\nInotropes\\nFunctional status\\n7\\nLife support\\nBlood group: A\\nPulmonary Vascular Resistance\\n8\\nBlood group: B\\nLife support\\nEducational level: none\\n9\\nInotropic support\\nBlood group: AB\\nVentricular assist type: LVAD + RVAD\\n10\\nEthnicity: black\\nBlood group: B\\nEducational level: grade school\\nTABLE V\\nEVALUATION ON THE TEST WITH THE 10 BEST FEATURES FOUND FOR\\nEACH TIME PERIOD.\\nDays\\nF1\\nF1\\n(micro)\\n(macro)\\n180\\n0.710\\n0.657\\n365\\n0.714\\n0.655\\n730\\n0.889\\n0.660\\nTABLE VI\\nTHE DISTRIBUTION OF OUTCOMES DEPENDING ON BLOOD GROUP FOR\\n365 DAYS\\nBlood group\\nDead (%)\\nTransplanted (%)\\nQueueing (%)\\nO\\n14.2\\n59.3\\n26.5\\nnot O\\n9.7\\n76.4\\n13.8\\nREFERENCES\\n[1]\\nMichele Barone et al. “ABO blood group-related\\nwaiting list disparities in liver transplant candidates:\\neffect of the MELD adoption.” In: Transplantation\\n85.6 (2008), pp. 844–849.\\n[2]\\nFranc¸ois Chollet. Keras. https://github.com/\\nfchollet/keras. 2015.\\n[3]\\nPetra Glander et al. “The ‘blood group O problem’ in\\nkidney transplantation—time to change?” In: Nephrol-\\nogy Dialysis Transplantation 25.6 (2010), p. 1998.\\n[4]\\nJ.C. Hussey, J. Parameshwar, and N.R. Banner. “Inﬂu-\\nence of Blood Group on Mortality and Waiting Time\\nBefore Heart Transplantation in the United Kingdom:\\nImplications for Equity of Access”. In: The Journal of\\nHeart and Lung Transplantation 26.1 (2007), pp. 30–\\n33. ISSN: 1053-2498.\\n[5]\\nD. Medved, P. Nugues, and J. Nilsson. “Selection of\\nan optimal feature set to predict heart transplanta-\\ntion outcomes”. In: 2016 38th Annual International\\nConference of the IEEE Engineering in Medicine and\\nBiology Society (EMBC). Aug. 2016, pp. 3290–3293.\\n[6]\\nJohan Nilsson et al. “The International Heart Trans-\\nplant Survival Algorithm (IHTSA): A New Model to\\nImprove Organ Sharing and Survival”. In: PLoS ONE\\n10.3 (2015), e0118644.\\n[7]\\nUnited Network for Organ Sharing. Organ Procure-\\nment and Transplantation Network Data. 2015. URL:\\nhttp : / / optn . transplant . hrsa . gov /\\nconverge / data / default . asp (visited on\\n11/19/2015).\\n[8]\\nDM Powers. “Evaluation: From Precision, Recall and\\nF Factor to ROC, Informedness, Markedness & Cor-\\nrealtion”. In: School of Informatics and Engineer-\\ning, Flinders University of South Australia Adelaide\\n(2007).\\n[9]\\nKazem Rahimi et al. “Risk Prediction in Patients\\nWith Heart Failure”. In: JACC: Heart Failure 2.5\\n(2014), pp. 440–446. ISSN: 2213-1779.\\n[10]\\nHelena Rexius, Folke Nilsson, and Anders Jeppsson.\\n“On the Allocation of Cardiac Allografts from Blood\\nGroup-O Donors”. In: Scandinavian Cardiovascular\\nJournal 36.6 (2002), pp. 342–344.\\n[11]\\nMichael Schemper and Georg Heinze. “Probability\\nimputation revisited for prognostic factor studies”. In:\\nStatistics in medicine 16.1 (1997), pp. 73–80.\\n[12]\\nNitish Srivastava et al. “Dropout: A Simple Way\\nto Prevent Neural Networks from Overﬁtting”. In:\\nJournal of Machine Learning Research 15 (2014),\\npp. 1929–1958.\\nURL: http : / / jmlr . org /\\npapers/v15/srivastava14a.html.\\n[13]\\nAdi L Tarca et al. “Machine learning and its appli-\\ncations to biology”. In: PLOS Computational Biology\\n3.6 (2007), e116.\\n[14]\\nTheano Development Team. “Theano: A Python\\nframework for fast computation of mathematical ex-\\npressions”. In: arXiv e-prints abs/1605.02688 (May\\n2016). URL: http://arxiv.org/abs/1605.\\n02688.\\n[15]\\nVincent Van Asch. Macro-and micro-averaged eval-\\nuation measures. Tech. rep. University of Antwerp,\\n2013.\\n[16]\\nEric S. Weiss et al. “Creation of a Quantitative\\nRecipient Risk Index for Mortality Prediction After\\nCardiac Transplantation (IMPACT)”. In: The Annals\\nof Thoracic Surgery 92.3 (2011), pp. 914–922.\\n77\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:26:59 UTC from IEEE Xplore.  Restrictions apply. \\n'}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/Predicting_the_outcome_for_patients_in_a_heart_transplantation_queue_using_deep_learning.pdf', 'text': 'Predicting the Outcome for Patients in a Heart Transplantation Queue\\nusing Deep Learning\\nDennis Medved1, Pierre Nugues1, and Johan Nilsson2\\nAbstract— Heart transplantations have made it possible to\\nextend the median survival time to 12 years for patients with\\nend-stage heart diseases. This operation is unfortunately limited\\nby the availability of donor organs and patients have to wait on\\naverage about 200 days in a waiting list before being operated.\\nThis waiting time varies considerably across the patients. In\\nthis paper, we studied the outcome for patients entering a\\ntransplantation waiting list using deep learning techniques. We\\nimplemented a model in the form of two-layer neural networks\\nand we predicted the outcome as still waiting, transplanted\\nor dead in the waiting list, at three different time points: 180\\ndays, 365 days, and 730 days. As data source, we used the\\nUnited Network for Organ Sharing (UNOS) registry, where\\nwe extracted adult patients (>17 years) from January 2000\\nto December 2011. We trained our model using the Keras\\nframework, and we report F1 macro scores of respectively\\n0.674, 0.680, and 0.680 compared to a baseline of 0.271. We\\nalso applied a backward elimination procedure, using our\\nneural network, to extract the 10 most signiﬁcant parameters\\npredicting the patient status for the three different time points.\\nI. INTRODUCTION\\nHeart transplantations have made it possible to extend the\\nmedian survival time to 12 years for patients with end-stage\\nheart diseases. Unfortunately, the need for donated hearts\\ngreatly exceeds supply and many candidates die awaiting\\ntransplantation. Estimating the probability of dying in the\\nwaiting list for a speciﬁc time period, could support the\\ndecision of surgeons on the priority of a transplantation.\\nIn addition, knowing the probability for a patient to be\\ntransplanted within a certain time frame would help plan\\noperation resources and inform the patient.\\nIn this study, we have used neural network models to\\npredict the outcome for patients entering a heart transplan-\\ntation waiting list. We carried out the prediction at three\\ndifferent time points: 180 days, 365 days and 730 days. We\\ncategorized the patient status with three possible outcomes:\\nstill waiting, transplanted, or dead in the waiting list.\\nII. PREVIOUS WORK\\nA few studies investigated waiting times of allografts.\\nThey include heart transplants [10, 4], liver [1], and kidney\\n[3], that all revealed increased waiting times for group O\\nrecipients. Other studies proposed models to predict the\\n*This research was supported by Heart Lung Fondation, The Swedish\\nResearch Council, and the eSSENCE program.\\n1Department of Computer Science, Lund University, Lund, Sweden\\n{dennis.medved, pierre.nugues}@cs.lth.se\\n2Department\\nof\\nClinical\\nSciences\\nLund,\\nCardiothoracic\\nSurgery,\\nLund\\nUniversity\\nand\\nSk˚ane\\nUniversity\\nHospital,\\nLund,\\nSweden\\njohan.nilsson@med.lu.se\\noutcome in heart failures and outlined lists of predictors. [9]\\nis a review of 64 such models, where the possible outcomes\\nwere death, hospitalization, and death or hospitalization,\\ndepending on the model. The authors could distill a list of 10\\nconsistently used predictors: age, renal function, blood pres-\\nsure, blood sodium level, etc. Other papers provide models to\\nestimate the survival time after a heart transplantation such\\nas [16, 6], while [5] describe a procedure to extract features\\npredicting the one, ﬁve, and ten year survival of patients.\\nIII. MATERIALS AND METHODS\\nA. Data Source\\nUNOS administers the only Organ Procurement and Trans-\\nplantation Network in the United States of America [7],\\nand is a non-proﬁt organization. The patient data that we\\nused was obtained from the UNOS database. The database\\ncontains data from October 1, 1987 and onwards. In the\\ndatabase, there is information that encompass recipient,\\ndonor and transplant data. It includes almost 500 variables\\nreﬂecting different attributes of the patients.\\nThe Ethics Committee for Clinical Research at Lund\\nUniversity, Sweden approved the study protocol. The data\\nwas de-identiﬁed prior to analyzing it and the institutional\\nreview board waived the need for written informed consent\\nfrom the participants.\\nB. Study population\\nWe included adult (> 17 years) heart transplantation (HT)\\npatients from January 2000 to December 2011, that either\\ndied in the queue, got transplanted, or were still waiting in\\nthe queue. We did not include patients, who were removed\\nfrom the list for other reasons, such as being too sick to\\nbe operated. We excluded these patients because they could\\npotentially confuse the model. We assumed that the features\\npredicting the death of a patient would probably be correlated\\nwith a removal from the queue.\\nWe used this data set to create three different temporal\\ncohorts, where we recorded the patients’ outcome after 180,\\n365, and 730 days. Table I shows the distribution of outcomes\\nin the different time periods.\\nThe total number of patients included in our data\\nset was of 27,444. We randomly divided the data in\\ntrain/validation/test in sets of 70%/15%/15% which translates\\nto 19210/4117/4117 patients, respectively.\\nAs features, we included 87 variables describing the\\npatients in the queue that were available at the time of listing\\nsuch as: age, sex, weight, and blood group.\\n978-1-5090-2809-2/17/$31.00 ©2017 IEEE\\n74\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:26:59 UTC from IEEE Xplore.  Restrictions apply. \\n\\nTABLE I\\nTHE THREE TEMPORAL COHORTS AND OUTCOME DISTRIBUTION\\nDays\\nDead (%)\\nTransplanted (%)\\nQueueing (%)\\n180\\n9.7\\n57.0\\n33.4\\n365\\n11.6\\n69.1\\n19.3\\n730\\n13.4\\n77.3\\n9.3\\nC. Imputation of Missing Data\\nAs with all large registries, there is missing patient data.\\nNo patient has a complete information record and excluding\\nthe patients with missing data ﬁelds from the cohorts would\\nhave reduced the data set to almost nothing. To mitigate this,\\nwe chose to impute the missing data, where we applied a\\nprobabilistic approach. For each variable, we replaced the\\nmissing values with a random value from a discrete uniform\\ndistribution of the non-missing values in this variable, fol-\\nlowing the method in [11].\\nD. Evaluation\\nTo evaluate the models, we used the F1 score, which is\\nthe harmonic mean of precision and recall [8]. These metrics\\nwere created for binary classes and to generalize them to\\nmore than two classes, we averaged the results using micro\\nand macro averages.\\nThe micro average method consists of summing up the\\nindividual true positives, false positives, and false negatives\\nof the system for the different classes and then calculating\\nthe average. The macro average takes the average of the\\nprecision and recall of the system on the different classes.\\nWhen the examples are unevenly distributed across the\\nclasses, the macro average method is less biased toward the\\nlargest class [15].\\nWe also computed a confusion matrix, where each column\\nof the matrix represents the instances of a predicted class,\\nwhile each row represents the actual class. The diagonal\\nthen represents the correctly classiﬁed outcomes. Confusion\\nmatrices make it easier to visualize the classiﬁcation errors\\nthat a model produces [13].\\nE. Implementation Details\\nWe used the Keras framework to train the model [2]. It\\nutilizes Python as a programming interface and enables the\\nuser to easily create and conﬁgure artiﬁcial neural networks\\n(ANN) of different architectures. It serves as a high level\\nabstraction, that utilizes Theano as the back-end [14].\\nWe created a network with two hidden layers and 128\\nnodes in each layer. The hidden layers used the rectiﬁed\\nlinear unit as activation function and the ﬁnal output layer\\nused a softmax activation. We selected categorical cross\\nentropy as the loss function and adamax as the optimizer\\nwith 30 epochs.\\nDropout is a regularization technique for reducing overﬁt-\\nting in neural networks [12]. The idea behind dropout is to\\nrandomly drop units, together with their connections, from\\nthe neural network during training. The dropout rate controls\\nthe probability of a neuron being removed. We chose to use\\na dropout rate of 0.5.\\nF. Feature Signiﬁcance\\nWe wanted to know which features contributed the most\\nto the result of the classiﬁcation. We utilized backward\\nelimination to ﬁnd these features.\\nBackward elimination starts with all the features and\\nremoves them one by one from the set. The resulting feature\\nset is then used to produce the classiﬁcation probabilities. We\\ncalculate the F1 macro metric for each of the new feature\\nsets and remove the feature that produced the best score when\\nexcluded. We repeat this process until the desired amount of\\nfeatures remain.\\nIV. RESULTS\\nWe optimized the hyperparameters on the validation set.\\nUsing these parameters, Table II shows the precision and\\nrecall values we obtained on the test set, while Table III\\nshows the F1 values for 180, 365, and 730 days, respectively.\\nWe included a baseline model in the table that always\\nclassiﬁes the most frequent class, in this case: the patient was\\ntransplanted. The best macro averaged F1 was achieved for\\n365 days: 0.680. Figure 1 shows the precision-recall curve\\nfor this time period.\\nTABLE II\\nTHE PRECISION AND RECALL VALUES FOR 180, 365, AND 730 DAYS\\nOBTAINED ON THE TEST SET\\nDays\\nClass\\nPrecision\\nRecall\\nF1\\n180\\nDead\\n0.680\\n0.644\\n0.664\\nTransplanted\\n0.764\\n0.887\\n0.820\\nQueuing\\n0.654\\n0.485\\n0.557\\n365\\nDead\\n0.782\\n0.684\\n0.705\\nTransplanted\\n0.842\\n0.967\\n0.900\\nQueuing\\n0.605\\n0.314\\n0.413\\n730\\nDead\\n0.770\\n0.747\\n0.759\\nTransplanted\\n0.918\\n0.992\\n0.954\\nQueuing\\n0.606\\n0.226\\n0.329\\nBaseline 180\\nDead\\n0.000\\n0.000\\n0.000\\nTransplanted\\n0.567\\n1.000\\n0.724\\nQueuing\\n0.000\\n0.000\\n0.000\\nBaseline 365\\nDead\\n0.000\\n0.000\\n0.000\\nTransplanted\\n0.77\\n1.000\\n0.869\\nQueuing\\n0.000\\n0.000\\n0.000\\nBaseline 730\\nDead\\n0.000\\n0.000\\n0.000\\nTransplanted\\n0.685\\n1.000\\n0.813\\nQueuing\\n0.000\\n0.000\\n0.000\\nUsing the neural network and backward elimination, we\\nextracted the ten most important features, shown in Table IV.\\nThe features are ranked within the sets, according to their\\nremoval order. We evaluated these feature sets and Table V\\nshows the results. Figure 2 shows the confusion matrix\\nfor the 365 days time period that reveals that the most\\nmisclassiﬁed outcome is queueing as transplanted.\\nWe wanted to look at the distributions of outcomes\\ndepending on the patient having blood group O or not,\\nmostly because previous studies had shown that it was a\\npredictor. In addition, it was also implicitly included in the\\nten most predictive features for 365 days. Table VI shows\\nthat there is a 17% absolute difference between the number\\nof transplanted.\\n75\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:26:59 UTC from IEEE Xplore.  Restrictions apply. \\n\\nFig. 1.\\nPrecision-recall curves for 365 days\\nTABLE III\\nTHE F1 VALUES FOR 180, 365, AND 730 DAYS OBTAINED ON THE TEST\\nSET\\nDays\\nF1\\nF1\\n(micro)\\n(macro)\\n180\\n0.750\\n0.675\\n365\\n0.760\\n0.680\\n730\\n0.888\\n0.680\\nBaseline 180\\n0.567\\n0.241\\nBaseline 365\\n0.685\\n0.271\\nBaseline 730\\n0.769\\n0.290\\nV. DISCUSSION\\nThe distribution of patient outcomes within the cohorts\\nis quite imbalanced, where transplanted is the outcome for\\n57-77% of the patients, during the chosen time periods.\\nWe tried a simple baseline, where we classiﬁed all the\\npatient outcomes as the most frequent, see Table III for the\\nresults. It produced quite good micro averaged values, mostly\\nbecause these metrics are biased towards the largest class, but\\ncomparatively bad macro values.\\nThe largest misclassiﬁcation error in Figure 2 corresponds\\nto queueing as transplanted. This is probably because it is\\nhard to differentiate between the patients that were trans-\\nplanted at a certain time point versus those that are still\\nwaiting in the queue, based on the available features.\\nWe carried out a backward elimination using our neural\\nnetwork and the ten most contributing features is shown\\nin Table IV. This results in a decrease of only about 2%\\n(absolute difference) from the F1 macro score with all the\\nfeatures, see Table V. This means that most of the predictive\\npower from the ANN comes from a few features. Neural\\nnetworks do a kind of feature selection naturally as part of the\\nmodel, weighing up more predictive features and weighing\\ndown the less predictive. Because of this, feature search\\nfor neural networks is usually not needed. But considering\\nit is hard to interpret the matrices produced by the ANN\\nmodel directly, we carried out a backward elimination to\\napproximate the features importance.\\nFig. 2.\\nConfusion matrix for 365 days time period.\\nThe features shared by all of the three sets are: urgency\\nstatus 2, weight, height and body mass index (BMI). BMI can\\nbe considered a feature transformation of weight and height\\nas BMI = weight × height2, but it provided extra predictive\\ninformation over the constituent variables. A sufﬁciently\\ncomplex neural network could probably approximate this\\ntransformation and therefore BMI would probably not be\\nneeded.\\nTable VI shows some discrepancy between the number\\nof transplanted patients depending on having blood group\\nO. This can probably be explained by the fact that only\\npatients that are blood-group compatible with the donor are\\ntransplanted. Even though type O is quite common, patients\\nof this group can only receive from donors from the same\\nblood group and can give to all other types.\\nA. Future Work\\nWe did not have time to fully optimize the hyperparam-\\neters of the neural network and there are some variables\\nthat are available that we did not include, both which could\\nproduce better results.\\nWe also plan to build a more advanced model based on\\nnetworks similar to those we described in this paper to be\\nable to estimate the probability the patient would die or\\nwould be transplanted depending on the time s/he spent in\\nthe waiting list.\\nACKNOWLEDGMENT\\nThis work is based on OPTN data as of October 1, 2013\\nand was supported in part by the Health Resources and\\nServices Administration contract 234-2005-370011C. The\\ncontent is the responsibility of the authors alone and does not\\nnecessarily reﬂect the views or policies of the Department\\nof Health and Human Services. This research was supported\\nby Heart Lung Foundation, The Swedish Research Council,\\nand the eSSENCE program.\\n76\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:26:59 UTC from IEEE Xplore.  Restrictions apply. \\n\\nTABLE IV\\nTHE TEN MOST CONTRIBUTING FEATURES FOR EACH TIME PERIOD USING BACKWARD ELIMINATION, IN ORDER OF IMPORTANCE.\\nRank\\n180 days\\n365 days\\n730 days\\n1\\nUrgency status 2\\nBMI\\nBMI\\n2\\nWeight\\nWeight\\nWeight\\n3\\nBMI\\nHeight\\nHeight\\n4\\nHeight\\nUrgency status 2\\nUrgency status 2\\n5\\nInotropes\\nCreatine clearance\\nCreatinine\\n6\\nBlood group: AB\\nInotropes\\nFunctional status\\n7\\nLife support\\nBlood group: A\\nPulmonary Vascular Resistance\\n8\\nBlood group: B\\nLife support\\nEducational level: none\\n9\\nInotropic support\\nBlood group: AB\\nVentricular assist type: LVAD + RVAD\\n10\\nEthnicity: black\\nBlood group: B\\nEducational level: grade school\\nTABLE V\\nEVALUATION ON THE TEST WITH THE 10 BEST FEATURES FOUND FOR\\nEACH TIME PERIOD.\\nDays\\nF1\\nF1\\n(micro)\\n(macro)\\n180\\n0.710\\n0.657\\n365\\n0.714\\n0.655\\n730\\n0.889\\n0.660\\nTABLE VI\\nTHE DISTRIBUTION OF OUTCOMES DEPENDING ON BLOOD GROUP FOR\\n365 DAYS\\nBlood group\\nDead (%)\\nTransplanted (%)\\nQueueing (%)\\nO\\n14.2\\n59.3\\n26.5\\nnot O\\n9.7\\n76.4\\n13.8\\nREFERENCES\\n[1]\\nMichele Barone et al. “ABO blood group-related\\nwaiting list disparities in liver transplant candidates:\\neffect of the MELD adoption.” In: Transplantation\\n85.6 (2008), pp. 844–849.\\n[2]\\nFranc¸ois Chollet. Keras. https://github.com/\\nfchollet/keras. 2015.\\n[3]\\nPetra Glander et al. “The ‘blood group O problem’ in\\nkidney transplantation—time to change?” In: Nephrol-\\nogy Dialysis Transplantation 25.6 (2010), p. 1998.\\n[4]\\nJ.C. Hussey, J. Parameshwar, and N.R. Banner. “Inﬂu-\\nence of Blood Group on Mortality and Waiting Time\\nBefore Heart Transplantation in the United Kingdom:\\nImplications for Equity of Access”. In: The Journal of\\nHeart and Lung Transplantation 26.1 (2007), pp. 30–\\n33. ISSN: 1053-2498.\\n[5]\\nD. Medved, P. Nugues, and J. Nilsson. “Selection of\\nan optimal feature set to predict heart transplanta-\\ntion outcomes”. In: 2016 38th Annual International\\nConference of the IEEE Engineering in Medicine and\\nBiology Society (EMBC). Aug. 2016, pp. 3290–3293.\\n[6]\\nJohan Nilsson et al. “The International Heart Trans-\\nplant Survival Algorithm (IHTSA): A New Model to\\nImprove Organ Sharing and Survival”. In: PLoS ONE\\n10.3 (2015), e0118644.\\n[7]\\nUnited Network for Organ Sharing. Organ Procure-\\nment and Transplantation Network Data. 2015. URL:\\nhttp : / / optn . transplant . hrsa . gov /\\nconverge / data / default . asp (visited on\\n11/19/2015).\\n[8]\\nDM Powers. “Evaluation: From Precision, Recall and\\nF Factor to ROC, Informedness, Markedness & Cor-\\nrealtion”. In: School of Informatics and Engineer-\\ning, Flinders University of South Australia Adelaide\\n(2007).\\n[9]\\nKazem Rahimi et al. “Risk Prediction in Patients\\nWith Heart Failure”. In: JACC: Heart Failure 2.5\\n(2014), pp. 440–446. ISSN: 2213-1779.\\n[10]\\nHelena Rexius, Folke Nilsson, and Anders Jeppsson.\\n“On the Allocation of Cardiac Allografts from Blood\\nGroup-O Donors”. In: Scandinavian Cardiovascular\\nJournal 36.6 (2002), pp. 342–344.\\n[11]\\nMichael Schemper and Georg Heinze. “Probability\\nimputation revisited for prognostic factor studies”. In:\\nStatistics in medicine 16.1 (1997), pp. 73–80.\\n[12]\\nNitish Srivastava et al. “Dropout: A Simple Way\\nto Prevent Neural Networks from Overﬁtting”. In:\\nJournal of Machine Learning Research 15 (2014),\\npp. 1929–1958.\\nURL: http : / / jmlr . org /\\npapers/v15/srivastava14a.html.\\n[13]\\nAdi L Tarca et al. “Machine learning and its appli-\\ncations to biology”. In: PLOS Computational Biology\\n3.6 (2007), e116.\\n[14]\\nTheano Development Team. “Theano: A Python\\nframework for fast computation of mathematical ex-\\npressions”. In: arXiv e-prints abs/1605.02688 (May\\n2016). URL: http://arxiv.org/abs/1605.\\n02688.\\n[15]\\nVincent Van Asch. Macro-and micro-averaged eval-\\nuation measures. Tech. rep. University of Antwerp,\\n2013.\\n[16]\\nEric S. Weiss et al. “Creation of a Quantitative\\nRecipient Risk Index for Mortality Prediction After\\nCardiac Transplantation (IMPACT)”. In: The Annals\\nof Thoracic Surgery 92.3 (2011), pp. 914–922.\\n77\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:26:59 UTC from IEEE Xplore.  Restrictions apply. \\n'}, {'source': '/Users/sir/Downloads/Data/PDF/Lung_transplant_outcome_prediction_using_UNOS_data.pdf', 'text': 'Lung Transplant Outcome Prediction using UNOS Data\\nAnkit Agrawal∗, Reda Al-Bahrani∗, Mark J. Russo†, Jaishankar Raman‡ and Alok Choudhary∗\\n∗Dept. of Electrical Engineering and Computer Science,\\nNorthwestern University,\\nEvanston, IL USA\\nEmail: {ankitag,rav650,choudhar}@eecs.northwestern.edu\\n†Department of Cardiothoracic Surgery,\\nBarnabas Health Heart Centers\\nLivingston, NJ USA\\nEmail: lmrusso@barnabashealth.org\\n‡Department of Cardiothoracic & Vascular Surgery,\\nRush University Medical Center\\nChicago, IL USA\\nEmail: jai raman@rush.edu\\nAbstract—We analyze lung transplant data from the United\\nNetwork for Organ Sharing (UNOS) program with the aim\\nof developing accurate risk prediction models for mortality\\nwithin 1 year of lung transplant using data mining techniques.\\nThe data used in this study is de-identiﬁed and consists of 62\\npredictor attributes, and 1-year posttranplant survial outcome\\nfor patients who underwent lung transplant between the years\\n2005 and 2009. Our dataset had 5,319 such patient instances.\\nSeveral data mining classiﬁcation techniques were used on\\nthis data along with various data mining optimizations and\\nvalidations to build predictive models for the abovementioned\\noutcome. Prediction results were evaluated using c-statistic\\nmetric, and the highest c-statistic obtained was 0.68. Further,\\nwe also applied feature selection techniques to reduce the\\nnumber of attributes in the model from 50 to 8, without any\\ndegradation in c-statistic. The ﬁnal model was also found to\\noutperform logistic regression, which is the most commonly\\nused technique in predictive healthcare informatics. We believe\\nthat the resulting predictive model on the reduced dataset can\\nbe quite useful to integrate in a risk calculator to aid both\\nphysicians and patients in risk assessment.\\nKeywords-lung transplant; predictive modeling; data mining;\\nI. INTRODUCTION\\nA lung transplant, or pulmonary transplant, is a surgical\\ntransplant procedure in which a patient’s diseased lungs are\\npartially or totally replaced by lungs which come from a\\ndonor. [1]. As of 2008, the survival rate for lung trans-\\nplant after 1 year was 83.6% [2]. Complications of lung\\ntransplantation include rejection of the transplanted lung and\\ninfection [3]. Typical expenses range from around $600,000\\nto $1,100,000 for single lung, double lung, and heart-lung\\ntransplant [4], [5].\\nAs organs available for transplant remain critically scarce,\\nachieving maximal beneﬁt from lung transplantation de-\\npends upon improved recipient and donor selection [6]. Thus\\naccurate estimation of lung transplant outcomes can improve\\nboth informed patient consent by helping patients better\\nunderstand its risks and beneﬁts, and also aid the physicians\\nin decision making by assessing the true patient-speciﬁc\\nrisks of the operation, rather than relying on population-wide\\nrisk assessments. To this end, accurate outcome prediction\\nof performing transplantation is extremely important.\\nThe United Network for Organ Sharing (UNOS) is a\\nprivate, non-proﬁt organization that manages the nation’s\\norgan transplant system under contract with the federal\\ngovernment [7]. UNOS is involved in many aspects of the\\norgan transplant and donation process, including maintaining\\nthe database that contains all organ transplant data for every\\ntransplant event that occurs in the US.\\nApplying data mining techniques to lung transplantation\\ndata can be useful to rank and link pretransplantation at-\\ntributes to the outcome. Here we use data mining tech-\\nniques on UNOS lung transplantation data to estimate 1-\\nyear survival of lung transplant patients, based on pretrans-\\nplant characteristics. Experiments with nearly 50 modeling\\ntechniques were conducted and the results compared to\\nﬁnd the best model for the data used in this study. It was\\nfound that rotation forest ensembles of alternation decision\\ntrees resulted in the best discrimination (c-statistic) between\\nsurvived and non-survived lung recepients. Further, feature\\nselection was used to ﬁnd a smaller subset of attributes that\\ncan potentially achieve similar prediction performance, but\\ncan result in a simpler model.\\nThe rest of the paper is organized as follows: Section\\n2 describes the data mining techniques used in this study\\nfollowed by a brief description of the UNOS data used in\\nthis study in Section 3. Experiments and results are presented\\nin Section 4, and the conclusion and future work is presented\\nin Section 5.\\n2013 IEEE International Conference on Big Data\\n978-1-4799-1293-3/13/$31.00 ©2013 IEEE\\n1\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:21:24 UTC from IEEE Xplore.  Restrictions apply. \\n\\nII. DATA MINING TECHNIQUES\\nA. Modeling\\nWe used 47 classiﬁcation schemes in this study, including\\nboth direct application of classiﬁcation techniques and also\\nconstructing their ensembles using various ensembling tech-\\nniques. Due to space limitations, here we brieﬂy describe\\nonly those classiﬁcation/ensembling techniques whose re-\\nsults we present in the next section.\\n1) Support vector machines: SVMs are based on the\\nStructural Risk Minimization(SRM) principle from\\nstatistical learning theory. A detailed description of\\nSVMs and SRM is available in [8]. In their basic\\nform, SVMs attempt to perform classiﬁcation by con-\\nstructing hyperplanes in a multidimensional space that\\nseparates the cases of different class labels. It supports\\nboth classiﬁcation and regression tasks and can handle\\nmultiple continuous and nominal variables.\\n2) Artiﬁcial neural networks: ANNs are networks of\\ninterconnected artiﬁcial neurons, and are commonly\\nused for non-linear statistical data modeling to model\\ncomplex relationships between inputs and outputs. The\\nnetwork includes a hidden layer of multiple artiﬁcial\\nneurons connected to the inputs and outputs with\\ndifferent edge weights. The internal edge weights are\\n’learnt’ during the training process using techniques\\nlike back propagation. Several good descriptions of\\nneural networks are available [9], [10]. It has been\\nused for accurate estimation in different areas such as\\nmobile health [11], drug abuse [12], civil engineering\\n[13], computer vision\\n[14] and video delivery [15],\\n[16].\\n3) Decision Table: Decision table typically constructs\\nrules involving different combinations of attributes,\\nwhich are selected using an attribute selection search\\nmethod. Simple decision table majority classiﬁer [17]\\nhas been shown to sometimes outperform state-of-the-\\nart classiﬁers.\\n4) KStar: KStar [18] is a lazy instance-based classiﬁer,\\ni.e., the class of a test instance is based upon the class\\nof those training instances similar to it, as determined\\nby some similarity function. It differs from other\\ninstance-based learners in that it uses an entropy-based\\ndistance function.\\n5) Reduced error pruning tree: Commonly known as\\nREPTree [19], it is a implementation of a fast decision\\ntree learner, which builds a decision/regression tree\\nusing information gain/variance and prunes it using\\nreduced-error pruning.\\n6) Random forest: The Random Forest [20] classiﬁer\\nconsists of multiple decision trees. The ﬁnal class\\nof an instance in a Random Forest is assigned by\\noutputting the class that is the mode of the outputs\\nof individual trees, which can produce robust and\\naccurate classiﬁcation, and ability to handle a very\\nlarge number of input variables. It is relatively robust\\nto overﬁtting and can handle datasets with highly\\nimbalanced class distributions.\\n7) Alternating decision tree: ADTree [21] is decision\\ntree classiﬁer which supports only binary classiﬁca-\\ntion. It consists of two types of nodes: decision nodes\\n(specifying a predicate condition, like ’age’ > 45)\\nand prediction nodes (containing a single real-value\\nnumber). ADTrees always have prediction nodes as\\nboth root and leaves. An instance is classiﬁed by\\nfollowing all paths for which all decision nodes are\\ntrue and summing the values of any prediction nodes\\nthat are traversed. This is different from the J48\\ndecision tree algorithm in which an instance follows\\nonly one path through the tree.\\n8) Decision stump: A decision stump [19] is a weak tree-\\nbased machine learning model consisting of a single-\\nlevel decision tree with a categorical or numeric class\\nlabel. Decision stumps are usually used in ensemble\\nmachine learning techniques.\\n9) Naive Bayes: The naive bayes classiﬁer [22] is a\\nsimple probabilistic classiﬁer that is based upon the\\nBayes theorem. This classiﬁer makes strong assump-\\ntions about the independence of the input features,\\nwhich may not always be true. It makes use of the\\nvariables contained in the data sample, by observing\\nand relating them individually to the target class,\\nindependent of each other. Despite this assumption,\\nthe naive bayes classiﬁer works well in practice for a\\nwide variety of datasets and often outperforms other\\ncomplex classiﬁers.\\n10) Bayesian Network: A Bayesian network is a graphical\\nmodel that encodes probabilistic relationships among\\na set of variables, representing a set of random vari-\\nables and their conditional dependencies via a directed\\nacyclic graph (DAG). Bayesian network learning can\\nbe used with various search algorithms for searching\\nthe network structures, and estimator algorithms for\\nﬁnding the conditional probability tables of the net-\\nwork.\\n11) Logistic Regression: Logistic Regression [23] is used\\nfor prediction of the probability of occurrence of an\\nevent by ﬁtting data to a sigmoidal S-shaped logistic\\ncurve. Logistic regression is often used with ridge\\nestimators [24] to improve the parameter estimates and\\nto reduce the error made by further predictions.\\n12) AdaBoost: AdaBoost [25] is a commonly used en-\\nsembling technique for boosting a nominal class clas-\\nsiﬁer. In general, boosting can be used to signiﬁcantly\\nreduce the error of any weak learning algorithm that\\nconsistently generates classiﬁers which need only be\\na little bit better than random guessing. It usually\\ndramatically improves performance, but is also prone\\n2\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:21:24 UTC from IEEE Xplore.  Restrictions apply. \\n\\nto overﬁtting.\\n13) LogitBoost: The LogitBoost algorithm is an ensem-\\nbling technique implementation of additive logistic\\nregression which performs classiﬁcation using a re-\\ngression scheme as the base learner, and can handle\\nmulti-class problems. In [26], the authors explain the\\ntheoretical connection between Boosting and additive\\nmodels.\\n14) Bagging: Bagging [27] is a meta-algorithm to improve\\nthe stability of classiﬁcation and regression algorithms\\nby reducing variance. Bagging is usually applied to\\ndecision tree models to boost their performance. It in-\\nvolves generating a number of new training sets (called\\nbootstrap modules) from the original set by sampling\\nuniformly with replacement. The bootstrap modules\\nare then used to generate models whose predictions\\nare averaged to generate the ﬁnal prediction. Bagging\\nhas been shown work better with decision trees than\\nwith linear models.\\n15) Random subspace: The Random Subspace classi-\\nﬁer [28] constructs a decision tree based classiﬁer\\nconsisting of multiple trees, which are constructed\\nsystematically by pseudo-randomly selecting subsets\\nof features, trying to achieve a balance between over-\\nﬁtting and achieving maximum accuracy. It maintains\\nhighest accuracy on training data and improves on\\ngeneralization accuracy as it grows in complexity.\\n16) Rotation Forest: Rotation forest [29] is a method\\nfor generating classiﬁer ensembles based on feature\\nextraction, which can work both with classiﬁcation\\nand regression base learners. The training data for\\na the base classiﬁer is created by applying Principal\\nComponent Analysis (PCA) [30] to K subsets of the\\nfeature set, followed by K axis rotations to form the\\nnew features for the base learner, to encourage simul-\\ntaneously individual accuracy and diversity within the\\nensemble.\\nB. Feature Selection\\nFeature selection techniques are typically used to select a\\nsubset of relevant features for use in a model. It is based\\non the assumption that data contains many redundant or\\nirrelevant attributes that do not add much to the information\\nprovided by other existing attributes. We used 2 feature\\nselection techniques in this study:\\n1) Correlation Feature Selection (CFS): CFS is used\\nto identify a subset of features highly correlated with\\nthe class variable and weakly correlated amongst them\\n[31]. CFS was used in conjunction with a greedy\\nstepwise search to ﬁnd a subset S with best average\\nmerit, which is given by:\\nMeritS =\\nn.rfo\\n\\x02\\nn + n(n −1).rff\\nwhere n is the number of features in S, rfo is the\\naverage value of feature-outcome correlations, and rff\\nis the average value of all feature-feature correlations.\\n2) Information Gain: This is used to assess the relative\\npredictive power of the predictor attributes, which\\nevaluates the worth of an attribute by measuring the\\ninformation gain with respect to the outcome status:\\nIG(Class, Attrib) = H(Class) −H(Class|Attrib)\\nwhere H(.) denotes the information entropy.\\nThe CFS technique evaluates subsets rather than individ-\\nual attributes, so it was ﬁrst used to ﬁnd a smaller subset of\\nattributes. Subsequently, information gain was used on the\\nreduced set of attributes to get a ranking of the attributes in\\nthe order of their predictive potential, as information gain\\nevaluates each attribute independently.\\nIII. UNOS DATA\\nThe UNOS STAR Thoracic Organ Transplant and Waiting\\nList File contains data on all transplant candidates and trans-\\nplant recipients of heart, lung, and heart-lungs that have been\\nlisted or performed in the U.S. and reported to the OPTN\\nsince October 1987. Data entry by all US transplant centers\\nis mandated by the 1984 National Transplantation Act. This\\ncohort totals over 37,000 observations. There is one record\\nper waiting list registration/transplant event. Each record\\nincludes the most recent follow-up data, including patient\\nand graft survival, waittime, and the patient’s list status\\n(e.g., waiting, transplanted, removed prior to transplant, or\\ndead). This dataset contains nearly 500 ﬁelds to charac-\\nterize candidate/recipient and donor information including\\ndemographics (eg, age, race, sex), social history, and clinical\\ninformation (eg, blood type, measures of lung function and\\nhemodynamic measures, past medical and surgical history,\\nserologies, and severity of co-morbid illness).\\nUNOS provided de-identiﬁed patient-level data (data\\nsource #01052011-6). These data include all lung transplant\\nrecipients and donors in the U.S. and reported to the Organ\\nProcurement and Transplantation Network between January\\n1, 2005 and December 31, 2009. The use of these data is\\nconsistent with the regulations of our Institutional Review\\nBoard.\\nThe study population included 5,319 lung transpalnt pa-\\ntients aged 18 years and older between January 1, 2005 and\\nDecember 31, 2009. Patients were monitored from the date\\nof transplantation to January 3, 2011, which was the last day\\nof follow-up provided by UNOS. 62 predictor attributes were\\nassessed. The primary outcome variable was 1-year post-\\ntransplant survival. We omit the details of all the input 62\\nattributes here due to space constraints. A brief description\\nof the selected subset 12 attributes used in the ﬁnal model\\nis presented later. Out of 5,319 patients, 1,061 patients\\n(19.95%) did not survive more than 1 year after transplant.\\n3\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:21:24 UTC from IEEE Xplore.  Restrictions apply. \\n\\nIV. EXPERIMENTS AND RESULTS\\nWe used the WEKA toolkit 3.6.7 for the implementation\\nof data mining techniques described earlier [32]. 3-fold\\ncross-validation was used for evaluation. Cross-validation is\\nroutinely used to evaluate the prediction performance of data\\nmining models to eliminate any chances of over-ﬁtting. In\\nk-fold cross-validation, the input data is randomly divided\\ninto k segments. k −1 segments are used to build the model\\nand the remaining 1 segment unseen by the model is used\\nto test/evaluate it. This is repeated k times with different\\ntest segments, and the results are aggregrated. In this way,\\neach instance of the dataset is run through a model that\\nhas not seen it during the training phase. Running a test\\ninstance through a trained model generates a probability\\ndistribution for that instance to belong to different possible\\nclass values (here, binary 1-year survival). A probability\\ncutoff is required to actually classify the test instance into\\none of the classes. For binary classiﬁcation, 50% cutoff is\\nmost commonly used.\\nA. Evaluation metrics\\nBinary classiﬁcation performance can be evaluated using\\nvarious metrics. We use the following in this work:\\n1) c-statistic (AUC): The ROC (Receiver operating char-\\nacteristic) curve is a graphical plot of true positive\\nrate and false positive rate. The area under the ROC\\ncurve (AUC or c-statistic) is an effective metric for\\nevaluating binary classiﬁcation performance, as it is\\nindependent of the probability cutoff and measures the\\ndiscrimination power of the model. This is the primary\\nmetric used in this work for inter-model comparison.\\n2) Overall accuracy: It is the percentage of predictions\\nthat are correct. For highly unbalanced classes where\\nthe minority class is the class of interest, overall\\naccuracy by itself may not be a very useful indica-\\ntor of classiﬁcation performance, since even a trivial\\nclassiﬁer that simply predicts the majority class would\\ngive high values of overall accuracy.\\nOverall accuracy =\\n(TP + TN)\\n(TP + TN + FP + FN)\\nwhere TP is the number of true positives (hits), TN\\nis number of true negatives (correct rejections), FP\\nis number of false positives (false alarms), and FN is\\nnumber of false negatives (misses).\\n3) Sensitivity (Recall): It is the percentage of positive\\nlabeled records that were predicted positive. Recall\\nmeasures the completeness of the positive predictions.\\nSensitivity =\\nTP\\n(TP + FN)\\n4) Speciﬁcity: It is the percentage of negative labeled\\nrecords that were predicted negative, thus measuring\\nthe completeness of the negative predictions.\\nSpecificity =\\nTN\\n(TN + FP)\\n5) Positive predictive value (Precision): It is the per-\\ncentage of positive predictions that are correct. Preci-\\nsion measures the correctness of positive predictions.\\nPositive predictive value =\\nTP\\n(TP + FP)\\n6) Negative predictive value: It is the percentage of neg-\\native predictions that are correct, thereby measuring\\nthe correctness of negative predictions.\\nNegative predictive value =\\nTN\\n(TN + FN)\\n7) F-measure: It is in general, possible to have either\\ngood precision or good recall, at the cost of the other,\\nand F-measure combines the two measures in a single\\nmetric by taking the harmonic mean of precision and\\nrecall.\\nF −measure =\\n2.precision.recall\\n(precision + recall)\\nB. Modeling and Feature Selection\\nAs mentioned before, we used 47 classiﬁcation schemes\\non this data. Fig. 1 present the results on 15 classiﬁcation\\nschemes for 1-year survival, consisting of most of the\\npopular classiﬁers. For each of the ensembling techniques,\\nmany underlying classﬁers were tried in the experiments\\nbut only the one with the best c-statistic is preented in the\\nﬁgure. Blue bars represent the c-statistic with the entire set\\nof 62 attributes, and red bars represent the results with the\\nreduced set after feature selection. Using correlation based\\nfeature selection (CFS) technique yielded a subset of only\\n12 features for the given outcome of 1-year survival.\\nThe ﬁgures clearly show that many of the evaluated\\nclassiﬁcation schemes perform comparably well for 1-year\\nsurvival. Of all the models used in this study, Rotation Forest\\nwith Alternate Decision Trees as the underlying classiﬁer\\ngave the best c-statistic of 0.677 with 62 attributes, and of\\n0.680 with 12 attributes. Thus, feature selection techniques\\nwere able to identify a much smaller subset without a loss\\nin c-statistic.\\nFigure 2 presents the relative predictive power of the\\nresulting smaller subset of attributes identiﬁed by CFS for\\n1-year survival. Following is a brief description of these 12\\nattributes:\\n1) Ability to perform daily activities: Functional status\\nis an individual’s ability to perform normal daily\\nactivities required to meet basic needs, fulﬁll usual\\nroles, and maintain health and well-being. Decline in\\nfunctional status is measured by an individual’s loss\\nof independence in activities of daily living (ADLs)\\nover a period of time [33].\\n4\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:21:24 UTC from IEEE Xplore.  Restrictions apply. \\n\\nFigure 1.\\nPrediction performance comparison for 1-year survival in terms of area under the ROC curve (c-statistic).\\nFigure 2.\\nRelative information gain of features resulting from the CFS technique for 1-year survival.\\n5\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:21:24 UTC from IEEE Xplore.  Restrictions apply. \\n\\n2) ICU at transplant: It represents whether the patient\\nwas in Intensive Care Unit at the time of transplant.\\nIntensive Care Units cater to patients with the life-\\nthreatening conditions that require close monitoring\\nand support in order to maintain normal bodily func-\\ntions [34].\\n3) Hospitalized at transplant: It represents whether the\\npatient was already hospitalized when transplantation\\nwas done. In general, patients who are already in\\nhospital tend to have lot of existing complications,\\ninfections, limited mobility, etc., which increases the\\nrisks of complications in addition to reducing the\\nchance of successful outcomes.\\n4) Lung Allocation Score at transplant: The lung\\nallocation score (LAS) is a numerical value used\\nby UNOS to assign relative priority for distributing\\ndonated lungs for transplantation within the US. It\\ntakes into account various measures of a patient’s\\nhealth in order to direct donated organs towards the\\npatients who would best beneﬁt from a lung transplant\\n[35]. This attribute represents the lung allocation score\\nat the time of translplant.\\n5) Lung Allocation Score at Listing: Patients who are\\ndetermined to be eligible for a lung transplant are\\nplaced on a waiting list. This waiting list is part of\\na national allocation system for donor organs run by\\nthe Organ Procurement and Transplantation Network\\n(OPTN) [36]. This attribute represents the lung allo-\\ncation score at the time of listing.\\n6) Intubated at transplant: Intubation refers to the\\ninsertion of a tube into an external or internal oriﬁce\\nof the body for the purpose of adding or removing\\nﬂuids [37].\\n7) GFR at transplant: Glomerular ﬁltration rate (GFR)\\nis a test used to check how well the kidneys are work-\\ning. Speciﬁcally, it estimates how much blood passes\\nthrough the tiny ﬁlters in the kidneys (glomeruli) per\\nminute [38]. This attribute represents the GFR at the\\ntime of transplant.\\n8) Previous lung transplant: It indicates whether the\\npatient has undergone a lung transplant in the past.\\n9) Age at transplant: The age of the patient at the time\\nof transplant.\\n10) ECMO at transplant: Extracorporeal membrane oxy-\\ngenation (ECMO) is an extracorporeal technique of\\nproviding both cardiac and respiratory support oxygen\\nto patients whose heart and lungs are so severely\\ndiseased or damaged that they can no longer serve\\ntheir function [39]. This is maximal life support, but\\nrequires continuous infusion of heparin and blood\\ncirculating through large tubes that exit the body.\\nThis attribute represents the ECMO at the time of\\ntransplant.\\n11) Previous lung transplant <90 days: It indicates\\nwhether the patient has undergone a lung transplant\\nwithin 90 days prior to the current transplant.\\n12) Previous cardiac surgery: It indicates whether the\\npatient has undergone a cardiac surgical procedure in\\nthe past.\\nUsing a predicted 1-year mortality risk ≥50% as a\\ncutoff, the sensitivity of the ﬁnal model for predicting 1-\\nyear mortality was 12%, the speciﬁcity 98%, and the F-\\nmeasure 20%. Table I summarizes the performance of the\\nﬁnal model comparing it to logistic regression, which is the\\nmost widely used technique in healthcare informatics for\\npredictive modeling.\\nWe believe that the preliminary results obtained in this\\nwork are quite encouraging and the fact that we can signif-\\nicantly reduce the number of attrbutes in the model without\\nsacriﬁcing accuracy motivates integration of such models in\\nclinical decision making.\\nV. CONCLUSION AND FUTURE WORK\\nIn this workshop paper, we present our preliminary results\\nof data mining on UNOS data on lung transplantation out-\\ncome. We evaluated nearly 50 classiﬁcation schemes for pre-\\ndicting 1-year survival after the transplant. c-statistic of up\\nto 0.68 was achieved. Further, feature selection techniques\\nwere able to signiﬁcantly reduce the number of attributes in\\nthe model, incurring no cost in c-statistic. We believe that\\nthe resulting models can be very useful to aid physicians\\nin decision making by providing them with patient-speciﬁc\\nrisk estimations.\\nFuture work includes developing more sophisticated mod-\\nels for the studied outcome, and also exploring conditional\\noutcome models using some post-transplant information\\n(e.g. risk of 2-year mortality, given that the patient has\\nalready survived 1 year after transplant), and exploring\\nthe use of undersampling/oversampling to deal with unbal-\\nanced data. We also plan to do similar analysis for other\\ntransplants, and integrate the current and future work into\\nhealthcare and clinical decision making in practice, in the\\nform of risk calculators.\\nVI. ACKNOWLEDGMENTS\\nThis\\nwork\\nis\\nsupported\\nin\\npart\\nby\\nthe\\nfollowing\\ngrants: NSF awards CCF-0833131, CNS-0830927, IIS-\\n0905205, CCF-0938000, CCF-1029166, and OCI-1144061;\\nDOE awards DE-FG02-08ER25848, DE-SC0001283, DE-\\nSC0005309, DESC0005340, and DESC0007456; AFOSR\\naward FA9550-12-1-0458.\\nREFERENCES\\n[1] “Url: Lung transplantation, wikipedia, http://en.wikipedia.org/\\nwiki/Lung transplantation, accessed may 22, 2013.”\\n[2] “Url:\\n2008\\noptn/srtr\\nannual\\nreport,\\nus\\nscientiﬁc\\nreg-\\nistry of transplant recipients, http://www.ustransplant.org/\\nannual reports/current/113 surv-new dh.htm, accessed may\\n22, 2013.”\\n6\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:21:24 UTC from IEEE Xplore.  Restrictions apply. \\n\\nTable I\\nACCURACY OF FINAL MODEL AND LOGISTIC REGRESSION FOR PREDICTING 1-YEAR SURVIVAL AFTER LUNG TRANSPLANT\\nEvaluation Metric\\nLogistic Regression\\nFinal Model\\nc-statistic (95% CI)\\n0.649 (0.630-0.668)\\n0.678 (0.660-0.696)\\nSensitivity (Recall)\\n10.7%\\n12.3%\\nSpeciﬁcity\\n98.2%\\n98.1%\\nPositive predictive value (Precision)\\n59.4%\\n61.0%\\nNegative predictive value\\n81.5%\\n81.8%\\nF-measure\\n18.2%\\n20.4%\\nOverall accuracy\\n80.7%\\n80.9%\\n[3] “Url: Lung transplantation, nlm nih, http://www.nlm.nih.\\ngov/medlineplus/lungtransplantation.html, accessed may 22,\\n2013.”\\n[4] “Url: 2008 u.s. organ and tissue transplant cost estimates and\\ndiscussion, millman inc., http://publications.milliman.com/\\nresearch/health-rr/pdfs/2008-us-organ-tisse-RR4-1-08.pdf,\\naccessed may 13, 2013.”\\n[5] “Url:\\nFinancing\\na\\ntransplant,\\ntranplant\\nliving,\\nhttp://www.transplantliving.org/before-the-transplant/\\nﬁnancing-a-transplant/the-costs/, accessed may 13, 2013.”\\n[6] J. Orens, M. Estenne, S. Arcasoy, J. Conte, P. Corris, J. Egan,\\nT. Egan, S. Keshavjee, C. Knoop, R. Kotloff, F. Martinez,\\nS. Nathan, S. Palmer, A. Patterson, L. Singer, G. Snell,\\nS. Studer, J. Vachiery, and A. Glanville, “International guide-\\nlines for the selection of lung transplant candidates: 2006\\nupdate–a consensus report from the pulmonary scientiﬁc\\ncouncil of the international society for heart and lung trans-\\nplantation,” J Heart Lung Transplant, vol. 25, no. 7, pp. 745–\\n55, 2006.\\n[7] “Url: United network for organ sharing, unos, http://www.\\nunos.org/about/index.php, accessed may 13, 2013.”\\n[8] V. N. Vapnik, “The nature of statistical learning theory,”\\nSpringer, 1995.\\n[9] C. Bishop, Neural Networks for Pattern Recognition. Oxford:\\nUniversity Press, 1995.\\n[10] L. Fausett, Fundamentals of Neural Networks.\\nNew York,\\nPrentice Hall, 1994.\\n[11] A. Pande, Y. Zeng, A. Das, P. Mohapatra, S. Miyamoto,\\nE. Seto, E. Henricson, and J. Han, “Energy expenditure esti-\\nmation with smartphone body sensors,” in 8th International\\nConference on Body Area Networks (Bodynets) 2013, 2013.\\n[12] R. E. Tarter, “Evaluation and treatment of adolescent sub-\\nstance abuse: A decision tree method,” The American journal\\nof drug and alcohol abuse, vol. 16, no. 1-2, pp. 1–46, 1990.\\n[13] S. Kim, K. Gopalakrishnan, and H. Ceylan, “Neural net-\\nworks application in pavement infrastructure materials,” in\\nIntelligent and Soft Computing in Infrastructure Systems\\nEngineering, 2009, pp. 47–66.\\n[14] B. Stenger, A. Thayananthan, P. H. Torr, and R. Cipolla,\\n“Filtering using a tree-based estimator,” in Computer Vision,\\n2003. Proceedings. Ninth IEEE International Conference on.\\nIEEE, 2003, pp. 1063–1070.\\n[15] S. Jana, A. Pande, A. Chan, and P. Mohapatra, “Network\\ncharacterization and perceptual evaluation of skype mobile\\nvideos,” 2013.\\n[16] V. Omwando, A. Pande, Y. Zeng, and P. Mohapatra, “Eval-\\nuating perceptual video quality in 802.11n wlan with mobile\\nclients,” in The 8th ACM International Workshop on Wireless\\nNetwork Testbeds, Experimental Evaluation and Characteri-\\nzation (ACM WiNTECH) 2013, 2013.\\n[17] R. Kohavi, “The power of decision tables,” in Proceedings\\nof the 8th European Conference on Machine Learning, ser.\\nECML ’95, 1995, pp. 174–189.\\n[18] J. G. Cleary and L. E. Trigg, “K*: An instance-based learner\\nusing an entropic distance measure,” in In Proceedings of\\nthe 12th International Conference on Machine Learning.\\nMorgan Kaufmann, 1995, pp. 108–114.\\n[19] I. Witten and E. Frank, Data Mining: Practical machine\\nlearning tools and techniques. Morgan Kaufmann Pub, 2005.\\n[20] L. Breiman, “Random forests,” Machine learning, vol. 45,\\nno. 1, pp. 5–32, 2001.\\n[21] Y. Freund and L. Mason, “The alternating decision tree learn-\\ning algorithm,” in Proceeding of the Sixteenth International\\nConference on Machine Learning.\\nCiteseer, 1999, pp. 124–\\n133.\\n[22] H. George, “John and Pat Langley. Estimating continuous\\ndistributions in bayesian classiﬁers,” in Proceedings of the\\nEleventh Conference on Uncertainty in Artiﬁcial Intelligence,\\n1995, pp. 338–345.\\n[23] D. Hosmer and S. Lemeshow, Applied Logistic Regression.\\nJohn Wiley and Sons, Inc., 1989.\\n[24] S. le Cessie and J. van Houwelingen, “Ridge estimators in\\nlogistic regression,” Applied Statistics, vol. 41, no. 1, pp. 191–\\n201, 1992.\\n[25] Y. Freund and R. E. Schapire, “Experiments with a new\\nboosting algorithm,” 1996.\\n[26] J. Friedman, T. Hastie, and R. Tibshirani, “Special invited\\npaper. additive logistic regression: A statistical view of boost-\\ning,” Annals of statistics, vol. 28, no. 2, pp. 337–374, 2000.\\n[27] L. Breiman, “Bagging predictors,” Machine Learning, vol. 24,\\nno. 2, pp. 123–140, 1996.\\n7\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:21:24 UTC from IEEE Xplore.  Restrictions apply. \\n\\n[28] T. Ho, “The random subspace method for constructing de-\\ncision forests,” IEEE Transactions on Pattern Analysis and\\nMachine Intelligence, vol. 20, no. 8, pp. 832–844, 1998.\\n[29] J. Rodriguez, L. Kuncheva, and C. Alonso, “Rotation forest:\\nA new classiﬁer ensemble method,” Pattern Analysis and\\nMachine Intelligence, IEEE Transactions on, vol. 28, no. 10,\\npp. 1619 –1630, oct. 2006.\\n[30] I.\\nT.\\nJolliffe,\\nPrincipal\\nComponent\\nAnalysis,\\n2nd\\ned.\\nSpringer, 2002.\\n[31] M. Hall, “Correlation-based feature selection for machine\\nlearning,” Ph.D. dissertation, Citeseer, 1999.\\n[32] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann,\\nand I. H. Witten, “The weka data mining software: An\\nupdate,” SIGKDD Explorations, vol. 11, no. 1, 2009.\\n[33] “Url:\\nFunctional\\nstatus,\\nnpcrc,\\nhttp://www.npcrc.org/\\nresources/resources show.htm?doc id=376169,\\naccessed\\nmay 22, 2013.”\\n[34] “Url: Intensive-care unit, wikipedia, https://en.wikipedia.org/\\nwiki/Intensive-care unit, accessed may 22, 2013.”\\n[35] “Url: Lung allocation score, wikipedia, http://en.wikipedia.\\norg/wiki/Lung allocation score, accessed may 22, 2013.”\\n[36] “Url: What to expect before a heart transplant, national heart\\nlung and blood institute, http://www.nhlbi.nih.gov/health//dci/\\nDiseases/ht/ht before.html, accessed may 15, 2013.”\\n[37] “Url:\\nIntubation,\\nwikipedia,\\nhttp://en.wikipedia.org/wiki/\\nIntubation, accessed may 15, 2013.”\\n[38] “Url: Glomerular ﬁltration rate, medlineplus, http://www.nlm.\\nnih.gov/medlineplus/ency/article/007305.htm, accessed may\\n15, 2013.”\\n[39] “Url:\\nExtracorporeal\\nmembrane\\noxygenation,\\nwikipedia,\\nhttp://en.wikipedia.org/wiki/Extracorporeal membrane\\noxygenation, accessed may 15, 2013.”\\n8\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:21:24 UTC from IEEE Xplore.  Restrictions apply. \\n'}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/Lung_transplant_outcome_prediction_using_UNOS_data.pdf', 'text': 'Lung Transplant Outcome Prediction using UNOS Data\\nAnkit Agrawal∗, Reda Al-Bahrani∗, Mark J. Russo†, Jaishankar Raman‡ and Alok Choudhary∗\\n∗Dept. of Electrical Engineering and Computer Science,\\nNorthwestern University,\\nEvanston, IL USA\\nEmail: {ankitag,rav650,choudhar}@eecs.northwestern.edu\\n†Department of Cardiothoracic Surgery,\\nBarnabas Health Heart Centers\\nLivingston, NJ USA\\nEmail: lmrusso@barnabashealth.org\\n‡Department of Cardiothoracic & Vascular Surgery,\\nRush University Medical Center\\nChicago, IL USA\\nEmail: jai raman@rush.edu\\nAbstract—We analyze lung transplant data from the United\\nNetwork for Organ Sharing (UNOS) program with the aim\\nof developing accurate risk prediction models for mortality\\nwithin 1 year of lung transplant using data mining techniques.\\nThe data used in this study is de-identiﬁed and consists of 62\\npredictor attributes, and 1-year posttranplant survial outcome\\nfor patients who underwent lung transplant between the years\\n2005 and 2009. Our dataset had 5,319 such patient instances.\\nSeveral data mining classiﬁcation techniques were used on\\nthis data along with various data mining optimizations and\\nvalidations to build predictive models for the abovementioned\\noutcome. Prediction results were evaluated using c-statistic\\nmetric, and the highest c-statistic obtained was 0.68. Further,\\nwe also applied feature selection techniques to reduce the\\nnumber of attributes in the model from 50 to 8, without any\\ndegradation in c-statistic. The ﬁnal model was also found to\\noutperform logistic regression, which is the most commonly\\nused technique in predictive healthcare informatics. We believe\\nthat the resulting predictive model on the reduced dataset can\\nbe quite useful to integrate in a risk calculator to aid both\\nphysicians and patients in risk assessment.\\nKeywords-lung transplant; predictive modeling; data mining;\\nI. INTRODUCTION\\nA lung transplant, or pulmonary transplant, is a surgical\\ntransplant procedure in which a patient’s diseased lungs are\\npartially or totally replaced by lungs which come from a\\ndonor. [1]. As of 2008, the survival rate for lung trans-\\nplant after 1 year was 83.6% [2]. Complications of lung\\ntransplantation include rejection of the transplanted lung and\\ninfection [3]. Typical expenses range from around $600,000\\nto $1,100,000 for single lung, double lung, and heart-lung\\ntransplant [4], [5].\\nAs organs available for transplant remain critically scarce,\\nachieving maximal beneﬁt from lung transplantation de-\\npends upon improved recipient and donor selection [6]. Thus\\naccurate estimation of lung transplant outcomes can improve\\nboth informed patient consent by helping patients better\\nunderstand its risks and beneﬁts, and also aid the physicians\\nin decision making by assessing the true patient-speciﬁc\\nrisks of the operation, rather than relying on population-wide\\nrisk assessments. To this end, accurate outcome prediction\\nof performing transplantation is extremely important.\\nThe United Network for Organ Sharing (UNOS) is a\\nprivate, non-proﬁt organization that manages the nation’s\\norgan transplant system under contract with the federal\\ngovernment [7]. UNOS is involved in many aspects of the\\norgan transplant and donation process, including maintaining\\nthe database that contains all organ transplant data for every\\ntransplant event that occurs in the US.\\nApplying data mining techniques to lung transplantation\\ndata can be useful to rank and link pretransplantation at-\\ntributes to the outcome. Here we use data mining tech-\\nniques on UNOS lung transplantation data to estimate 1-\\nyear survival of lung transplant patients, based on pretrans-\\nplant characteristics. Experiments with nearly 50 modeling\\ntechniques were conducted and the results compared to\\nﬁnd the best model for the data used in this study. It was\\nfound that rotation forest ensembles of alternation decision\\ntrees resulted in the best discrimination (c-statistic) between\\nsurvived and non-survived lung recepients. Further, feature\\nselection was used to ﬁnd a smaller subset of attributes that\\ncan potentially achieve similar prediction performance, but\\ncan result in a simpler model.\\nThe rest of the paper is organized as follows: Section\\n2 describes the data mining techniques used in this study\\nfollowed by a brief description of the UNOS data used in\\nthis study in Section 3. Experiments and results are presented\\nin Section 4, and the conclusion and future work is presented\\nin Section 5.\\n2013 IEEE International Conference on Big Data\\n978-1-4799-1293-3/13/$31.00 ©2013 IEEE\\n1\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:21:24 UTC from IEEE Xplore.  Restrictions apply. \\n\\nII. DATA MINING TECHNIQUES\\nA. Modeling\\nWe used 47 classiﬁcation schemes in this study, including\\nboth direct application of classiﬁcation techniques and also\\nconstructing their ensembles using various ensembling tech-\\nniques. Due to space limitations, here we brieﬂy describe\\nonly those classiﬁcation/ensembling techniques whose re-\\nsults we present in the next section.\\n1) Support vector machines: SVMs are based on the\\nStructural Risk Minimization(SRM) principle from\\nstatistical learning theory. A detailed description of\\nSVMs and SRM is available in [8]. In their basic\\nform, SVMs attempt to perform classiﬁcation by con-\\nstructing hyperplanes in a multidimensional space that\\nseparates the cases of different class labels. It supports\\nboth classiﬁcation and regression tasks and can handle\\nmultiple continuous and nominal variables.\\n2) Artiﬁcial neural networks: ANNs are networks of\\ninterconnected artiﬁcial neurons, and are commonly\\nused for non-linear statistical data modeling to model\\ncomplex relationships between inputs and outputs. The\\nnetwork includes a hidden layer of multiple artiﬁcial\\nneurons connected to the inputs and outputs with\\ndifferent edge weights. The internal edge weights are\\n’learnt’ during the training process using techniques\\nlike back propagation. Several good descriptions of\\nneural networks are available [9], [10]. It has been\\nused for accurate estimation in different areas such as\\nmobile health [11], drug abuse [12], civil engineering\\n[13], computer vision\\n[14] and video delivery [15],\\n[16].\\n3) Decision Table: Decision table typically constructs\\nrules involving different combinations of attributes,\\nwhich are selected using an attribute selection search\\nmethod. Simple decision table majority classiﬁer [17]\\nhas been shown to sometimes outperform state-of-the-\\nart classiﬁers.\\n4) KStar: KStar [18] is a lazy instance-based classiﬁer,\\ni.e., the class of a test instance is based upon the class\\nof those training instances similar to it, as determined\\nby some similarity function. It differs from other\\ninstance-based learners in that it uses an entropy-based\\ndistance function.\\n5) Reduced error pruning tree: Commonly known as\\nREPTree [19], it is a implementation of a fast decision\\ntree learner, which builds a decision/regression tree\\nusing information gain/variance and prunes it using\\nreduced-error pruning.\\n6) Random forest: The Random Forest [20] classiﬁer\\nconsists of multiple decision trees. The ﬁnal class\\nof an instance in a Random Forest is assigned by\\noutputting the class that is the mode of the outputs\\nof individual trees, which can produce robust and\\naccurate classiﬁcation, and ability to handle a very\\nlarge number of input variables. It is relatively robust\\nto overﬁtting and can handle datasets with highly\\nimbalanced class distributions.\\n7) Alternating decision tree: ADTree [21] is decision\\ntree classiﬁer which supports only binary classiﬁca-\\ntion. It consists of two types of nodes: decision nodes\\n(specifying a predicate condition, like ’age’ > 45)\\nand prediction nodes (containing a single real-value\\nnumber). ADTrees always have prediction nodes as\\nboth root and leaves. An instance is classiﬁed by\\nfollowing all paths for which all decision nodes are\\ntrue and summing the values of any prediction nodes\\nthat are traversed. This is different from the J48\\ndecision tree algorithm in which an instance follows\\nonly one path through the tree.\\n8) Decision stump: A decision stump [19] is a weak tree-\\nbased machine learning model consisting of a single-\\nlevel decision tree with a categorical or numeric class\\nlabel. Decision stumps are usually used in ensemble\\nmachine learning techniques.\\n9) Naive Bayes: The naive bayes classiﬁer [22] is a\\nsimple probabilistic classiﬁer that is based upon the\\nBayes theorem. This classiﬁer makes strong assump-\\ntions about the independence of the input features,\\nwhich may not always be true. It makes use of the\\nvariables contained in the data sample, by observing\\nand relating them individually to the target class,\\nindependent of each other. Despite this assumption,\\nthe naive bayes classiﬁer works well in practice for a\\nwide variety of datasets and often outperforms other\\ncomplex classiﬁers.\\n10) Bayesian Network: A Bayesian network is a graphical\\nmodel that encodes probabilistic relationships among\\na set of variables, representing a set of random vari-\\nables and their conditional dependencies via a directed\\nacyclic graph (DAG). Bayesian network learning can\\nbe used with various search algorithms for searching\\nthe network structures, and estimator algorithms for\\nﬁnding the conditional probability tables of the net-\\nwork.\\n11) Logistic Regression: Logistic Regression [23] is used\\nfor prediction of the probability of occurrence of an\\nevent by ﬁtting data to a sigmoidal S-shaped logistic\\ncurve. Logistic regression is often used with ridge\\nestimators [24] to improve the parameter estimates and\\nto reduce the error made by further predictions.\\n12) AdaBoost: AdaBoost [25] is a commonly used en-\\nsembling technique for boosting a nominal class clas-\\nsiﬁer. In general, boosting can be used to signiﬁcantly\\nreduce the error of any weak learning algorithm that\\nconsistently generates classiﬁers which need only be\\na little bit better than random guessing. It usually\\ndramatically improves performance, but is also prone\\n2\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:21:24 UTC from IEEE Xplore.  Restrictions apply. \\n\\nto overﬁtting.\\n13) LogitBoost: The LogitBoost algorithm is an ensem-\\nbling technique implementation of additive logistic\\nregression which performs classiﬁcation using a re-\\ngression scheme as the base learner, and can handle\\nmulti-class problems. In [26], the authors explain the\\ntheoretical connection between Boosting and additive\\nmodels.\\n14) Bagging: Bagging [27] is a meta-algorithm to improve\\nthe stability of classiﬁcation and regression algorithms\\nby reducing variance. Bagging is usually applied to\\ndecision tree models to boost their performance. It in-\\nvolves generating a number of new training sets (called\\nbootstrap modules) from the original set by sampling\\nuniformly with replacement. The bootstrap modules\\nare then used to generate models whose predictions\\nare averaged to generate the ﬁnal prediction. Bagging\\nhas been shown work better with decision trees than\\nwith linear models.\\n15) Random subspace: The Random Subspace classi-\\nﬁer [28] constructs a decision tree based classiﬁer\\nconsisting of multiple trees, which are constructed\\nsystematically by pseudo-randomly selecting subsets\\nof features, trying to achieve a balance between over-\\nﬁtting and achieving maximum accuracy. It maintains\\nhighest accuracy on training data and improves on\\ngeneralization accuracy as it grows in complexity.\\n16) Rotation Forest: Rotation forest [29] is a method\\nfor generating classiﬁer ensembles based on feature\\nextraction, which can work both with classiﬁcation\\nand regression base learners. The training data for\\na the base classiﬁer is created by applying Principal\\nComponent Analysis (PCA) [30] to K subsets of the\\nfeature set, followed by K axis rotations to form the\\nnew features for the base learner, to encourage simul-\\ntaneously individual accuracy and diversity within the\\nensemble.\\nB. Feature Selection\\nFeature selection techniques are typically used to select a\\nsubset of relevant features for use in a model. It is based\\non the assumption that data contains many redundant or\\nirrelevant attributes that do not add much to the information\\nprovided by other existing attributes. We used 2 feature\\nselection techniques in this study:\\n1) Correlation Feature Selection (CFS): CFS is used\\nto identify a subset of features highly correlated with\\nthe class variable and weakly correlated amongst them\\n[31]. CFS was used in conjunction with a greedy\\nstepwise search to ﬁnd a subset S with best average\\nmerit, which is given by:\\nMeritS =\\nn.rfo\\n\\x02\\nn + n(n −1).rff\\nwhere n is the number of features in S, rfo is the\\naverage value of feature-outcome correlations, and rff\\nis the average value of all feature-feature correlations.\\n2) Information Gain: This is used to assess the relative\\npredictive power of the predictor attributes, which\\nevaluates the worth of an attribute by measuring the\\ninformation gain with respect to the outcome status:\\nIG(Class, Attrib) = H(Class) −H(Class|Attrib)\\nwhere H(.) denotes the information entropy.\\nThe CFS technique evaluates subsets rather than individ-\\nual attributes, so it was ﬁrst used to ﬁnd a smaller subset of\\nattributes. Subsequently, information gain was used on the\\nreduced set of attributes to get a ranking of the attributes in\\nthe order of their predictive potential, as information gain\\nevaluates each attribute independently.\\nIII. UNOS DATA\\nThe UNOS STAR Thoracic Organ Transplant and Waiting\\nList File contains data on all transplant candidates and trans-\\nplant recipients of heart, lung, and heart-lungs that have been\\nlisted or performed in the U.S. and reported to the OPTN\\nsince October 1987. Data entry by all US transplant centers\\nis mandated by the 1984 National Transplantation Act. This\\ncohort totals over 37,000 observations. There is one record\\nper waiting list registration/transplant event. Each record\\nincludes the most recent follow-up data, including patient\\nand graft survival, waittime, and the patient’s list status\\n(e.g., waiting, transplanted, removed prior to transplant, or\\ndead). This dataset contains nearly 500 ﬁelds to charac-\\nterize candidate/recipient and donor information including\\ndemographics (eg, age, race, sex), social history, and clinical\\ninformation (eg, blood type, measures of lung function and\\nhemodynamic measures, past medical and surgical history,\\nserologies, and severity of co-morbid illness).\\nUNOS provided de-identiﬁed patient-level data (data\\nsource #01052011-6). These data include all lung transplant\\nrecipients and donors in the U.S. and reported to the Organ\\nProcurement and Transplantation Network between January\\n1, 2005 and December 31, 2009. The use of these data is\\nconsistent with the regulations of our Institutional Review\\nBoard.\\nThe study population included 5,319 lung transpalnt pa-\\ntients aged 18 years and older between January 1, 2005 and\\nDecember 31, 2009. Patients were monitored from the date\\nof transplantation to January 3, 2011, which was the last day\\nof follow-up provided by UNOS. 62 predictor attributes were\\nassessed. The primary outcome variable was 1-year post-\\ntransplant survival. We omit the details of all the input 62\\nattributes here due to space constraints. A brief description\\nof the selected subset 12 attributes used in the ﬁnal model\\nis presented later. Out of 5,319 patients, 1,061 patients\\n(19.95%) did not survive more than 1 year after transplant.\\n3\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:21:24 UTC from IEEE Xplore.  Restrictions apply. \\n\\nIV. EXPERIMENTS AND RESULTS\\nWe used the WEKA toolkit 3.6.7 for the implementation\\nof data mining techniques described earlier [32]. 3-fold\\ncross-validation was used for evaluation. Cross-validation is\\nroutinely used to evaluate the prediction performance of data\\nmining models to eliminate any chances of over-ﬁtting. In\\nk-fold cross-validation, the input data is randomly divided\\ninto k segments. k −1 segments are used to build the model\\nand the remaining 1 segment unseen by the model is used\\nto test/evaluate it. This is repeated k times with different\\ntest segments, and the results are aggregrated. In this way,\\neach instance of the dataset is run through a model that\\nhas not seen it during the training phase. Running a test\\ninstance through a trained model generates a probability\\ndistribution for that instance to belong to different possible\\nclass values (here, binary 1-year survival). A probability\\ncutoff is required to actually classify the test instance into\\none of the classes. For binary classiﬁcation, 50% cutoff is\\nmost commonly used.\\nA. Evaluation metrics\\nBinary classiﬁcation performance can be evaluated using\\nvarious metrics. We use the following in this work:\\n1) c-statistic (AUC): The ROC (Receiver operating char-\\nacteristic) curve is a graphical plot of true positive\\nrate and false positive rate. The area under the ROC\\ncurve (AUC or c-statistic) is an effective metric for\\nevaluating binary classiﬁcation performance, as it is\\nindependent of the probability cutoff and measures the\\ndiscrimination power of the model. This is the primary\\nmetric used in this work for inter-model comparison.\\n2) Overall accuracy: It is the percentage of predictions\\nthat are correct. For highly unbalanced classes where\\nthe minority class is the class of interest, overall\\naccuracy by itself may not be a very useful indica-\\ntor of classiﬁcation performance, since even a trivial\\nclassiﬁer that simply predicts the majority class would\\ngive high values of overall accuracy.\\nOverall accuracy =\\n(TP + TN)\\n(TP + TN + FP + FN)\\nwhere TP is the number of true positives (hits), TN\\nis number of true negatives (correct rejections), FP\\nis number of false positives (false alarms), and FN is\\nnumber of false negatives (misses).\\n3) Sensitivity (Recall): It is the percentage of positive\\nlabeled records that were predicted positive. Recall\\nmeasures the completeness of the positive predictions.\\nSensitivity =\\nTP\\n(TP + FN)\\n4) Speciﬁcity: It is the percentage of negative labeled\\nrecords that were predicted negative, thus measuring\\nthe completeness of the negative predictions.\\nSpecificity =\\nTN\\n(TN + FP)\\n5) Positive predictive value (Precision): It is the per-\\ncentage of positive predictions that are correct. Preci-\\nsion measures the correctness of positive predictions.\\nPositive predictive value =\\nTP\\n(TP + FP)\\n6) Negative predictive value: It is the percentage of neg-\\native predictions that are correct, thereby measuring\\nthe correctness of negative predictions.\\nNegative predictive value =\\nTN\\n(TN + FN)\\n7) F-measure: It is in general, possible to have either\\ngood precision or good recall, at the cost of the other,\\nand F-measure combines the two measures in a single\\nmetric by taking the harmonic mean of precision and\\nrecall.\\nF −measure =\\n2.precision.recall\\n(precision + recall)\\nB. Modeling and Feature Selection\\nAs mentioned before, we used 47 classiﬁcation schemes\\non this data. Fig. 1 present the results on 15 classiﬁcation\\nschemes for 1-year survival, consisting of most of the\\npopular classiﬁers. For each of the ensembling techniques,\\nmany underlying classﬁers were tried in the experiments\\nbut only the one with the best c-statistic is preented in the\\nﬁgure. Blue bars represent the c-statistic with the entire set\\nof 62 attributes, and red bars represent the results with the\\nreduced set after feature selection. Using correlation based\\nfeature selection (CFS) technique yielded a subset of only\\n12 features for the given outcome of 1-year survival.\\nThe ﬁgures clearly show that many of the evaluated\\nclassiﬁcation schemes perform comparably well for 1-year\\nsurvival. Of all the models used in this study, Rotation Forest\\nwith Alternate Decision Trees as the underlying classiﬁer\\ngave the best c-statistic of 0.677 with 62 attributes, and of\\n0.680 with 12 attributes. Thus, feature selection techniques\\nwere able to identify a much smaller subset without a loss\\nin c-statistic.\\nFigure 2 presents the relative predictive power of the\\nresulting smaller subset of attributes identiﬁed by CFS for\\n1-year survival. Following is a brief description of these 12\\nattributes:\\n1) Ability to perform daily activities: Functional status\\nis an individual’s ability to perform normal daily\\nactivities required to meet basic needs, fulﬁll usual\\nroles, and maintain health and well-being. Decline in\\nfunctional status is measured by an individual’s loss\\nof independence in activities of daily living (ADLs)\\nover a period of time [33].\\n4\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:21:24 UTC from IEEE Xplore.  Restrictions apply. \\n\\nFigure 1.\\nPrediction performance comparison for 1-year survival in terms of area under the ROC curve (c-statistic).\\nFigure 2.\\nRelative information gain of features resulting from the CFS technique for 1-year survival.\\n5\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:21:24 UTC from IEEE Xplore.  Restrictions apply. \\n\\n2) ICU at transplant: It represents whether the patient\\nwas in Intensive Care Unit at the time of transplant.\\nIntensive Care Units cater to patients with the life-\\nthreatening conditions that require close monitoring\\nand support in order to maintain normal bodily func-\\ntions [34].\\n3) Hospitalized at transplant: It represents whether the\\npatient was already hospitalized when transplantation\\nwas done. In general, patients who are already in\\nhospital tend to have lot of existing complications,\\ninfections, limited mobility, etc., which increases the\\nrisks of complications in addition to reducing the\\nchance of successful outcomes.\\n4) Lung Allocation Score at transplant: The lung\\nallocation score (LAS) is a numerical value used\\nby UNOS to assign relative priority for distributing\\ndonated lungs for transplantation within the US. It\\ntakes into account various measures of a patient’s\\nhealth in order to direct donated organs towards the\\npatients who would best beneﬁt from a lung transplant\\n[35]. This attribute represents the lung allocation score\\nat the time of translplant.\\n5) Lung Allocation Score at Listing: Patients who are\\ndetermined to be eligible for a lung transplant are\\nplaced on a waiting list. This waiting list is part of\\na national allocation system for donor organs run by\\nthe Organ Procurement and Transplantation Network\\n(OPTN) [36]. This attribute represents the lung allo-\\ncation score at the time of listing.\\n6) Intubated at transplant: Intubation refers to the\\ninsertion of a tube into an external or internal oriﬁce\\nof the body for the purpose of adding or removing\\nﬂuids [37].\\n7) GFR at transplant: Glomerular ﬁltration rate (GFR)\\nis a test used to check how well the kidneys are work-\\ning. Speciﬁcally, it estimates how much blood passes\\nthrough the tiny ﬁlters in the kidneys (glomeruli) per\\nminute [38]. This attribute represents the GFR at the\\ntime of transplant.\\n8) Previous lung transplant: It indicates whether the\\npatient has undergone a lung transplant in the past.\\n9) Age at transplant: The age of the patient at the time\\nof transplant.\\n10) ECMO at transplant: Extracorporeal membrane oxy-\\ngenation (ECMO) is an extracorporeal technique of\\nproviding both cardiac and respiratory support oxygen\\nto patients whose heart and lungs are so severely\\ndiseased or damaged that they can no longer serve\\ntheir function [39]. This is maximal life support, but\\nrequires continuous infusion of heparin and blood\\ncirculating through large tubes that exit the body.\\nThis attribute represents the ECMO at the time of\\ntransplant.\\n11) Previous lung transplant <90 days: It indicates\\nwhether the patient has undergone a lung transplant\\nwithin 90 days prior to the current transplant.\\n12) Previous cardiac surgery: It indicates whether the\\npatient has undergone a cardiac surgical procedure in\\nthe past.\\nUsing a predicted 1-year mortality risk ≥50% as a\\ncutoff, the sensitivity of the ﬁnal model for predicting 1-\\nyear mortality was 12%, the speciﬁcity 98%, and the F-\\nmeasure 20%. Table I summarizes the performance of the\\nﬁnal model comparing it to logistic regression, which is the\\nmost widely used technique in healthcare informatics for\\npredictive modeling.\\nWe believe that the preliminary results obtained in this\\nwork are quite encouraging and the fact that we can signif-\\nicantly reduce the number of attrbutes in the model without\\nsacriﬁcing accuracy motivates integration of such models in\\nclinical decision making.\\nV. CONCLUSION AND FUTURE WORK\\nIn this workshop paper, we present our preliminary results\\nof data mining on UNOS data on lung transplantation out-\\ncome. We evaluated nearly 50 classiﬁcation schemes for pre-\\ndicting 1-year survival after the transplant. c-statistic of up\\nto 0.68 was achieved. Further, feature selection techniques\\nwere able to signiﬁcantly reduce the number of attributes in\\nthe model, incurring no cost in c-statistic. We believe that\\nthe resulting models can be very useful to aid physicians\\nin decision making by providing them with patient-speciﬁc\\nrisk estimations.\\nFuture work includes developing more sophisticated mod-\\nels for the studied outcome, and also exploring conditional\\noutcome models using some post-transplant information\\n(e.g. risk of 2-year mortality, given that the patient has\\nalready survived 1 year after transplant), and exploring\\nthe use of undersampling/oversampling to deal with unbal-\\nanced data. We also plan to do similar analysis for other\\ntransplants, and integrate the current and future work into\\nhealthcare and clinical decision making in practice, in the\\nform of risk calculators.\\nVI. ACKNOWLEDGMENTS\\nThis\\nwork\\nis\\nsupported\\nin\\npart\\nby\\nthe\\nfollowing\\ngrants: NSF awards CCF-0833131, CNS-0830927, IIS-\\n0905205, CCF-0938000, CCF-1029166, and OCI-1144061;\\nDOE awards DE-FG02-08ER25848, DE-SC0001283, DE-\\nSC0005309, DESC0005340, and DESC0007456; AFOSR\\naward FA9550-12-1-0458.\\nREFERENCES\\n[1] “Url: Lung transplantation, wikipedia, http://en.wikipedia.org/\\nwiki/Lung transplantation, accessed may 22, 2013.”\\n[2] “Url:\\n2008\\noptn/srtr\\nannual\\nreport,\\nus\\nscientiﬁc\\nreg-\\nistry of transplant recipients, http://www.ustransplant.org/\\nannual reports/current/113 surv-new dh.htm, accessed may\\n22, 2013.”\\n6\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:21:24 UTC from IEEE Xplore.  Restrictions apply. \\n\\nTable I\\nACCURACY OF FINAL MODEL AND LOGISTIC REGRESSION FOR PREDICTING 1-YEAR SURVIVAL AFTER LUNG TRANSPLANT\\nEvaluation Metric\\nLogistic Regression\\nFinal Model\\nc-statistic (95% CI)\\n0.649 (0.630-0.668)\\n0.678 (0.660-0.696)\\nSensitivity (Recall)\\n10.7%\\n12.3%\\nSpeciﬁcity\\n98.2%\\n98.1%\\nPositive predictive value (Precision)\\n59.4%\\n61.0%\\nNegative predictive value\\n81.5%\\n81.8%\\nF-measure\\n18.2%\\n20.4%\\nOverall accuracy\\n80.7%\\n80.9%\\n[3] “Url: Lung transplantation, nlm nih, http://www.nlm.nih.\\ngov/medlineplus/lungtransplantation.html, accessed may 22,\\n2013.”\\n[4] “Url: 2008 u.s. organ and tissue transplant cost estimates and\\ndiscussion, millman inc., http://publications.milliman.com/\\nresearch/health-rr/pdfs/2008-us-organ-tisse-RR4-1-08.pdf,\\naccessed may 13, 2013.”\\n[5] “Url:\\nFinancing\\na\\ntransplant,\\ntranplant\\nliving,\\nhttp://www.transplantliving.org/before-the-transplant/\\nﬁnancing-a-transplant/the-costs/, accessed may 13, 2013.”\\n[6] J. Orens, M. Estenne, S. Arcasoy, J. Conte, P. Corris, J. Egan,\\nT. Egan, S. Keshavjee, C. Knoop, R. Kotloff, F. Martinez,\\nS. Nathan, S. Palmer, A. Patterson, L. Singer, G. Snell,\\nS. Studer, J. Vachiery, and A. Glanville, “International guide-\\nlines for the selection of lung transplant candidates: 2006\\nupdate–a consensus report from the pulmonary scientiﬁc\\ncouncil of the international society for heart and lung trans-\\nplantation,” J Heart Lung Transplant, vol. 25, no. 7, pp. 745–\\n55, 2006.\\n[7] “Url: United network for organ sharing, unos, http://www.\\nunos.org/about/index.php, accessed may 13, 2013.”\\n[8] V. N. Vapnik, “The nature of statistical learning theory,”\\nSpringer, 1995.\\n[9] C. Bishop, Neural Networks for Pattern Recognition. Oxford:\\nUniversity Press, 1995.\\n[10] L. Fausett, Fundamentals of Neural Networks.\\nNew York,\\nPrentice Hall, 1994.\\n[11] A. Pande, Y. Zeng, A. Das, P. Mohapatra, S. Miyamoto,\\nE. Seto, E. Henricson, and J. Han, “Energy expenditure esti-\\nmation with smartphone body sensors,” in 8th International\\nConference on Body Area Networks (Bodynets) 2013, 2013.\\n[12] R. E. Tarter, “Evaluation and treatment of adolescent sub-\\nstance abuse: A decision tree method,” The American journal\\nof drug and alcohol abuse, vol. 16, no. 1-2, pp. 1–46, 1990.\\n[13] S. Kim, K. Gopalakrishnan, and H. Ceylan, “Neural net-\\nworks application in pavement infrastructure materials,” in\\nIntelligent and Soft Computing in Infrastructure Systems\\nEngineering, 2009, pp. 47–66.\\n[14] B. Stenger, A. Thayananthan, P. H. Torr, and R. Cipolla,\\n“Filtering using a tree-based estimator,” in Computer Vision,\\n2003. Proceedings. Ninth IEEE International Conference on.\\nIEEE, 2003, pp. 1063–1070.\\n[15] S. Jana, A. Pande, A. Chan, and P. Mohapatra, “Network\\ncharacterization and perceptual evaluation of skype mobile\\nvideos,” 2013.\\n[16] V. Omwando, A. Pande, Y. Zeng, and P. Mohapatra, “Eval-\\nuating perceptual video quality in 802.11n wlan with mobile\\nclients,” in The 8th ACM International Workshop on Wireless\\nNetwork Testbeds, Experimental Evaluation and Characteri-\\nzation (ACM WiNTECH) 2013, 2013.\\n[17] R. Kohavi, “The power of decision tables,” in Proceedings\\nof the 8th European Conference on Machine Learning, ser.\\nECML ’95, 1995, pp. 174–189.\\n[18] J. G. Cleary and L. E. Trigg, “K*: An instance-based learner\\nusing an entropic distance measure,” in In Proceedings of\\nthe 12th International Conference on Machine Learning.\\nMorgan Kaufmann, 1995, pp. 108–114.\\n[19] I. Witten and E. Frank, Data Mining: Practical machine\\nlearning tools and techniques. Morgan Kaufmann Pub, 2005.\\n[20] L. Breiman, “Random forests,” Machine learning, vol. 45,\\nno. 1, pp. 5–32, 2001.\\n[21] Y. Freund and L. Mason, “The alternating decision tree learn-\\ning algorithm,” in Proceeding of the Sixteenth International\\nConference on Machine Learning.\\nCiteseer, 1999, pp. 124–\\n133.\\n[22] H. George, “John and Pat Langley. Estimating continuous\\ndistributions in bayesian classiﬁers,” in Proceedings of the\\nEleventh Conference on Uncertainty in Artiﬁcial Intelligence,\\n1995, pp. 338–345.\\n[23] D. Hosmer and S. Lemeshow, Applied Logistic Regression.\\nJohn Wiley and Sons, Inc., 1989.\\n[24] S. le Cessie and J. van Houwelingen, “Ridge estimators in\\nlogistic regression,” Applied Statistics, vol. 41, no. 1, pp. 191–\\n201, 1992.\\n[25] Y. Freund and R. E. Schapire, “Experiments with a new\\nboosting algorithm,” 1996.\\n[26] J. Friedman, T. Hastie, and R. Tibshirani, “Special invited\\npaper. additive logistic regression: A statistical view of boost-\\ning,” Annals of statistics, vol. 28, no. 2, pp. 337–374, 2000.\\n[27] L. Breiman, “Bagging predictors,” Machine Learning, vol. 24,\\nno. 2, pp. 123–140, 1996.\\n7\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:21:24 UTC from IEEE Xplore.  Restrictions apply. \\n\\n[28] T. Ho, “The random subspace method for constructing de-\\ncision forests,” IEEE Transactions on Pattern Analysis and\\nMachine Intelligence, vol. 20, no. 8, pp. 832–844, 1998.\\n[29] J. Rodriguez, L. Kuncheva, and C. Alonso, “Rotation forest:\\nA new classiﬁer ensemble method,” Pattern Analysis and\\nMachine Intelligence, IEEE Transactions on, vol. 28, no. 10,\\npp. 1619 –1630, oct. 2006.\\n[30] I.\\nT.\\nJolliffe,\\nPrincipal\\nComponent\\nAnalysis,\\n2nd\\ned.\\nSpringer, 2002.\\n[31] M. Hall, “Correlation-based feature selection for machine\\nlearning,” Ph.D. dissertation, Citeseer, 1999.\\n[32] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann,\\nand I. H. Witten, “The weka data mining software: An\\nupdate,” SIGKDD Explorations, vol. 11, no. 1, 2009.\\n[33] “Url:\\nFunctional\\nstatus,\\nnpcrc,\\nhttp://www.npcrc.org/\\nresources/resources show.htm?doc id=376169,\\naccessed\\nmay 22, 2013.”\\n[34] “Url: Intensive-care unit, wikipedia, https://en.wikipedia.org/\\nwiki/Intensive-care unit, accessed may 22, 2013.”\\n[35] “Url: Lung allocation score, wikipedia, http://en.wikipedia.\\norg/wiki/Lung allocation score, accessed may 22, 2013.”\\n[36] “Url: What to expect before a heart transplant, national heart\\nlung and blood institute, http://www.nhlbi.nih.gov/health//dci/\\nDiseases/ht/ht before.html, accessed may 15, 2013.”\\n[37] “Url:\\nIntubation,\\nwikipedia,\\nhttp://en.wikipedia.org/wiki/\\nIntubation, accessed may 15, 2013.”\\n[38] “Url: Glomerular ﬁltration rate, medlineplus, http://www.nlm.\\nnih.gov/medlineplus/ency/article/007305.htm, accessed may\\n15, 2013.”\\n[39] “Url:\\nExtracorporeal\\nmembrane\\noxygenation,\\nwikipedia,\\nhttp://en.wikipedia.org/wiki/Extracorporeal membrane\\noxygenation, accessed may 15, 2013.”\\n8\\nAuthorized licensed use limited to: DePaul University. Downloaded on October 19,2023 at 16:21:24 UTC from IEEE Xplore.  Restrictions apply. \\n'}, {'source': '/Users/sir/Downloads/Data/PDF/Personalized Donor-Recipient Matching for Organ Transplantation.pdf', 'text': 'Personalized Donor-Recipient\\nMatching for Organ Transplantation\\nJinsung Yoon,1 Ahmed M. Alaa,1 Martin Cadeiras,2 Mihaela van der Schaar1\\n1 Department of Electrical Engineering, University of California, Los Angeles (UCLA), CA, 90095, USA\\n2 David Geffen School of Medicine, University of California, Los Angeles (UCLA), CA, 90095, USA\\njsyoon0823@g.ucla.edu, ahmedmalaa@g.ucla.edu, MCardeiras@mednet.ucla.edu, mihaela@ee.ucla.edu\\nAbstract\\nOrgan transplants can improve the life expectancy and qual-\\nity of life for the recipient but carry the risk of serious post-\\noperative complications, such as septic shock and organ re-\\njection. The probability of a successful transplant depends in\\na very subtle fashion on compatibility between the donor and\\nthe recipient - but current medical practice is short of do-\\nmain knowledge regarding the complex nature of recipient-\\ndonor compatibility. Hence a data-driven approach for learn-\\ning compatibility has the potential for signiﬁcant improve-\\nments in match quality. This paper proposes a novel system\\n(ConﬁdentMatch) that is trained using data from electronic\\nhealth records. ConﬁdentMatch predicts the success of an or-\\ngan transplant (in terms of the 3-year survival rates) on the\\nbasis of clinical and demographic traits of the donor and\\nrecipient. ConﬁdentMatch captures the heterogeneity of the\\ndonor and recipient traits by optimally dividing the feature\\nspace into clusters and constructing different optimal predic-\\ntive models to each cluster. The system controls the complex-\\nity of the learned predictive model in a way that allows for\\nassuring more granular and accurate predictions for a larger\\nnumber of potential recipient-donor pairs, thereby ensuring\\nthat predictions are “personalized” and tailored to individ-\\nual characteristics to the ﬁnest possible granularity. Experi-\\nments conducted on the UNOS heart transplant dataset show\\nthe superiority of the prognostic value of ConﬁdentMatch to\\nother competing benchmarks; ConﬁdentMatch can provide\\npredictions of success with 95% accuracy for 5,489 patients\\nof a total population of 9,620 patients, which corresponds to\\n410 more patients than the most competitive benchmark al-\\ngorithm (DeepBoost).\\nIntroduction\\nOrgan transplantation is the therapy of choice for patients\\nwith end-stage diseases who are refractory to medical ther-\\napies (Shah 2012). Even though organ transplantation can\\nincrease the life expectancy and quality of life for the re-\\ncipient, the operation can entail various complications, in-\\ncluding infection, acute and chronic rejection and malig-\\nnancy (Huynh 2014). The pre-operative anticipation of the\\nrisk associated with organ transplantation is a regular task\\nthat transplant centers perform in order to determine which\\npatients would beneﬁt from transplantation and accurately\\nCopyright c⃝2017, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.\\nidentify these for whom the risk of transplantation is too\\nhigh and would therefore provide no survival beneﬁts. Such\\na risk assessment task is quite complicated for that post-\\noperative patient survival depends on different types of risk\\nfactors: recipient-related factors (e.g. cardiovascular dis-\\nease severity of heart recipients (Wozniak 2014; Nwakanma\\n2007; Russo 2006; Silva 2016)), recipient-donor match-\\ning factors (e.g. weight ratio and human leukocyte antigen\\n(HLA) (Jayarajan 2013), blood group compatibility (Jawitz\\n2013), race (Allen 2010), etc), and donor-related factors\\n(e.g. diabetes (Arnaoutakis 2012; Taghavi 2013)). The in-\\nteractions among all these risk factors make the progno-\\nsis problem for the organ transplant outcomes highly com-\\nplex; the National Heart, Lung and Blood Institute (NHLBI)\\nworking group suggests resolving this problem by enhanc-\\ning the phenotypic compatibility characterization of the\\npre-transplant recipient-donor population (Mancini 2010;\\nCollins 2015; Shah 2012).\\nIn the light of the above, we seek an enhanced pheno-\\ntypic characterization for the compatibility of patient-donor\\npairs via a precision medicine approach (Collins 2015) in\\nwhich we construct personalized predictive models that are\\ntailored to the individual traits of both the donor and the re-\\ncipient to the ﬁnest possible granularity. The advent of elec-\\ntronic health records (EHR) inspire a data-driven approach\\nfor constructing such predictive models in which the com-\\nplex recipient-donor compatibility patterns are discovered\\nfrom observational data. To that end, we develop Conﬁdent-\\nMatch: an automated system that learns the recipient-donor\\ncompatibility patterns from the EHR data in terms of the\\nprobability of transplant success for given recipient-donor\\npairs. The clinicians can utilize ConﬁdentMatch as a prog-\\nnostic tool for managing organ transplantation selection de-\\ncisions in which information about the donor and recipient\\nare fed to the system, and the output comes as the probabil-\\nity of the transplant’s success. The system can also be used\\nin conjunction with any matching algorithms, such as the\\nNobel prize-winning algorithm of Shapley and Roth (Shap-\\nley 1962; Roth 2003). More speciﬁcally, a set of patients is\\nmatched to a set of donors using the compatibility score that\\nConﬁdentMatch computes as an input to the matching algo-\\nrithm.\\nIn order to learn the highly complex recipient-donor com-\\npatibility patterns, ConﬁdentMatch adopts a novel learn-\\nProceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)\\n1647\\n\\ning framework in which the recipient-donor feature space\\nis partitioned into disjoint subsets, and a separate predic-\\ntive model (learner) is assigned to each partition. Such a\\nlearning approach gives rise to a highly complex overall\\npredictive model for mapping recipient-donor features to\\ntransplant success probabilities. Over-ﬁtting is controlled by\\npenalizing the number of partitions in the recipient-donor\\nfeature space and the complexity of the learners assigned\\nto the different partitions. Unlike existing meta-learning al-\\ngorithms, ConﬁdentMatch solves an optimization problem\\nthrough which it jointly determines how to partition the\\nrecipient-donor feature space, and what predictive model to\\nassign to each partition. Since such an optimization prob-\\nlem is intractable, we propose an efﬁcient greedy algorithm\\nthat proceeds iteratively by ﬁrst ﬁxing the number of par-\\ntitions in the recipient-donor feature space, optimizing the\\npredictive models assigned to each partition, and then fur-\\nther stratifying each partition and re-assigning more “spe-\\ncialized” predictive models to the new, ﬁner partitions. The\\nalgorithm stops stratifying the recipient-donor feature space\\nwhen further stratiﬁcation would lead to new partitions with\\nno enough per-partition training data for learning ﬁner pre-\\ndictive models.\\nExperiments conducted on the United Network for Or-\\ngan Sharing (UNOS) heart transplant dataset (Cecka 1996)\\nshow that ConﬁdentMatch can provide predictions of suc-\\ncess with 95% accuracy for 5,489 patients of a total popula-\\ntion of 9,620 patients – 410 more patients than for the best\\nstate-of-the-art machine learning algorithm (DeepBoost).\\nRelated Work\\nWe identify three broad categories of learning algorithms\\nthat are capable of combining multiple predictive models\\nor stratifying the feature space into ﬁne clusters: ensemble\\nlearning algorithms, clustering algorithms and tree-based al-\\ngorithms. We compare ConﬁdentMatch with these methods\\nhereunder.\\nEnsemble learning algorithms\\nMethods based on ensemble learning, such as Random For-\\nest (Liaw 2002), LogitBoost (Friedman et al. 2000), Adap-\\ntive Boosting (Freund 1997) and DeepBoost (Kuznetsov\\n2014), operate by allocating different sets of training data\\nto different weak learners, and then aggregating the predic-\\ntions of these weak learners through a weighted sum to is-\\nsue a ﬁnal prediction. Among many applications, ensemble\\nlearning has also been successfully used in (Dai et al. 2016)\\nand (Tekin et al. 2016). While these methods can learn com-\\nplex functions through the synergy of multiple weak learn-\\ners, they do not integrate the allocation of the training data\\nto the different learners (in both the bagging and boosting\\napproaches) as part of their loss minimization problems.\\nTherefore, these methods do not - in principle - learn a gran-\\nular predictive model that performs well uniformly over the\\nfeature space, but rather learn a predictive model that works\\nwell “on average”. Contrarily, ConﬁdentMatch jointly opti-\\nmizes the partitioning of the feature space together with the\\npredictive model associated with each partition, and hence,\\nit learns a reﬁned recipient-donor phenotypic compatibility\\ncharacterization in which the predictions are tailored to ﬁne\\nsegments of the recipient-donor feature space, leading to an\\noverall improved performance as compared to conventional\\nensemble methods. We will demonstrate the superiority of\\nConﬁdentMatch to ensemble learning algorithms in the “Re-\\nsults and Discussion” Section.\\nClustering algorithms\\nClustering is a natural approach for identifying pheno-\\ntypic characterizations by grouping “similar” patients (or\\nrecipient-donor pairs) into distinct clusters. Clustering algo-\\nrithms can be divided into two categories: unsupervised and\\nsupervised clustering. Unsupervised clustering algorithms,\\nsuch as the k-means algorithm (MacQueen 1967), utilize\\nthe feature space solely to learn the partitions that maximize\\nsome given objective, and hence they cannot address the or-\\ngan transplant prognosis problem since they do not consider\\nthe transplant outcomes (i.e. labels) in the clustering process.\\nSupervised clustering algorithms utilize both the feature\\nspace and the label space for constructing clusters (Eick\\n2004; Finley 2005); however, the predictive models assigned\\nto each partition of the feature space are limited to indica-\\ntor functions (an example for such algorithms is the regres-\\nsion tree (Strobl 2009)). For the organ transplant setting, the\\ncomplex interactions between the donor and recipient fea-\\ntures create highly complex patterns of recipient-donor com-\\npatibility (transplant success probabilities) that would ex-\\nhibit very high per-partition impurity under conventional su-\\npervised clustering or tree learning algorithms. This means\\nthat learning such complex medical concepts would face the\\ndilemma of exhibiting large over-ﬁtting errors when adopt-\\ning a large number of clusters (or very deep decision trees)\\nto resolve the per-partition impurity, and exhibiting a large\\nbias error when restricting the number of clusters (or restrict-\\ning the depth of decision trees). ConﬁdentMatch approaches\\nthis problem by providing a versatile framework for com-\\nplexity control where complex predictive models can be as-\\nsigned to every partition to reduce the per-partition bias er-\\nror, which enables learning complex functions with fewer\\npartitions and hence utilize the training data more efﬁciently.\\nTree-based algorithms\\nAs we will show in the next Section, ConﬁdentMatch can be\\ninterpreted as a nonparametric method for learning complex\\nfunctions by constructing a “tree of base learners”. While\\nConﬁdentMatch displays a tree structure, the algorithm is\\nnot a conventional “decision tree” (Quinlan 1986, DeSalvo\\nand Mohri 2016, Kim 2004) since the feature space parti-\\ntions do not correspond to different “decisions” or labels,\\nbut rather correspond to different “base learners” that intro-\\nduce complexity in the earlier levels of the tree, allowing for\\na more ﬂexible complexity control for the learned hypothe-\\nsis.\\nMethods\\nLet the D-dimensional recipient-donor feature space be de-\\nnoted as X; every instance in X corresponds to a recipient-\\n1648\\n\\ndonor pair with certain given characteristics. Denote the cor-\\nresponding label space which designates the success or fail-\\nure of an organ transplant for a given recipient-donor pair as\\nY; every label instance can be deﬁned as the speciﬁc event of\\ntransplant success or failure if Y = {0, 1}, or the probability\\nof the transplant’s success if Y = [0, 1]. Let T be a dataset\\nextracted from the EHR; we split the dataset T into two sep-\\narate sets: a training set S = {(xs\\n1, ys\\n1), ..., (xs\\nm, ys\\nm)} and a\\nvalidation set V = {(xv\\n1, yv\\n1), ..., (xv\\nn, yv\\nn)}, where S and V\\nare disjoint (S ∩V = ∅). Every entry in S and V comprise a\\nrecipient-donor feature pair and a transplant outcome.\\nThe goal of ConﬁdentMatch is to construct a predictive\\nmodel h ∈H, h : X →Y that maps recipient-donor pairs\\nto anticipated transplant outcomes; such a model has to be\\nlearned from the dataset T = {S, V}, and can be used for\\nout-of-sample recipient-donor pair in order to assess the risk\\nof a recipient’s transplant operation. The problem of learn-\\ning the predictive model h ∈H from the labeled dataset T\\nis a standard supervised learning problem (Shalev-Shwartz\\n2014).\\nThe expected loss of a predictive model h is deﬁned as\\nLF(h) = EF[l(h(x), y)] where l(h(x), y) is a general loss\\nfunction, and F is the joint recipient-donor feature-label dis-\\ntribution, which is unknown to the clinicians. The optimal\\npredictive model is deﬁned as h∗= arg minh∈H LF(h);\\nsince F is unknown, we cannot directly ﬁnd the optimal\\npredictive model, and hence we resort to minimizing the\\nempirical loss as measured over the training and validation\\nsets. The empirical loss for the training set is deﬁned as\\nLS(h) = 1\\nm\\n\\x02m\\ni=1 l(h(xs\\nj), ys\\nj), and it can be deﬁned simi-\\nlarly for the validation set..\\nNote that as pointed out in the previous section, the true\\nhypothesis is likely to be of a very complex structure as it\\nabstracts a complex medical concept, i.e. the interactions\\nbetween the recipient and donor features and their effect\\non the transplant outcomes. A poor initial choice for the\\nspace of possible models H, e.g. letting H be a hypothe-\\nsis class with a small VC dimension, may lead to a large\\nbias in the loss function of true hypothesis due to the classic\\nbias-complexity trade-off, and hence we need a more ver-\\nsatile learning framework for which the complexity of the\\npredictive model h adapts to the complexity of the underly-\\ning medical concept being learned.\\nConﬁdentMatch adopts a novel framework for crafting\\ncomplex predictive models out of simpler baseline models\\nby creating a phenotypic characterization of the recipient-\\ndonor feature space in which separate predictive models are\\nassigned to disjoint partitions of the feature space. That is,\\nConﬁdentMatch outputs a set of partitions that cover the en-\\ntire recipient-donor feature space, together with a set of pre-\\ndictive models, each tailored to a given partition, thereby\\nleading to an overall complex, granular predictive model.\\nFormally, we divide the recipient-donor feature space X into\\nk disjoint subsets, where k is to be determined based on the\\ngiven dataset, in such a way that for each subset, we can\\nhave a separate optimal predictive model that minimizes the\\noverall expected risk. We write {X1, ..., Xk} to denote a par-\\ntition of the feature space X, where all such partitions are en-\\nsured to be disjoint and cover X. The partition {X1, ..., Xk}\\ninduces a partition of the training set S and validation set\\nV, i.e. the training set is partitioned as {S1, ..., Sk}, where\\nSi = {(x, y) : (x, y) ∈S and x ∈Xi}. From now on, i in-\\ndicates the partition index, and j indicaes the instance index\\nGiven the above construct, the learning problem be-\\ncomes a problem of (jointly) ﬁnding the optimal partitioning\\n{X1, ..., Xk} of the recipient-donor feature space, together\\nwith the optimal predictive model hi ∈H associated with\\nevery partition i, i.e. assuming that we know the distribu-\\ntion F, the optimal predictive model is found by solving the\\nfollowing optimization problem\\nmin\\n{X1,...,Xk}\\n\\x02\\nmin\\nh1,...,hk∈H\\nk\\n\\x03\\ni=1\\nF(X ∈Xi) × EFi[l(hi(x), y)]\\n\\x04\\nsubject to\\nX =\\nk\\x05\\ni=1\\nXi, and Xi ∩Xl = ∅∀i ̸= l.\\n(1)\\nWe break down the problem in (1) into two nested optimiza-\\ntion problems; we ﬁrst focus on the solution of the inner op-\\ntimization problem and deﬁne its solution (for a given parti-\\ntioning {X1, ..., Xk}) as\\nd({X1, ..., Xk}) =\\nmin\\nh1,...,hk∈H\\nk\\n\\x02\\ni=1\\nF(X ∈Xi) × EFi[l(hi(x), y)].\\nNote that given a partition {X1, ..., Xk}, the solutions to the\\ninner optimizations are separable, i.e. the optimal predictor\\nof one partition can be determined independent of the choice\\nof the predictors for the other partitions. Hence, we can sim-\\nplify the inner optimization problem as follows.\\nmin\\nh1,...,hk∈H\\nk\\n\\x03\\ni=1\\nF(X ∈Xi) × EFi[l(hi(x), y)] =\\nk\\n\\x03\\ni=1\\nmin\\nhi∈H F(X ∈Xi) × EFi[l(hi(x), y)].\\nSince ConﬁdentMatch has no access to the true dis-\\ntribution F, the algorithm has to learn the partition-\\ning {X1, ..., Xk} and the corresponding predictive models\\n{hi}k\\ni=1 from the dataset T\\n= S ∪V) in such a way\\nthat it reaches a loss function that is as close as possi-\\nble to the true loss in (1). To achieve this, we construct\\na proxy for the objective in (1) by replacing the terms\\nF(X ∈Xi) and EFi[l(hi(x), y)], which depend on the un-\\nknown F, with their sample estimates |Vi|\\nn and LVi(hi) =\\n1\\n|Vi|\\n\\x02\\n(xv\\nj ,yv\\nj )∈Vi l(hi(xv\\nj ), yv\\nj ). Hence, empirical loss mini-\\nmization over the validation dataset V can be formulated as\\nfollows\\nmin\\nk,X1,...,Xk\\n\\x04\\nk\\n\\x03\\ni=1\\nmin\\nhi∈H\\n|Vi|\\nn ×\\n1\\n|Vi|\\n\\x03\\n(xv\\nj ,yv\\nj )∈Vi\\nl(hi(xv\\nj ), yv\\nj )\\n\\x05\\nsubject to\\nX =\\nk\\x06\\ni=1\\nXi, and Xi ∩Xl = ∅for ∀i ̸= l.\\n(2)\\n1649\\n\\nConﬁdentMatch constructs the hypothesis class H in (2),\\nfrom which we select a hypothesis (or a predictive model) hi\\nfor every partition i, by combining the outputs of a ﬁnite set\\nof M learners {A1, ..., AM}, each of which can learn a pre-\\ndictive model that belongs to some hypothesis class H(A).\\nThat is, for a ﬁxed partition Xi, using the corresponding\\ntraining set Si, the predictive model that is learned by the\\nlearning algorithm Ai for partition i is Ai(Si). Therefore,\\nthe set of all predictive models that can be learned by all\\nthe learning algorithms operating on data set Si is given as\\nˆH(Si) = {A1(Si), ..., AM(Si)}. ConﬁdentMatch decides\\nthe optimal partitioning and the optimal predictor for each\\npartition i that belongs to a set of learnable predictors ˆH(Si)\\nby minimizing the empirical loss with respect to the valida-\\ntion data set as follows\\nmin\\nk,X1,...,Xk\\n\\x02\\nk\\n\\x03\\ni=1\\nmin\\nhi∈ˆ\\nH(Si)\\n|Vi|\\nn ×\\n1\\n|Vi|\\n\\x03\\n(xv\\nj ,yv\\nj )∈Vi\\nl(hi(xv\\nj ), yv\\nj )\\n\\x04\\nsubject to\\nX =\\nk\\x05\\ni=1\\nXi,and Xi ∩Xl = ∅for ∀i ̸= l.\\n(3)\\nNote that the formulation in (3) does not account for the\\nout-of-sample error (or over-ﬁtting); to handle that, we re-\\nformulate problem (3) by replacing the objective function\\nwith a tight upper bound on the true loss that appropriately\\npenalizes over-ﬁtting. Deﬁne\\nˆd({X1, ..Xk}) =\\nk\\n\\x03\\ni=1\\nmin\\nhi∈ˆ\\nH(Si)\\n\\x04\\n1\\nn\\n\\x03\\n(xv\\nj ,yv\\nj )∈Vi\\nl(hi(xv\\nj ), yv\\nj )\\n\\x05\\n+ α\\n\\x07\\nk2 log M\\nn\\n,\\n(4)\\nwhere α ≥0 is a penalty parameter. The expression in (4)\\ncomprises the sample estimate of the objective and a penalty\\nterm α\\n\\x07\\nk log M\\nn/k\\nthat penalizes: the number of partitions\\nk, the average size of a partition n/k, and the number of\\npredictive model M from which we chose one model to as-\\nsign to a given partition. It can be shown that if the penalty\\nparameter α ≥\\n\\x07\\n1\\n2 +\\n1\\n2 log M log(\\n2\\n1 −(1 −δ)1/k), then\\nthe probability that d({X1, ..Xk}) is bounded above by\\nˆd({X1, ..Xk}) is greater than 1−δ, i.e. P(d({X1, ..., Xk}) <\\nˆd({X1, ..., Xk})) ≥1−δ (the proof can be found in the sup-\\nporting material). By using the upper bound ˆd({X1, ..Xk})\\nas the objective, the empirical loss minimization problem\\nbecomes\\nmin\\n{X1,...,Xk}\\n\\x04\\nk\\n\\x03\\ni=1\\nmin\\nhi∈\\nˆ\\nH(Si)\\n1\\nn\\n\\x03\\n(xv\\nj ,yv\\nj )∈Vi\\nl(h(xv\\nj ), yv\\nj )\\n\\x05\\n+ α\\n\\x07\\nk2 log M\\nn\\nsubject to\\nX =\\nk\\x06\\ni=1\\nXi, and Xi ∩Xl = ∅for ∀i ̸= l\\n(5)\\nSolving (5) is computationally intractable with exhaustive\\nevaluation because the number of possible partitions for n\\npoints is the Bell number (recursively, it can be deﬁned as\\nBn+1 = \\x02n\\nk=0 Bk (Wilf 1994)). Therefore, to address this\\nproblem, ConﬁdentMatch adopts an efﬁcient greedy algo-\\nrithm for approximating the solution to (5). As a ﬁrst step\\nto construct such an algorithm, we reformulate (5) by incor-\\nporating two more constraints. First, we restrict the parti-\\ntions of the recipient-donor feature space to be hypercubes.\\nA hypercubic partition of the feature space X is deﬁned as\\n{X1, .., Xk} where Xi = \\x08D\\nl=1[ail, bil], ail ≤bil, ail ∈\\nR∗, bil ∈R∗. (R∗is deﬁned as R ∪{−∞, ∞}). Second,\\nwe restrict the number of partitions to be γ ∈N. The op-\\ntimization problem in (5) with these additional constraints\\ncan be stated as follows.\\nmin\\n{X1,...,Xk}\\n\\x04\\nk\\n\\x03\\ni=1\\nmin\\nhi∈ˆ\\nH(Si)\\n1\\nn\\n\\x03\\n(xv\\nj ,yv\\nj )∈Vi\\nl(h(xv\\nj ), yv\\nj )\\n\\x05\\n+ α\\n\\x07\\nk2 log M\\nn\\nsubject to\\nXi =\\nD\\n\\t\\nl=1\\n[ail, bil], ail ≤bil, ail ∈R∗, bil ∈R∗\\nk ≤γ, where k ∈Z+\\nX =\\nk\\x06\\ni=1\\nXi, and Xi ∩Xl = ∅for ∀i ̸= l.\\n(6)\\nLet Opt(X) be the optimal partition that solves the opti-\\nmization problem in (6) (it can be solved by the exhaustive\\nevaluation method); we construct a greedy algorithm which\\niteratively solve the optimization problem (6) to achieve the\\napproximate solution for (5) as follows. We write the par-\\ntition that is generated when X is input to the optimization\\nproblem (6) as Opt(X) = {X1, X2, .., Xk} where k < γ.\\nWe apply the same procedure recursively on each Xi sepa-\\nrately up to the point where we do not expect to improve the\\nobjective function (i.e. the optimal k is 1). The ﬁnal partition\\nis the union of the partitions that are generated by applying\\nthis procedure recursively to each Xi. We write the ﬁnal par-\\ntition achieved by the greedy algorithm as { ˆ\\nX ∗\\n1 , ..., ˆ\\nX ∗\\nk }.\\nThe pseudo-code for ConﬁdentMatch is given in Fig. 1.\\nThe algorithm learns the recipient-donor compatibility pat-\\nterns in an off-line manner using the procedure described\\n1650\\n\\nOff-line Stage I: Discovering Optimal Partitions\\n(Greedy Approach)\\nInput: X\\nOutput: Greedy(X)\\n————————————————————–\\nFunction: Greedy(X)\\nY = ∅: Initialize the output as the empty set\\nC =Opt(X)\\nif |C| > 1 then\\nfor i = 1 to |C| do\\nY = Y ∪Greedy(Ci) where Ci is i-th subset of\\nC\\nend for\\nelse\\nY = Y ∪C\\nend if\\nFunction Output: Y = { ˆ\\nX ∗\\n1 , ..., ˆ\\nX ∗\\nk }\\n————————————————————–\\nOff-line Stage II: Learning Optimal Predictive\\nModels\\nInput: { ˆ\\nX ∗\\n1 , ..., ˆ\\nX ∗\\nk }, S, V, H\\nfor i = 1 to k do\\nˆh∗\\ni =\\nmin\\nhi∈ˆ\\nH(Si)\\n\\x02\\n(xv\\nj ,yv\\nj )∈Vi l(h(xv\\nj ), yv\\nj )\\nend for\\nOutput: {ˆh∗\\n1, ..., ˆh∗\\nk}\\n————————————————————–\\nOn-line Stage: Computing Compatibility Scores\\nInput: x, {ˆh∗\\n1, ..., ˆh∗\\nk}, { ˆ\\nX ∗\\n1 , ..., ˆ\\nX ∗\\nk }\\nˆh∗(x) = \\x02k\\ni=1 I(x ∈ˆ\\nX ∗\\ni ) × ˆh∗\\ni (x)\\nOutput: ˆh∗(x)\\nFigure 1: Pseudo-code for ConﬁdentMatch.\\nabove: it jointly optimizes the partitioning of the recipient-\\ndonor feature space (off-line stage I), and then optimizes\\nthe predictive model associated with each partition (off-line\\nstage II). Having learned the recipient-donor compatibilities,\\nthe algorithm operates in an on-line stage for new recipi-\\nents and donors by computing a compatibility score, i.e. the\\nprobability of transplant success, for a given recipient-donor\\npair. The compatibility score is displayed to the clinicians\\nand based on it, the clinicians/patients can make decisions\\non whether a transplant should be conducted, or whether the\\nrecipient should be matched with another donor.\\nIn what follows, we specify the computational complex-\\nity of ConﬁdentMatch. Let the complexity of the algorithm\\nAl(Si) for learning a predictive model using the dataset Si\\nbe Tl(|Si|, D). Based on this, it can be shown that the worst-\\ncase complexity for computing Opt(X) in the optimiza-\\ntion problem (6) is O(γγ+1nγDγ−1 \\x02M\\nl=1 Tl(n, D)), and\\nthe complexity of the greedy algorithm described in Fig. 1\\nis O(γγ+1nγ+1Dγ−1 \\x02M\\nl=1 Tl(n, D)) (proofs are provided\\nin the supporting material).\\nResults and Discussion\\nExperiments were conducted using the UNOS database for\\npatients who underwent a heart transplant over the years\\nfrom 1987 to 2015 (Cecka 1996). We use the “Thoracic\\nDATA” dataset in the UNOS database as our root dataset.\\nIn this dataset, all patients were followed-up until death, i.e.\\nthe post-transplant survival times for all patients are avail-\\nable in the dataset. Of the 148,512 patients in the “Thoracic\\nDATA” who underwent either heart or lung transplant, we\\nextract 60,516 patients who underwent a heart transplant.\\nOf the 60,516 patients who underwent a heart transplant,\\nwe exclude 3,800 patients (6.28%) who are still alive (right-\\ncensored), and we only use the 56,716 patients for whom we\\nhave the exact survival (lifetime) information.\\nFor each patient in the dataset, a total of 504 features are\\nprovided; these include a combination of both the patient’s\\nand the donor’s information. We discard 12 features that are\\nnormally obtained after the transplant. Of the remaining 492\\nfeatures, we extract 70 features for which we have less than\\n10% missing information in order to reduce the noise of im-\\nputation. We use the k-nearest-neighbor (KNN) imputation\\nmethod to impute the missing data (Hastie 1999).\\nWe compared the performance of ConﬁdentMatch in pre-\\ndicting the success of transplants with the following bench-\\nmark algorithms: logistic regression (Logit), Lasso regular-\\nized logistic regression (Lasso), decision tree (DTree), Ran-\\ndom Forest (RForest), AdaBoost (ABoost), and DeepBoost\\n(DBoost). We use the correlation feature selection (CFS)\\nmethod to discover the relevant features for the predic-\\ntive models of both ConﬁdentMatch and benchmarks (Hall\\n1999). The validation set calibrated all parameters of Con-\\nﬁdentMatch (α) and the benchmark algorithms. We adopt\\nthe following metric for quantifying the performance of the\\ndifferent algorithms. The transplant’s success probability is\\nquantiﬁed via the 3-year post-transplant survival rate (long-\\nterm survival rate). We say that an algorithm provides a pre-\\ndiction for the transplant’s success (probability of 3-year\\npost-transplant survival) for a certain recipient-donor pair\\nwith an accuracy level X% if the algorithm’s probability of\\ncorrect prediction is X% for that recipient-donor pair. Based\\non this deﬁnition, we deﬁne the gain of ConﬁdentMatch at\\nan accuracy level of X% as the number of recipient-donor\\ninstances in the testing dataset for which ConﬁdentMatch\\nprovides an accurate prediction of the transplant success,\\nwhereas the best-competing benchmark does not.\\nWe split the dataset into a training/validation set com-\\nprising the recipient-donor instances in which the recipient\\nunderwent the transplant before the year 2010 (former pa-\\ntients), and a testing set that comprises recipients who un-\\nderwent the transplant after 2010 (current patients). Of the\\n56,716 recipient-donor pairs, 37,677 pairs (66.43%) were\\nused for training, 9,419 pairs (16.61%) were used for val-\\nidating and 9,620 pairs (16.96%) were used for testing. We\\nvaried the accuracy level from 80% to 95% and evaluated\\nthe performance within this range of the accuracy levels. The\\nexecution time of the ConﬁdentMatch on this dataset is less\\nthan 5 hours on MATLAB R2015a with Intel i5 (1.5GHz)\\nprocessor with 4 GB RAM.\\nTable 2 and Fig 2 show that ConﬁdentMatch consis-\\ntently outperforms the benchmarks to predict the success of\\nheart transplant regarding 3-year mortality prediction. For\\ninstance, ConﬁdentMatch boosts the number of recipient-\\ndonor pairs who get 95% accurate predictions by 410 as\\n1651\\n\\nRelevant Features\\nRank\\nOverall Recipient-donor Population\\nGroup A\\nGroup B\\n1\\nVentilator Assist\\nVentilator Assist\\nECMO Assist\\n2\\nECMO Assist\\nECMO Assist\\nVentilator Assist\\n3\\nOther Life Support\\nOther Vent Support\\nDonor VDRL Result\\n4\\nOther Vent Support\\nOther Life Support\\nDays in State 1A\\n5\\nDays in State 1A\\nVAD support\\nPrior Cardiac Surgery\\n6\\nDiagnosis Code\\nDiagnosis Code\\nBlood Type Matching\\n7\\nDonor Status\\nDonor Status\\nDonor Blood Type (O)\\n8\\nTransplant Type\\nMalignancy\\nDonor Status\\n9\\nDonor HEP-P Antigen\\nTransplant Type\\nDonor HEP-B Antigen\\n10\\nPrevious MI History\\nPrevious Transplant\\nInhaled Assist\\nTable 1: Top 10 relevant features for the prediction of heart transplant success. (ECMO: Extracorporeal membrane oxygenation,\\nVDRL: Venereal disease research laboratory, VAD: Ventricular assist device, HEP: Hepatitis, MI: Myocardial infarction)\\nAccu.∗\\nCM∗\\nLASSO\\nRF∗\\nAB∗\\nDB∗\\nDTree\\n80%\\n9269\\n7721\\n9057\\n9063\\n9067\\n7893\\n85%\\n7798\\n7096\\n7605\\n7577\\n7614\\n7316\\n90%\\n6467\\n4431\\n6065\\n6076\\n6080\\n4526\\n95%\\n5489\\n2796\\n5065\\n5058\\n5079\\n3124\\nTable 2: Predictions for the success of heart transplant by\\ndifferent algorithms (∗Accu. = Accuracy, CM = Conﬁdent-\\nMatch, RF = random forest, AB = ABoost, DB = DBoost).\\nFigure 2: The number of recipient-donor pairs with conﬁ-\\ndent prediction based on ConﬁdentMatch as compared to\\nDBoost.\\ncompared to the best performing benchmark (DBoost); this\\nmeans that ConﬁdentMatch can allow an additional number\\nof 410 recipient-donor pairs to make better pre-transplant\\ndecisions such as whether or not the transplant should be\\nconducted, and whether the recipient should be matched\\nwith another donor.\\nThe performance gains achieved by ConﬁdentMatch can\\nbe attributed to the improved phenotypic characterization\\nof the recipient-donor pairs that the algorithm achieves by\\nFigure 3: Trade-offs associated with increasing the num-\\nber of partitions in ConﬁdentMatch (gain is with respect to\\nDBoost).\\nstratifying the recipient-donor feature space. The ﬁne and\\ngranular phenotypic characterization achieved by Conﬁdent-\\nMatch is restricted by the size of the training data; the more\\nrecipient-donor instances are available in the training set, the\\nlarger is the number of partitions that ConﬁdentMatch can\\nconstruct and cast a specialized predictive model. Fig 3 illus-\\ntrates the trade-off associated with increasing the complex-\\nity of ConﬁdentMatch’s predictive model by increasing the\\nnumber of partitions; if the number of partitions increases,\\nthe gain of ConﬁdentMatch also increases as it copes with\\nthe underlying complexity of the recipient-donor compati-\\nbility patterns, until a certain number of partitions when the\\ngain starts to decrease due to over-ﬁtting.\\nConﬁdentMatch does not only improve the quality of\\nprognosis, but can also draw clinical insights on the patterns\\nof recipient-donor compatibility. To illustrate this, we list the\\nﬁrst two partitions through which ConﬁdentMatch stratiﬁes\\nthe recipient feature space: ConﬁdentMatch forms two re-\\ncipient groups, group A comprises patients whose length of\\nstay in status 1A (urgent transplant wait-list) is shorter than\\n10 days, whereas group B comprises the remaining patients.\\n1652\\n\\nTable 1 lists the most relevant features that are predictive\\nof the transplant outcome for each group. It can be seen\\nthat group B patients are more sensitive to the donor char-\\nacteristics; the donor’s Venereal disease research laboratory\\n(VDRL) result and blood type are relevant to the transplant\\noutcome, whereas group A patients appear to be less sensi-\\ntive to these features. Thus, the more the patient waits in sta-\\ntus 1A, the more it becomes essential to consider the extent\\nof her compatibility with the donors. As more training data\\nbecomes available, ConﬁdentMatch can reveal ﬁner parti-\\ntions and identify the relevant features for more granular re-\\ncipient subgroups.\\nConclusions\\nOrgan transplants for patients with end-stage diseases car-\\nries the risk of various serious post-operative complica-\\ntions, the pre-operative anticipation of the transplant out-\\ncome depends on the compatibility between the donor and\\nthe recipient. In this paper, we have developed Conﬁdent-\\nMatch, a data-driven system that learns complex recipient-\\ndonor compatibility patterns from the outcomes of previous\\ntransplants. ConﬁdentMatch captures the complexity of such\\ncompatibility patterns by optimally dividing the recipient-\\ndonor feature space into clusters and assigning different op-\\ntimal predictive models to each cluster, thereby ensuring\\nthat predictions are “personalized” and tailored to individual\\ncharacteristics of both the donors and the recipients. Experi-\\nments conducted on a public heart transplant dataset demon-\\nstrate the superiority of ConﬁdentMatch to other competing\\nbenchmark algorithms.\\nAcknowledgments\\nThe authors would like to thank the reviewers for their help-\\nful comments. The research presented in this paper was sup-\\nported by Natural Science Foundation (NSF) under Grant\\nNumber NSF IIP1533983 and NSF ECCS1462245.\\nReferences\\nAllen J. G., Weiss E. S., Arnaoutakis G. J., Russell S. D.,\\nBaumgartner W. A., Conte J. V., and Shah A. S. 2010. The\\nimpact of race on survival after heart transplantation: an\\nanalysis of more than 20,000 patients. The Annals of tho-\\nracic surgery 89:1956–1963.\\nArnaoutakis G. J., George T. J., Allen J. G., Russell S. D.,\\nShah A. S., Conte J. V. and Weiss E. S. 2012 Institutional\\nvolume and the effect of recipient risk on short-term mor-\\ntality after orthotopic heart transplant. Journal on Thoracic\\nCardiovascular Surgery 143: 157–167.\\nCecka J. M. 1996. The unos scientic renal transplant reg-\\nistryten years of kidney transplants. Clinical transplants\\n114.\\nCollins F. S., Varmus H. 2015. A new initiative on precision\\nmedicine. New England Journal of Medicine. 372: 793–795.\\nDai, P., Gwadry-Sridhar, F., Bauer, M., Borrie, M. 2016.\\nBagging Ensembles for the Diagnosis and Prognostication\\nof Alzheimer’s Disease. Thirtieth AAAI Conference on Arti-\\nﬁcial Intelligence.\\nDeSalvo G., Mehryar, M. 2014. Random Composite Forests.\\nThirtieth AAAI Conference on Artiﬁcial Intelligence.\\nEick, C. F., Zeidat, N., Zhao, Z. 2004. Supervised clustering-\\nalgorithms and benets. In IEEE ICTAI 774–776.\\nEnciso, J. S. et al. 2014. Effect of Peripheral Vascular Dis-\\nease on Mortality in Cardiac Transplant Recipients (from the\\nUnited Network of Organ Sharing Database). The American\\njournal of cardiology 114: 1111–1115.\\nFinley, T., Joachims, T. 2005. Supervised clustering with\\nsupport vector machines. In ICML 217–224.\\nFreund, Y., Schapire, R. E. 1997. A decision-theoretic gener-\\nalization of on-line learning and an application to boosting.\\nJournal of computer and system sciences 55: 119–139.\\nFriedman, J., Hastie, T., Tibshirani, R. et al. 2000. Additive\\nlogistic regression: a statistical view of boosting. The annals\\nof statistics 28: 337–407.\\nGale, D., Shapley, L. S. 1962. College Admissions and the\\nStability of Marriage. American Mathematical Monthly 69:\\n9–14.\\nHall, M. A. 1999. Correlation-based feature selection for\\nmachine learning. PhD thesis, The University of Waikato.\\nHastie, T., Tibshirani, R., Sherlock, G., Eisen, M., Brown,\\nP., and Botstein D. 1999, Imputing missing data for gene\\nexpression arrays.\\nHuynh, T. N., Kleerup, E. C., Raj, P. P., and Wenger, N. S.\\n2014. The opportunity cost of futile treatment in the inten-\\nsive care unit. Critical Care Medicine, 42: 1977–1982.\\nJawitz O. K., Jawitz N., Yuh D. D., Bonde P. 2013. Impact\\nof ABO compatibility on outcomes after heart transplanta-\\ntion in a national cohort during the past decade. Journal on\\nThoracic Cardiovascular Surgery 146: 1239-1245.\\nJayarajan S. N., Taghavi S., Komaroff E., Mangi A. A.\\n2013. Impact of low donor to recipient weight ratios on car-\\ndiac transplantation. Journal on Thoracic Cardiovascular\\nSurgery 146: 1538–1543.\\nKause, J. et al. 2004. A comparison of antecedents to car-\\ndiac arrests, deaths and emergency intensive care admissions\\nin Australia and New Zealand, and the United Kingdomthe\\nACADEMIA study. Resuscitation 62: 275–282.\\nKim, D. 2004. Structural risk minimization on decision trees\\nusing an evolutionary multiobjective optimization. In Euro-\\npean Conference on Genetic Programming 338–348.\\nKuznetsov, V., Mohri, M., Syed, U. 2014. Multi-class deep\\nboosting. In NIPS, 2501–2509.\\nLiaw, A., Wiener, M. 2002. Classication and regression by\\nrandom-forest. R news 3: 18–22.\\nMancini D., Lietz K. 2010. Selection of cardiac transplanta-\\ntion candidates in 2010. Circulation 122: 173–83.\\nMacQueen, J. et al. 1967. Some methods for classication and\\nanalysis of multivariate observations. In Proceedings of the\\nfth Berkeley symposium on mathematical statistics and prob-\\nability 1: 281–297.\\nNilsson J., Ohlsson M., Hglund P., Ekmehag B., Koul B.,\\nand Andersson B. 2015. The International Heart Transplant\\n1653\\n\\nSurvival Algorithm (IHTSA): a new model to improve organ\\nsharing and survival. PLoS One.\\nNwakanma L. U., Williams J. A., Weiss E. S., Russell S. D.,\\nBaumgartner W. A., Conte J. V. 2007. Inﬂuence of pretrans-\\nplant panel-reactive antibody on outcomes in 8,160 heart\\ntransplant recipients in recent era. The Annals of Thoracic\\nSurgery 84: 1556–1562.\\nQuinlan, J. Ross., 1986. Induction of decision trees. Ma-\\nchine learning 1: 81-106.\\nRoth, A. E., Sonmez, T., Unver, M. U. 2003. Kidney ex-\\nchange. National Bureau of Economic Research.\\nRusso M. J., et al. 2006. Survival after heart transplantation\\nis not diminished among recipients with uncomplicated dia-\\nbetes mellitus: an analysis of the United Network of Organ\\nSharing database. Circulation. 114: 2280–2287.\\nShah M. R., Starling R. C., Schwartz L. L., Mehra M. R.\\n2012. Heart transplantation research in the next decade–a\\ngoal to achieving evidence-based outcomes: National Heart,\\nLung, And Blood Institute Working Group. J Am Coll Car-\\ndiol, 59: 1263–1269.\\nShalev-Shwartz, S., Ben-David, S. 2014. Understanding ma-\\nchine learning: From theory to algorithms. Cambridge Uni-\\nversity Press.\\nStrobl, C., Malley, J., Tutz, G. 2009. An introduction to\\nrecur-sive partitioning: rationale, application, and character-\\nistics of classication and regression trees, bagging, and ran-\\ndom forests. Psychological methods 14: 323.\\nTaghavi S., Jayarajan S. N., Wilson L. M., Komaroff E., Tes-\\ntani J. M., Mangi A. A. 2013. Cardiac transplantation can be\\nsafely performed using selected diabetic donors. Journal on\\nThoracic Cardiovascular Surgery 146: 442–447.\\nTekin, C., Yoon, J., van der Schaar, M. 2016. Adaptive en-\\nsemble learning with conﬁdence bounds for personalized di-\\nagnosis. AAAI Workshop on Expanding the Boundaries of\\nHealth Informatics using AI (HIAI16): Making Proactive,\\nPersonalized, and Participatory Medicine A Reality.\\nWozniak C. J., Stehlik J., Baird B. C., et al. 2014. Ventricular\\nAssist Devices or Inotropic Agents in Status 1A Patients?\\nSurvival Analysis of the United Network of Organ Sharing\\nDatabase. The Annals of thoracic surgery 97: 1364–1372.\\nWilf. H. S., 1994. Generatingfunctionology. Elseviers 2: 23.\\n1654\\n'}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/Personalized Donor-Recipient Matching for Organ Transplantation.pdf', 'text': 'Personalized Donor-Recipient\\nMatching for Organ Transplantation\\nJinsung Yoon,1 Ahmed M. Alaa,1 Martin Cadeiras,2 Mihaela van der Schaar1\\n1 Department of Electrical Engineering, University of California, Los Angeles (UCLA), CA, 90095, USA\\n2 David Geffen School of Medicine, University of California, Los Angeles (UCLA), CA, 90095, USA\\njsyoon0823@g.ucla.edu, ahmedmalaa@g.ucla.edu, MCardeiras@mednet.ucla.edu, mihaela@ee.ucla.edu\\nAbstract\\nOrgan transplants can improve the life expectancy and qual-\\nity of life for the recipient but carry the risk of serious post-\\noperative complications, such as septic shock and organ re-\\njection. The probability of a successful transplant depends in\\na very subtle fashion on compatibility between the donor and\\nthe recipient - but current medical practice is short of do-\\nmain knowledge regarding the complex nature of recipient-\\ndonor compatibility. Hence a data-driven approach for learn-\\ning compatibility has the potential for signiﬁcant improve-\\nments in match quality. This paper proposes a novel system\\n(ConﬁdentMatch) that is trained using data from electronic\\nhealth records. ConﬁdentMatch predicts the success of an or-\\ngan transplant (in terms of the 3-year survival rates) on the\\nbasis of clinical and demographic traits of the donor and\\nrecipient. ConﬁdentMatch captures the heterogeneity of the\\ndonor and recipient traits by optimally dividing the feature\\nspace into clusters and constructing different optimal predic-\\ntive models to each cluster. The system controls the complex-\\nity of the learned predictive model in a way that allows for\\nassuring more granular and accurate predictions for a larger\\nnumber of potential recipient-donor pairs, thereby ensuring\\nthat predictions are “personalized” and tailored to individ-\\nual characteristics to the ﬁnest possible granularity. Experi-\\nments conducted on the UNOS heart transplant dataset show\\nthe superiority of the prognostic value of ConﬁdentMatch to\\nother competing benchmarks; ConﬁdentMatch can provide\\npredictions of success with 95% accuracy for 5,489 patients\\nof a total population of 9,620 patients, which corresponds to\\n410 more patients than the most competitive benchmark al-\\ngorithm (DeepBoost).\\nIntroduction\\nOrgan transplantation is the therapy of choice for patients\\nwith end-stage diseases who are refractory to medical ther-\\napies (Shah 2012). Even though organ transplantation can\\nincrease the life expectancy and quality of life for the re-\\ncipient, the operation can entail various complications, in-\\ncluding infection, acute and chronic rejection and malig-\\nnancy (Huynh 2014). The pre-operative anticipation of the\\nrisk associated with organ transplantation is a regular task\\nthat transplant centers perform in order to determine which\\npatients would beneﬁt from transplantation and accurately\\nCopyright c⃝2017, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.\\nidentify these for whom the risk of transplantation is too\\nhigh and would therefore provide no survival beneﬁts. Such\\na risk assessment task is quite complicated for that post-\\noperative patient survival depends on different types of risk\\nfactors: recipient-related factors (e.g. cardiovascular dis-\\nease severity of heart recipients (Wozniak 2014; Nwakanma\\n2007; Russo 2006; Silva 2016)), recipient-donor match-\\ning factors (e.g. weight ratio and human leukocyte antigen\\n(HLA) (Jayarajan 2013), blood group compatibility (Jawitz\\n2013), race (Allen 2010), etc), and donor-related factors\\n(e.g. diabetes (Arnaoutakis 2012; Taghavi 2013)). The in-\\nteractions among all these risk factors make the progno-\\nsis problem for the organ transplant outcomes highly com-\\nplex; the National Heart, Lung and Blood Institute (NHLBI)\\nworking group suggests resolving this problem by enhanc-\\ning the phenotypic compatibility characterization of the\\npre-transplant recipient-donor population (Mancini 2010;\\nCollins 2015; Shah 2012).\\nIn the light of the above, we seek an enhanced pheno-\\ntypic characterization for the compatibility of patient-donor\\npairs via a precision medicine approach (Collins 2015) in\\nwhich we construct personalized predictive models that are\\ntailored to the individual traits of both the donor and the re-\\ncipient to the ﬁnest possible granularity. The advent of elec-\\ntronic health records (EHR) inspire a data-driven approach\\nfor constructing such predictive models in which the com-\\nplex recipient-donor compatibility patterns are discovered\\nfrom observational data. To that end, we develop Conﬁdent-\\nMatch: an automated system that learns the recipient-donor\\ncompatibility patterns from the EHR data in terms of the\\nprobability of transplant success for given recipient-donor\\npairs. The clinicians can utilize ConﬁdentMatch as a prog-\\nnostic tool for managing organ transplantation selection de-\\ncisions in which information about the donor and recipient\\nare fed to the system, and the output comes as the probabil-\\nity of the transplant’s success. The system can also be used\\nin conjunction with any matching algorithms, such as the\\nNobel prize-winning algorithm of Shapley and Roth (Shap-\\nley 1962; Roth 2003). More speciﬁcally, a set of patients is\\nmatched to a set of donors using the compatibility score that\\nConﬁdentMatch computes as an input to the matching algo-\\nrithm.\\nIn order to learn the highly complex recipient-donor com-\\npatibility patterns, ConﬁdentMatch adopts a novel learn-\\nProceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)\\n1647\\n\\ning framework in which the recipient-donor feature space\\nis partitioned into disjoint subsets, and a separate predic-\\ntive model (learner) is assigned to each partition. Such a\\nlearning approach gives rise to a highly complex overall\\npredictive model for mapping recipient-donor features to\\ntransplant success probabilities. Over-ﬁtting is controlled by\\npenalizing the number of partitions in the recipient-donor\\nfeature space and the complexity of the learners assigned\\nto the different partitions. Unlike existing meta-learning al-\\ngorithms, ConﬁdentMatch solves an optimization problem\\nthrough which it jointly determines how to partition the\\nrecipient-donor feature space, and what predictive model to\\nassign to each partition. Since such an optimization prob-\\nlem is intractable, we propose an efﬁcient greedy algorithm\\nthat proceeds iteratively by ﬁrst ﬁxing the number of par-\\ntitions in the recipient-donor feature space, optimizing the\\npredictive models assigned to each partition, and then fur-\\nther stratifying each partition and re-assigning more “spe-\\ncialized” predictive models to the new, ﬁner partitions. The\\nalgorithm stops stratifying the recipient-donor feature space\\nwhen further stratiﬁcation would lead to new partitions with\\nno enough per-partition training data for learning ﬁner pre-\\ndictive models.\\nExperiments conducted on the United Network for Or-\\ngan Sharing (UNOS) heart transplant dataset (Cecka 1996)\\nshow that ConﬁdentMatch can provide predictions of suc-\\ncess with 95% accuracy for 5,489 patients of a total popula-\\ntion of 9,620 patients – 410 more patients than for the best\\nstate-of-the-art machine learning algorithm (DeepBoost).\\nRelated Work\\nWe identify three broad categories of learning algorithms\\nthat are capable of combining multiple predictive models\\nor stratifying the feature space into ﬁne clusters: ensemble\\nlearning algorithms, clustering algorithms and tree-based al-\\ngorithms. We compare ConﬁdentMatch with these methods\\nhereunder.\\nEnsemble learning algorithms\\nMethods based on ensemble learning, such as Random For-\\nest (Liaw 2002), LogitBoost (Friedman et al. 2000), Adap-\\ntive Boosting (Freund 1997) and DeepBoost (Kuznetsov\\n2014), operate by allocating different sets of training data\\nto different weak learners, and then aggregating the predic-\\ntions of these weak learners through a weighted sum to is-\\nsue a ﬁnal prediction. Among many applications, ensemble\\nlearning has also been successfully used in (Dai et al. 2016)\\nand (Tekin et al. 2016). While these methods can learn com-\\nplex functions through the synergy of multiple weak learn-\\ners, they do not integrate the allocation of the training data\\nto the different learners (in both the bagging and boosting\\napproaches) as part of their loss minimization problems.\\nTherefore, these methods do not - in principle - learn a gran-\\nular predictive model that performs well uniformly over the\\nfeature space, but rather learn a predictive model that works\\nwell “on average”. Contrarily, ConﬁdentMatch jointly opti-\\nmizes the partitioning of the feature space together with the\\npredictive model associated with each partition, and hence,\\nit learns a reﬁned recipient-donor phenotypic compatibility\\ncharacterization in which the predictions are tailored to ﬁne\\nsegments of the recipient-donor feature space, leading to an\\noverall improved performance as compared to conventional\\nensemble methods. We will demonstrate the superiority of\\nConﬁdentMatch to ensemble learning algorithms in the “Re-\\nsults and Discussion” Section.\\nClustering algorithms\\nClustering is a natural approach for identifying pheno-\\ntypic characterizations by grouping “similar” patients (or\\nrecipient-donor pairs) into distinct clusters. Clustering algo-\\nrithms can be divided into two categories: unsupervised and\\nsupervised clustering. Unsupervised clustering algorithms,\\nsuch as the k-means algorithm (MacQueen 1967), utilize\\nthe feature space solely to learn the partitions that maximize\\nsome given objective, and hence they cannot address the or-\\ngan transplant prognosis problem since they do not consider\\nthe transplant outcomes (i.e. labels) in the clustering process.\\nSupervised clustering algorithms utilize both the feature\\nspace and the label space for constructing clusters (Eick\\n2004; Finley 2005); however, the predictive models assigned\\nto each partition of the feature space are limited to indica-\\ntor functions (an example for such algorithms is the regres-\\nsion tree (Strobl 2009)). For the organ transplant setting, the\\ncomplex interactions between the donor and recipient fea-\\ntures create highly complex patterns of recipient-donor com-\\npatibility (transplant success probabilities) that would ex-\\nhibit very high per-partition impurity under conventional su-\\npervised clustering or tree learning algorithms. This means\\nthat learning such complex medical concepts would face the\\ndilemma of exhibiting large over-ﬁtting errors when adopt-\\ning a large number of clusters (or very deep decision trees)\\nto resolve the per-partition impurity, and exhibiting a large\\nbias error when restricting the number of clusters (or restrict-\\ning the depth of decision trees). ConﬁdentMatch approaches\\nthis problem by providing a versatile framework for com-\\nplexity control where complex predictive models can be as-\\nsigned to every partition to reduce the per-partition bias er-\\nror, which enables learning complex functions with fewer\\npartitions and hence utilize the training data more efﬁciently.\\nTree-based algorithms\\nAs we will show in the next Section, ConﬁdentMatch can be\\ninterpreted as a nonparametric method for learning complex\\nfunctions by constructing a “tree of base learners”. While\\nConﬁdentMatch displays a tree structure, the algorithm is\\nnot a conventional “decision tree” (Quinlan 1986, DeSalvo\\nand Mohri 2016, Kim 2004) since the feature space parti-\\ntions do not correspond to different “decisions” or labels,\\nbut rather correspond to different “base learners” that intro-\\nduce complexity in the earlier levels of the tree, allowing for\\na more ﬂexible complexity control for the learned hypothe-\\nsis.\\nMethods\\nLet the D-dimensional recipient-donor feature space be de-\\nnoted as X; every instance in X corresponds to a recipient-\\n1648\\n\\ndonor pair with certain given characteristics. Denote the cor-\\nresponding label space which designates the success or fail-\\nure of an organ transplant for a given recipient-donor pair as\\nY; every label instance can be deﬁned as the speciﬁc event of\\ntransplant success or failure if Y = {0, 1}, or the probability\\nof the transplant’s success if Y = [0, 1]. Let T be a dataset\\nextracted from the EHR; we split the dataset T into two sep-\\narate sets: a training set S = {(xs\\n1, ys\\n1), ..., (xs\\nm, ys\\nm)} and a\\nvalidation set V = {(xv\\n1, yv\\n1), ..., (xv\\nn, yv\\nn)}, where S and V\\nare disjoint (S ∩V = ∅). Every entry in S and V comprise a\\nrecipient-donor feature pair and a transplant outcome.\\nThe goal of ConﬁdentMatch is to construct a predictive\\nmodel h ∈H, h : X →Y that maps recipient-donor pairs\\nto anticipated transplant outcomes; such a model has to be\\nlearned from the dataset T = {S, V}, and can be used for\\nout-of-sample recipient-donor pair in order to assess the risk\\nof a recipient’s transplant operation. The problem of learn-\\ning the predictive model h ∈H from the labeled dataset T\\nis a standard supervised learning problem (Shalev-Shwartz\\n2014).\\nThe expected loss of a predictive model h is deﬁned as\\nLF(h) = EF[l(h(x), y)] where l(h(x), y) is a general loss\\nfunction, and F is the joint recipient-donor feature-label dis-\\ntribution, which is unknown to the clinicians. The optimal\\npredictive model is deﬁned as h∗= arg minh∈H LF(h);\\nsince F is unknown, we cannot directly ﬁnd the optimal\\npredictive model, and hence we resort to minimizing the\\nempirical loss as measured over the training and validation\\nsets. The empirical loss for the training set is deﬁned as\\nLS(h) = 1\\nm\\n\\x02m\\ni=1 l(h(xs\\nj), ys\\nj), and it can be deﬁned simi-\\nlarly for the validation set..\\nNote that as pointed out in the previous section, the true\\nhypothesis is likely to be of a very complex structure as it\\nabstracts a complex medical concept, i.e. the interactions\\nbetween the recipient and donor features and their effect\\non the transplant outcomes. A poor initial choice for the\\nspace of possible models H, e.g. letting H be a hypothe-\\nsis class with a small VC dimension, may lead to a large\\nbias in the loss function of true hypothesis due to the classic\\nbias-complexity trade-off, and hence we need a more ver-\\nsatile learning framework for which the complexity of the\\npredictive model h adapts to the complexity of the underly-\\ning medical concept being learned.\\nConﬁdentMatch adopts a novel framework for crafting\\ncomplex predictive models out of simpler baseline models\\nby creating a phenotypic characterization of the recipient-\\ndonor feature space in which separate predictive models are\\nassigned to disjoint partitions of the feature space. That is,\\nConﬁdentMatch outputs a set of partitions that cover the en-\\ntire recipient-donor feature space, together with a set of pre-\\ndictive models, each tailored to a given partition, thereby\\nleading to an overall complex, granular predictive model.\\nFormally, we divide the recipient-donor feature space X into\\nk disjoint subsets, where k is to be determined based on the\\ngiven dataset, in such a way that for each subset, we can\\nhave a separate optimal predictive model that minimizes the\\noverall expected risk. We write {X1, ..., Xk} to denote a par-\\ntition of the feature space X, where all such partitions are en-\\nsured to be disjoint and cover X. The partition {X1, ..., Xk}\\ninduces a partition of the training set S and validation set\\nV, i.e. the training set is partitioned as {S1, ..., Sk}, where\\nSi = {(x, y) : (x, y) ∈S and x ∈Xi}. From now on, i in-\\ndicates the partition index, and j indicaes the instance index\\nGiven the above construct, the learning problem be-\\ncomes a problem of (jointly) ﬁnding the optimal partitioning\\n{X1, ..., Xk} of the recipient-donor feature space, together\\nwith the optimal predictive model hi ∈H associated with\\nevery partition i, i.e. assuming that we know the distribu-\\ntion F, the optimal predictive model is found by solving the\\nfollowing optimization problem\\nmin\\n{X1,...,Xk}\\n\\x02\\nmin\\nh1,...,hk∈H\\nk\\n\\x03\\ni=1\\nF(X ∈Xi) × EFi[l(hi(x), y)]\\n\\x04\\nsubject to\\nX =\\nk\\x05\\ni=1\\nXi, and Xi ∩Xl = ∅∀i ̸= l.\\n(1)\\nWe break down the problem in (1) into two nested optimiza-\\ntion problems; we ﬁrst focus on the solution of the inner op-\\ntimization problem and deﬁne its solution (for a given parti-\\ntioning {X1, ..., Xk}) as\\nd({X1, ..., Xk}) =\\nmin\\nh1,...,hk∈H\\nk\\n\\x02\\ni=1\\nF(X ∈Xi) × EFi[l(hi(x), y)].\\nNote that given a partition {X1, ..., Xk}, the solutions to the\\ninner optimizations are separable, i.e. the optimal predictor\\nof one partition can be determined independent of the choice\\nof the predictors for the other partitions. Hence, we can sim-\\nplify the inner optimization problem as follows.\\nmin\\nh1,...,hk∈H\\nk\\n\\x03\\ni=1\\nF(X ∈Xi) × EFi[l(hi(x), y)] =\\nk\\n\\x03\\ni=1\\nmin\\nhi∈H F(X ∈Xi) × EFi[l(hi(x), y)].\\nSince ConﬁdentMatch has no access to the true dis-\\ntribution F, the algorithm has to learn the partition-\\ning {X1, ..., Xk} and the corresponding predictive models\\n{hi}k\\ni=1 from the dataset T\\n= S ∪V) in such a way\\nthat it reaches a loss function that is as close as possi-\\nble to the true loss in (1). To achieve this, we construct\\na proxy for the objective in (1) by replacing the terms\\nF(X ∈Xi) and EFi[l(hi(x), y)], which depend on the un-\\nknown F, with their sample estimates |Vi|\\nn and LVi(hi) =\\n1\\n|Vi|\\n\\x02\\n(xv\\nj ,yv\\nj )∈Vi l(hi(xv\\nj ), yv\\nj ). Hence, empirical loss mini-\\nmization over the validation dataset V can be formulated as\\nfollows\\nmin\\nk,X1,...,Xk\\n\\x04\\nk\\n\\x03\\ni=1\\nmin\\nhi∈H\\n|Vi|\\nn ×\\n1\\n|Vi|\\n\\x03\\n(xv\\nj ,yv\\nj )∈Vi\\nl(hi(xv\\nj ), yv\\nj )\\n\\x05\\nsubject to\\nX =\\nk\\x06\\ni=1\\nXi, and Xi ∩Xl = ∅for ∀i ̸= l.\\n(2)\\n1649\\n\\nConﬁdentMatch constructs the hypothesis class H in (2),\\nfrom which we select a hypothesis (or a predictive model) hi\\nfor every partition i, by combining the outputs of a ﬁnite set\\nof M learners {A1, ..., AM}, each of which can learn a pre-\\ndictive model that belongs to some hypothesis class H(A).\\nThat is, for a ﬁxed partition Xi, using the corresponding\\ntraining set Si, the predictive model that is learned by the\\nlearning algorithm Ai for partition i is Ai(Si). Therefore,\\nthe set of all predictive models that can be learned by all\\nthe learning algorithms operating on data set Si is given as\\nˆH(Si) = {A1(Si), ..., AM(Si)}. ConﬁdentMatch decides\\nthe optimal partitioning and the optimal predictor for each\\npartition i that belongs to a set of learnable predictors ˆH(Si)\\nby minimizing the empirical loss with respect to the valida-\\ntion data set as follows\\nmin\\nk,X1,...,Xk\\n\\x02\\nk\\n\\x03\\ni=1\\nmin\\nhi∈ˆ\\nH(Si)\\n|Vi|\\nn ×\\n1\\n|Vi|\\n\\x03\\n(xv\\nj ,yv\\nj )∈Vi\\nl(hi(xv\\nj ), yv\\nj )\\n\\x04\\nsubject to\\nX =\\nk\\x05\\ni=1\\nXi,and Xi ∩Xl = ∅for ∀i ̸= l.\\n(3)\\nNote that the formulation in (3) does not account for the\\nout-of-sample error (or over-ﬁtting); to handle that, we re-\\nformulate problem (3) by replacing the objective function\\nwith a tight upper bound on the true loss that appropriately\\npenalizes over-ﬁtting. Deﬁne\\nˆd({X1, ..Xk}) =\\nk\\n\\x03\\ni=1\\nmin\\nhi∈ˆ\\nH(Si)\\n\\x04\\n1\\nn\\n\\x03\\n(xv\\nj ,yv\\nj )∈Vi\\nl(hi(xv\\nj ), yv\\nj )\\n\\x05\\n+ α\\n\\x07\\nk2 log M\\nn\\n,\\n(4)\\nwhere α ≥0 is a penalty parameter. The expression in (4)\\ncomprises the sample estimate of the objective and a penalty\\nterm α\\n\\x07\\nk log M\\nn/k\\nthat penalizes: the number of partitions\\nk, the average size of a partition n/k, and the number of\\npredictive model M from which we chose one model to as-\\nsign to a given partition. It can be shown that if the penalty\\nparameter α ≥\\n\\x07\\n1\\n2 +\\n1\\n2 log M log(\\n2\\n1 −(1 −δ)1/k), then\\nthe probability that d({X1, ..Xk}) is bounded above by\\nˆd({X1, ..Xk}) is greater than 1−δ, i.e. P(d({X1, ..., Xk}) <\\nˆd({X1, ..., Xk})) ≥1−δ (the proof can be found in the sup-\\nporting material). By using the upper bound ˆd({X1, ..Xk})\\nas the objective, the empirical loss minimization problem\\nbecomes\\nmin\\n{X1,...,Xk}\\n\\x04\\nk\\n\\x03\\ni=1\\nmin\\nhi∈\\nˆ\\nH(Si)\\n1\\nn\\n\\x03\\n(xv\\nj ,yv\\nj )∈Vi\\nl(h(xv\\nj ), yv\\nj )\\n\\x05\\n+ α\\n\\x07\\nk2 log M\\nn\\nsubject to\\nX =\\nk\\x06\\ni=1\\nXi, and Xi ∩Xl = ∅for ∀i ̸= l\\n(5)\\nSolving (5) is computationally intractable with exhaustive\\nevaluation because the number of possible partitions for n\\npoints is the Bell number (recursively, it can be deﬁned as\\nBn+1 = \\x02n\\nk=0 Bk (Wilf 1994)). Therefore, to address this\\nproblem, ConﬁdentMatch adopts an efﬁcient greedy algo-\\nrithm for approximating the solution to (5). As a ﬁrst step\\nto construct such an algorithm, we reformulate (5) by incor-\\nporating two more constraints. First, we restrict the parti-\\ntions of the recipient-donor feature space to be hypercubes.\\nA hypercubic partition of the feature space X is deﬁned as\\n{X1, .., Xk} where Xi = \\x08D\\nl=1[ail, bil], ail ≤bil, ail ∈\\nR∗, bil ∈R∗. (R∗is deﬁned as R ∪{−∞, ∞}). Second,\\nwe restrict the number of partitions to be γ ∈N. The op-\\ntimization problem in (5) with these additional constraints\\ncan be stated as follows.\\nmin\\n{X1,...,Xk}\\n\\x04\\nk\\n\\x03\\ni=1\\nmin\\nhi∈ˆ\\nH(Si)\\n1\\nn\\n\\x03\\n(xv\\nj ,yv\\nj )∈Vi\\nl(h(xv\\nj ), yv\\nj )\\n\\x05\\n+ α\\n\\x07\\nk2 log M\\nn\\nsubject to\\nXi =\\nD\\n\\t\\nl=1\\n[ail, bil], ail ≤bil, ail ∈R∗, bil ∈R∗\\nk ≤γ, where k ∈Z+\\nX =\\nk\\x06\\ni=1\\nXi, and Xi ∩Xl = ∅for ∀i ̸= l.\\n(6)\\nLet Opt(X) be the optimal partition that solves the opti-\\nmization problem in (6) (it can be solved by the exhaustive\\nevaluation method); we construct a greedy algorithm which\\niteratively solve the optimization problem (6) to achieve the\\napproximate solution for (5) as follows. We write the par-\\ntition that is generated when X is input to the optimization\\nproblem (6) as Opt(X) = {X1, X2, .., Xk} where k < γ.\\nWe apply the same procedure recursively on each Xi sepa-\\nrately up to the point where we do not expect to improve the\\nobjective function (i.e. the optimal k is 1). The ﬁnal partition\\nis the union of the partitions that are generated by applying\\nthis procedure recursively to each Xi. We write the ﬁnal par-\\ntition achieved by the greedy algorithm as { ˆ\\nX ∗\\n1 , ..., ˆ\\nX ∗\\nk }.\\nThe pseudo-code for ConﬁdentMatch is given in Fig. 1.\\nThe algorithm learns the recipient-donor compatibility pat-\\nterns in an off-line manner using the procedure described\\n1650\\n\\nOff-line Stage I: Discovering Optimal Partitions\\n(Greedy Approach)\\nInput: X\\nOutput: Greedy(X)\\n————————————————————–\\nFunction: Greedy(X)\\nY = ∅: Initialize the output as the empty set\\nC =Opt(X)\\nif |C| > 1 then\\nfor i = 1 to |C| do\\nY = Y ∪Greedy(Ci) where Ci is i-th subset of\\nC\\nend for\\nelse\\nY = Y ∪C\\nend if\\nFunction Output: Y = { ˆ\\nX ∗\\n1 , ..., ˆ\\nX ∗\\nk }\\n————————————————————–\\nOff-line Stage II: Learning Optimal Predictive\\nModels\\nInput: { ˆ\\nX ∗\\n1 , ..., ˆ\\nX ∗\\nk }, S, V, H\\nfor i = 1 to k do\\nˆh∗\\ni =\\nmin\\nhi∈ˆ\\nH(Si)\\n\\x02\\n(xv\\nj ,yv\\nj )∈Vi l(h(xv\\nj ), yv\\nj )\\nend for\\nOutput: {ˆh∗\\n1, ..., ˆh∗\\nk}\\n————————————————————–\\nOn-line Stage: Computing Compatibility Scores\\nInput: x, {ˆh∗\\n1, ..., ˆh∗\\nk}, { ˆ\\nX ∗\\n1 , ..., ˆ\\nX ∗\\nk }\\nˆh∗(x) = \\x02k\\ni=1 I(x ∈ˆ\\nX ∗\\ni ) × ˆh∗\\ni (x)\\nOutput: ˆh∗(x)\\nFigure 1: Pseudo-code for ConﬁdentMatch.\\nabove: it jointly optimizes the partitioning of the recipient-\\ndonor feature space (off-line stage I), and then optimizes\\nthe predictive model associated with each partition (off-line\\nstage II). Having learned the recipient-donor compatibilities,\\nthe algorithm operates in an on-line stage for new recipi-\\nents and donors by computing a compatibility score, i.e. the\\nprobability of transplant success, for a given recipient-donor\\npair. The compatibility score is displayed to the clinicians\\nand based on it, the clinicians/patients can make decisions\\non whether a transplant should be conducted, or whether the\\nrecipient should be matched with another donor.\\nIn what follows, we specify the computational complex-\\nity of ConﬁdentMatch. Let the complexity of the algorithm\\nAl(Si) for learning a predictive model using the dataset Si\\nbe Tl(|Si|, D). Based on this, it can be shown that the worst-\\ncase complexity for computing Opt(X) in the optimiza-\\ntion problem (6) is O(γγ+1nγDγ−1 \\x02M\\nl=1 Tl(n, D)), and\\nthe complexity of the greedy algorithm described in Fig. 1\\nis O(γγ+1nγ+1Dγ−1 \\x02M\\nl=1 Tl(n, D)) (proofs are provided\\nin the supporting material).\\nResults and Discussion\\nExperiments were conducted using the UNOS database for\\npatients who underwent a heart transplant over the years\\nfrom 1987 to 2015 (Cecka 1996). We use the “Thoracic\\nDATA” dataset in the UNOS database as our root dataset.\\nIn this dataset, all patients were followed-up until death, i.e.\\nthe post-transplant survival times for all patients are avail-\\nable in the dataset. Of the 148,512 patients in the “Thoracic\\nDATA” who underwent either heart or lung transplant, we\\nextract 60,516 patients who underwent a heart transplant.\\nOf the 60,516 patients who underwent a heart transplant,\\nwe exclude 3,800 patients (6.28%) who are still alive (right-\\ncensored), and we only use the 56,716 patients for whom we\\nhave the exact survival (lifetime) information.\\nFor each patient in the dataset, a total of 504 features are\\nprovided; these include a combination of both the patient’s\\nand the donor’s information. We discard 12 features that are\\nnormally obtained after the transplant. Of the remaining 492\\nfeatures, we extract 70 features for which we have less than\\n10% missing information in order to reduce the noise of im-\\nputation. We use the k-nearest-neighbor (KNN) imputation\\nmethod to impute the missing data (Hastie 1999).\\nWe compared the performance of ConﬁdentMatch in pre-\\ndicting the success of transplants with the following bench-\\nmark algorithms: logistic regression (Logit), Lasso regular-\\nized logistic regression (Lasso), decision tree (DTree), Ran-\\ndom Forest (RForest), AdaBoost (ABoost), and DeepBoost\\n(DBoost). We use the correlation feature selection (CFS)\\nmethod to discover the relevant features for the predic-\\ntive models of both ConﬁdentMatch and benchmarks (Hall\\n1999). The validation set calibrated all parameters of Con-\\nﬁdentMatch (α) and the benchmark algorithms. We adopt\\nthe following metric for quantifying the performance of the\\ndifferent algorithms. The transplant’s success probability is\\nquantiﬁed via the 3-year post-transplant survival rate (long-\\nterm survival rate). We say that an algorithm provides a pre-\\ndiction for the transplant’s success (probability of 3-year\\npost-transplant survival) for a certain recipient-donor pair\\nwith an accuracy level X% if the algorithm’s probability of\\ncorrect prediction is X% for that recipient-donor pair. Based\\non this deﬁnition, we deﬁne the gain of ConﬁdentMatch at\\nan accuracy level of X% as the number of recipient-donor\\ninstances in the testing dataset for which ConﬁdentMatch\\nprovides an accurate prediction of the transplant success,\\nwhereas the best-competing benchmark does not.\\nWe split the dataset into a training/validation set com-\\nprising the recipient-donor instances in which the recipient\\nunderwent the transplant before the year 2010 (former pa-\\ntients), and a testing set that comprises recipients who un-\\nderwent the transplant after 2010 (current patients). Of the\\n56,716 recipient-donor pairs, 37,677 pairs (66.43%) were\\nused for training, 9,419 pairs (16.61%) were used for val-\\nidating and 9,620 pairs (16.96%) were used for testing. We\\nvaried the accuracy level from 80% to 95% and evaluated\\nthe performance within this range of the accuracy levels. The\\nexecution time of the ConﬁdentMatch on this dataset is less\\nthan 5 hours on MATLAB R2015a with Intel i5 (1.5GHz)\\nprocessor with 4 GB RAM.\\nTable 2 and Fig 2 show that ConﬁdentMatch consis-\\ntently outperforms the benchmarks to predict the success of\\nheart transplant regarding 3-year mortality prediction. For\\ninstance, ConﬁdentMatch boosts the number of recipient-\\ndonor pairs who get 95% accurate predictions by 410 as\\n1651\\n\\nRelevant Features\\nRank\\nOverall Recipient-donor Population\\nGroup A\\nGroup B\\n1\\nVentilator Assist\\nVentilator Assist\\nECMO Assist\\n2\\nECMO Assist\\nECMO Assist\\nVentilator Assist\\n3\\nOther Life Support\\nOther Vent Support\\nDonor VDRL Result\\n4\\nOther Vent Support\\nOther Life Support\\nDays in State 1A\\n5\\nDays in State 1A\\nVAD support\\nPrior Cardiac Surgery\\n6\\nDiagnosis Code\\nDiagnosis Code\\nBlood Type Matching\\n7\\nDonor Status\\nDonor Status\\nDonor Blood Type (O)\\n8\\nTransplant Type\\nMalignancy\\nDonor Status\\n9\\nDonor HEP-P Antigen\\nTransplant Type\\nDonor HEP-B Antigen\\n10\\nPrevious MI History\\nPrevious Transplant\\nInhaled Assist\\nTable 1: Top 10 relevant features for the prediction of heart transplant success. (ECMO: Extracorporeal membrane oxygenation,\\nVDRL: Venereal disease research laboratory, VAD: Ventricular assist device, HEP: Hepatitis, MI: Myocardial infarction)\\nAccu.∗\\nCM∗\\nLASSO\\nRF∗\\nAB∗\\nDB∗\\nDTree\\n80%\\n9269\\n7721\\n9057\\n9063\\n9067\\n7893\\n85%\\n7798\\n7096\\n7605\\n7577\\n7614\\n7316\\n90%\\n6467\\n4431\\n6065\\n6076\\n6080\\n4526\\n95%\\n5489\\n2796\\n5065\\n5058\\n5079\\n3124\\nTable 2: Predictions for the success of heart transplant by\\ndifferent algorithms (∗Accu. = Accuracy, CM = Conﬁdent-\\nMatch, RF = random forest, AB = ABoost, DB = DBoost).\\nFigure 2: The number of recipient-donor pairs with conﬁ-\\ndent prediction based on ConﬁdentMatch as compared to\\nDBoost.\\ncompared to the best performing benchmark (DBoost); this\\nmeans that ConﬁdentMatch can allow an additional number\\nof 410 recipient-donor pairs to make better pre-transplant\\ndecisions such as whether or not the transplant should be\\nconducted, and whether the recipient should be matched\\nwith another donor.\\nThe performance gains achieved by ConﬁdentMatch can\\nbe attributed to the improved phenotypic characterization\\nof the recipient-donor pairs that the algorithm achieves by\\nFigure 3: Trade-offs associated with increasing the num-\\nber of partitions in ConﬁdentMatch (gain is with respect to\\nDBoost).\\nstratifying the recipient-donor feature space. The ﬁne and\\ngranular phenotypic characterization achieved by Conﬁdent-\\nMatch is restricted by the size of the training data; the more\\nrecipient-donor instances are available in the training set, the\\nlarger is the number of partitions that ConﬁdentMatch can\\nconstruct and cast a specialized predictive model. Fig 3 illus-\\ntrates the trade-off associated with increasing the complex-\\nity of ConﬁdentMatch’s predictive model by increasing the\\nnumber of partitions; if the number of partitions increases,\\nthe gain of ConﬁdentMatch also increases as it copes with\\nthe underlying complexity of the recipient-donor compati-\\nbility patterns, until a certain number of partitions when the\\ngain starts to decrease due to over-ﬁtting.\\nConﬁdentMatch does not only improve the quality of\\nprognosis, but can also draw clinical insights on the patterns\\nof recipient-donor compatibility. To illustrate this, we list the\\nﬁrst two partitions through which ConﬁdentMatch stratiﬁes\\nthe recipient feature space: ConﬁdentMatch forms two re-\\ncipient groups, group A comprises patients whose length of\\nstay in status 1A (urgent transplant wait-list) is shorter than\\n10 days, whereas group B comprises the remaining patients.\\n1652\\n\\nTable 1 lists the most relevant features that are predictive\\nof the transplant outcome for each group. It can be seen\\nthat group B patients are more sensitive to the donor char-\\nacteristics; the donor’s Venereal disease research laboratory\\n(VDRL) result and blood type are relevant to the transplant\\noutcome, whereas group A patients appear to be less sensi-\\ntive to these features. Thus, the more the patient waits in sta-\\ntus 1A, the more it becomes essential to consider the extent\\nof her compatibility with the donors. As more training data\\nbecomes available, ConﬁdentMatch can reveal ﬁner parti-\\ntions and identify the relevant features for more granular re-\\ncipient subgroups.\\nConclusions\\nOrgan transplants for patients with end-stage diseases car-\\nries the risk of various serious post-operative complica-\\ntions, the pre-operative anticipation of the transplant out-\\ncome depends on the compatibility between the donor and\\nthe recipient. In this paper, we have developed Conﬁdent-\\nMatch, a data-driven system that learns complex recipient-\\ndonor compatibility patterns from the outcomes of previous\\ntransplants. ConﬁdentMatch captures the complexity of such\\ncompatibility patterns by optimally dividing the recipient-\\ndonor feature space into clusters and assigning different op-\\ntimal predictive models to each cluster, thereby ensuring\\nthat predictions are “personalized” and tailored to individual\\ncharacteristics of both the donors and the recipients. Experi-\\nments conducted on a public heart transplant dataset demon-\\nstrate the superiority of ConﬁdentMatch to other competing\\nbenchmark algorithms.\\nAcknowledgments\\nThe authors would like to thank the reviewers for their help-\\nful comments. The research presented in this paper was sup-\\nported by Natural Science Foundation (NSF) under Grant\\nNumber NSF IIP1533983 and NSF ECCS1462245.\\nReferences\\nAllen J. G., Weiss E. S., Arnaoutakis G. J., Russell S. D.,\\nBaumgartner W. A., Conte J. V., and Shah A. S. 2010. The\\nimpact of race on survival after heart transplantation: an\\nanalysis of more than 20,000 patients. The Annals of tho-\\nracic surgery 89:1956–1963.\\nArnaoutakis G. J., George T. J., Allen J. G., Russell S. D.,\\nShah A. S., Conte J. V. and Weiss E. S. 2012 Institutional\\nvolume and the effect of recipient risk on short-term mor-\\ntality after orthotopic heart transplant. Journal on Thoracic\\nCardiovascular Surgery 143: 157–167.\\nCecka J. M. 1996. The unos scientic renal transplant reg-\\nistryten years of kidney transplants. Clinical transplants\\n114.\\nCollins F. S., Varmus H. 2015. A new initiative on precision\\nmedicine. New England Journal of Medicine. 372: 793–795.\\nDai, P., Gwadry-Sridhar, F., Bauer, M., Borrie, M. 2016.\\nBagging Ensembles for the Diagnosis and Prognostication\\nof Alzheimer’s Disease. Thirtieth AAAI Conference on Arti-\\nﬁcial Intelligence.\\nDeSalvo G., Mehryar, M. 2014. Random Composite Forests.\\nThirtieth AAAI Conference on Artiﬁcial Intelligence.\\nEick, C. F., Zeidat, N., Zhao, Z. 2004. Supervised clustering-\\nalgorithms and benets. In IEEE ICTAI 774–776.\\nEnciso, J. S. et al. 2014. Effect of Peripheral Vascular Dis-\\nease on Mortality in Cardiac Transplant Recipients (from the\\nUnited Network of Organ Sharing Database). The American\\njournal of cardiology 114: 1111–1115.\\nFinley, T., Joachims, T. 2005. Supervised clustering with\\nsupport vector machines. In ICML 217–224.\\nFreund, Y., Schapire, R. E. 1997. A decision-theoretic gener-\\nalization of on-line learning and an application to boosting.\\nJournal of computer and system sciences 55: 119–139.\\nFriedman, J., Hastie, T., Tibshirani, R. et al. 2000. Additive\\nlogistic regression: a statistical view of boosting. The annals\\nof statistics 28: 337–407.\\nGale, D., Shapley, L. S. 1962. College Admissions and the\\nStability of Marriage. American Mathematical Monthly 69:\\n9–14.\\nHall, M. A. 1999. Correlation-based feature selection for\\nmachine learning. PhD thesis, The University of Waikato.\\nHastie, T., Tibshirani, R., Sherlock, G., Eisen, M., Brown,\\nP., and Botstein D. 1999, Imputing missing data for gene\\nexpression arrays.\\nHuynh, T. N., Kleerup, E. C., Raj, P. P., and Wenger, N. S.\\n2014. The opportunity cost of futile treatment in the inten-\\nsive care unit. Critical Care Medicine, 42: 1977–1982.\\nJawitz O. K., Jawitz N., Yuh D. D., Bonde P. 2013. Impact\\nof ABO compatibility on outcomes after heart transplanta-\\ntion in a national cohort during the past decade. Journal on\\nThoracic Cardiovascular Surgery 146: 1239-1245.\\nJayarajan S. N., Taghavi S., Komaroff E., Mangi A. A.\\n2013. Impact of low donor to recipient weight ratios on car-\\ndiac transplantation. Journal on Thoracic Cardiovascular\\nSurgery 146: 1538–1543.\\nKause, J. et al. 2004. A comparison of antecedents to car-\\ndiac arrests, deaths and emergency intensive care admissions\\nin Australia and New Zealand, and the United Kingdomthe\\nACADEMIA study. Resuscitation 62: 275–282.\\nKim, D. 2004. Structural risk minimization on decision trees\\nusing an evolutionary multiobjective optimization. In Euro-\\npean Conference on Genetic Programming 338–348.\\nKuznetsov, V., Mohri, M., Syed, U. 2014. Multi-class deep\\nboosting. In NIPS, 2501–2509.\\nLiaw, A., Wiener, M. 2002. Classication and regression by\\nrandom-forest. R news 3: 18–22.\\nMancini D., Lietz K. 2010. Selection of cardiac transplanta-\\ntion candidates in 2010. Circulation 122: 173–83.\\nMacQueen, J. et al. 1967. Some methods for classication and\\nanalysis of multivariate observations. In Proceedings of the\\nfth Berkeley symposium on mathematical statistics and prob-\\nability 1: 281–297.\\nNilsson J., Ohlsson M., Hglund P., Ekmehag B., Koul B.,\\nand Andersson B. 2015. The International Heart Transplant\\n1653\\n\\nSurvival Algorithm (IHTSA): a new model to improve organ\\nsharing and survival. PLoS One.\\nNwakanma L. U., Williams J. A., Weiss E. S., Russell S. D.,\\nBaumgartner W. A., Conte J. V. 2007. Inﬂuence of pretrans-\\nplant panel-reactive antibody on outcomes in 8,160 heart\\ntransplant recipients in recent era. The Annals of Thoracic\\nSurgery 84: 1556–1562.\\nQuinlan, J. Ross., 1986. Induction of decision trees. Ma-\\nchine learning 1: 81-106.\\nRoth, A. E., Sonmez, T., Unver, M. U. 2003. Kidney ex-\\nchange. National Bureau of Economic Research.\\nRusso M. J., et al. 2006. Survival after heart transplantation\\nis not diminished among recipients with uncomplicated dia-\\nbetes mellitus: an analysis of the United Network of Organ\\nSharing database. Circulation. 114: 2280–2287.\\nShah M. R., Starling R. C., Schwartz L. L., Mehra M. R.\\n2012. Heart transplantation research in the next decade–a\\ngoal to achieving evidence-based outcomes: National Heart,\\nLung, And Blood Institute Working Group. J Am Coll Car-\\ndiol, 59: 1263–1269.\\nShalev-Shwartz, S., Ben-David, S. 2014. Understanding ma-\\nchine learning: From theory to algorithms. Cambridge Uni-\\nversity Press.\\nStrobl, C., Malley, J., Tutz, G. 2009. An introduction to\\nrecur-sive partitioning: rationale, application, and character-\\nistics of classication and regression trees, bagging, and ran-\\ndom forests. Psychological methods 14: 323.\\nTaghavi S., Jayarajan S. N., Wilson L. M., Komaroff E., Tes-\\ntani J. M., Mangi A. A. 2013. Cardiac transplantation can be\\nsafely performed using selected diabetic donors. Journal on\\nThoracic Cardiovascular Surgery 146: 442–447.\\nTekin, C., Yoon, J., van der Schaar, M. 2016. Adaptive en-\\nsemble learning with conﬁdence bounds for personalized di-\\nagnosis. AAAI Workshop on Expanding the Boundaries of\\nHealth Informatics using AI (HIAI16): Making Proactive,\\nPersonalized, and Participatory Medicine A Reality.\\nWozniak C. J., Stehlik J., Baird B. C., et al. 2014. Ventricular\\nAssist Devices or Inotropic Agents in Status 1A Patients?\\nSurvival Analysis of the United Network of Organ Sharing\\nDatabase. The Annals of thoracic surgery 97: 1364–1372.\\nWilf. H. S., 1994. Generatingfunctionology. Elseviers 2: 23.\\n1654\\n'}, {'source': '/Users/sir/Downloads/Data/PDF/Are You Solving the Right Problems?.pdf', 'text': 'Summary.\\xa0\\xa0\\xa0\\nDecision Making And Problem Solving\\nAre You Solving the Right\\nProblems?\\nReframing them can reveal unexpected solutions. by Thomas Wedell-\\nWedellsborg\\nFrom the Magazine (January–February 2017)\\nFredrik Broden\\nIn surveys of 106 C-suite executives representing 91 private- and public-sector\\ncompanies from 17 countries, the author found that a full 85% agreed that their organizations\\nwere bad at problem diagnosis, and 87% agreed that this ﬂaw carried signiﬁcant costs....\\nHow good is your company at problem solving? Probably quite good,\\nif your managers are like those at the companies I’ve studied. What they\\nstruggle with, it turns out, is not solving problems but figuring out what\\nthe problems are. In surveys of 106 C-suite executives who represented\\n91 private and public-sector companies in 17 countries, I found that a\\nfull 85% strongly agreed or agreed that their organizations were bad at\\nproblem diagnosis, and 87% strongly agreed or agreed that this flaw\\ncarried significant costs. Fewer than one in 10 said they were unaffected\\nby the issue. The pattern is clear: Spurred by a penchant for action,\\nmanagers tend to switch quickly into solution mode without checking\\nwhether they really understand the problem.\\nIt has been 40 years since Mihaly Csikszentmihalyi and Jacob Getzels\\nempirically demonstrated the central role of problem framing in\\ncreativity. Thinkers from Albert Einstein to Peter Drucker have\\nemphasized the importance of properly diagnosing your problems. So\\nwhy do organizations still struggle to get it right?\\nPart of the reason is that we tend to overengineer the diagnostic process.\\nMany existing frameworks—TRIZ, Six Sigma, Scrum, and others—are\\nquite comprehensive. When properly applied, they can be tremendously\\npowerful. But their very thoroughness also makes them too complex and\\ntime-consuming to fit into a regular workday. The setting in which\\npeople most need to be better at problem diagnosis is not the annual\\nstrategy seminar but the daily meeting—so we need tools that don’t\\nrequire the entire organization to undergo weeks-long training\\nprograms.\\nBut even when people apply simpler problem-diagnosis frameworks,\\nsuch as root cause analysis and the related 5 Whys questioning\\ntechnique, they often find themselves digging deeper into the problem\\nthey’ve already defined rather than arriving at another diagnosis. That\\ncan be helpful, certainly. But creative solutions nearly always come\\nfrom an alternative definition of your problem.\\nThrough my research on corporate innovation, much of it conducted\\nwith my colleague Paddy Miller, I have spent close to 10 years working\\nwith and studying reframing—first in the narrow context of\\norganizational change and then more broadly. In the following pages I\\noffer a new approach to problem diagnosis that can be applied quickly\\nand, I’ve found, frequently leads to creative solutions by unearthing\\nradically different framings of familiar and persistent problems. To put\\nreframing in context, I’ll explain more precisely just what this approach\\nis trying to achieve.\\nThe Slow Elevator Problem\\nImagine this: You are the owner of an office building, and your tenants\\nare complaining about the elevator. It’s old and slow, and they have to\\nwait a lot. Several tenants are threatening to break their leases if you\\ndon’t fix the problem.\\nWhen asked, most people quickly identify some solutions: replace the\\nlift, install a stronger motor, or perhaps upgrade the algorithm that runs\\nthe lift. These suggestions fall into what I call a solution space: a cluster\\nof solutions that share assumptions about what the problem is—in this\\ncase, that the elevator is slow. This framing is illustrated below.\\nHowever, when the problem is presented to building managers, they\\nsuggest a much more elegant solution: Put up mirrors next to the\\nelevator. This simple measure has proved wonderfully effective in\\nreducing complaints, because people tend to lose track of time when\\ngiven something utterly fascinating to look at—namely, themselves.\\nThe mirror solution is particularly interesting because in fact it is not a\\nsolution to the stated problem: It doesn’t make the elevator faster.\\nInstead it proposes a different understanding of the problem.\\nNote that the initial framing of the problem is not necessarily wrong.\\nInstalling a new lift would probably work. The point of reframing is not\\nto find the “real” problem but, rather, to see if there is a better one to\\nsolve. In fact, the very idea that a single root problem exists may be\\nmisleading; problems are typically multicausal and can be addressed in\\nmany ways. The elevator issue, for example, could be reframed as a peak\\ndemand problem—too many people need the lift at the same time—\\nleading to a solution that focuses on spreading out the demand, such as\\nby staggering people’s lunch breaks.\\nIdentifying a different aspect of the problem can sometimes deliver\\nradical improvements—and even spark solutions to problems that have\\nseemed intractable for decades. I recently saw this in action when\\nstudying an often overlooked problem in the pet industry: the number\\nof dogs in shelters.\\nAmerica’s Dog-Adoption Problem\\nDogs are very popular in America: Industry statistics suggest that more\\nthan 40% of U.S. households have one. But this fondness for dogs has a\\ndownside: According to estimates by the ASPCA, one of the largest\\nanimal-welfare groups in the United States, more than 3 million dogs\\nenter a shelter each year and are put up for adoption.\\nShelters and other animal-welfare organizations work hard to raise\\nawareness of this issue. A typical ad or poster will show a neglected, sad-\\nlooking dog, carefully chosen to evoke compassion, along with a line\\nsuch as “Save a life—adopt a dog” or perhaps a request to donate to the\\ncause. Through this and other initiatives, this notoriously underfunded\\nsystem manages to get about 1.4 million dogs adopted each year. But\\nthat leaves more than a million unadopted dogs—and doesn’t account\\nfor the many cats and other pets in the same situation. There is just a\\nlimited amount of compassion to go around. So despite the impressive\\nefforts of shelters and rescue groups, the shortage of pet adopters has\\npersisted for decades.\\nLori Weise, the founder of Downtown Dog Rescue in Los Angeles, has\\ndemonstrated that adoption is not the only way to frame the problem.\\nWeise is one of the pioneers of an approach that is currently spreading\\nwithin the industry—the shelter intervention program. Rather than seek\\nto get more dogs adopted, Weise tries to keep them with their original\\nfamilies so that they never enter shelters in the first place. It turns out\\nthat about 30% of the dogs that enter a shelter are “owner surrenders,”\\ndeliberately relinquished by their owners. In a volunteer-driven\\ncommunity united by a deep love of animals, those people have often\\nbeen heavily criticized for heartlessly discarding their pets as if they\\nwere just another consumer good. To prevent dogs from ending up with\\nsuch “bad” owners, many shelters, despite their chronic overpopulation,\\nrequire potential adopters to undergo laborious background checks.\\nWeise has a different take. “Owner surrenders are not a people\\nproblem,” she says. “By and large, they are a poverty problem. These\\nfamilies love their dogs as much as we do, but they are also\\nexceptionally poor. We’re talking about people who in some cases aren’t\\nentirely sure how they will feed their kids at the end of the month. So\\nwhen a new landlord suddenly demands a deposit to house the dog, they\\nsimply have no way to get the money. In other cases, the dog needs a $10\\nrabies shot, but the family has no access to a vet, or may be afraid to\\napproach any kind of authority. Handing over their pet to a shelter is\\noften the last option they believe they have.”\\nWeise started her program in April 2013, collaborating with a shelter in\\nSouth Los Angeles. The idea is simple: Whenever a family comes in to\\nhand over a pet, a staff member asks without judgment if the family\\nwould prefer to keep the pet. If the answer is yes, the staff member tries\\nto help resolve the problem, drawing on his or her network and\\nknowledge of the system.\\nWithin the first year it was clear that the program was a remarkable\\nsuccess. In prior years Weise’s organization had spent an average of $85\\nper pet it helped. The new program brought that cost down to about $60\\nwhile keeping shelter space free for other animals in need. And, Weise\\ntold me, that was just the immediate impact: “The wider effect on the\\ncommunity is the real point. The program helps families learn problem\\nsolving, lets them know their rights and responsibilities, and teaches the\\ncommunity that help is available. It also shifted the industry’s\\nperception of the pet owners: We found that when offered assistance, a\\nfull 75% of them actually wanted to keep their pets.”\\nYou won’t know which problems can\\nbenefit from being reframed until you\\ntry.\\nAs of this writing, Weise’s program has helped close to 5,000 pets and\\nfamilies and has gained the formal support of the ASPCA. Weise has\\nreleased a book, First Home, Forever Home, that explains to other rescue\\ngroups how to run an intervention program. Thanks to her reframing of\\nthe problem, overcrowded shelters may someday be a thing of the past.\\nHow might you find a similarly insightful reframing for your problem?\\nSeven Practices for Effective Reframing\\nIn my experience, reframing is best taught as a quick, iterative process.\\nYou might think of it as a cognitive counterpoint to rapid prototyping.\\nThe practices I outline here can be used in one of two ways, depending\\non how much control you have over the situation. One way is to\\nmethodically apply all seven to the problem. That can be done in about\\n30 minutes, and it has the benefit of familiarizing everyone with the\\nmethod.\\nThe other way is suitable when you don’t control the situation and have\\nto scale the method according to how much time is available. Perhaps a\\nteam member ambushes you in the hallway and you have only five\\nminutes to help him or her rethink a problem. If so, simply select the\\none or two practices that seem most appropriate.\\nFive minutes may sound like too little time to even describe a problem,\\nmuch less reframe it. But surprisingly, I have found that such short\\ninterventions are often sufficient to kick-start new thinking—and once\\nin a while they can trigger an aha moment and radically shift your view\\nof a problem. Proximity to your own problems can make it easy to get\\nlost in the weeds, endlessly ruminating about why a colleague, a spouse,\\nor your children won’t listen. Sometimes all you need is someone to\\nsuggest, “Well, could the trouble be that you are bad at listening to\\nthem?”\\nOf course, not all problems are that simple. Often multiple rounds of\\nreframing—interspersed with observation, conversation, and\\nprototyping—are necessary. And in some cases reframing won’t help at\\nall. But you won’t know which problems can benefit from being\\nreframed until you try. Once you’ve mastered the five-minute version,\\nyou can apply reframing to pretty much any problem you face.\\nHere are the seven practices:\\n1. Establish legitimacy.\\nIt’s difficult to use reframing if you are the only person in the room who\\nunderstands the method. Other people, driven by a desire to find\\nsolutions, may feel that your insistence on discussing the problem is\\ncounterproductive. If the group has a power imbalance, such as when\\nyou’re facing clients or more-senior colleagues, they may well shut you\\ndown before you even get started. And even powerful executives may\\nfind it hard to use the method when people are accustomed to getting\\nanswers rather than questions from their leaders.\\nYour first job, therefore, is to establish the method’s legitimacy within\\nthe group, creating the conversational space necessary to employ\\nreframing. I suggest two ways to do this. The first is to share this article\\nwith the people you are meeting. Even if they don’t read it, simply\\nseeing it may persuade them to listen to you. The second is to relate the\\nslow elevator problem, which is my go-to example when I have less than\\n30 seconds to explain the concept. I have found it to be a powerful way\\nto quickly explain reframing—how it differs from merely diagnosing a\\nproblem and how it can potentially create dramatically better results.\\n2. Bring outsiders into the discussion.\\nThis is the single most helpful reframing practice. I saw it in action eight\\nyears ago when the management team of a small European company\\nwas wrestling with a lack of innovation in its workforce. The managers\\nhad recently encountered a specific innovation training technique they\\nall liked, so they started discussing how best to implement it within the\\norganization.\\nSensing that the group lacked an outside voice, the general manager\\nasked his personal assistant, Charlotte, to take part in their discussion.\\n“I’ve been working here for 12 years,” Charlotte told the group, “and in\\nthat time I have seen three different management teams try to roll out\\nsome new innovation framework. None of them worked. I don’t think\\npeople would react well to the introduction of another set of\\nbuzzwords.”\\nCharlotte’s observation prompted the managers to realize that they had\\nfallen in love with a solution—introducing an innovation framework—\\nbefore they fully understood the problem. They soon concluded that\\ntheir initial diagnosis had been wrong: Many of their employees already\\nknew how to innovate, but they didn’t feel very engaged in the\\ncompany, so they were unlikely to take initiative beyond what their job\\ndescriptions mandated. What the managers had first framed as a skill-\\nset problem was better approached as a motivation problem.\\nThey abandoned all talk of innovation workshops and instead focused\\non improving employee engagement by (among other things) giving\\npeople more autonomy, introducing flexible working hours, and\\nswitching to a more participatory decision-making style. The remedy\\nworked. Within 18 months workplace satisfaction scores had doubled\\nand employee turnover had fallen dramatically. And as people started\\nbringing their creative abilities to bear at work, financial results\\nimproved markedly. Four years later the company won an award for\\nbeing the country’s best place to work.\\nAs this story shows, getting an outsider’s perspective can be\\ninstrumental in rethinking a problem quickly and properly. To do so\\nmost effectively:\\nLook for “boundary spanners.” As research by Michael Tushman and\\nmany others has shown, the most useful input tends to come from\\npeople who understand but are not fully part of your world. Charlotte\\nwas close enough to the front lines of the company to know how the\\nemployees really felt, but she was also close enough to management to\\nunderstand its priorities and speak its language, making her ideally\\nsuited for the task. In contrast, calling on an innovation expert might\\nwell have led the team’s members further down the innovation path\\ninstead of inspiring them to rethink their problem.\\nChoose someone who will speak freely. By virtue of her long tenure and\\nher closeness to the general manager, Charlotte felt free to challenge the\\nmanagement team while remaining committed to its objectives. This\\nsense of psychological safety, as Harvard’s Amy C. Edmondson calls it,\\nhas been proved to help groups perform better. You might consider\\nturning to someone whose career advancement will not be determined\\nby the group in question or who has a track record of (constructively)\\nspeaking truth to power.\\nExpect input, not solutions. Crucially, Charlotte did not try to provide\\nthe group with a solution; rather, her observation made the managers\\nthemselves rethink their problem. This pattern is typical. By definition,\\noutsiders are not experts on the situation and thus will rarely be able to\\nsolve the problem. That’s not their function. They are there to stimulate\\nthe problem owners to think differently. So when you bring them in, ask\\nthem specifically to challenge the group’s thinking, and prime the\\nproblem owners to listen and look for input rather than answers.\\n3. Get people’s deﬁnitions in writing.\\nIt’s not unusual for people to leave a meeting thinking they all agree on\\nwhat the problem is after a loose oral description, only to discover weeks\\nor months later that they had different views of the issue. Moreover, a\\nsuccessful reframing may well lurk in one of those views.\\nFor instance, a management team may agree that the company’s\\nproblem is a lack of innovation. But if you ask each member to describe\\nwhat’s wrong in a sentence or two, you will quickly see how framings\\ndiffer. Some people will claim, “Our employees aren’t motivated to\\ninnovate” or “They don’t understand the urgency of the situation.”\\nOthers will say, “People don’t have the right skill set,” “Our customers\\naren’t willing to pay for innovation,” or “We don’t reward people for\\ninnovation.” Pay close attention to the wording, because even seemingly\\ninconsequential word choices can surface a new perspective on the\\nproblem.\\nI saw a memorable demonstration of this when I was working with a\\ngroup of managers in the construction industry, exploring what they\\ncould do as individual leaders to deliver better results. As we tried to\\nidentify the barriers each one faced, I asked them to write their\\nproblems on flip charts, after which we jointly analyzed the statements.\\nThe very first comment from the group had the greatest impact: “Almost\\nnone of the definitions include the word ‘I.’” With one exception, the\\nproblems were consistently worded in a way that diffused individual\\nresponsibility, such as “My team doesn’t…,” “The market doesn’t…,”\\nand, in a few cases, “We don’t…” That one observation shifted the tenor\\nof the meeting, pushing the participants to take more ownership of the\\nchallenges they faced.\\nThese individual definitions of the problem should ideally be gathered\\nin advance of a discussion. If possible, ask people to send you a few lines\\nin a confidential e-mail, and insist that they write in sentence form—\\nbullet points are simply too condensed. Then copy the definitions\\nyou’ve collected on a flip chart so that everyone can see them and react\\nto them in the meeting. Don’t attribute them, because you want to\\nensure that people’s judgment of a definition isn’t affected by the\\ndefiner’s identity or status.\\nReceiving these multiple definitions will sensitize you to the\\nperspectives of other stakeholders. We all appreciate in theory that\\nothers may experience a problem differently (or not see it at all). But as\\ndemonstrated in a recent study by Johannes Hattula, of Imperial College\\nLondon, if managers try to imagine a customer’s perspective\\nthemselves, they typically get it wrong. To understand what other\\nstakeholders think, you need to hear it from them.\\n4. Ask what’s missing.\\nWhen faced with the description of a problem, people tend to delve into\\nthe details of what has been stated, paying less attention to what the\\ndescription might be leaving out. To rectify this, make sure to ask\\nexplicitly what has not been captured or mentioned.\\nRecently I worked with a team of senior executives in Brazil who had\\nbeen asked to provide their CEO with ideas for improving the market’s\\nperception of the company’s stock price. The team had expertly\\nanalyzed the components affecting a stock’s value—the P/E ratio\\nforecast, the debt ratio, earnings per share, and so on. Of course, none of\\nthis was news to the CEO, nor were these factors particularly easy to\\naffect, leading to mild despondency on the team.\\nBut when I prompted the executives to zoom out and consider what was\\nmissing from their definition of the problem, something new came up. It\\nturned out that when external financial analysts asked to speak with\\nexecutives from the company, the task of responding was typically\\ndelegated to slightly more junior leaders, none of whom had received\\ntraining in how to talk to analysts. As soon as this point was raised, the\\ngroup saw that it had found a potential recommendation for the CEO.\\n(The observation came not from the team’s finance expert but from a\\nboundary-spanning HR executive.)\\n5. Consider multiple categories.\\nAs Lori Weise’s story demonstrates, powerful change can come from\\ntransforming people’s perception of a problem. One way to trigger this\\nkind of paradigm shift is to invite people to identify specifically what\\ncategory of problem they think the group is facing. Is it an incentive\\nproblem? An expectations problem? An attitude problem? Then try to\\nsuggest other categories.\\nA manager I know named Jeremiah Zinn did this when he led the\\nproduct development team of the popular children’s entertainment\\nchannel Nickelodeon. The team was launching a promising new app,\\nand lots of kids downloaded it. But actually activating the app was\\nsomewhat complicated, because it required logging in to the\\nhousehold’s cable TV service. At that point in the sign-up process,\\nalmost every kid dropped out.\\nSeeing the problem as one of usability, the team put its expertise to work\\nand ran hundreds of A/B tests on various sign-up flows, seeking to make\\nthe process less complex. Nothing helped.\\nThe shift came when Zinn realized that the team members had been\\nthinking of the problem too narrowly. They had focused on the kids’\\nactions, carefully tracking every click and swipe—but they had not\\nexplored how the kids felt during the sign-up process. That turned out to\\nbe critical. As the team started looking for emotional reactions, it\\ndiscovered that the request for the cable password made the kids fear\\ngetting in trouble: To a 10-year-old kid, a password request signals\\nforbidden territory. Equipped with that insight, Zinn’s team simply\\nadded a short video explaining that it was OK to ask parents for the\\npassword—and saw a rapid 10-fold increase in the sign-up rate for the\\napp.\\nBy explicitly highlighting how the group thinks about a problem—what\\nis sometimes called metacognition, or thinking about thinking—you can\\noften help people reframe it, even if you don’t have other frames to\\nsuggest. And it’s a useful way of sorting through written definitions if\\nyou managed to gather them in advance.\\nZinn’s story also exposes a typical pitfall in problem solving, first\\nexpressed by Abraham Kaplan in his famous law of the instrument:\\nmore\\nPost\\nPost\\nShare\\nSave\\nBuy Copies\\nPrint\\nLatest\\nMagazine\\nAscend\\nTopics\\nPodcasts\\nStore\\nThe Big Idea\\nData & Visuals\\nCase Selections\\nDecision Making And Problem Solving \\xa0\\xa0|\\xa0\\xa0 Are You Solving the Right Problems?\\nSubscribe\\nSign In\\n\\nStart my subscription!\\nExplore HBR\\nThe Latest\\nAll Topics\\nMagazine Archive\\nThe Big Idea\\nReading Lists\\nCase Selections\\nPodcasts\\nWebinars\\nData & Visuals\\nMy Library\\nNewsletters\\nHBR Press\\nHBR Ascend\\nHBR Store\\nArticle Reprints\\nBooks\\nCases\\nCollections\\nMagazine Issues\\nHBR Guide Series\\nHBR 20-Minute Managers\\nHBR Emotional Intelligence Series\\nHBR Must Reads\\nTools\\nAbout HBR\\nContact Us\\nAdvertise with Us\\nInformation for\\nBooksellers/Retailers\\nMasthead\\nGlobal Editions\\nMedia Inquiries\\nGuidelines for Authors\\nHBR Analytic Services\\nCopyright Permissions\\nManage My Account\\nMy Library\\nTopic Feeds\\nOrders\\nAccount Settings\\nEmail Preferences\\nAccount FAQ\\nHelp Center\\nContact Customer Service\\nFollow HBR\\nAbout Us\\n|\\n Careers\\n|\\n Privacy Policy\\n|\\n Cookie Policy\\n|\\n Copyright Information\\n|\\n Trademark Policy\\n|\\n Terms of Use\\nHarvard Business Publishing:  Higher Education\\n|\\n Corporate Learning\\n|\\n Harvard Business Review\\n|\\n Harvard Business School\\nCopyright ©2024\\xa0 Harvard Business School Publishing. All rights reserved. Harvard Business Publishing is an afﬁliate of Harvard Business School.\\nexpressed by Abraham Kaplan in his famous law of the instrument:\\n“Give a small boy a hammer, and he will find that everything he\\nencounters needs pounding.” At Nickelodeon, because the team\\nmembers were usability experts, they defaulted to thinking the problem\\nwas one of usability.\\n6. Analyze positive exceptions.\\nTo find additional problem framings, look to instances when the\\nproblem did not occur, asking, “What was different about that\\nsituation?” Exploring such positive exceptions, sometimes called bright\\nspots, can often uncover hidden factors whose influence the group may\\nnot have considered.\\nA lawyer I spoke to, for instance, told me that the partners at his firm\\nwould occasionally meet to discuss initiatives that might grow their\\nbusiness in the longer term. But to his frustration, the instant one of\\nthose meetings ended, he and the other partners went back to focusing\\non landing the next short-term project. When prompted to think of\\npositive exceptions, he remembered one longer-term initiative that had\\nin fact gone forward.\\nWhat was different about that one? I asked. It was that the meeting,\\nunusually, had included not just partners but also an associate who was\\nconsidered a rising star—and it was she who had pursued the idea. That\\nimmediately suggested that talented associates be included in future\\nmeetings. The associates felt privileged and energized by being invited\\nto the strategic discussions, and unlike the partners, they had a clear\\nshort-term incentive to move on long-term projects—namely, to impress\\nthe partners and gain an edge in the competition against their peers.\\nA checklist for problem diagnosis\\ntends to discourage actual thinking.\\nLooking at positive exceptions can also make the discussion less\\nthreatening. Especially in a large group or other public setting,\\ndissecting a string of failures can quickly become confrontational and\\nmake people overly defensive. If, instead, you ask the group’s members\\nto analyze a positive outcome, it becomes easier for them to examine\\ntheir own behavior.\\n7. Question the objective.\\nIn the negotiation classic Getting to Yes, Roger Fisher, William L. Ury,\\nand Bruce Patton share the early management thinker Mary Parker\\nFollett’s story about two people fighting over whether to keep a window\\nopen or closed. The underlying goals of the two turn out to differ: One\\nperson wants fresh air, while the other wants to avoid a draft. Only when\\nthese hidden objectives are brought to light through the questions of a\\nthird person is the problem resolved—by opening a window in the next\\nroom.\\nThat story highlights another way to reframe a problem—by paying\\nexplicit attention to the objectives of the parties involved, first clarifying\\nand then challenging them. Weise’s shelter intervention program, for\\ninstance, hinged on a shift in the objective, from increasing adoption to\\nkeeping more pets with their original owners. The story of Charlotte,\\ntoo, included a shift in the stated goals of the management team, from\\nteaching innovation skills to boosting employee engagement.\\nAs described in Fred Kaplan’s book The Insurgents, a famous\\ncontemporary example is the change in U.S. military doctrine pioneered\\nby General David Petraeus, among others. In traditional warfare, the aim\\nof a battle is to defeat the enemy forces. But Petraeus and his allies\\nargued that when dealing with insurgencies, the army had to pursue a\\ndifferent, broader objective to prevent new enemies from cropping up—\\nnamely, get the populace on its side, thereby removing the source of\\nrecruits and other forms of local support the insurgency needed to\\noperate in the area. That approach was eventually adopted by the\\nmilitary—because a small group of rogue thinkers took it upon\\nthemselves to question the predefined and long-standing objectives of\\ntheir organization.\\nCONCLUSION\\nPowerful as reframing can be, it takes time and practice to get good at it.\\nOne senior executive from the defense industry told me, “I was shocked\\nby how difficult it is to reframe problems, but also how effective it is.” As\\nyou start to work more with the method, urge your team to trust the\\nprocess, and be prepared for it to feel messy and confusing at times.\\nIn leading more and more reframing discussions, you may also be\\ntempted to create a diagnostic checklist. I strongly caution you against\\nthat—or at least against making the checklist evident to the group you’re\\nengaging with. A checklist for problem diagnosis tends to discourage\\nactual thinking, which of course defeats the very purpose of engaging in\\nreframing. As Neil Gaiman reminds us in The Sandman, tools can be the\\nsubtlest of traps.\\nFinally, combine reframing with real-world testing. The method is\\nultimately limited by the knowledge and perspectives of the people in\\nthe room—and as Steve Blank, of Stanford, and others have repeatedly\\nshown, it is fatal to think you can figure it all out within the comfy\\nconfines of your own office. The next time you face a problem, start by\\nreframing it—but don’t wait too long before getting out of the building\\nto observe your customers and prototype your ideas. It is neither\\nthinking nor testing alone, but a marriage of the two, that holds the key\\nto radically better results.\\nThe ﬁrst appearance in print of the elevator problem, to the best of the author’s\\nknowledge, was in Russell L. Ackoﬀ, “Systems, Organizations, and Interdisciplinary\\nResearch,” General Systems, vol. V (1960).\\nA version of this article appeared in the January–February 2017 issue (pp.76–83) of Harvard\\nBusiness Review.\\nRead more on Decision making\\nand problem solving or related\\ntopics Business communication\\nand Management skills\\nThomas Wedell-Wedellsborg is an\\nindependent consultant and speaker and a\\ncoauthor of Innovation as Usual: How to Help\\nYour People Bring Great Ideas to Life (Harvard\\nBusiness Review Press, March 2013)\\nPost\\nPost\\nShare\\nSave\\nBuy Copies\\nPrint\\nRecommended For You\\nPODCAST\\nThe Secret to Better\\nProblem Solving\\nHow to Make Better\\nDecisions with Less Data\\nHow Structured Debate\\nHelps Your Team Grow\\nTwo Things to Do After\\nEvery Meeting\\nPartner Center\\nFacebook\\n\\ue021\\nX Corp.\\nLinkedIn\\n\\ue022\\nInstagram\\n\\ue085\\nYour Newsreader\\n\\ue083\\nYou have 1 free article left\\nthis month.\\nSubscribe for unlimited access.\\n'}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/Are You Solving the Right Problems?.pdf', 'text': 'Summary.\\xa0\\xa0\\xa0\\nDecision Making And Problem Solving\\nAre You Solving the Right\\nProblems?\\nReframing them can reveal unexpected solutions. by Thomas Wedell-\\nWedellsborg\\nFrom the Magazine (January–February 2017)\\nFredrik Broden\\nIn surveys of 106 C-suite executives representing 91 private- and public-sector\\ncompanies from 17 countries, the author found that a full 85% agreed that their organizations\\nwere bad at problem diagnosis, and 87% agreed that this ﬂaw carried signiﬁcant costs....\\nHow good is your company at problem solving? Probably quite good,\\nif your managers are like those at the companies I’ve studied. What they\\nstruggle with, it turns out, is not solving problems but figuring out what\\nthe problems are. In surveys of 106 C-suite executives who represented\\n91 private and public-sector companies in 17 countries, I found that a\\nfull 85% strongly agreed or agreed that their organizations were bad at\\nproblem diagnosis, and 87% strongly agreed or agreed that this flaw\\ncarried significant costs. Fewer than one in 10 said they were unaffected\\nby the issue. The pattern is clear: Spurred by a penchant for action,\\nmanagers tend to switch quickly into solution mode without checking\\nwhether they really understand the problem.\\nIt has been 40 years since Mihaly Csikszentmihalyi and Jacob Getzels\\nempirically demonstrated the central role of problem framing in\\ncreativity. Thinkers from Albert Einstein to Peter Drucker have\\nemphasized the importance of properly diagnosing your problems. So\\nwhy do organizations still struggle to get it right?\\nPart of the reason is that we tend to overengineer the diagnostic process.\\nMany existing frameworks—TRIZ, Six Sigma, Scrum, and others—are\\nquite comprehensive. When properly applied, they can be tremendously\\npowerful. But their very thoroughness also makes them too complex and\\ntime-consuming to fit into a regular workday. The setting in which\\npeople most need to be better at problem diagnosis is not the annual\\nstrategy seminar but the daily meeting—so we need tools that don’t\\nrequire the entire organization to undergo weeks-long training\\nprograms.\\nBut even when people apply simpler problem-diagnosis frameworks,\\nsuch as root cause analysis and the related 5 Whys questioning\\ntechnique, they often find themselves digging deeper into the problem\\nthey’ve already defined rather than arriving at another diagnosis. That\\ncan be helpful, certainly. But creative solutions nearly always come\\nfrom an alternative definition of your problem.\\nThrough my research on corporate innovation, much of it conducted\\nwith my colleague Paddy Miller, I have spent close to 10 years working\\nwith and studying reframing—first in the narrow context of\\norganizational change and then more broadly. In the following pages I\\noffer a new approach to problem diagnosis that can be applied quickly\\nand, I’ve found, frequently leads to creative solutions by unearthing\\nradically different framings of familiar and persistent problems. To put\\nreframing in context, I’ll explain more precisely just what this approach\\nis trying to achieve.\\nThe Slow Elevator Problem\\nImagine this: You are the owner of an office building, and your tenants\\nare complaining about the elevator. It’s old and slow, and they have to\\nwait a lot. Several tenants are threatening to break their leases if you\\ndon’t fix the problem.\\nWhen asked, most people quickly identify some solutions: replace the\\nlift, install a stronger motor, or perhaps upgrade the algorithm that runs\\nthe lift. These suggestions fall into what I call a solution space: a cluster\\nof solutions that share assumptions about what the problem is—in this\\ncase, that the elevator is slow. This framing is illustrated below.\\nHowever, when the problem is presented to building managers, they\\nsuggest a much more elegant solution: Put up mirrors next to the\\nelevator. This simple measure has proved wonderfully effective in\\nreducing complaints, because people tend to lose track of time when\\ngiven something utterly fascinating to look at—namely, themselves.\\nThe mirror solution is particularly interesting because in fact it is not a\\nsolution to the stated problem: It doesn’t make the elevator faster.\\nInstead it proposes a different understanding of the problem.\\nNote that the initial framing of the problem is not necessarily wrong.\\nInstalling a new lift would probably work. The point of reframing is not\\nto find the “real” problem but, rather, to see if there is a better one to\\nsolve. In fact, the very idea that a single root problem exists may be\\nmisleading; problems are typically multicausal and can be addressed in\\nmany ways. The elevator issue, for example, could be reframed as a peak\\ndemand problem—too many people need the lift at the same time—\\nleading to a solution that focuses on spreading out the demand, such as\\nby staggering people’s lunch breaks.\\nIdentifying a different aspect of the problem can sometimes deliver\\nradical improvements—and even spark solutions to problems that have\\nseemed intractable for decades. I recently saw this in action when\\nstudying an often overlooked problem in the pet industry: the number\\nof dogs in shelters.\\nAmerica’s Dog-Adoption Problem\\nDogs are very popular in America: Industry statistics suggest that more\\nthan 40% of U.S. households have one. But this fondness for dogs has a\\ndownside: According to estimates by the ASPCA, one of the largest\\nanimal-welfare groups in the United States, more than 3 million dogs\\nenter a shelter each year and are put up for adoption.\\nShelters and other animal-welfare organizations work hard to raise\\nawareness of this issue. A typical ad or poster will show a neglected, sad-\\nlooking dog, carefully chosen to evoke compassion, along with a line\\nsuch as “Save a life—adopt a dog” or perhaps a request to donate to the\\ncause. Through this and other initiatives, this notoriously underfunded\\nsystem manages to get about 1.4 million dogs adopted each year. But\\nthat leaves more than a million unadopted dogs—and doesn’t account\\nfor the many cats and other pets in the same situation. There is just a\\nlimited amount of compassion to go around. So despite the impressive\\nefforts of shelters and rescue groups, the shortage of pet adopters has\\npersisted for decades.\\nLori Weise, the founder of Downtown Dog Rescue in Los Angeles, has\\ndemonstrated that adoption is not the only way to frame the problem.\\nWeise is one of the pioneers of an approach that is currently spreading\\nwithin the industry—the shelter intervention program. Rather than seek\\nto get more dogs adopted, Weise tries to keep them with their original\\nfamilies so that they never enter shelters in the first place. It turns out\\nthat about 30% of the dogs that enter a shelter are “owner surrenders,”\\ndeliberately relinquished by their owners. In a volunteer-driven\\ncommunity united by a deep love of animals, those people have often\\nbeen heavily criticized for heartlessly discarding their pets as if they\\nwere just another consumer good. To prevent dogs from ending up with\\nsuch “bad” owners, many shelters, despite their chronic overpopulation,\\nrequire potential adopters to undergo laborious background checks.\\nWeise has a different take. “Owner surrenders are not a people\\nproblem,” she says. “By and large, they are a poverty problem. These\\nfamilies love their dogs as much as we do, but they are also\\nexceptionally poor. We’re talking about people who in some cases aren’t\\nentirely sure how they will feed their kids at the end of the month. So\\nwhen a new landlord suddenly demands a deposit to house the dog, they\\nsimply have no way to get the money. In other cases, the dog needs a $10\\nrabies shot, but the family has no access to a vet, or may be afraid to\\napproach any kind of authority. Handing over their pet to a shelter is\\noften the last option they believe they have.”\\nWeise started her program in April 2013, collaborating with a shelter in\\nSouth Los Angeles. The idea is simple: Whenever a family comes in to\\nhand over a pet, a staff member asks without judgment if the family\\nwould prefer to keep the pet. If the answer is yes, the staff member tries\\nto help resolve the problem, drawing on his or her network and\\nknowledge of the system.\\nWithin the first year it was clear that the program was a remarkable\\nsuccess. In prior years Weise’s organization had spent an average of $85\\nper pet it helped. The new program brought that cost down to about $60\\nwhile keeping shelter space free for other animals in need. And, Weise\\ntold me, that was just the immediate impact: “The wider effect on the\\ncommunity is the real point. The program helps families learn problem\\nsolving, lets them know their rights and responsibilities, and teaches the\\ncommunity that help is available. It also shifted the industry’s\\nperception of the pet owners: We found that when offered assistance, a\\nfull 75% of them actually wanted to keep their pets.”\\nYou won’t know which problems can\\nbenefit from being reframed until you\\ntry.\\nAs of this writing, Weise’s program has helped close to 5,000 pets and\\nfamilies and has gained the formal support of the ASPCA. Weise has\\nreleased a book, First Home, Forever Home, that explains to other rescue\\ngroups how to run an intervention program. Thanks to her reframing of\\nthe problem, overcrowded shelters may someday be a thing of the past.\\nHow might you find a similarly insightful reframing for your problem?\\nSeven Practices for Effective Reframing\\nIn my experience, reframing is best taught as a quick, iterative process.\\nYou might think of it as a cognitive counterpoint to rapid prototyping.\\nThe practices I outline here can be used in one of two ways, depending\\non how much control you have over the situation. One way is to\\nmethodically apply all seven to the problem. That can be done in about\\n30 minutes, and it has the benefit of familiarizing everyone with the\\nmethod.\\nThe other way is suitable when you don’t control the situation and have\\nto scale the method according to how much time is available. Perhaps a\\nteam member ambushes you in the hallway and you have only five\\nminutes to help him or her rethink a problem. If so, simply select the\\none or two practices that seem most appropriate.\\nFive minutes may sound like too little time to even describe a problem,\\nmuch less reframe it. But surprisingly, I have found that such short\\ninterventions are often sufficient to kick-start new thinking—and once\\nin a while they can trigger an aha moment and radically shift your view\\nof a problem. Proximity to your own problems can make it easy to get\\nlost in the weeds, endlessly ruminating about why a colleague, a spouse,\\nor your children won’t listen. Sometimes all you need is someone to\\nsuggest, “Well, could the trouble be that you are bad at listening to\\nthem?”\\nOf course, not all problems are that simple. Often multiple rounds of\\nreframing—interspersed with observation, conversation, and\\nprototyping—are necessary. And in some cases reframing won’t help at\\nall. But you won’t know which problems can benefit from being\\nreframed until you try. Once you’ve mastered the five-minute version,\\nyou can apply reframing to pretty much any problem you face.\\nHere are the seven practices:\\n1. Establish legitimacy.\\nIt’s difficult to use reframing if you are the only person in the room who\\nunderstands the method. Other people, driven by a desire to find\\nsolutions, may feel that your insistence on discussing the problem is\\ncounterproductive. If the group has a power imbalance, such as when\\nyou’re facing clients or more-senior colleagues, they may well shut you\\ndown before you even get started. And even powerful executives may\\nfind it hard to use the method when people are accustomed to getting\\nanswers rather than questions from their leaders.\\nYour first job, therefore, is to establish the method’s legitimacy within\\nthe group, creating the conversational space necessary to employ\\nreframing. I suggest two ways to do this. The first is to share this article\\nwith the people you are meeting. Even if they don’t read it, simply\\nseeing it may persuade them to listen to you. The second is to relate the\\nslow elevator problem, which is my go-to example when I have less than\\n30 seconds to explain the concept. I have found it to be a powerful way\\nto quickly explain reframing—how it differs from merely diagnosing a\\nproblem and how it can potentially create dramatically better results.\\n2. Bring outsiders into the discussion.\\nThis is the single most helpful reframing practice. I saw it in action eight\\nyears ago when the management team of a small European company\\nwas wrestling with a lack of innovation in its workforce. The managers\\nhad recently encountered a specific innovation training technique they\\nall liked, so they started discussing how best to implement it within the\\norganization.\\nSensing that the group lacked an outside voice, the general manager\\nasked his personal assistant, Charlotte, to take part in their discussion.\\n“I’ve been working here for 12 years,” Charlotte told the group, “and in\\nthat time I have seen three different management teams try to roll out\\nsome new innovation framework. None of them worked. I don’t think\\npeople would react well to the introduction of another set of\\nbuzzwords.”\\nCharlotte’s observation prompted the managers to realize that they had\\nfallen in love with a solution—introducing an innovation framework—\\nbefore they fully understood the problem. They soon concluded that\\ntheir initial diagnosis had been wrong: Many of their employees already\\nknew how to innovate, but they didn’t feel very engaged in the\\ncompany, so they were unlikely to take initiative beyond what their job\\ndescriptions mandated. What the managers had first framed as a skill-\\nset problem was better approached as a motivation problem.\\nThey abandoned all talk of innovation workshops and instead focused\\non improving employee engagement by (among other things) giving\\npeople more autonomy, introducing flexible working hours, and\\nswitching to a more participatory decision-making style. The remedy\\nworked. Within 18 months workplace satisfaction scores had doubled\\nand employee turnover had fallen dramatically. And as people started\\nbringing their creative abilities to bear at work, financial results\\nimproved markedly. Four years later the company won an award for\\nbeing the country’s best place to work.\\nAs this story shows, getting an outsider’s perspective can be\\ninstrumental in rethinking a problem quickly and properly. To do so\\nmost effectively:\\nLook for “boundary spanners.” As research by Michael Tushman and\\nmany others has shown, the most useful input tends to come from\\npeople who understand but are not fully part of your world. Charlotte\\nwas close enough to the front lines of the company to know how the\\nemployees really felt, but she was also close enough to management to\\nunderstand its priorities and speak its language, making her ideally\\nsuited for the task. In contrast, calling on an innovation expert might\\nwell have led the team’s members further down the innovation path\\ninstead of inspiring them to rethink their problem.\\nChoose someone who will speak freely. By virtue of her long tenure and\\nher closeness to the general manager, Charlotte felt free to challenge the\\nmanagement team while remaining committed to its objectives. This\\nsense of psychological safety, as Harvard’s Amy C. Edmondson calls it,\\nhas been proved to help groups perform better. You might consider\\nturning to someone whose career advancement will not be determined\\nby the group in question or who has a track record of (constructively)\\nspeaking truth to power.\\nExpect input, not solutions. Crucially, Charlotte did not try to provide\\nthe group with a solution; rather, her observation made the managers\\nthemselves rethink their problem. This pattern is typical. By definition,\\noutsiders are not experts on the situation and thus will rarely be able to\\nsolve the problem. That’s not their function. They are there to stimulate\\nthe problem owners to think differently. So when you bring them in, ask\\nthem specifically to challenge the group’s thinking, and prime the\\nproblem owners to listen and look for input rather than answers.\\n3. Get people’s deﬁnitions in writing.\\nIt’s not unusual for people to leave a meeting thinking they all agree on\\nwhat the problem is after a loose oral description, only to discover weeks\\nor months later that they had different views of the issue. Moreover, a\\nsuccessful reframing may well lurk in one of those views.\\nFor instance, a management team may agree that the company’s\\nproblem is a lack of innovation. But if you ask each member to describe\\nwhat’s wrong in a sentence or two, you will quickly see how framings\\ndiffer. Some people will claim, “Our employees aren’t motivated to\\ninnovate” or “They don’t understand the urgency of the situation.”\\nOthers will say, “People don’t have the right skill set,” “Our customers\\naren’t willing to pay for innovation,” or “We don’t reward people for\\ninnovation.” Pay close attention to the wording, because even seemingly\\ninconsequential word choices can surface a new perspective on the\\nproblem.\\nI saw a memorable demonstration of this when I was working with a\\ngroup of managers in the construction industry, exploring what they\\ncould do as individual leaders to deliver better results. As we tried to\\nidentify the barriers each one faced, I asked them to write their\\nproblems on flip charts, after which we jointly analyzed the statements.\\nThe very first comment from the group had the greatest impact: “Almost\\nnone of the definitions include the word ‘I.’” With one exception, the\\nproblems were consistently worded in a way that diffused individual\\nresponsibility, such as “My team doesn’t…,” “The market doesn’t…,”\\nand, in a few cases, “We don’t…” That one observation shifted the tenor\\nof the meeting, pushing the participants to take more ownership of the\\nchallenges they faced.\\nThese individual definitions of the problem should ideally be gathered\\nin advance of a discussion. If possible, ask people to send you a few lines\\nin a confidential e-mail, and insist that they write in sentence form—\\nbullet points are simply too condensed. Then copy the definitions\\nyou’ve collected on a flip chart so that everyone can see them and react\\nto them in the meeting. Don’t attribute them, because you want to\\nensure that people’s judgment of a definition isn’t affected by the\\ndefiner’s identity or status.\\nReceiving these multiple definitions will sensitize you to the\\nperspectives of other stakeholders. We all appreciate in theory that\\nothers may experience a problem differently (or not see it at all). But as\\ndemonstrated in a recent study by Johannes Hattula, of Imperial College\\nLondon, if managers try to imagine a customer’s perspective\\nthemselves, they typically get it wrong. To understand what other\\nstakeholders think, you need to hear it from them.\\n4. Ask what’s missing.\\nWhen faced with the description of a problem, people tend to delve into\\nthe details of what has been stated, paying less attention to what the\\ndescription might be leaving out. To rectify this, make sure to ask\\nexplicitly what has not been captured or mentioned.\\nRecently I worked with a team of senior executives in Brazil who had\\nbeen asked to provide their CEO with ideas for improving the market’s\\nperception of the company’s stock price. The team had expertly\\nanalyzed the components affecting a stock’s value—the P/E ratio\\nforecast, the debt ratio, earnings per share, and so on. Of course, none of\\nthis was news to the CEO, nor were these factors particularly easy to\\naffect, leading to mild despondency on the team.\\nBut when I prompted the executives to zoom out and consider what was\\nmissing from their definition of the problem, something new came up. It\\nturned out that when external financial analysts asked to speak with\\nexecutives from the company, the task of responding was typically\\ndelegated to slightly more junior leaders, none of whom had received\\ntraining in how to talk to analysts. As soon as this point was raised, the\\ngroup saw that it had found a potential recommendation for the CEO.\\n(The observation came not from the team’s finance expert but from a\\nboundary-spanning HR executive.)\\n5. Consider multiple categories.\\nAs Lori Weise’s story demonstrates, powerful change can come from\\ntransforming people’s perception of a problem. One way to trigger this\\nkind of paradigm shift is to invite people to identify specifically what\\ncategory of problem they think the group is facing. Is it an incentive\\nproblem? An expectations problem? An attitude problem? Then try to\\nsuggest other categories.\\nA manager I know named Jeremiah Zinn did this when he led the\\nproduct development team of the popular children’s entertainment\\nchannel Nickelodeon. The team was launching a promising new app,\\nand lots of kids downloaded it. But actually activating the app was\\nsomewhat complicated, because it required logging in to the\\nhousehold’s cable TV service. At that point in the sign-up process,\\nalmost every kid dropped out.\\nSeeing the problem as one of usability, the team put its expertise to work\\nand ran hundreds of A/B tests on various sign-up flows, seeking to make\\nthe process less complex. Nothing helped.\\nThe shift came when Zinn realized that the team members had been\\nthinking of the problem too narrowly. They had focused on the kids’\\nactions, carefully tracking every click and swipe—but they had not\\nexplored how the kids felt during the sign-up process. That turned out to\\nbe critical. As the team started looking for emotional reactions, it\\ndiscovered that the request for the cable password made the kids fear\\ngetting in trouble: To a 10-year-old kid, a password request signals\\nforbidden territory. Equipped with that insight, Zinn’s team simply\\nadded a short video explaining that it was OK to ask parents for the\\npassword—and saw a rapid 10-fold increase in the sign-up rate for the\\napp.\\nBy explicitly highlighting how the group thinks about a problem—what\\nis sometimes called metacognition, or thinking about thinking—you can\\noften help people reframe it, even if you don’t have other frames to\\nsuggest. And it’s a useful way of sorting through written definitions if\\nyou managed to gather them in advance.\\nZinn’s story also exposes a typical pitfall in problem solving, first\\nexpressed by Abraham Kaplan in his famous law of the instrument:\\nmore\\nPost\\nPost\\nShare\\nSave\\nBuy Copies\\nPrint\\nLatest\\nMagazine\\nAscend\\nTopics\\nPodcasts\\nStore\\nThe Big Idea\\nData & Visuals\\nCase Selections\\nDecision Making And Problem Solving \\xa0\\xa0|\\xa0\\xa0 Are You Solving the Right Problems?\\nSubscribe\\nSign In\\n\\nStart my subscription!\\nExplore HBR\\nThe Latest\\nAll Topics\\nMagazine Archive\\nThe Big Idea\\nReading Lists\\nCase Selections\\nPodcasts\\nWebinars\\nData & Visuals\\nMy Library\\nNewsletters\\nHBR Press\\nHBR Ascend\\nHBR Store\\nArticle Reprints\\nBooks\\nCases\\nCollections\\nMagazine Issues\\nHBR Guide Series\\nHBR 20-Minute Managers\\nHBR Emotional Intelligence Series\\nHBR Must Reads\\nTools\\nAbout HBR\\nContact Us\\nAdvertise with Us\\nInformation for\\nBooksellers/Retailers\\nMasthead\\nGlobal Editions\\nMedia Inquiries\\nGuidelines for Authors\\nHBR Analytic Services\\nCopyright Permissions\\nManage My Account\\nMy Library\\nTopic Feeds\\nOrders\\nAccount Settings\\nEmail Preferences\\nAccount FAQ\\nHelp Center\\nContact Customer Service\\nFollow HBR\\nAbout Us\\n|\\n Careers\\n|\\n Privacy Policy\\n|\\n Cookie Policy\\n|\\n Copyright Information\\n|\\n Trademark Policy\\n|\\n Terms of Use\\nHarvard Business Publishing:  Higher Education\\n|\\n Corporate Learning\\n|\\n Harvard Business Review\\n|\\n Harvard Business School\\nCopyright ©2024\\xa0 Harvard Business School Publishing. All rights reserved. Harvard Business Publishing is an afﬁliate of Harvard Business School.\\nexpressed by Abraham Kaplan in his famous law of the instrument:\\n“Give a small boy a hammer, and he will find that everything he\\nencounters needs pounding.” At Nickelodeon, because the team\\nmembers were usability experts, they defaulted to thinking the problem\\nwas one of usability.\\n6. Analyze positive exceptions.\\nTo find additional problem framings, look to instances when the\\nproblem did not occur, asking, “What was different about that\\nsituation?” Exploring such positive exceptions, sometimes called bright\\nspots, can often uncover hidden factors whose influence the group may\\nnot have considered.\\nA lawyer I spoke to, for instance, told me that the partners at his firm\\nwould occasionally meet to discuss initiatives that might grow their\\nbusiness in the longer term. But to his frustration, the instant one of\\nthose meetings ended, he and the other partners went back to focusing\\non landing the next short-term project. When prompted to think of\\npositive exceptions, he remembered one longer-term initiative that had\\nin fact gone forward.\\nWhat was different about that one? I asked. It was that the meeting,\\nunusually, had included not just partners but also an associate who was\\nconsidered a rising star—and it was she who had pursued the idea. That\\nimmediately suggested that talented associates be included in future\\nmeetings. The associates felt privileged and energized by being invited\\nto the strategic discussions, and unlike the partners, they had a clear\\nshort-term incentive to move on long-term projects—namely, to impress\\nthe partners and gain an edge in the competition against their peers.\\nA checklist for problem diagnosis\\ntends to discourage actual thinking.\\nLooking at positive exceptions can also make the discussion less\\nthreatening. Especially in a large group or other public setting,\\ndissecting a string of failures can quickly become confrontational and\\nmake people overly defensive. If, instead, you ask the group’s members\\nto analyze a positive outcome, it becomes easier for them to examine\\ntheir own behavior.\\n7. Question the objective.\\nIn the negotiation classic Getting to Yes, Roger Fisher, William L. Ury,\\nand Bruce Patton share the early management thinker Mary Parker\\nFollett’s story about two people fighting over whether to keep a window\\nopen or closed. The underlying goals of the two turn out to differ: One\\nperson wants fresh air, while the other wants to avoid a draft. Only when\\nthese hidden objectives are brought to light through the questions of a\\nthird person is the problem resolved—by opening a window in the next\\nroom.\\nThat story highlights another way to reframe a problem—by paying\\nexplicit attention to the objectives of the parties involved, first clarifying\\nand then challenging them. Weise’s shelter intervention program, for\\ninstance, hinged on a shift in the objective, from increasing adoption to\\nkeeping more pets with their original owners. The story of Charlotte,\\ntoo, included a shift in the stated goals of the management team, from\\nteaching innovation skills to boosting employee engagement.\\nAs described in Fred Kaplan’s book The Insurgents, a famous\\ncontemporary example is the change in U.S. military doctrine pioneered\\nby General David Petraeus, among others. In traditional warfare, the aim\\nof a battle is to defeat the enemy forces. But Petraeus and his allies\\nargued that when dealing with insurgencies, the army had to pursue a\\ndifferent, broader objective to prevent new enemies from cropping up—\\nnamely, get the populace on its side, thereby removing the source of\\nrecruits and other forms of local support the insurgency needed to\\noperate in the area. That approach was eventually adopted by the\\nmilitary—because a small group of rogue thinkers took it upon\\nthemselves to question the predefined and long-standing objectives of\\ntheir organization.\\nCONCLUSION\\nPowerful as reframing can be, it takes time and practice to get good at it.\\nOne senior executive from the defense industry told me, “I was shocked\\nby how difficult it is to reframe problems, but also how effective it is.” As\\nyou start to work more with the method, urge your team to trust the\\nprocess, and be prepared for it to feel messy and confusing at times.\\nIn leading more and more reframing discussions, you may also be\\ntempted to create a diagnostic checklist. I strongly caution you against\\nthat—or at least against making the checklist evident to the group you’re\\nengaging with. A checklist for problem diagnosis tends to discourage\\nactual thinking, which of course defeats the very purpose of engaging in\\nreframing. As Neil Gaiman reminds us in The Sandman, tools can be the\\nsubtlest of traps.\\nFinally, combine reframing with real-world testing. The method is\\nultimately limited by the knowledge and perspectives of the people in\\nthe room—and as Steve Blank, of Stanford, and others have repeatedly\\nshown, it is fatal to think you can figure it all out within the comfy\\nconfines of your own office. The next time you face a problem, start by\\nreframing it—but don’t wait too long before getting out of the building\\nto observe your customers and prototype your ideas. It is neither\\nthinking nor testing alone, but a marriage of the two, that holds the key\\nto radically better results.\\nThe ﬁrst appearance in print of the elevator problem, to the best of the author’s\\nknowledge, was in Russell L. Ackoﬀ, “Systems, Organizations, and Interdisciplinary\\nResearch,” General Systems, vol. V (1960).\\nA version of this article appeared in the January–February 2017 issue (pp.76–83) of Harvard\\nBusiness Review.\\nRead more on Decision making\\nand problem solving or related\\ntopics Business communication\\nand Management skills\\nThomas Wedell-Wedellsborg is an\\nindependent consultant and speaker and a\\ncoauthor of Innovation as Usual: How to Help\\nYour People Bring Great Ideas to Life (Harvard\\nBusiness Review Press, March 2013)\\nPost\\nPost\\nShare\\nSave\\nBuy Copies\\nPrint\\nRecommended For You\\nPODCAST\\nThe Secret to Better\\nProblem Solving\\nHow to Make Better\\nDecisions with Less Data\\nHow Structured Debate\\nHelps Your Team Grow\\nTwo Things to Do After\\nEvery Meeting\\nPartner Center\\nFacebook\\n\\ue021\\nX Corp.\\nLinkedIn\\n\\ue022\\nInstagram\\n\\ue085\\nYour Newsreader\\n\\ue083\\nYou have 1 free article left\\nthis month.\\nSubscribe for unlimited access.\\n'}, {'source': '/Users/sir/Downloads/Data/PDF/Improving prediction of heart transplantation outcome using deep learning techniques.pdf', 'text': \"1\\nSCieNTifiC REPOrTS |  (2018) 8:3613  | DOI:10.1038/s41598-018-21417-7\\nwww.nature.com/scientificreports\\nImproving prediction of heart \\ntransplantation outcome using \\ndeep learning techniques\\nDennis Medved1, Mattias Ohlsson2, Peter Höglund\\u200a \\u200a3, Bodil Andersson4, Pierre Nugues1 & \\nJohan Nilsson\\n5\\nThe primary objective of this study is to compare the accuracy of two risk models, International \\nHeart Transplantation Survival Algorithm (IHTSA), developed using deep learning technique, and \\nIndex for Mortality Prediction After Cardiac Transplantation (IMPACT), to predict survival after heart \\ntransplantation. Data from adult heart transplanted patients between January 1997 to December \\n2011 were collected from the UNOS registry. The study included 27,860 heart transplantations, \\ncorresponding to 27,705 patients. The study cohorts were divided into patients transplanted before \\n2009 (derivation cohort) and from 2009 (test cohort). The receiver operating characteristic (ROC) \\nvalues, for the validation cohort, computed for one-year mortality, were 0.654 (95% CI: 0.629–0.679) \\nfor IHTSA and 0.608 (0.583–0.634) for the IMPACT model. The discrimination reached a C-index for \\nlong-term survival of 0.627 (0.608–0.646) for IHTSA, compared with 0.584 (0.564–0.605) for the IMPACT \\nmodel. These figures correspond to an error reduction of 12% for ROC and 10% for C-index by using \\ndeep learning technique. The predicted one-year mortality rates for were 12% and 22% for IHTSA \\nand IMPACT, respectively, versus an actual mortality rate of 10%. The IHTSA model showed superior \\ndiscriminatory power to predict one-year mortality and survival over time after heart transplantation \\ncompared to the IMPACT model.\\nHeart transplantation (HT) is a life-saving operation for patients with end-stage heart disease. Despite this reality, \\nthe transplantation number does not increase over the years. One of the most limiting factors is the lack of donor \\norgans and a conservative allocation policy that results in the loss of about half of the organs being offered1. An \\nimproved prediction of the outcome would augment the confidence in the post-transplantation performance and \\nmake it possible to optimise the allocation of organs. Furthermore, it would enable practitioners to determine the \\nrisk of early and late graft dysfunction more accurately and improve donor and recipient management.\\nAlthough there exist several survival models within cardiac surgery, currently there is no accepted tool for \\nestimating the outcome after heart transplantation. In recent years, some risk score algorithms designed to pre-\\ndict post-transplantation performance have been developed, which almost all have been derivate on the sin-\\ngle national, multi institutional United Network for Organ Sharing (UNOS) registry2. The most notable ones \\nare: Donor Risk Index (DRI), Risk Stratification Score (RSS), and Index for Mortality Prediction After Cardiac \\nTransplantation (IMPACT)3–5. The IMPACT model has additionally been validated on the International Society \\nof Heart and Lung Transplantation (ISHLT) registry and showed an acceptable accuracy in predicting mortality. \\nRecently a multinational model, the International Heart Transplantation Survival Algorithm (IHTSA), devel-\\noped on the ISHLT registry was published6. This model was designed to predict both short-term and long-term \\nmortality and, in contrast to previous models, it utilises deep learning techniques. The results it obtained showed \\nan improved discrimination compared with the DRI, RSS, and IMPACT models. However, the validation was \\nperformed on the ISHLT registry, which was also used for the development of the model6.\\nEven if the validation cohort was separated from the derivation cohort, the IHTSA model might be biased \\ntowards this registry.\\n1Department of Computer Science, Lund University, Lund, Sweden. 2Department of Astronomy and Theoretical \\nPhysics, Computational Biology and Biological Physics, Lund University, Lund, Sweden. 3Department of \\nLaboratory Medicine Lund, Clinical Chemistry and Pharmacology, Lund University, Lund, Sweden. 4Department \\nof Clinical Sciences Lund, Surgery, Lund University and Skåne University Hospital, Lund, Sweden. 5Department \\nof Clinical Sciences Lund, Cardiothoracic Surgery, Lund University and Skåne University Hospital, Lund, Sweden. \\nCorrespondence and requests for materials should be addressed to J.N. (email: johan.nilsson@med.lu.se)\\nReceived: 10 October 2017\\nAccepted: 1 February 2018\\nPublished: xx xx xxxx\\nOPEN\\n\\nwww.nature.com/scientificreports/\\n2\\nSCieNTifiC REPOrTS |  (2018) 8:3613  | DOI:10.1038/s41598-018-21417-7\\nThe aim of this study was to determine the most suitable risk stratification model for heart transplantation by \\napplying the IMPACT and IHTSA algorithms to the UNOS registry.\\nResults\\nCharacteristics of the Study Population.\\u2003\\nThe preoperative characteristics of the recipients are listed in \\nTable\\xa01 and for the donors in Table\\xa02. The number of adult HT with a follow-up time of at least one year, from \\nJanuary 1997 to December 2011, was of 27,860, corresponding to 27,705 patients. Over the time span, the cumu-\\nlative sum of follow-up years was of 165,206. The median survival time was 12 years (Interquartile Range [IQR]: \\n5–16). The one-year mortality was of 13% (n\\u2009=\\u20093,561). The average age of the recipients was 52\\u2009±\\u200913 years, with \\na range from 18 to 78 years. Most of the recipients were males 76% (n\\u2009=\\u200921,151). Multi-organ transplants were \\nmarginal (2.5%). The number of transplants contained in the derivation cohort was of 22,263, and the number of \\ntransplants in the test cohort was of 5,597.\\nIMPACT versus IHTSA.\\u2003\\nThe IHTSA model includes 32 recipient risk variables, while the IMPACT model \\nhas 18 variables; five of these variables are shared between the models: female gender, diagnosis: ischemic cardi-\\nomyopathy, diagnosis: congenital, infection within two weeks, and mechanical ventilation. Additionally, IHTSA \\nalso has 11 donor variables, while IMPACT has no donor variables.\\nWe evaluated the original IHTSA model in the test cohort (2009–2011) for one-year mortality; it had an area \\nunder receiver operating characteristic (AUROC) of 0.643 (95% CI: 0.619–0.667), while IMPACT had an AUROC \\nof 0.608 (0.583–0.634), P\\u2009=\\u20090.004, see Table\\xa03. As shown in Fig.\\xa01 and Table\\xa03, the recalibrated IHTSA model has \\na significantly higher discrimination compared with the IMPACT model for one-year mortality, P\\u2009=\\u20090.001, corre-\\nsponding to an error reduction of 11.7%. Harrell’s C-index for the recalibrated IHTSA compared with IMPACT \\nwas substantially larger, as shown in Table\\xa04, with about a 4% absolute difference for the later time era. This corre-\\nsponds to an error reduction of 10.3%. On the time era 1997–2008, on which the models were trained using 5-fold \\ncross-validation technique, the recalibrated IHTSA had an AUROC of 0.688 (0.678–0.699), and IMPACT had \\n0.606 (0.595–0.617) for one-year mortality, P\\u2009=\\u20090.001, Table\\xa03. The absolute difference in C-index was 5% higher \\nfor the IHTSA model compared with the IMPACT model, P\\u2009<\\u20090.001, Table\\xa04.\\nWe analysed the sensitivity of both models relatively to the deceased patients after one year at the levels of \\n25%, 50%, and 75%. Out of the transplants in the test cohort (N\\u2009=\\u20095,597), the numbers of correctly classified \\npatients after one year were 4,812, 3,890, and 2,582 patients respectively for IHTSA, and 4,539, 3,396, and 2,140 \\npatients respectively for IMPACT. See Fig.\\xa02 for a graph of the difference in correctly classified patients.\\nWe furthermore compared the predicted one-year mortality rate for IMPACT and IHTSA, with the true mor-\\ntality rate. The predicted one-year mortality for the second time-era (test cohort) was 12% and 22% for the recal-\\nibrated IHTSA and IMPACT, respectively, versus an actual mortality rate of 10%. The Hosmer-Lemeshow (HL) \\nchi-square for one-year, using ten groups, was of 40 in the IHTSA model and 101 for the IMPACT model, both \\nwith a P-value less than 0.05. As shown in the calibration plot, Fig.\\xa03, the predictive mortality compared with \\nactual mortality was more consistent over all deciles for the ITHSA model compared with the IMPACT model.\\nTo evaluate difference in methodology approach (deep learning versus logistic regression), we performed \\ntwo additional experiments. We quantify the difference between the deep learning technique used by the IHTSA \\nmodel and the more traditional logistic regression approach used by the IMPACT model, by letting the two sys-\\ntems use identical features. The second experiment was to assess the difference between a model that include and \\nexclude donor variables.\\nAs shown in Tables\\xa05 and 6, a recalibrated IHTSA model including only the same risk variables as the IMPACT \\nmodel still showed a substantial improvement in the AUROC (about 2%) and C-index in the test cohort com-\\npared with the IMPACT model. The recalibrated IHTSA model excluding the donor variables showed a decrease \\nin discrimination compared with the original IHTSA model, however the difference was minor, producing nearly \\nthe same AUROC.\\nDiscussion\\nThe purpose of this study was to compare the IMPACT and IHTSA models with regards to the prediction accu-\\nracy of one-year mortality on the UNOS database. There exist some biases in both models when used on the \\nUNOS data set for the time era 1997–2008. Because IMPACT was developed on these data and IHTSA on the \\nISHLT dataset, which consists in part of the same UNOS data, the models may be subjected to a non-negligible \\noverfit to the data, skewing the result towards a more positive value. Therefore, we chose to validate the models on \\na later time era, which has no overlapping patients with the training set.\\nThe results show that the IHTSA model exhibited improved performance and accuracy compared to the \\nIMPACT model. Even though IMPACT was designed to predict one-year mortality and IHTSA was created for \\nlong-term survival, IHTSA shows better discrimination on one-year mortality.\\nThis study could also prove the benefits of using deep learning modelling techniques. Such techniques are \\ninspired by the human brain. They consist of a network of “neurons” that emulate the properties of their real \\ncounterparts. Using multiple processing layers makes it possible to learn representations of data with multiple \\nlevels of abstraction7. These methods have improved the state-of-the-art in speech recognition, visual object rec-\\nognition, object detection and many other domains8.\\nOur results show that the IHTSA model can be applied to predict short-term mortality with greater accuracy \\nthan a more traditional risk-based model based on logistic regression. Although the comparison of ROC curves \\nto evaluate models in a statistically valid manner is controversial, the ROC curve is currently the most developed \\nstatistical tool for describing performance9,10. The improvements seen can be explained by the difference in the \\nvariable selection, such as the absence of donor risk factors in the IMPACT model, but also by the the neural net-\\nwork’s ability to handle interactions between variables and nonlinearities. An increased donor age has in previous \\n\\nwww.nature.com/scientificreports/\\n3\\nSCieNTifiC REPOrTS |  (2018) 8:3613  | DOI:10.1038/s41598-018-21417-7\\nFeature\\nN\\nTime era \\n1997–2008\\nTime era \\n2009–2011\\np-Value\\nIMPACT\\nIHTSA\\n(n\\u2009=\\u200922,263)\\n(n\\u2009=\\u20095,597)\\nDemographic data\\n\\xa0\\xa0Age (years)\\n27,860\\n52\\u2009±\\u200913\\n53\\u2009±\\u200913\\n0.001\\n✓\\n\\xa0\\xa0Age >60 years\\n27,860\\n5,707 (26%)\\n1,809 (32%)\\n0.001\\n✓\\n\\xa0\\xa0Female gender\\n27,860\\n5,298 (24%)\\n1,411 (25%)\\n0.029\\n✓\\n✓\\n\\xa0\\xa0Height (cm)\\n27,740\\n174\\u2009±\\u200910\\n174\\u2009±\\u200910\\n0.835\\n✓\\n\\xa0\\xa0Weight (kg)\\n27,760\\n80\\u2009±\\u200917\\n82\\u2009±\\u200917\\n0.001\\n✓\\n\\xa0\\xa0Race: African American\\n27,860\\n3,324 (15%)\\n1,103 (20%)\\n0.001\\n✓\\nDiagnosis\\n\\xa0\\xa0Ischemic cardiomyopathy\\n27,859\\n9,976 (45%)\\n2,793 (50%)\\n0.001\\n✓\\n✓\\n\\xa0\\xa0Non-ischemic cardiomyopathy\\n27,859\\n10,247 (46%)\\n2,119 (38%)\\n0.001\\n✓\\n\\xa0\\xa0Congenital\\n27,859\\n518 (2%)\\n149 (3%)\\n0.159\\n✓\\n✓\\n\\xa0\\xa0Other\\n27,859\\n852 (3%)\\n247 (4%)\\n0.001\\n✓\\n\\xa0\\xa0Graft failure\\n27,859\\n669 (3%)\\n197 (4%)\\n0.058\\n✓\\n\\xa0\\xa0Diabetes mellitus#\\n27,597\\n4,735 (22%)\\n1,500 (27%)\\n0.001\\n✓\\n\\xa0\\xa0Hypertension†\\n17,876\\n7,108 (40%)\\n—\\n✓\\n\\xa0\\xa0Infection within two weeks‡\\n26,543\\n2,333 (11%)\\n594 (11%)\\n0.550\\n✓\\n✓\\n\\xa0\\xa0Antiarrhythmic drugs prior transplant\\n17,266\\n6,371 (37%)\\n—\\n✓\\n\\xa0\\xa0Amiodarone prior to transplant\\n17,530\\n4,726 (27%)\\n—\\n✓\\n\\xa0\\xa0Dialysis prior to transplant\\n27,002\\n706 (3%)\\n185 (3%)\\n0.510\\n✓\\n\\xa0\\xa0Previous blood transfusion\\n15,221\\n5,285 (35%)\\n27 (29%)\\n0.247\\n✓\\n\\xa0\\xa0Previously transplanted*\\n27,860\\n680 (3%)\\n199 (4%)\\n0.067\\n✓\\n\\xa0\\xa0Previous cardiac surgery\\n14,069\\n1,866 (22%)\\n1,483 (27%)\\n0.001\\n✓\\n\\xa0\\xa0ICU\\n27,860\\n7,991 (36%)\\n1,493 (27%)\\n0.001\\n✓\\n\\xa0\\xa0Mechanical ventilation\\n27,860\\n625 (3%)\\n166 (3%)\\n0.532\\n✓\\n✓\\n\\xa0\\xa0ECMO\\n27,860\\n90 (0.04%)\\n48 (1%)\\n0.001\\n✓\\n\\xa0\\xa0IABP\\n27,860\\n1193 (5%)\\n263 (5%)\\n0.039\\n✓\\n✓\\n\\xa0\\xa0Ventricular assist device\\n24,357\\n4,665 (25%)\\n2,191 (39%)\\n0.001\\n✓\\n\\xa0\\xa0Early generationa\\n6,856\\n911 (20%)\\n114 (5%)\\n0.001\\n✓\\n\\xa0\\xa0Late generationb\\n6,856\\n536 (11%)\\n1,610 (74%)\\n0.001\\n✓\\n\\xa0\\xa0Other/Unknown\\n6,856\\n3,218 (69%)\\n467 (21%)\\n0.001\\n\\xa0\\xa0Temporary circulatory supportc\\n27,860\\n209 (1%)\\n113 (2%)\\n0.001\\n✓\\nTransplant era\\n\\xa0\\xa01996–2000\\n27,860\\n7781 (35%)\\n—\\n✓\\n\\xa0\\xa02001–2005\\n27,860\\n8981 (40%)\\n—\\n✓\\n\\xa0\\xa0>2005\\n27,860\\n5501 (25%)\\n5,598 (100%)\\n0.001\\n✓\\nHemodynamic status\\n\\xa0\\xa0PVR (wood units)\\n21,782\\n2.5\\u2009±\\u20091.8\\n2.4\\u2009±\\u20091.8\\n0.205\\n✓\\n\\xa0\\xa0SPP (mmHg)\\n25,100\\n43\\u2009±\\u200914\\n42\\u2009±\\u200914\\n0.001\\n✓\\nLaboratory values\\n\\xa0\\xa0Creatinine (mg/dl)\\n27,027 1\\n1.4\\u2009±\\u20090.8\\n1.3\\u2009±\\u20090.8\\n0.038\\n✓\\nCreatinine clearance (mL/min)\\n\\xa0\\xa030–49\\n27,054\\n2,964 (14%)\\n698 (12%)\\n0.008\\n✓\\n\\xa0\\xa0<30\\n27,054\\n674 (3%)\\n189 (3%)\\n0.376\\n✓\\n\\xa0\\xa0Serum bilirubin (mg/dl)\\n26,224\\n1.3\\u2009±\\u20092\\n1.2\\u2009±\\u20092\\n0.001\\n✓\\n\\xa0\\xa01.00–1.99\\n26,224\\n6,117 (30%)\\n1,562 (28%)\\n0.102\\n✓\\n\\xa0\\xa02.00–3.99\\n26,224\\n1261 (6%)\\n300 (5%)\\n0.070\\n✓\\n\\xa0\\xa0≥4\\n26,224\\n1314 (6%)\\n297 (5%)\\n0.007\\n✓\\nImmunology status\\n\\xa0\\xa0PRA\\u2009>\\u200910%\\n18,351\\n1,113 (8%)\\n1,114 (20%)\\n0.001\\n✓\\n\\xa0\\xa0HLA-DR, 2 mismatch\\n23,858\\n10,289 (55%)\\n2,746 (55%)\\n0.906\\n✓\\nRecipient blood group\\n\\xa0\\xa0A\\n27,860\\n9,543 (43%)\\n2,313 (41%)\\n0.036\\n✓\\n\\xa0\\xa0B\\n27,860\\n3,040 (14%)\\n795 (14%)\\n0.343\\n✓\\n\\xa0\\xa0AB\\n27,860\\n1,143 (5%)\\n295 (5%)\\n0.597\\n✓\\n\\xa0\\xa0O\\n27,860\\n8,549 (38%)\\n2,198 (39%)\\n0.092\\n✓\\n\\nwww.nature.com/scientificreports/\\n4\\nSCieNTifiC REPOrTS |  (2018) 8:3613  | DOI:10.1038/s41598-018-21417-7\\nreports been shown to have a negative influence on short-term survival6,11. To examine this, we compared the \\ndifference of the deep learning model and the logistic regression model using the same variables. Here, we show \\na substantial improvement when using the deep learning approach compared with the traditional approach. \\nFurthermore, we could show that the predictive availability for the deep learning model was less dependent on \\nthe variables included compared with a standard model. Donor variables showed to be of less importance than \\nexpected. A possible explanation for that may be the deep learning technology has an increased ability to identify \\nnew patterns with the data it has available. It is interesting to note that the two models do not show a considerable \\noverlap of features. Only five features are shared by the two models out of 18 for IMPACT and 43 for IHTSA. If we \\ncompare the overlapping variables with the seven most important variables for IHTSA, we find that three of them \\nare shared: age, diagnosis, and mechanical ventilation6.\\nOne disadvantage of the deep learning technique is that it yields a black box model with a limited ability to \\nexplicitly identify possible causal relationships. Logistic regression, on the contrary, makes it feasible to determine \\nthe strongly predictive variables based on the size of the coefficients. To cope with the lack of a well-established \\nTable 1.\\u2002 The recipient features used in the IMPACT and IHTSA Models. N, number of transplants with non-\\nmissing values. n, total number of transplants. Qualitative data are expressed as n (%), and quantitative data \\nas mean\\u2009±\\u2009SD. #Drug or insulin treated diabetes mellitus. †Drug treated systemic hypertension. ‡Infection \\nrequiring intravenous antibiotic therapy within two weeks prior to transplant. *Previous transplant—previous \\nkidney, liver, pancreas, pancreas islet cells, heart, lung, intestine and/or bone marrow transplant. aEarly \\ngeneration includes para and intracorporeal pulsatile VADs: Abiomed AB5000, Heartmate I, XE, and XVE, \\nThortecIVAD, Toyobo, Medos and LionHeart. bLater generation continuous VADs including Heartmate \\nII, Jarvik, Micromed, Debakey, and VentrAssist. cIncludes ECMO and [or] extracorporeal VADs: Abiomed \\nBVS5000, Bio-Medicus, TandemHeart, and Levitronix/Centrimag. ECMO, extracorporeal membrane \\noxygenation; ICU, intensive care unit; IHTSA, international heart transplantation survival algorithm; IMPACT, \\nindex for mortality prediction after cardiac transplantation; HLA, human leukocyte antigen; PRA, panel \\nreactive antibody; PVR, pulmonary vascular resistance; SD, standard deviation; SPP, systolic pulmonary \\npressure. The t-test and chi-squared test was used for continuous respectively categorical values.\\nFeature\\nN\\nTime era \\n1997–2008\\nTime era \\n2009–2011\\np-Value\\nIMPACT\\nIHTSA\\n(n\\u2009=\\u200922,263)\\n(n\\u2009=\\u20095,597)\\nDemographic data\\n\\xa0\\xa0Age (years)\\n27,075\\n32\\u2009±\\u200912\\n32\\u2009±\\u200912\\n0.515\\n✓\\n\\xa0\\xa0Female gender\\n27,860\\n6,546 (29%)\\n1,645 (29%)\\n0.979\\n✓\\n\\xa0\\xa0Weight (kg)\\n27,838\\n79\\u2009±\\u200919\\n82\\u2009±\\u200919\\n0.001\\n✓\\n\\xa0\\xa0Duration of ischemia (min)\\n26,029\\n189\\u2009±\\u200963\\n194\\u2009±\\u200910\\n0.001\\n✓\\n\\xa0\\xa0CODD: Head Trauma\\n27,825\\n13,733 (62%)\\n3,068 (55%)\\n0.001\\n✓\\n\\xa0\\xa0CODD: Cerebrovascular event\\n27,825\\n5,894 (27%)\\n1,297 (23%)\\n0.001\\n✓\\nDonor blood group\\n\\xa0\\xa0A\\n27,859\\n8,232 (37%)\\n1,983(35%)\\n0.030\\n✓\\n\\xa0\\xa0B\\n27,859\\n2,284 (10%)\\n617 (11%)\\n0.102\\n✓\\n\\xa0\\xa0AB\\n27,859\\n477 (2%)\\n125 (2%)\\n0.682\\n✓\\n\\xa0\\xa0O\\n27,859\\n11269 (40%)\\n2,873 (51%)\\n0.001\\n✓\\n\\xa0\\xa0Recipient-donor weight ratio\\n27,739\\n1.03\\u2009±\\u20090.22\\n1.02\\u2009±\\u20090.20\\n0.001\\n✓\\n\\xa0\\xa0Recipient-donor height ratio\\n27,660\\n0.998\\u2009±\\u20090.06\\n0.999\\u2009±\\u20090.06\\n0.068\\n✓\\nTable 2.\\u2002 The donor features used in the IHTSA model. N, number of transplants with non-missing values. \\nn, total number of transplants. Qualitative data are expressed as n (%), and quantitative data as mean\\u2009±\\u2009SD. \\nCODD, cause of donor death; IHTSA, international heart transplantation survival algorithm; IMPACT, index \\nfor mortality prediction after cardiac transplantation. The t-test and chi-squared test was used for continuous \\nrespectively categorical values.\\nTime era\\nAUROC (95% CI)\\nIMPACT\\nIHTSA\\nP-Value\\nIHTSA cal.\\nP-Value\\n1997–2008\\n0.61 (0.59–0.62)\\n0.66 (0.64–0.67)\\n0.001\\n0.69 (0.68–0.70)\\n0.001\\n2009–2011\\n0.61 (0.58–0.63)\\n0.64 (0.62–0.67)\\n0.004\\n0.65 (0.63–0.68)\\n0.001\\nTable 3.\\u2002 The AUROC for one-year mortality for the different cohorts using IMPACT and IHTSA respectively. \\nAUROC, area under the receiver-operating curve; CI, confidence interval; IHTSA, international heart \\ntransplantation survival algorithm; cal, the recalibrated version; IMPACT, index for mortality prediction after \\ncardiac transplantation.; P, probability that the result is the same as IMPACT.\\n\\nwww.nature.com/scientificreports/\\n5\\nSCieNTifiC REPOrTS |  (2018) 8:3613  | DOI:10.1038/s41598-018-21417-7\\nmethod for interpreting the weights of a connection matrix in a neural network, the developers of the IHTSA \\nalgorithm used a classification and regression tree (CART), fitted to the predicted median survival time, to assess \\nthe relative importance of the features6. Furthermore, the web-based calculator (http://ihtsa.cs.lth.se) makes it \\npossible to estimate the survival on a computer or mobile device.\\nDuring 2011, approximately 17,000 donors were reported12. Unfortunately, not more than one-third of all \\ndonors could be utilised for heart transplantation. One explanation for this may be the uncertainty in the risk \\nof early and late graft dysfunction, which means that some suitable donors are not accepted. Although there are \\nmany donor predictors of allograft discard in the current era, these characteristics seem to have little effect on \\nrecipient outcomes when the hearts are transplanted, which also is confirmed in this study13. A more liberal use of \\nFigure 1.\\u2002 The ROC curves show the sensitivity of prediction of one-year mortality vs. 1-specificity for the \\nIMPACT (short-long dashed line) and the recalibrated IHTSA (solid line) risk algorithms is plotted on the test \\ncohort (2009–2011). The gray dashed line represents the absence of discrimination.\\nTime era\\nC-index (95% CI)\\nIMPACT\\nIHTSA\\nP-Value\\nIHTSA cal.\\nP-Value\\n1997–2008\\n0.56 (0.56–0.56)\\n0.59 (0.59–0.60)\\n0.001\\n0.62 (0.61–0.62)\\n0.001\\n2009–2011\\n0.58 (0.56–0.61)\\n0.61 (0.59–0.63)\\n0.002\\n0.63 (0.61–0.65)\\n0.001\\nTable 4.\\u2002 The Harrells C-index for survival for the different cohorts using IMPACT and IHTSA respectively. CI, \\nconfidence interval; IHTSA, international heart transplantation survival algorithm; cal, the recalibrated version; \\nIMPACT, index for mortality prediction after cardiac transplantation; P, probability that the result is the same as \\nIMPACT.\\nFigure 2.\\u2002 The sensitivity of prediction of one-year mortality versus the total number of additional correctly \\nclassified patients by IHTSA compared with IMPACT, both in absolute numbers and percentage, plotted on the \\ntest cohort (2009–2011).\\n\\nwww.nature.com/scientificreports/\\n6\\nSCieNTifiC REPOrTS |  (2018) 8:3613  | DOI:10.1038/s41598-018-21417-7\\ncardiac allografts with relative contraindications may be warranted. A calculator would allow us to conveniently \\nperform batch estimation of survival for multiple patients at the same time. This would allow the IHTSA model \\nto be used as a virtual recipient-donor matching tool that models survival for potential recipients on a waiting \\nlist when there is a donor heart available. This could potentially increase the number of organs that could be used \\ncompared with a traditional criterion-based model6. Additionally, it will make it easier for other research groups \\nto validate the model.\\nThe results of this study carry limitations associated with the retrospective analysis of a registry database, the \\nquality of the source data, the number of missing data, and the lack of standardization associated with multi-\\ncenter studies (such as different immunosuppressive regimens and different matching criteria). However, those \\nlimitations are the same for both models. Even if a comparison of risk models remains controversial, the C-index \\nis probably the best statistical tool for describing performance. A C-index of <0.7 may seem low, but it should \\nbe kept in mind that the IHTSA model predicts long term survival, and to the best of our knowledge, it is higher \\nthan previously reported studies.\\nConclusions\\nIn this study, we have shown that a flexible nonlinear artificial neural network model (IHTSA), utilising deep \\nlearning techniques, exhibits better discrimination and accuracy than a more traditional risk score model \\n(IMPACT) for predicting one-year mortality. We made public the results of this model in the form of a web-based \\nFigure 3.\\u2002 The observed (gray bars) and expected mortality (black bars), in percent, for each decile, for the \\nIMPACT and IHTSA models, in the test cohort (2009–2011). The patients are divided into deciles according to \\ntheir expected mortality, and the observed mortality was derived for each decile.\\nTime era\\nAUROC (95% CI)\\nIMPACT\\nANN I\\nP-Value\\nANN II\\nP-Value\\n2009–2011\\n0.61 (0.58–0.63)\\n0.63 (0.60–0.65)\\n0.027\\n0.65 (0.63–0.68)\\n0.001\\nTable 5.\\u2002 The AUROC for one-year mortality for the test cohort (2009–2011) using an artificial neural network \\nmodel derived on the derivation cohort (1997–2008) with IMPACT features only (ANN I) and with IHTSA \\nrecipient features only (ANN II). AUROC, area under the receiver-operating curve; CI, confidence interval; \\nIHTSA, international heart transplantation survival algorithm; IMPACT, index for mortality prediction after \\ncardiac transplantation.; P, probability that the result is the same as IMPACT.\\nTime era\\nC-index (95% CI)\\nIMPACT\\nANN I\\nP-Value\\nANN II\\nP-Value\\n2009–2011\\n0.58 (0.56–0.61)\\n0.60 (0.58–0.62)\\n0.002\\n0.62 (0.60–0.64)\\n0.001\\nTable 6.\\u2002 The Harrells C-index for one-year mortality for the test cohort (2009–2011) using an artificial neural \\nnetwork model derived on the derivation cohort (1997–2008) with IMPACT features only (ANN I) and with \\nIHTSA recipient features only (ANN II). CI, confidence interval; IHTSA, international heart transplantation \\nsurvival algorithm; IMPACT, index for mortality prediction after cardiac transplantation.; P, probability that the \\nresult is the same as IMPACT.\\n\\nwww.nature.com/scientificreports/\\n7\\nSCieNTifiC REPOrTS |  (2018) 8:3613  | DOI:10.1038/s41598-018-21417-7\\nbatch calculator that could be used as a virtual recipient-donor matching tool. This is a first step in the imple-\\nmentation of a deep learning architecture for transplantation data that, we hope, will pave the way for further \\nimprovements and an even more accurate model.\\nMaterials and Methods\\nData Source.\\u2003\\nThe data set of heart transplant patients was obtained from the UNOS database. UNOS is a \\nnon-profit organisation that administers the only Organ Procurement and Transplantation Network (OPTN) in \\nthe United States of America14. The database contains data from October 1, 1987, onwards and includes almost \\n500 variables that encompass recipient, donor, and transplant information. It consists of both deceased- and liv-\\ning-recipient transplants. The Ethics Committee for Clinical Research at Lund University, Sweden approved the \\nstudy protocol. The data was anonymized and de-identified prior to analysis and the institutional review board \\nwaived the need for written informed consent from the participants.\\nStudy Population.\\u2003\\nWe included all the adult HT patients (>17 years) from January 1997 to December 2011. \\nThe latest annual follow-up was on September 30, 2013. The data set was divided into two temporal cohorts: \\ntransplantation done before 2009 (derivation cohort) and after or during 2009 (test cohort). These time periods \\nwere chosen because both IMPACT and IHTSA were developed on patients between 1997–2008 and we wanted \\ndisjoint sets (derivation and test) to evaluate the prediction performance. The number of variables extracted from \\nthe database was 56 in total, where IHTSA uses 43 of them and IMPACT 18. The primary endpoint was one-year \\nmortality and the second endpoint was all-cause cumulative mortality during the study period.\\nStoring the Data.\\u2003\\nWe converted the complete UNOS database containing heart transplants until 2011, \\nexcept a few variables, into a Resource Description Framework (RDF) database following the procedure outlined \\nin a previously published report15. This enabled us to use the SPARQL language to query the data and easily \\nretrieve the variables used by both the IMPACT and IHTSA model to predict the mortality of the transplants16.\\nStatistical Analysis.\\u2003\\nWe performed the statistical analyses using the Stata MP statistical package version \\n13 (2013) (StataCorp LP, College Station, TX), and with RStudio Desktop 0.99.441 (RStudio, Boston, MA) using \\nR version 3.3.1. Data are presented as means with standard deviation (SD), and frequency as appropriate. The \\nAnderson-Darling test was used to assess the normality of the variables17. We used the t-test and chi-squared test \\nfor continuous, respectively categorical values, to test if the data was significantly different from each other. As \\nwith all patient registries, the dataset contains missing values. We applied a probability imputation technique by \\ncreating a list for each variable in the data set, containing the non-missing values for that variable, and then we \\nimputed each missing value with a value from the list, chosen from a uniform distribution18. In consequence, the \\ndistribution of the imputed values should follow that of the non-missing ones.\\nThe discriminatory power for one-year mortality was assessed by calculating the AUROC19. We compared \\nthe statistical significance of the difference between the AUROC of the two models using the non-parametric \\nDeLong’s test20. To evaluate the discrimination for long-term survival of the patients, we utilised the Harrell’s \\nconcordance index (C-index)21. We used a z-score test to compare the C-indexes22. The AUROC and C-index \\nvalues are both presented with 95% confidence limits. The predictive accuracy of the models was assessed by \\ncomparing the observed and expected mortality for equal-sized quantiles of risk by using the Hosmer–Lemeshow \\ngoodness-of-fit test23.\\nThe IMPACT model.\\u2003\\nIMPACT was created with a data set of heart transplant patients between 1997 to 2008 \\nthat were collected from the UNOS database. IMPACT only utilises recipient variables. Creatinine clearance was \\nnot directly available from the data set and had to be calculated using the Cockcroft-Gault equation24. By appor-\\ntioning points according to the relative importance of the variables for the one-year mortality, a risk index was \\ncreated. The minimum number of scoring points a patient can have is 0 and the maximum is 50. The points are \\nafter that converted to a predicted probability of one-year mortality by a formula derived from logistic regression5.\\nThe IHTSA model.\\u2003\\nThe data set used in developing IHTSA was extracted from the ISHLT containing HT \\npatients who were transplanted between 1994 and 2010. IHTSA utilises both recipient and donor variables. The \\nsurvival model consists of a flexible nonlinear generalisation of the standard Cox proportional hazard model. \\nInstead of using a single prediction model, this model integrates ensembles of artificial neural networks (ANNs). \\nIn addition, its prediction capability is not limited to one year6.\\nHowever, the variables hypertension and antiarrhythmic drugs are not recorded in the UNOS database from \\n2007 and onward. To handle this problem, we first imputed them with random values taken from the earlier \\ntime era. Secondly, we excluded these two variables, and retrained (calibrated) the neural network, utilizing a \\n5-fold cross validation of the patients between 1997 and 2008 in UNOS. The same training procedure was used as \\ndescribed in the original IHTSA article, but we did not carry out any new variable selection6. We called this model \\nthe recalibrated IHTSA model.\\nWeb-Based IHTSA Calculator.\\u2003\\nThe IHTSA model is available via a web application (ihtsa.cs.lth.se), where \\na user can either input a single patient’s data or submit a file of multiple patients in a batch calculator. To com-\\npute the results, the user then selects one of the two prediction models developed either on UNOS or IHSLT \\ndata, corresponding to American or international patients respectively. The submitted file should consist of \\ncomma-separated values (CSV) reflecting the patient data in a table format. The batch calculator uses this data \\nto predict one-, five-, and ten-year survival respectively and median survival time. Once processed, the result \\n\\nwww.nature.com/scientificreports/\\n8\\nSCieNTifiC REPOrTS |  (2018) 8:3613  | DOI:10.1038/s41598-018-21417-7\\nconsisting of relevant survival and mortality numbers is either emailed back to the user in a CSV format, in the \\ncase of the batch calculator, or presented directly in the web interface.\\nThe applications were implemented as a Java program, for the graphical user interface part and a Matlab (ver-\\nsion 2010A and 2015b) application for running the survival models.\\nData availability.\\u2003\\nThe data that support the findings of this study are available from UNOS but restrictions \\napply to the availability of these data, which were used under license for the current study, and so are not publicly \\navailable.\\nReferences\\n\\t 1.\\t Klein, A. S. et al. Organ donation and utilization in the United States, 1999–2008. Am J Transplant 10, 973–986, https://doi.\\norg/10.1111/j.1600-6143.2009.03008.x (2010).\\n\\t 2.\\t Nilsson, J., Algotsson, L., Höglund, P., Lührs, C. & Brandt, J. Comparison of 19 pre-operative risk stratification models in open-heart \\nsurgery. Eur Heart J 27, 867–874, https://doi.org/10.1093/eurheartj/ehi720 (2006).\\n\\t 3.\\t Weiss, E. S. et al. Development of a quantitative donor risk index to predict short-term mortality in orthotopic heart transplantation. \\nThe Journal of Heart and Lung Transplantation 31, 266–273 (2012).\\n\\t 4.\\t Hong, K. N. et al. Who is the high-risk recipient? Predicting mortality after heart transplant using pretransplant donor and recipient \\nrisk factors. The Annals of thoracic surgery 92, 520–527 (2011).\\n\\t 5.\\t Weiss, E. S. et al. Creation of a Quantitative Recipient Risk Index for Mortality Prediction After Cardiac Transplantation (IMPACT). \\nThe Annals of Thoracic Surgery 92, 914–922 (2011).\\n\\t 6.\\t Nilsson, J. et al. The International Heart Transplant Survival Algorithm (IHTSA): A New Model to Improve Organ Sharing and \\nSurvival. PloS one 10, e0118644 (2015).\\n\\t 7.\\t LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521, 436–444, https://doi.org/10.1038/nature14539 (2015).\\n\\t 8.\\t Cucchetti, A. et al. Artificial neural network is superior to MELD in predicting mortality of patients with end-stage liver disease. Gut \\n56, 253–258 (2007).\\n\\t 9.\\t Bradley, A. P. The use of the area under the ROC curve in the evaluation of machine learning algorithms. Pattern recognition 30, \\n1145–1159 (1997).\\n\\t10.\\t Kumar, R. & Indrayan, A. Receiver operating characteristic (ROC) curve for medical researchers. Indian pediatrics 48, 277–287 \\n(2011).\\n\\t11.\\t Ammirati, E. et al. A prospective comparison of mid-term outcomes in patients treated with heart transplantation with advanced \\nage donors versus left ventricular assist device implantation. Interact Cardiovasc Thorac Surg 23, 584–592, https://doi.org/10.1093/\\nicvts/ivw164 (2016).\\n\\t12.\\t Matesanz, R. International Figures on Donation and Transplantation −2012. 74 pages (Council of Europe European committee on \\norgan transplantation, Global Observatory on Donation & Transplantation, 2013).\\n\\t13.\\t Khush, K. K., Menza, R., Nguyen, J., Zaroff, J. G. & Goldstein, B. A. Donor Predictors of Allograft Use and Recipient Outcomes After \\nHeartTransplantation. Circ-Heart Fai l6, 300–309, https://doi.org/10.1161/Circheartfailure.112.000165 (2013).\\n\\t14.\\t Organ Procurement and Transplantation Network, https://optn.transplant.hrsa.gov/data/ (2015).\\n\\t15.\\t Medved, D., Nilsson, J. & Nugues, P. Streamlining a Transplantation Survival Prediction Program with a RDF Triplestore. Paper \\npresented at 9th International Conference on Data Integration in the Life Sciences https://doi.org/10.1007/978-3-642-39437-9 (2013).\\n\\t16.\\t Prud, E., Seaborne, A. & others. Sparql query language for rdf. (2006).\\n\\t17.\\t Anderson, T. W. & Darling, D. A. Asymptotic theory of certain “goodness of fit” criteria based on stochastic processes. The annals of \\nmathematical statistics, 193–212 (1952).\\n\\t18.\\t Schemper, M. & Heinze, G. Probability imputation revisited for prognostic factor studies. Statistics in medicine 16, 73–80 (1997).\\n\\t19.\\t Fawcett, T. An introduction to ROC analysis. Pattern Recognition Letters 27, 861–874 (2006).\\n\\t20.\\t DeLong, E. R., DeLong, D. M. & Clarke-Pearson, D. L. Comparing the areas under two or more correlated receiver operating \\ncharacteristic curves: a nonparametric approach. Biometrics, 837–845 (1988).\\n\\t21.\\t Harrell, F. E., Califf, R. M., Pryor, D. B., Lee, K. L. & Rosati, R. A. Evaluating the yield of medical tests. Jama 247, 2543–2546 (1982).\\n\\t22.\\t Kang, L., Chen, W., Petrick, N. A. & Gallas, B. D. Comparing two correlated C indices with right‐censored survival outcome: a \\none‐shot nonparametric approach. Statistics in medicine 34, 685–703 (2015).\\n\\t23.\\t Hosmer, D. W. Jr & Lemeshow, S. Applied logistic regression. (John Wiley & Sons, 2004).\\n\\t24.\\t Cockcroft, D. W. & Gault, M. H. Prediction of creatinine clearance from serum creatinine. Nephron 16, 31–41 (1976).\\nAcknowledgements\\nThis work is based on OPTN data as of October 1, 2013 and was supported in part by the Health Resources and \\nServices Administration contract 234-2005-370011C. The content is the responsibility of the authors alone and \\ndoes not necessarily reflect the views or policies of the Department of Health and Human Services, nor does \\nmention of trade names, commercial products, or organisations imply endorsement by the U.S. Government. This \\nresearch was supported by Swedish National Infrastructure for Computing, Swedish Heart-Lung Foundation, \\nSwedish Society of Medicine, Government grant for clinical research, Region Skåne Research Funds, Donation \\nFunds of Skane University Hospital, Anna-Lisa and Sven Eric Lundgrens Foundation, the Crafoord Foundation, \\nthe Swedish Research Council, and the eSSENCE program. The supporting sources had no involvement in the \\nstudy.\\nAuthor Contributions\\nAll authors contributed to the study design and data interpretation. D.M., P.H., P.N. and J.N., undertook the \\nanalysis and validation of the data. D.M., P.N. and M.O. performed the computer programming. D.M., P.N. and \\nJ.N. drafted the initial report, and all authors contributed to the final draft. Data was provided from the UNOS \\nregistry by staff at the US, United Network for Organ Sharing, and compiled by B.A. and J.N.\\nAdditional Information\\nCompeting Interests: The authors declare no competing interests.\\nPublisher's note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and \\ninstitutional affiliations.\\n\\nwww.nature.com/scientificreports/\\n9\\nSCieNTifiC REPOrTS |  (2018) 8:3613  | DOI:10.1038/s41598-018-21417-7\\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \\nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \\nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Cre-\\native Commons license, and indicate if changes were made. The images or other third party material in this \\narticle are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the \\nmaterial. If material is not included in the article’s Creative Commons license and your intended use is not per-\\nmitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the \\ncopyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.\\n \\n© The Author(s) 2018\\n\"}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/Improving prediction of heart transplantation outcome using deep learning techniques.pdf', 'text': \"1\\nSCieNTifiC REPOrTS |  (2018) 8:3613  | DOI:10.1038/s41598-018-21417-7\\nwww.nature.com/scientificreports\\nImproving prediction of heart \\ntransplantation outcome using \\ndeep learning techniques\\nDennis Medved1, Mattias Ohlsson2, Peter Höglund\\u200a \\u200a3, Bodil Andersson4, Pierre Nugues1 & \\nJohan Nilsson\\n5\\nThe primary objective of this study is to compare the accuracy of two risk models, International \\nHeart Transplantation Survival Algorithm (IHTSA), developed using deep learning technique, and \\nIndex for Mortality Prediction After Cardiac Transplantation (IMPACT), to predict survival after heart \\ntransplantation. Data from adult heart transplanted patients between January 1997 to December \\n2011 were collected from the UNOS registry. The study included 27,860 heart transplantations, \\ncorresponding to 27,705 patients. The study cohorts were divided into patients transplanted before \\n2009 (derivation cohort) and from 2009 (test cohort). The receiver operating characteristic (ROC) \\nvalues, for the validation cohort, computed for one-year mortality, were 0.654 (95% CI: 0.629–0.679) \\nfor IHTSA and 0.608 (0.583–0.634) for the IMPACT model. The discrimination reached a C-index for \\nlong-term survival of 0.627 (0.608–0.646) for IHTSA, compared with 0.584 (0.564–0.605) for the IMPACT \\nmodel. These figures correspond to an error reduction of 12% for ROC and 10% for C-index by using \\ndeep learning technique. The predicted one-year mortality rates for were 12% and 22% for IHTSA \\nand IMPACT, respectively, versus an actual mortality rate of 10%. The IHTSA model showed superior \\ndiscriminatory power to predict one-year mortality and survival over time after heart transplantation \\ncompared to the IMPACT model.\\nHeart transplantation (HT) is a life-saving operation for patients with end-stage heart disease. Despite this reality, \\nthe transplantation number does not increase over the years. One of the most limiting factors is the lack of donor \\norgans and a conservative allocation policy that results in the loss of about half of the organs being offered1. An \\nimproved prediction of the outcome would augment the confidence in the post-transplantation performance and \\nmake it possible to optimise the allocation of organs. Furthermore, it would enable practitioners to determine the \\nrisk of early and late graft dysfunction more accurately and improve donor and recipient management.\\nAlthough there exist several survival models within cardiac surgery, currently there is no accepted tool for \\nestimating the outcome after heart transplantation. In recent years, some risk score algorithms designed to pre-\\ndict post-transplantation performance have been developed, which almost all have been derivate on the sin-\\ngle national, multi institutional United Network for Organ Sharing (UNOS) registry2. The most notable ones \\nare: Donor Risk Index (DRI), Risk Stratification Score (RSS), and Index for Mortality Prediction After Cardiac \\nTransplantation (IMPACT)3–5. The IMPACT model has additionally been validated on the International Society \\nof Heart and Lung Transplantation (ISHLT) registry and showed an acceptable accuracy in predicting mortality. \\nRecently a multinational model, the International Heart Transplantation Survival Algorithm (IHTSA), devel-\\noped on the ISHLT registry was published6. This model was designed to predict both short-term and long-term \\nmortality and, in contrast to previous models, it utilises deep learning techniques. The results it obtained showed \\nan improved discrimination compared with the DRI, RSS, and IMPACT models. However, the validation was \\nperformed on the ISHLT registry, which was also used for the development of the model6.\\nEven if the validation cohort was separated from the derivation cohort, the IHTSA model might be biased \\ntowards this registry.\\n1Department of Computer Science, Lund University, Lund, Sweden. 2Department of Astronomy and Theoretical \\nPhysics, Computational Biology and Biological Physics, Lund University, Lund, Sweden. 3Department of \\nLaboratory Medicine Lund, Clinical Chemistry and Pharmacology, Lund University, Lund, Sweden. 4Department \\nof Clinical Sciences Lund, Surgery, Lund University and Skåne University Hospital, Lund, Sweden. 5Department \\nof Clinical Sciences Lund, Cardiothoracic Surgery, Lund University and Skåne University Hospital, Lund, Sweden. \\nCorrespondence and requests for materials should be addressed to J.N. (email: johan.nilsson@med.lu.se)\\nReceived: 10 October 2017\\nAccepted: 1 February 2018\\nPublished: xx xx xxxx\\nOPEN\\n\\nwww.nature.com/scientificreports/\\n2\\nSCieNTifiC REPOrTS |  (2018) 8:3613  | DOI:10.1038/s41598-018-21417-7\\nThe aim of this study was to determine the most suitable risk stratification model for heart transplantation by \\napplying the IMPACT and IHTSA algorithms to the UNOS registry.\\nResults\\nCharacteristics of the Study Population.\\u2003\\nThe preoperative characteristics of the recipients are listed in \\nTable\\xa01 and for the donors in Table\\xa02. The number of adult HT with a follow-up time of at least one year, from \\nJanuary 1997 to December 2011, was of 27,860, corresponding to 27,705 patients. Over the time span, the cumu-\\nlative sum of follow-up years was of 165,206. The median survival time was 12 years (Interquartile Range [IQR]: \\n5–16). The one-year mortality was of 13% (n\\u2009=\\u20093,561). The average age of the recipients was 52\\u2009±\\u200913 years, with \\na range from 18 to 78 years. Most of the recipients were males 76% (n\\u2009=\\u200921,151). Multi-organ transplants were \\nmarginal (2.5%). The number of transplants contained in the derivation cohort was of 22,263, and the number of \\ntransplants in the test cohort was of 5,597.\\nIMPACT versus IHTSA.\\u2003\\nThe IHTSA model includes 32 recipient risk variables, while the IMPACT model \\nhas 18 variables; five of these variables are shared between the models: female gender, diagnosis: ischemic cardi-\\nomyopathy, diagnosis: congenital, infection within two weeks, and mechanical ventilation. Additionally, IHTSA \\nalso has 11 donor variables, while IMPACT has no donor variables.\\nWe evaluated the original IHTSA model in the test cohort (2009–2011) for one-year mortality; it had an area \\nunder receiver operating characteristic (AUROC) of 0.643 (95% CI: 0.619–0.667), while IMPACT had an AUROC \\nof 0.608 (0.583–0.634), P\\u2009=\\u20090.004, see Table\\xa03. As shown in Fig.\\xa01 and Table\\xa03, the recalibrated IHTSA model has \\na significantly higher discrimination compared with the IMPACT model for one-year mortality, P\\u2009=\\u20090.001, corre-\\nsponding to an error reduction of 11.7%. Harrell’s C-index for the recalibrated IHTSA compared with IMPACT \\nwas substantially larger, as shown in Table\\xa04, with about a 4% absolute difference for the later time era. This corre-\\nsponds to an error reduction of 10.3%. On the time era 1997–2008, on which the models were trained using 5-fold \\ncross-validation technique, the recalibrated IHTSA had an AUROC of 0.688 (0.678–0.699), and IMPACT had \\n0.606 (0.595–0.617) for one-year mortality, P\\u2009=\\u20090.001, Table\\xa03. The absolute difference in C-index was 5% higher \\nfor the IHTSA model compared with the IMPACT model, P\\u2009<\\u20090.001, Table\\xa04.\\nWe analysed the sensitivity of both models relatively to the deceased patients after one year at the levels of \\n25%, 50%, and 75%. Out of the transplants in the test cohort (N\\u2009=\\u20095,597), the numbers of correctly classified \\npatients after one year were 4,812, 3,890, and 2,582 patients respectively for IHTSA, and 4,539, 3,396, and 2,140 \\npatients respectively for IMPACT. See Fig.\\xa02 for a graph of the difference in correctly classified patients.\\nWe furthermore compared the predicted one-year mortality rate for IMPACT and IHTSA, with the true mor-\\ntality rate. The predicted one-year mortality for the second time-era (test cohort) was 12% and 22% for the recal-\\nibrated IHTSA and IMPACT, respectively, versus an actual mortality rate of 10%. The Hosmer-Lemeshow (HL) \\nchi-square for one-year, using ten groups, was of 40 in the IHTSA model and 101 for the IMPACT model, both \\nwith a P-value less than 0.05. As shown in the calibration plot, Fig.\\xa03, the predictive mortality compared with \\nactual mortality was more consistent over all deciles for the ITHSA model compared with the IMPACT model.\\nTo evaluate difference in methodology approach (deep learning versus logistic regression), we performed \\ntwo additional experiments. We quantify the difference between the deep learning technique used by the IHTSA \\nmodel and the more traditional logistic regression approach used by the IMPACT model, by letting the two sys-\\ntems use identical features. The second experiment was to assess the difference between a model that include and \\nexclude donor variables.\\nAs shown in Tables\\xa05 and 6, a recalibrated IHTSA model including only the same risk variables as the IMPACT \\nmodel still showed a substantial improvement in the AUROC (about 2%) and C-index in the test cohort com-\\npared with the IMPACT model. The recalibrated IHTSA model excluding the donor variables showed a decrease \\nin discrimination compared with the original IHTSA model, however the difference was minor, producing nearly \\nthe same AUROC.\\nDiscussion\\nThe purpose of this study was to compare the IMPACT and IHTSA models with regards to the prediction accu-\\nracy of one-year mortality on the UNOS database. There exist some biases in both models when used on the \\nUNOS data set for the time era 1997–2008. Because IMPACT was developed on these data and IHTSA on the \\nISHLT dataset, which consists in part of the same UNOS data, the models may be subjected to a non-negligible \\noverfit to the data, skewing the result towards a more positive value. Therefore, we chose to validate the models on \\na later time era, which has no overlapping patients with the training set.\\nThe results show that the IHTSA model exhibited improved performance and accuracy compared to the \\nIMPACT model. Even though IMPACT was designed to predict one-year mortality and IHTSA was created for \\nlong-term survival, IHTSA shows better discrimination on one-year mortality.\\nThis study could also prove the benefits of using deep learning modelling techniques. Such techniques are \\ninspired by the human brain. They consist of a network of “neurons” that emulate the properties of their real \\ncounterparts. Using multiple processing layers makes it possible to learn representations of data with multiple \\nlevels of abstraction7. These methods have improved the state-of-the-art in speech recognition, visual object rec-\\nognition, object detection and many other domains8.\\nOur results show that the IHTSA model can be applied to predict short-term mortality with greater accuracy \\nthan a more traditional risk-based model based on logistic regression. Although the comparison of ROC curves \\nto evaluate models in a statistically valid manner is controversial, the ROC curve is currently the most developed \\nstatistical tool for describing performance9,10. The improvements seen can be explained by the difference in the \\nvariable selection, such as the absence of donor risk factors in the IMPACT model, but also by the the neural net-\\nwork’s ability to handle interactions between variables and nonlinearities. An increased donor age has in previous \\n\\nwww.nature.com/scientificreports/\\n3\\nSCieNTifiC REPOrTS |  (2018) 8:3613  | DOI:10.1038/s41598-018-21417-7\\nFeature\\nN\\nTime era \\n1997–2008\\nTime era \\n2009–2011\\np-Value\\nIMPACT\\nIHTSA\\n(n\\u2009=\\u200922,263)\\n(n\\u2009=\\u20095,597)\\nDemographic data\\n\\xa0\\xa0Age (years)\\n27,860\\n52\\u2009±\\u200913\\n53\\u2009±\\u200913\\n0.001\\n✓\\n\\xa0\\xa0Age >60 years\\n27,860\\n5,707 (26%)\\n1,809 (32%)\\n0.001\\n✓\\n\\xa0\\xa0Female gender\\n27,860\\n5,298 (24%)\\n1,411 (25%)\\n0.029\\n✓\\n✓\\n\\xa0\\xa0Height (cm)\\n27,740\\n174\\u2009±\\u200910\\n174\\u2009±\\u200910\\n0.835\\n✓\\n\\xa0\\xa0Weight (kg)\\n27,760\\n80\\u2009±\\u200917\\n82\\u2009±\\u200917\\n0.001\\n✓\\n\\xa0\\xa0Race: African American\\n27,860\\n3,324 (15%)\\n1,103 (20%)\\n0.001\\n✓\\nDiagnosis\\n\\xa0\\xa0Ischemic cardiomyopathy\\n27,859\\n9,976 (45%)\\n2,793 (50%)\\n0.001\\n✓\\n✓\\n\\xa0\\xa0Non-ischemic cardiomyopathy\\n27,859\\n10,247 (46%)\\n2,119 (38%)\\n0.001\\n✓\\n\\xa0\\xa0Congenital\\n27,859\\n518 (2%)\\n149 (3%)\\n0.159\\n✓\\n✓\\n\\xa0\\xa0Other\\n27,859\\n852 (3%)\\n247 (4%)\\n0.001\\n✓\\n\\xa0\\xa0Graft failure\\n27,859\\n669 (3%)\\n197 (4%)\\n0.058\\n✓\\n\\xa0\\xa0Diabetes mellitus#\\n27,597\\n4,735 (22%)\\n1,500 (27%)\\n0.001\\n✓\\n\\xa0\\xa0Hypertension†\\n17,876\\n7,108 (40%)\\n—\\n✓\\n\\xa0\\xa0Infection within two weeks‡\\n26,543\\n2,333 (11%)\\n594 (11%)\\n0.550\\n✓\\n✓\\n\\xa0\\xa0Antiarrhythmic drugs prior transplant\\n17,266\\n6,371 (37%)\\n—\\n✓\\n\\xa0\\xa0Amiodarone prior to transplant\\n17,530\\n4,726 (27%)\\n—\\n✓\\n\\xa0\\xa0Dialysis prior to transplant\\n27,002\\n706 (3%)\\n185 (3%)\\n0.510\\n✓\\n\\xa0\\xa0Previous blood transfusion\\n15,221\\n5,285 (35%)\\n27 (29%)\\n0.247\\n✓\\n\\xa0\\xa0Previously transplanted*\\n27,860\\n680 (3%)\\n199 (4%)\\n0.067\\n✓\\n\\xa0\\xa0Previous cardiac surgery\\n14,069\\n1,866 (22%)\\n1,483 (27%)\\n0.001\\n✓\\n\\xa0\\xa0ICU\\n27,860\\n7,991 (36%)\\n1,493 (27%)\\n0.001\\n✓\\n\\xa0\\xa0Mechanical ventilation\\n27,860\\n625 (3%)\\n166 (3%)\\n0.532\\n✓\\n✓\\n\\xa0\\xa0ECMO\\n27,860\\n90 (0.04%)\\n48 (1%)\\n0.001\\n✓\\n\\xa0\\xa0IABP\\n27,860\\n1193 (5%)\\n263 (5%)\\n0.039\\n✓\\n✓\\n\\xa0\\xa0Ventricular assist device\\n24,357\\n4,665 (25%)\\n2,191 (39%)\\n0.001\\n✓\\n\\xa0\\xa0Early generationa\\n6,856\\n911 (20%)\\n114 (5%)\\n0.001\\n✓\\n\\xa0\\xa0Late generationb\\n6,856\\n536 (11%)\\n1,610 (74%)\\n0.001\\n✓\\n\\xa0\\xa0Other/Unknown\\n6,856\\n3,218 (69%)\\n467 (21%)\\n0.001\\n\\xa0\\xa0Temporary circulatory supportc\\n27,860\\n209 (1%)\\n113 (2%)\\n0.001\\n✓\\nTransplant era\\n\\xa0\\xa01996–2000\\n27,860\\n7781 (35%)\\n—\\n✓\\n\\xa0\\xa02001–2005\\n27,860\\n8981 (40%)\\n—\\n✓\\n\\xa0\\xa0>2005\\n27,860\\n5501 (25%)\\n5,598 (100%)\\n0.001\\n✓\\nHemodynamic status\\n\\xa0\\xa0PVR (wood units)\\n21,782\\n2.5\\u2009±\\u20091.8\\n2.4\\u2009±\\u20091.8\\n0.205\\n✓\\n\\xa0\\xa0SPP (mmHg)\\n25,100\\n43\\u2009±\\u200914\\n42\\u2009±\\u200914\\n0.001\\n✓\\nLaboratory values\\n\\xa0\\xa0Creatinine (mg/dl)\\n27,027 1\\n1.4\\u2009±\\u20090.8\\n1.3\\u2009±\\u20090.8\\n0.038\\n✓\\nCreatinine clearance (mL/min)\\n\\xa0\\xa030–49\\n27,054\\n2,964 (14%)\\n698 (12%)\\n0.008\\n✓\\n\\xa0\\xa0<30\\n27,054\\n674 (3%)\\n189 (3%)\\n0.376\\n✓\\n\\xa0\\xa0Serum bilirubin (mg/dl)\\n26,224\\n1.3\\u2009±\\u20092\\n1.2\\u2009±\\u20092\\n0.001\\n✓\\n\\xa0\\xa01.00–1.99\\n26,224\\n6,117 (30%)\\n1,562 (28%)\\n0.102\\n✓\\n\\xa0\\xa02.00–3.99\\n26,224\\n1261 (6%)\\n300 (5%)\\n0.070\\n✓\\n\\xa0\\xa0≥4\\n26,224\\n1314 (6%)\\n297 (5%)\\n0.007\\n✓\\nImmunology status\\n\\xa0\\xa0PRA\\u2009>\\u200910%\\n18,351\\n1,113 (8%)\\n1,114 (20%)\\n0.001\\n✓\\n\\xa0\\xa0HLA-DR, 2 mismatch\\n23,858\\n10,289 (55%)\\n2,746 (55%)\\n0.906\\n✓\\nRecipient blood group\\n\\xa0\\xa0A\\n27,860\\n9,543 (43%)\\n2,313 (41%)\\n0.036\\n✓\\n\\xa0\\xa0B\\n27,860\\n3,040 (14%)\\n795 (14%)\\n0.343\\n✓\\n\\xa0\\xa0AB\\n27,860\\n1,143 (5%)\\n295 (5%)\\n0.597\\n✓\\n\\xa0\\xa0O\\n27,860\\n8,549 (38%)\\n2,198 (39%)\\n0.092\\n✓\\n\\nwww.nature.com/scientificreports/\\n4\\nSCieNTifiC REPOrTS |  (2018) 8:3613  | DOI:10.1038/s41598-018-21417-7\\nreports been shown to have a negative influence on short-term survival6,11. To examine this, we compared the \\ndifference of the deep learning model and the logistic regression model using the same variables. Here, we show \\na substantial improvement when using the deep learning approach compared with the traditional approach. \\nFurthermore, we could show that the predictive availability for the deep learning model was less dependent on \\nthe variables included compared with a standard model. Donor variables showed to be of less importance than \\nexpected. A possible explanation for that may be the deep learning technology has an increased ability to identify \\nnew patterns with the data it has available. It is interesting to note that the two models do not show a considerable \\noverlap of features. Only five features are shared by the two models out of 18 for IMPACT and 43 for IHTSA. If we \\ncompare the overlapping variables with the seven most important variables for IHTSA, we find that three of them \\nare shared: age, diagnosis, and mechanical ventilation6.\\nOne disadvantage of the deep learning technique is that it yields a black box model with a limited ability to \\nexplicitly identify possible causal relationships. Logistic regression, on the contrary, makes it feasible to determine \\nthe strongly predictive variables based on the size of the coefficients. To cope with the lack of a well-established \\nTable 1.\\u2002 The recipient features used in the IMPACT and IHTSA Models. N, number of transplants with non-\\nmissing values. n, total number of transplants. Qualitative data are expressed as n (%), and quantitative data \\nas mean\\u2009±\\u2009SD. #Drug or insulin treated diabetes mellitus. †Drug treated systemic hypertension. ‡Infection \\nrequiring intravenous antibiotic therapy within two weeks prior to transplant. *Previous transplant—previous \\nkidney, liver, pancreas, pancreas islet cells, heart, lung, intestine and/or bone marrow transplant. aEarly \\ngeneration includes para and intracorporeal pulsatile VADs: Abiomed AB5000, Heartmate I, XE, and XVE, \\nThortecIVAD, Toyobo, Medos and LionHeart. bLater generation continuous VADs including Heartmate \\nII, Jarvik, Micromed, Debakey, and VentrAssist. cIncludes ECMO and [or] extracorporeal VADs: Abiomed \\nBVS5000, Bio-Medicus, TandemHeart, and Levitronix/Centrimag. ECMO, extracorporeal membrane \\noxygenation; ICU, intensive care unit; IHTSA, international heart transplantation survival algorithm; IMPACT, \\nindex for mortality prediction after cardiac transplantation; HLA, human leukocyte antigen; PRA, panel \\nreactive antibody; PVR, pulmonary vascular resistance; SD, standard deviation; SPP, systolic pulmonary \\npressure. The t-test and chi-squared test was used for continuous respectively categorical values.\\nFeature\\nN\\nTime era \\n1997–2008\\nTime era \\n2009–2011\\np-Value\\nIMPACT\\nIHTSA\\n(n\\u2009=\\u200922,263)\\n(n\\u2009=\\u20095,597)\\nDemographic data\\n\\xa0\\xa0Age (years)\\n27,075\\n32\\u2009±\\u200912\\n32\\u2009±\\u200912\\n0.515\\n✓\\n\\xa0\\xa0Female gender\\n27,860\\n6,546 (29%)\\n1,645 (29%)\\n0.979\\n✓\\n\\xa0\\xa0Weight (kg)\\n27,838\\n79\\u2009±\\u200919\\n82\\u2009±\\u200919\\n0.001\\n✓\\n\\xa0\\xa0Duration of ischemia (min)\\n26,029\\n189\\u2009±\\u200963\\n194\\u2009±\\u200910\\n0.001\\n✓\\n\\xa0\\xa0CODD: Head Trauma\\n27,825\\n13,733 (62%)\\n3,068 (55%)\\n0.001\\n✓\\n\\xa0\\xa0CODD: Cerebrovascular event\\n27,825\\n5,894 (27%)\\n1,297 (23%)\\n0.001\\n✓\\nDonor blood group\\n\\xa0\\xa0A\\n27,859\\n8,232 (37%)\\n1,983(35%)\\n0.030\\n✓\\n\\xa0\\xa0B\\n27,859\\n2,284 (10%)\\n617 (11%)\\n0.102\\n✓\\n\\xa0\\xa0AB\\n27,859\\n477 (2%)\\n125 (2%)\\n0.682\\n✓\\n\\xa0\\xa0O\\n27,859\\n11269 (40%)\\n2,873 (51%)\\n0.001\\n✓\\n\\xa0\\xa0Recipient-donor weight ratio\\n27,739\\n1.03\\u2009±\\u20090.22\\n1.02\\u2009±\\u20090.20\\n0.001\\n✓\\n\\xa0\\xa0Recipient-donor height ratio\\n27,660\\n0.998\\u2009±\\u20090.06\\n0.999\\u2009±\\u20090.06\\n0.068\\n✓\\nTable 2.\\u2002 The donor features used in the IHTSA model. N, number of transplants with non-missing values. \\nn, total number of transplants. Qualitative data are expressed as n (%), and quantitative data as mean\\u2009±\\u2009SD. \\nCODD, cause of donor death; IHTSA, international heart transplantation survival algorithm; IMPACT, index \\nfor mortality prediction after cardiac transplantation. The t-test and chi-squared test was used for continuous \\nrespectively categorical values.\\nTime era\\nAUROC (95% CI)\\nIMPACT\\nIHTSA\\nP-Value\\nIHTSA cal.\\nP-Value\\n1997–2008\\n0.61 (0.59–0.62)\\n0.66 (0.64–0.67)\\n0.001\\n0.69 (0.68–0.70)\\n0.001\\n2009–2011\\n0.61 (0.58–0.63)\\n0.64 (0.62–0.67)\\n0.004\\n0.65 (0.63–0.68)\\n0.001\\nTable 3.\\u2002 The AUROC for one-year mortality for the different cohorts using IMPACT and IHTSA respectively. \\nAUROC, area under the receiver-operating curve; CI, confidence interval; IHTSA, international heart \\ntransplantation survival algorithm; cal, the recalibrated version; IMPACT, index for mortality prediction after \\ncardiac transplantation.; P, probability that the result is the same as IMPACT.\\n\\nwww.nature.com/scientificreports/\\n5\\nSCieNTifiC REPOrTS |  (2018) 8:3613  | DOI:10.1038/s41598-018-21417-7\\nmethod for interpreting the weights of a connection matrix in a neural network, the developers of the IHTSA \\nalgorithm used a classification and regression tree (CART), fitted to the predicted median survival time, to assess \\nthe relative importance of the features6. Furthermore, the web-based calculator (http://ihtsa.cs.lth.se) makes it \\npossible to estimate the survival on a computer or mobile device.\\nDuring 2011, approximately 17,000 donors were reported12. Unfortunately, not more than one-third of all \\ndonors could be utilised for heart transplantation. One explanation for this may be the uncertainty in the risk \\nof early and late graft dysfunction, which means that some suitable donors are not accepted. Although there are \\nmany donor predictors of allograft discard in the current era, these characteristics seem to have little effect on \\nrecipient outcomes when the hearts are transplanted, which also is confirmed in this study13. A more liberal use of \\nFigure 1.\\u2002 The ROC curves show the sensitivity of prediction of one-year mortality vs. 1-specificity for the \\nIMPACT (short-long dashed line) and the recalibrated IHTSA (solid line) risk algorithms is plotted on the test \\ncohort (2009–2011). The gray dashed line represents the absence of discrimination.\\nTime era\\nC-index (95% CI)\\nIMPACT\\nIHTSA\\nP-Value\\nIHTSA cal.\\nP-Value\\n1997–2008\\n0.56 (0.56–0.56)\\n0.59 (0.59–0.60)\\n0.001\\n0.62 (0.61–0.62)\\n0.001\\n2009–2011\\n0.58 (0.56–0.61)\\n0.61 (0.59–0.63)\\n0.002\\n0.63 (0.61–0.65)\\n0.001\\nTable 4.\\u2002 The Harrells C-index for survival for the different cohorts using IMPACT and IHTSA respectively. CI, \\nconfidence interval; IHTSA, international heart transplantation survival algorithm; cal, the recalibrated version; \\nIMPACT, index for mortality prediction after cardiac transplantation; P, probability that the result is the same as \\nIMPACT.\\nFigure 2.\\u2002 The sensitivity of prediction of one-year mortality versus the total number of additional correctly \\nclassified patients by IHTSA compared with IMPACT, both in absolute numbers and percentage, plotted on the \\ntest cohort (2009–2011).\\n\\nwww.nature.com/scientificreports/\\n6\\nSCieNTifiC REPOrTS |  (2018) 8:3613  | DOI:10.1038/s41598-018-21417-7\\ncardiac allografts with relative contraindications may be warranted. A calculator would allow us to conveniently \\nperform batch estimation of survival for multiple patients at the same time. This would allow the IHTSA model \\nto be used as a virtual recipient-donor matching tool that models survival for potential recipients on a waiting \\nlist when there is a donor heart available. This could potentially increase the number of organs that could be used \\ncompared with a traditional criterion-based model6. Additionally, it will make it easier for other research groups \\nto validate the model.\\nThe results of this study carry limitations associated with the retrospective analysis of a registry database, the \\nquality of the source data, the number of missing data, and the lack of standardization associated with multi-\\ncenter studies (such as different immunosuppressive regimens and different matching criteria). However, those \\nlimitations are the same for both models. Even if a comparison of risk models remains controversial, the C-index \\nis probably the best statistical tool for describing performance. A C-index of <0.7 may seem low, but it should \\nbe kept in mind that the IHTSA model predicts long term survival, and to the best of our knowledge, it is higher \\nthan previously reported studies.\\nConclusions\\nIn this study, we have shown that a flexible nonlinear artificial neural network model (IHTSA), utilising deep \\nlearning techniques, exhibits better discrimination and accuracy than a more traditional risk score model \\n(IMPACT) for predicting one-year mortality. We made public the results of this model in the form of a web-based \\nFigure 3.\\u2002 The observed (gray bars) and expected mortality (black bars), in percent, for each decile, for the \\nIMPACT and IHTSA models, in the test cohort (2009–2011). The patients are divided into deciles according to \\ntheir expected mortality, and the observed mortality was derived for each decile.\\nTime era\\nAUROC (95% CI)\\nIMPACT\\nANN I\\nP-Value\\nANN II\\nP-Value\\n2009–2011\\n0.61 (0.58–0.63)\\n0.63 (0.60–0.65)\\n0.027\\n0.65 (0.63–0.68)\\n0.001\\nTable 5.\\u2002 The AUROC for one-year mortality for the test cohort (2009–2011) using an artificial neural network \\nmodel derived on the derivation cohort (1997–2008) with IMPACT features only (ANN I) and with IHTSA \\nrecipient features only (ANN II). AUROC, area under the receiver-operating curve; CI, confidence interval; \\nIHTSA, international heart transplantation survival algorithm; IMPACT, index for mortality prediction after \\ncardiac transplantation.; P, probability that the result is the same as IMPACT.\\nTime era\\nC-index (95% CI)\\nIMPACT\\nANN I\\nP-Value\\nANN II\\nP-Value\\n2009–2011\\n0.58 (0.56–0.61)\\n0.60 (0.58–0.62)\\n0.002\\n0.62 (0.60–0.64)\\n0.001\\nTable 6.\\u2002 The Harrells C-index for one-year mortality for the test cohort (2009–2011) using an artificial neural \\nnetwork model derived on the derivation cohort (1997–2008) with IMPACT features only (ANN I) and with \\nIHTSA recipient features only (ANN II). CI, confidence interval; IHTSA, international heart transplantation \\nsurvival algorithm; IMPACT, index for mortality prediction after cardiac transplantation.; P, probability that the \\nresult is the same as IMPACT.\\n\\nwww.nature.com/scientificreports/\\n7\\nSCieNTifiC REPOrTS |  (2018) 8:3613  | DOI:10.1038/s41598-018-21417-7\\nbatch calculator that could be used as a virtual recipient-donor matching tool. This is a first step in the imple-\\nmentation of a deep learning architecture for transplantation data that, we hope, will pave the way for further \\nimprovements and an even more accurate model.\\nMaterials and Methods\\nData Source.\\u2003\\nThe data set of heart transplant patients was obtained from the UNOS database. UNOS is a \\nnon-profit organisation that administers the only Organ Procurement and Transplantation Network (OPTN) in \\nthe United States of America14. The database contains data from October 1, 1987, onwards and includes almost \\n500 variables that encompass recipient, donor, and transplant information. It consists of both deceased- and liv-\\ning-recipient transplants. The Ethics Committee for Clinical Research at Lund University, Sweden approved the \\nstudy protocol. The data was anonymized and de-identified prior to analysis and the institutional review board \\nwaived the need for written informed consent from the participants.\\nStudy Population.\\u2003\\nWe included all the adult HT patients (>17 years) from January 1997 to December 2011. \\nThe latest annual follow-up was on September 30, 2013. The data set was divided into two temporal cohorts: \\ntransplantation done before 2009 (derivation cohort) and after or during 2009 (test cohort). These time periods \\nwere chosen because both IMPACT and IHTSA were developed on patients between 1997–2008 and we wanted \\ndisjoint sets (derivation and test) to evaluate the prediction performance. The number of variables extracted from \\nthe database was 56 in total, where IHTSA uses 43 of them and IMPACT 18. The primary endpoint was one-year \\nmortality and the second endpoint was all-cause cumulative mortality during the study period.\\nStoring the Data.\\u2003\\nWe converted the complete UNOS database containing heart transplants until 2011, \\nexcept a few variables, into a Resource Description Framework (RDF) database following the procedure outlined \\nin a previously published report15. This enabled us to use the SPARQL language to query the data and easily \\nretrieve the variables used by both the IMPACT and IHTSA model to predict the mortality of the transplants16.\\nStatistical Analysis.\\u2003\\nWe performed the statistical analyses using the Stata MP statistical package version \\n13 (2013) (StataCorp LP, College Station, TX), and with RStudio Desktop 0.99.441 (RStudio, Boston, MA) using \\nR version 3.3.1. Data are presented as means with standard deviation (SD), and frequency as appropriate. The \\nAnderson-Darling test was used to assess the normality of the variables17. We used the t-test and chi-squared test \\nfor continuous, respectively categorical values, to test if the data was significantly different from each other. As \\nwith all patient registries, the dataset contains missing values. We applied a probability imputation technique by \\ncreating a list for each variable in the data set, containing the non-missing values for that variable, and then we \\nimputed each missing value with a value from the list, chosen from a uniform distribution18. In consequence, the \\ndistribution of the imputed values should follow that of the non-missing ones.\\nThe discriminatory power for one-year mortality was assessed by calculating the AUROC19. We compared \\nthe statistical significance of the difference between the AUROC of the two models using the non-parametric \\nDeLong’s test20. To evaluate the discrimination for long-term survival of the patients, we utilised the Harrell’s \\nconcordance index (C-index)21. We used a z-score test to compare the C-indexes22. The AUROC and C-index \\nvalues are both presented with 95% confidence limits. The predictive accuracy of the models was assessed by \\ncomparing the observed and expected mortality for equal-sized quantiles of risk by using the Hosmer–Lemeshow \\ngoodness-of-fit test23.\\nThe IMPACT model.\\u2003\\nIMPACT was created with a data set of heart transplant patients between 1997 to 2008 \\nthat were collected from the UNOS database. IMPACT only utilises recipient variables. Creatinine clearance was \\nnot directly available from the data set and had to be calculated using the Cockcroft-Gault equation24. By appor-\\ntioning points according to the relative importance of the variables for the one-year mortality, a risk index was \\ncreated. The minimum number of scoring points a patient can have is 0 and the maximum is 50. The points are \\nafter that converted to a predicted probability of one-year mortality by a formula derived from logistic regression5.\\nThe IHTSA model.\\u2003\\nThe data set used in developing IHTSA was extracted from the ISHLT containing HT \\npatients who were transplanted between 1994 and 2010. IHTSA utilises both recipient and donor variables. The \\nsurvival model consists of a flexible nonlinear generalisation of the standard Cox proportional hazard model. \\nInstead of using a single prediction model, this model integrates ensembles of artificial neural networks (ANNs). \\nIn addition, its prediction capability is not limited to one year6.\\nHowever, the variables hypertension and antiarrhythmic drugs are not recorded in the UNOS database from \\n2007 and onward. To handle this problem, we first imputed them with random values taken from the earlier \\ntime era. Secondly, we excluded these two variables, and retrained (calibrated) the neural network, utilizing a \\n5-fold cross validation of the patients between 1997 and 2008 in UNOS. The same training procedure was used as \\ndescribed in the original IHTSA article, but we did not carry out any new variable selection6. We called this model \\nthe recalibrated IHTSA model.\\nWeb-Based IHTSA Calculator.\\u2003\\nThe IHTSA model is available via a web application (ihtsa.cs.lth.se), where \\na user can either input a single patient’s data or submit a file of multiple patients in a batch calculator. To com-\\npute the results, the user then selects one of the two prediction models developed either on UNOS or IHSLT \\ndata, corresponding to American or international patients respectively. The submitted file should consist of \\ncomma-separated values (CSV) reflecting the patient data in a table format. The batch calculator uses this data \\nto predict one-, five-, and ten-year survival respectively and median survival time. Once processed, the result \\n\\nwww.nature.com/scientificreports/\\n8\\nSCieNTifiC REPOrTS |  (2018) 8:3613  | DOI:10.1038/s41598-018-21417-7\\nconsisting of relevant survival and mortality numbers is either emailed back to the user in a CSV format, in the \\ncase of the batch calculator, or presented directly in the web interface.\\nThe applications were implemented as a Java program, for the graphical user interface part and a Matlab (ver-\\nsion 2010A and 2015b) application for running the survival models.\\nData availability.\\u2003\\nThe data that support the findings of this study are available from UNOS but restrictions \\napply to the availability of these data, which were used under license for the current study, and so are not publicly \\navailable.\\nReferences\\n\\t 1.\\t Klein, A. S. et al. Organ donation and utilization in the United States, 1999–2008. Am J Transplant 10, 973–986, https://doi.\\norg/10.1111/j.1600-6143.2009.03008.x (2010).\\n\\t 2.\\t Nilsson, J., Algotsson, L., Höglund, P., Lührs, C. & Brandt, J. Comparison of 19 pre-operative risk stratification models in open-heart \\nsurgery. Eur Heart J 27, 867–874, https://doi.org/10.1093/eurheartj/ehi720 (2006).\\n\\t 3.\\t Weiss, E. S. et al. Development of a quantitative donor risk index to predict short-term mortality in orthotopic heart transplantation. \\nThe Journal of Heart and Lung Transplantation 31, 266–273 (2012).\\n\\t 4.\\t Hong, K. N. et al. Who is the high-risk recipient? Predicting mortality after heart transplant using pretransplant donor and recipient \\nrisk factors. The Annals of thoracic surgery 92, 520–527 (2011).\\n\\t 5.\\t Weiss, E. S. et al. Creation of a Quantitative Recipient Risk Index for Mortality Prediction After Cardiac Transplantation (IMPACT). \\nThe Annals of Thoracic Surgery 92, 914–922 (2011).\\n\\t 6.\\t Nilsson, J. et al. The International Heart Transplant Survival Algorithm (IHTSA): A New Model to Improve Organ Sharing and \\nSurvival. PloS one 10, e0118644 (2015).\\n\\t 7.\\t LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521, 436–444, https://doi.org/10.1038/nature14539 (2015).\\n\\t 8.\\t Cucchetti, A. et al. Artificial neural network is superior to MELD in predicting mortality of patients with end-stage liver disease. Gut \\n56, 253–258 (2007).\\n\\t 9.\\t Bradley, A. P. The use of the area under the ROC curve in the evaluation of machine learning algorithms. Pattern recognition 30, \\n1145–1159 (1997).\\n\\t10.\\t Kumar, R. & Indrayan, A. Receiver operating characteristic (ROC) curve for medical researchers. Indian pediatrics 48, 277–287 \\n(2011).\\n\\t11.\\t Ammirati, E. et al. A prospective comparison of mid-term outcomes in patients treated with heart transplantation with advanced \\nage donors versus left ventricular assist device implantation. Interact Cardiovasc Thorac Surg 23, 584–592, https://doi.org/10.1093/\\nicvts/ivw164 (2016).\\n\\t12.\\t Matesanz, R. International Figures on Donation and Transplantation −2012. 74 pages (Council of Europe European committee on \\norgan transplantation, Global Observatory on Donation & Transplantation, 2013).\\n\\t13.\\t Khush, K. K., Menza, R., Nguyen, J., Zaroff, J. G. & Goldstein, B. A. Donor Predictors of Allograft Use and Recipient Outcomes After \\nHeartTransplantation. Circ-Heart Fai l6, 300–309, https://doi.org/10.1161/Circheartfailure.112.000165 (2013).\\n\\t14.\\t Organ Procurement and Transplantation Network, https://optn.transplant.hrsa.gov/data/ (2015).\\n\\t15.\\t Medved, D., Nilsson, J. & Nugues, P. Streamlining a Transplantation Survival Prediction Program with a RDF Triplestore. Paper \\npresented at 9th International Conference on Data Integration in the Life Sciences https://doi.org/10.1007/978-3-642-39437-9 (2013).\\n\\t16.\\t Prud, E., Seaborne, A. & others. Sparql query language for rdf. (2006).\\n\\t17.\\t Anderson, T. W. & Darling, D. A. Asymptotic theory of certain “goodness of fit” criteria based on stochastic processes. The annals of \\nmathematical statistics, 193–212 (1952).\\n\\t18.\\t Schemper, M. & Heinze, G. Probability imputation revisited for prognostic factor studies. Statistics in medicine 16, 73–80 (1997).\\n\\t19.\\t Fawcett, T. An introduction to ROC analysis. Pattern Recognition Letters 27, 861–874 (2006).\\n\\t20.\\t DeLong, E. R., DeLong, D. M. & Clarke-Pearson, D. L. Comparing the areas under two or more correlated receiver operating \\ncharacteristic curves: a nonparametric approach. Biometrics, 837–845 (1988).\\n\\t21.\\t Harrell, F. E., Califf, R. M., Pryor, D. B., Lee, K. L. & Rosati, R. A. Evaluating the yield of medical tests. Jama 247, 2543–2546 (1982).\\n\\t22.\\t Kang, L., Chen, W., Petrick, N. A. & Gallas, B. D. Comparing two correlated C indices with right‐censored survival outcome: a \\none‐shot nonparametric approach. Statistics in medicine 34, 685–703 (2015).\\n\\t23.\\t Hosmer, D. W. Jr & Lemeshow, S. Applied logistic regression. (John Wiley & Sons, 2004).\\n\\t24.\\t Cockcroft, D. W. & Gault, M. H. Prediction of creatinine clearance from serum creatinine. Nephron 16, 31–41 (1976).\\nAcknowledgements\\nThis work is based on OPTN data as of October 1, 2013 and was supported in part by the Health Resources and \\nServices Administration contract 234-2005-370011C. The content is the responsibility of the authors alone and \\ndoes not necessarily reflect the views or policies of the Department of Health and Human Services, nor does \\nmention of trade names, commercial products, or organisations imply endorsement by the U.S. Government. This \\nresearch was supported by Swedish National Infrastructure for Computing, Swedish Heart-Lung Foundation, \\nSwedish Society of Medicine, Government grant for clinical research, Region Skåne Research Funds, Donation \\nFunds of Skane University Hospital, Anna-Lisa and Sven Eric Lundgrens Foundation, the Crafoord Foundation, \\nthe Swedish Research Council, and the eSSENCE program. The supporting sources had no involvement in the \\nstudy.\\nAuthor Contributions\\nAll authors contributed to the study design and data interpretation. D.M., P.H., P.N. and J.N., undertook the \\nanalysis and validation of the data. D.M., P.N. and M.O. performed the computer programming. D.M., P.N. and \\nJ.N. drafted the initial report, and all authors contributed to the final draft. Data was provided from the UNOS \\nregistry by staff at the US, United Network for Organ Sharing, and compiled by B.A. and J.N.\\nAdditional Information\\nCompeting Interests: The authors declare no competing interests.\\nPublisher's note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and \\ninstitutional affiliations.\\n\\nwww.nature.com/scientificreports/\\n9\\nSCieNTifiC REPOrTS |  (2018) 8:3613  | DOI:10.1038/s41598-018-21417-7\\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \\nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \\nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Cre-\\native Commons license, and indicate if changes were made. The images or other third party material in this \\narticle are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the \\nmaterial. If material is not included in the article’s Creative Commons license and your intended use is not per-\\nmitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the \\ncopyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.\\n \\n© The Author(s) 2018\\n\"}, {'source': '/Users/sir/Downloads/Data/PDF/DataAugmentationApproachesforNLP.pdf', 'text': 'A Survey of Data Augmentation Approaches for NLP\\nSteven Y. Feng∗, 1 Varun Gangal∗, 1 Jason Wei†, 2 Sarath Chandar,3\\nSoroush Vosoughi,4 Teruko Mitamura,1 Eduard Hovy1\\n1Carnegie Mellon University, 2Google Research\\n3Mila - Quebec AI Institute, 4Dartmouth College\\n{syfeng,vgangal,teruko,hovy}@cs.cmu.edu\\njasonwei@google.com\\nsarath.chandar@mila.quebec\\nsoroush@dartmouth.edu\\nAbstract\\nData augmentation has recently seen increased\\ninterest in NLP due to more work in low-\\nresource domains, new tasks, and the popu-\\nlarity of large-scale neural networks that re-\\nquire large amounts of training data.\\nDe-\\nspite this recent upsurge, this area is still rel-\\natively underexplored, perhaps due to the chal-\\nlenges posed by the discrete nature of language\\ndata. In this paper, we present a comprehen-\\nsive and unifying survey of data augmenta-\\ntion for NLP by summarizing the literature in\\na structured manner. We ﬁrst introduce and\\nmotivate data augmentation for NLP, and then\\ndiscuss major methodologically representative\\napproaches.\\nNext, we highlight techniques\\nthat are used for popular NLP applications and\\ntasks. We conclude by outlining current chal-\\nlenges and directions for future research. Over-\\nall, our paper aims to clarify the landscape\\nof existing literature in data augmentation for\\nNLP and motivate additional work in this area.\\nWe also present a GitHub repository with a pa-\\nper list that will be continuously updated at\\nhttps://github.com/styfeng/DataAug4NLP.\\n1\\nIntroduction\\nData augmentation (DA) refers to strategies for in-\\ncreasing the diversity of training examples without\\nexplicitly collecting new data. It has received active\\nattention in recent machine learning (ML) research\\nin the form of well-received, general-purpose tech-\\nniques such as UDA (Xie et al., 2020) (3.1), which\\nused backtranslation (Sennrich et al., 2016), Au-\\ntoAugment (Cubuk et al., 2018), and RandAugment\\n(Cubuk et al., 2020), and MIXUP (Zhang et al.,\\n2017) (3.2). These are often ﬁrst explored in com-\\nputer vision (CV), and DA’s adaptation for natural\\nlanguage processing (NLP) seems secondary and\\n∗Equal contribution by the two authors.\\n† AI Resident.\\ncomparatively underexplored, perhaps due to chal-\\nlenges presented by the discrete nature of language,\\nwhich rules out continuous noising and makes it\\nmore difﬁcult to maintain invariance.\\nDespite these challenges, there has been in-\\ncreased interest and demand for DA for NLP. As\\nNLP grows due to off-the-shelf availability of large\\npretrained models, there are increasingly more\\ntasks and domains to explore. Many of these are\\nlow-resource, and have a paucity of training exam-\\nples, creating many use-cases for which DA can\\nplay an important role. Particularly, for many non-\\nclassiﬁcation NLP tasks such as span-based tasks\\nand generation, DA research is relatively sparse\\ndespite their ubiquity in real-world settings.\\nOur paper aims to sensitize the NLP community\\ntowards this growing area of work, which has also\\nseen increasing interest in ML overall (as seen in\\nFigure 1). As interest and work on this topic con-\\ntinue to increase, this is an opportune time for a\\npaper of our kind to (i) give a bird’s eye view of\\nDA for NLP, and (ii) identify key challenges to\\neffectively motivate and orient interest in this area.\\nTo the best of our knowledge, this is the ﬁrst survey\\nto take a detailed look at DA methods for NLP.1\\nThis paper is structured as follows.\\nSection\\n2 discusses what DA is, its goals and trade-offs,\\nand why it works. Section 3 describes popular\\nmethodologically representative DA techniques for\\nNLP—which we categorize into rule-based (3.1),\\nexample interpolation-based (3.2), or model-based\\n(3.3). Section 4 discusses useful NLP applications\\nfor DA, including low-resource languages (4.1),\\nmitigating bias (4.2), ﬁxing class imbalance (4.3),\\nfew-shot learning (4.4), and adversarial examples\\n(4.5). Section 5 describes DA methods for common\\n1Liu et al. (2020a) present a smaller-scale text data aug-\\nmentation survey that is concise and focused. Our work serves\\nas a more comprehensive survey with larger coverage and is\\nmore up-to-date.\\narXiv:2105.03075v5  [cs.CL]  1 Dec 2021\\n\\nFigure 1: Weekly Google Trends scores for the search\\nterm \"data augmentation\", with a control, uneventful\\nML search term (\"minibatch\") for comparison.\\nNLP tasks including summarization (5.1), question\\nanswering (5.2), sequence tagging tasks (5.3), pars-\\ning tasks (5.4), grammatical error correction (5.5),\\nneural machine translation (5.6), data-to-text NLG\\n(5.7), open-ended and conditional text generation\\n(5.8), dialogue (5.9), and multimodal tasks (5.10).\\nFinally, Section 6 discusses challenges and future\\ndirections in DA for NLP. Appendix A lists useful\\nblog posts and code repositories.\\nThrough this work, we hope to emulate past pa-\\npers which have surveyed DA methods for other\\ntypes of data, such as images (Shorten and Khosh-\\ngoftaar, 2019), faces (Wang et al., 2019b), and time\\nseries (Iwana and Uchida, 2020). We hope to draw\\nfurther attention, elicit broader interest, and moti-\\nvate additional work in DA, particularly for NLP.\\n2\\nBackground\\nWhat is data augmentation? Data augmentation\\n(DA) encompasses methods of increasing training\\ndata diversity without directly collecting more data.\\nMost strategies either add slightly modiﬁed copies\\nof existing data or create synthetic data, aiming for\\nthe augmented data to act as a regularizer and re-\\nduce overﬁtting when training ML models (Shorten\\nand Khoshgoftaar, 2019; Hernández-García and\\nKönig, 2020). DA has been commonly used in\\nCV, where techniques like cropping, ﬂipping, and\\ncolor jittering are a standard component of model\\ntraining. In NLP, where the input space is discrete,\\nhow to generate effective augmented examples that\\ncapture the desired invariances is less obvious.\\nWhat are the goals and trade-offs?\\nDespite\\nchallenges associated with text, many DA tech-\\nniques for NLP have been proposed, ranging from\\nrule-based manipulations (Zhang et al., 2015) to\\nmore complicated generative approaches (Liu et al.,\\n2020b). As DA aims to provide an alternative to\\ncollecting more data, an ideal DA technique should\\nbe both easy-to-implement and improve model per-\\nformance. Most offer trade-offs between these two.\\nRule-based techniques are easy-to-implement\\nbut usually offer incremental performance improve-\\nments (Li et al., 2017; Wei and Zou, 2019; Wei\\net al., 2021b). Techniques leveraging trained mod-\\nels may be more costly to implement but introduce\\nmore data variation, leading to better performance\\nboosts. Model-based techniques customized for\\ndownstream tasks can have strong effects on per-\\nformance but be difﬁcult to develop and utilize.\\nFurther, the distribution of augmented data\\nshould neither be too similar nor too different from\\nthe original. This may lead to greater overﬁtting\\nor poor performance through training on examples\\nnot representative of the given domain, respectively.\\nEffective DA approaches should aim for a balance.\\nKasheﬁ\\nand\\nHwa\\n(2020)\\ndevise\\na\\nKL-\\nDivergence-based unsupervised procedure to pre-\\nemptively choose among DA heuristics, rather than\\na typical \"run-all-heuristics\" comparison, which\\ncan be very time and cost intensive.\\nInterpretation of DA\\nDao et al. (2019) note that\\n\"data augmentation is typically performed in an ad-\\nhoc manner with little understanding of the under-\\nlying theoretical principles\", and claim the typical\\nexplanation of DA as regularization to be insufﬁ-\\ncient. Overall, there indeed appears to be a lack of\\nresearch on why exactly DA works. Existing work\\non this topic is mainly surface-level, and rarely\\ninvestigates the theoretical underpinnings and prin-\\nciples. We discuss this challenge more in §6, and\\nhighlight some of the existing work below.\\nBishop (1995) show training with noised exam-\\nples is reducible to Tikhonov regularization (sub-\\nsumes L2). Rajput et al. (2019) show that DA can\\nincrease the positive margin for classiﬁers, but only\\nwhen augmenting exponentially many examples\\nfor common DA methods.\\nDao et al. (2019) think of DA transformations\\nas kernels, and ﬁnd two ways DA helps: averaging\\nof features and variance regularization. Chen et al.\\n(2020d) show that DA leads to variance reduction\\nby averaging over orbits of the group that keep the\\ndata distribution approximately invariant.\\n3\\nTechniques & Methods\\nWe now discuss some methodologically represen-\\ntative DA techniques which are relevant to all tasks\\n\\nvia the extensibility of their formulation.2\\n3.1\\nRule-Based Techniques\\nHere, we cover DA primitives which use easy-\\nto-compute, predetermined transforms sans model\\ncomponents. Feature space DA approaches gen-\\nerate augmented examples in the model’s feature\\nspace rather than input data. Many few-shot learn-\\ning approaches (Hariharan and Girshick, 2017;\\nSchwartz et al., 2018) leverage estimated feature\\nspace \"analogy\" transformations between exam-\\nples of known classes to augment for novel classes\\n(see §4.4).\\nPaschali et al. (2019) use iterative\\nafﬁne transformations and projections to maximally\\n\"stretch\" an example along the class-manifold.\\nWei and Zou (2019) propose EASY DATA AUG-\\nMENTATION (EDA), a set of token-level random\\nperturbation operations including random insertion,\\ndeletion, and swap. They show improved perfor-\\nmance on many text classiﬁcation tasks. UDA (Xie\\net al., 2020) show how supervised DA methods can\\nbe exploited for unsupervised data through consis-\\ntency training on (x, DA(x)) pairs.\\nFor paraphrase identiﬁcation, Chen et al. (2020b)\\nconstruct a signed graph over the data, with indi-\\nvidual sentences as nodes and pair labels as signed\\nedges. They use balance theory and transitivity\\nto infer augmented sentence pairs from this graph.\\nMotivated by image cropping and rotation, ¸Sahin\\nand Steedman (2018) propose dependency tree mor-\\nphing. For dependency-annotated sentences, chil-\\ndren of the same parent are swapped (à la rotation)\\nor some deleted (à la cropping), as seen in Figure 2.\\nThis is most beneﬁcial for language families with\\nrich case marking systems (e.g. Baltic and Slavic).\\n3.2\\nExample Interpolation Techniques\\nAnother class of DA techniques, pioneered by\\nMIXUP (Zhang et al., 2017), interpolates the in-\\nputs and labels of two or more real examples. This\\nclass of techniques is also sometimes referred to as\\nMixed Sample Data Augmentation (MSDA). Ensu-\\ning work has explored interpolating inner compo-\\nnents (Verma et al., 2019; Faramarzi et al., 2020),\\nmore general mixing schemes (Guo, 2020), and\\nadding adversaries (Beckham et al., 2019).\\nAnother class of extensions of MIXUP which has\\nbeen growing in the vision community attempts to\\nfuse raw input image pairs together into a single\\n2Table 1 compares several DA methods by various aspects\\nrelating to their applicability, dependencies, and requirements.\\nFigure 2: Dependency tree morphing DA applied to a\\nTurkish sentence, ¸Sahin and Steedman (2018)\\ninput image, rather than improve the continuous in-\\nterpolation mechanism. Examples of this paradigm\\ninclude CUTMIX (Yun et al., 2019), CUTOUT (De-\\nVries and Taylor, 2017) and COPY-PASTE (Ghiasi\\net al., 2020). For instance, CUTMIX replaces a\\nsmall sub-region of Image A with a patch sampled\\nfrom Image B, with the labels mixed in proportion\\nto sub-region sizes. There is potential to borrow\\nideas and inspiration from these works for NLP,\\ne.g. for multimodal work involving both images\\nand text (see \"Multimodal challenges\" in §6).\\nA bottleneck to using MIXUP for NLP tasks\\nwas the requirement of continuous inputs. This has\\nbeen overcome by mixing embeddings or higher\\nhidden layers (Chen et al., 2020c). Later variants\\npropose speech-tailored mixing schemes (Jindal\\net al., 2020b) and interpolation with adversarial\\nexamples (Cheng et al., 2020), among others.\\nSEQ2MIXUP (Guo et al., 2020) generalizes\\nMIXUP for sequence transduction tasks in two\\nways - the \"hard\" version samples a binary mask\\n(from a Bernoulli with a β(α, α) prior) and picks\\nfrom one of two sequences at each token position,\\nwhile the \"soft\" version softly interpolates between\\nsequences based on a coefﬁcient sampled from\\nβ(α, α). The \"soft\" version is found to outperform\\nthe \"hard\" version and earlier interpolation-based\\ntechniques like SWITCHOUT (Wang et al., 2018a).\\n3.3\\nModel-Based Techniques\\nSeq2seq and language models have also been used\\nfor DA. The popular BACKTRANSLATION method\\n(Sennrich et al., 2016) translates a sequence into\\nanother language and then back into the original\\n\\nFigure 3: Contextual Augmentation, Kobayashi (2018)\\nlanguage. Kumar et al. (2019a) train seq2seq mod-\\nels with their proposed method DiPS which learns\\nto generate diverse paraphrases of input text using\\na modiﬁed decoder with a submodular objective,\\nand show its effectiveness as DA for several classi-\\nﬁcation tasks. Pretrained language models such as\\nRNNs (Kobayashi, 2018) and transformers (Yang\\net al., 2020) have also been used for augmentation.\\nKobayashi (2018) generate augmented examples\\nby replacing words with others randomly drawn\\naccording to the recurrent language model’s dis-\\ntribution based on the current context (illustra-\\ntion in Figure 3). Yang et al. (2020) propose G-\\nDAUGc which generates synthetic examples using\\npretrained transformer language models, and se-\\nlects the most informative and diverse set for aug-\\nmentation. Gao et al. (2019) advocate retaining the\\nfull distribution through \"soft\" augmented exam-\\nples, showing gains on machine translation.\\nNie et al. (2020) augment word representations\\nwith a context-sensitive attention-based mixture of\\ntheir semantic neighbors from a pretrained embed-\\nding space, and show its effectiveness for NER\\non social media text. Inspired by denoising au-\\ntoencoders, Ng et al. (2020) use a corrupt-and-\\nreconstruct approach, with the corruption function\\nq(x′|x) masking an arbitrary number of word po-\\nsitions and the reconstruction function r(x|x′) un-\\nmasking them using BERT (Devlin et al., 2019).\\nTheir approach works well on domain-shifted test\\nsets across 9 datasets on sentiment, NLI, and NMT.\\nFeng et al. (2019) propose a task called SEMAN-\\nTIC TEXT EXCHANGE (STE) which involves ad-\\njusting the overall semantics of a text to ﬁt the\\ncontext of a new word/phrase that is inserted called\\nthe replacement entity (RE). They do so by using a\\nsystem called SMERTI and a masked LM approach.\\nWhile not proposed directly for DA, it can be used\\nas such, as investigated in Feng et al. (2020).\\nRather than starting from an existing exam-\\nple and modifying it, some model-based DA ap-\\nproaches directly estimate a generative process\\nfrom the training set and sample from it. Anaby-\\nTavor et al. (2020) learn a label-conditioned gen-\\nerator by ﬁnetuning GPT-2 (Radford et al., 2019)\\non the training data, using this to generate candi-\\ndate examples per class. A classiﬁer trained on the\\noriginal training set is then used to select top k can-\\ndidate examples which conﬁdently belong to the\\nrespective class for augmentation. Quteineh et al.\\n(2020) use a similar label-conditioned GPT-2 gen-\\neration method, and demonstrate its effectiveness\\nas a DA method in an active learning setup.\\nOther approaches include syntactic or controlled\\nparaphrasing (Iyyer et al., 2018; Kumar et al.,\\n2020), document-level paraphrasing (Gangal et al.,\\n2021), augmenting misclassiﬁed examples (Dreossi\\net al., 2018), BERT cross-encoder labeling of new\\ninputs (Thakur et al., 2021), guided generation us-\\ning large-scale generative language models (Liu\\net al., 2020b,c), and automated text augmentation\\n(Hu et al., 2019; Cai et al., 2020). Models can also\\nlearn to combine together simpler DA primitives\\n(Cubuk et al., 2018; Ratner et al., 2017) or add\\nhuman-in-the-loop (Kaushik et al., 2020, 2021).\\n4\\nApplications\\nIn this section, we discuss several DA methods for\\nsome common NLP applications.2\\n4.1\\nLow-Resource Languages\\nLow-resource languages are an important and chal-\\nlenging application for DA, typically for neural\\nmachine translation (NMT). Techniques using ex-\\nternal knowledge such as WordNet (Miller, 1995)\\nmay be difﬁcult to use effectively here.3 There\\nare ways to leverage high-resource languages for\\nlow-resource languages, particularly if they have\\nsimilar linguistic properties. Xia et al. (2019) use\\nthis approach to improve low-resource NMT.\\nLi et al. (2020b) use backtranslation and self-\\nlearning to generate augmented training data. In-\\nspired by work in CV, Fadaee et al. (2017) gener-\\nate additional training examples that contain low-\\nfrequency (rare) words in synthetically created con-\\ntexts. Qin et al. (2020) present a DA framework to\\ngenerate multi-lingual code-switching data to ﬁne-\\ntune multilingual-BERT. It encourages the align-\\n3Low-resource language challenges discussed more in §6.\\n\\nDA Method\\nExt.Know Pretrained\\nPreprocess\\nLevel\\nTask-Agnostic\\nSYNONYM REPLACEMENT (Zhang et al., 2015)\\n\\x13\\n×\\ntok\\nInput\\n\\x13\\nRANDOM DELETION (Wei and Zou, 2019)\\n×\\n×\\ntok\\nInput\\n\\x13\\nRANDOM SWAP (Wei and Zou, 2019)\\n×\\n×\\ntok\\nInput\\n\\x13\\nBACKTRANSLATION (Sennrich et al., 2016)\\n×\\n\\x13\\nDepends\\nInput\\n\\x13\\nSCPN (Wieting and Gimpel, 2017)\\n×\\n\\x13\\nconst\\nInput\\n\\x13\\nSEMANTIC TEXT EXCHANGE (Feng et al., 2019)\\n×\\n\\x13\\nconst\\nInput\\n\\x13\\nCONTEXTUALAUG (Kobayashi, 2018)\\n×\\n\\x13\\n-\\nInput\\n\\x13\\nLAMBADA (Anaby-Tavor et al., 2020)\\n×\\n\\x13\\n-\\nInput\\n×\\nGECA (Andreas, 2020)\\n×\\n×\\ntok\\nInput\\n×\\nSEQMIXUP (Guo et al., 2020)\\n×\\n×\\ntok\\nInput\\n×\\nSWITCHOUT (Wang et al., 2018b)\\n×\\n×\\ntok\\nInput\\n×\\nEMIX (Jindal et al., 2020a)\\n×\\n×\\n-\\nEmb/Hidden\\n\\x13\\nSPEECHMIX (Jindal et al., 2020b)\\n×\\n×\\n-\\nEmb/Hidden Speech/Audio\\nMIXTEXT (Chen et al., 2020c)\\n×\\n×\\n-\\nEmb/Hidden\\n\\x13\\nSIGNEDGRAPH (Chen et al., 2020b)\\n×\\n×\\n-\\nInput\\n×\\nDTREEMORPH (¸Sahin and Steedman, 2018)\\n×\\n×\\ndep\\nInput\\n\\x13\\nSub2 (Shi et al., 2021)\\n×\\n×\\ndep\\nInput\\nSubstructural\\nDAGA (Ding et al., 2020)\\n×\\n×\\ntok\\nInput+Label\\n×\\nWN-HYPERS (Feng et al., 2020)\\n\\x13\\n×\\nconst+KWE\\nInput\\n\\x13\\nSYNTHETIC NOISE (Feng et al., 2020)\\n×\\n×\\ntok\\nInput\\n\\x13\\nUEDIN-MS (DA part) (Grundkiewicz et al., 2019)\\n\\x13\\n×\\ntok\\nInput\\n\\x13\\nNONCE (Gulordava et al., 2018)\\n\\x13\\n×\\nconst\\nInput\\n\\x13\\nXLDA (Singh et al., 2019)\\n×\\n\\x13\\nDepends\\nInput\\n\\x13\\nSEQMIX (Zhang et al., 2020)\\n×\\n\\x13\\ntok\\nInput+Label\\n×\\nSLOT-SUB-LM (Louvan and Magnini, 2020)\\n×\\n\\x13\\ntok\\nInput\\n\\x13\\nUBT & TBT (Vaibhav et al., 2019)\\n×\\n\\x13\\nDepends\\nInput\\n\\x13\\nSOFT CONTEXTUAL DA (Gao et al., 2019)\\n×\\n\\x13\\ntok\\nEmb/Hidden\\n\\x13\\nDATA DIVERSIFICATION (Nguyen et al., 2020)\\n×\\n\\x13\\nDepends\\nInput\\n\\x13\\nDIPS (Kumar et al., 2019a)\\n×\\n\\x13\\ntok\\nInput\\n\\x13\\nAUGMENTED SBERT (Thakur et al., 2021)\\n×\\n\\x13\\n-\\nInput+Label Sentence Pairs\\nTable 1: Comparing a selection of DA methods by various aspects relating to their applicability, dependencies, and\\nrequirements. Ext.Know, KWE, tok, const, and dep stand for External Knowledge, keyword extraction, tokeniza-\\ntion, constituency parsing, and dependency parsing, respectively. Ext.Know refers to whether the DA method re-\\nquires external knowledge (e.g. WordNet) and Pretrained if it requires a pretrained model (e.g. BERT). Preprocess\\ndenotes preprocessing required, Level denotes the depth at which data is modiﬁed by the DA, and Task-Agnostic\\nrefers to whether the DA method can be applied to different tasks. See Appendix B for further explanation.\\nment of representations from source and multiple\\ntarget languages once by mixing their context in-\\nformation. They see improved performance across\\n5 tasks with 19 languages.\\n4.2\\nMitigating Bias\\nZhao et al. (2018) attempt to mitigate gender\\nbias in coreference resolution by creating an aug-\\nmented dataset identical to the original but biased\\ntowards the underrepresented gender (using gen-\\nder swapping of entities such as replacing \"he\"\\nwith \"she\") and train on the union of the two\\ndatasets. Lu et al. (2020) formally propose COUN-\\nTERFACTUAL DA (CDA) for gender bias mitiga-\\ntion, which involves causal interventions that break\\nassociations between gendered and gender-neutral\\nwords. Zmigrod et al. (2019) and Hall Maudslay\\net al. (2019) propose further improvements to CDA.\\nMoosavi et al. (2020) augment training sentences\\nwith their corresponding predicate-argument struc-\\ntures, improving the robustness of transformer mod-\\nels against various types of biases.\\n4.3\\nFixing Class Imbalance\\nFixing class imbalance typically involves a combi-\\nnation of undersampling and oversampling. SYN-\\nTHETIC MINORITY OVERSAMPLING TECHNIQUE\\n(SMOTE) (Chawla et al., 2002), which gener-\\nates augmented minority class examples through\\ninterpolation, still remains popular (Fernández\\net al., 2018). MULTILABEL SMOTE (MLSMOTE)\\n(Charte et al., 2015) modiﬁes SMOTE to balance\\nclasses for multi-label classiﬁcation, where classi-\\nﬁers predict more than one class at the same time.\\nOther techniques such as EDA (Wei and Zou, 2019)\\ncan possibly be used for oversampling as well.\\n4.4\\nFew-Shot Learning\\nDA methods can ease few-shot learning by adding\\nmore examples for novel classes introduced in the\\nfew-shot phase. Hariharan and Girshick (2017)\\nuse learned analogy transformations φ(z1, z2, x)\\n\\nbetween example pairs from a non-novel class\\nz1 →z2 to generate augmented examples x →x′\\nfor novel classes. Schwartz et al. (2018) generalize\\nthis to beyond just linear offsets, through their \"∆-\\nnetwork\" autoencoder which learns the distribution\\nP(z2|z1, C) from all y∗\\nz1 = y∗\\nz2 = C pairs, where\\nC is a class and y is the ground-truth labelling\\nfunction. Both these methods are applied only on\\nimage tasks, but their theoretical formulations are\\ngenerally applicable, and hence we discuss them.\\nKumar et al. (2019b) apply these and other\\nDA methods for few-shot learning of novel intent\\nclasses in task-oriented dialog. Wei et al. (2021a)\\nshow that data augmentation facilitates curriculum\\nlearning for training triplet networks for few-shot\\ntext classiﬁcation. Lee et al. (2021) use T5 to gen-\\nerate additional examples for data-scarce classes.\\n4.5\\nAdversarial Examples (AVEs)\\nAdversarial examples can be generated using\\ninnocuous label-preserving transformations (e.g.\\nparaphrasing) that fool state-of-the-art NLP mod-\\nels, as shown in Jia et al. (2019). Speciﬁcally,\\nthey add sentences with distractor spans to pas-\\nsages to construct AVEs for span-based QA. Zhang\\net al. (2019d) construct AVEs for paraphrase de-\\ntection using word swapping. Kang et al. (2018)\\nand Glockner et al. (2018) create AVEs for textual\\nentailment using WordNet relations.\\n5\\nTasks\\nIn this section, we discuss several DA works\\nfor common NLP tasks.2 We focus on non-\\nclassiﬁcation tasks as classiﬁcation is worked on\\nby default, and well covered in earlier sections (e.g.\\n§3 and §4). Numerous previously mentioned DA\\ntechniques, e.g. (Wei and Zou, 2019; Chen et al.,\\n2020b; Anaby-Tavor et al., 2020), have been used\\nor can be used for text classiﬁcation tasks.\\n5.1\\nSummarization\\nFabbri et al. (2020) investigate backtranslation as a\\nDA method for few-shot abstractive summarization\\nwith the use of a consistency loss inspired by UDA.\\nParida and Motlicek (2019) propose an iterative DA\\napproach for abstractive summarization that uses a\\nmix of synthetic and real data, where the former is\\ngenerated from Common Crawl. Zhu et al. (2019)\\nintroduce a query-focused summarization (Dang,\\n2005) dataset collected using Wikipedia called\\nWIKIREF which can be used for DA. Pasunuru et al.\\n(2021) use DA methods to construct two training\\ndatasets for Query-focused Multi-Document Sum-\\nmarization (QMDS) called QMDSCNN and QMD-\\nSIR by modifying CNN/DM (Hermann et al., 2015)\\nand mining search-query logs, respectively.\\n5.2\\nQuestion Answering (QA)\\nLongpre et al. (2019) investigate various DA and\\nsampling techniques for domain-agnostic QA in-\\ncluding paraphrasing by backtranslation. Yang\\net al. (2019) propose a DA method using distant\\nsupervision to improve BERT ﬁnetuning for open-\\ndomain QA. Riabi et al. (2020) leverage Question\\nGeneration models to produce augmented exam-\\nples for zero-shot cross-lingual QA. Singh et al.\\n(2019) propose XLDA, or CROSS-LINGUAL DA,\\nwhich substitutes a portion of the input text with\\nits translation in another language, improving per-\\nformance across multiple languages on NLI tasks\\nincluding the SQuAD QA task. Asai and Hajishirzi\\n(2020) use logical and linguistic knowledge to gen-\\nerate additional training data to improve the accu-\\nracy and consistency of QA responses by models.\\nYu et al. (2018) introduce a new QA architecture\\ncalled QANet that shows improved performance\\non SQuAD when combined with augmented data\\ngenerated using backtranslation.\\n5.3\\nSequence Tagging Tasks\\nDing et al. (2020) propose DAGA, a two-step DA\\nprocess. First, a language model over sequences of\\ntags and words linearized as per a certain scheme is\\nlearned. Second, sequences are sampled from this\\nlanguage model and de-linearized to generate new\\nexamples. ¸Sahin and Steedman (2018), discussed\\nin §3.1, use dependency tree morphing (Figure 2)\\nto generate additional training examples on the\\ndownstream task of part-of-speech (POS) tagging.\\nDai and Adel (2020) modify DA techniques pro-\\nposed for sentence-level tasks for named entity\\nrecognition (NER), including label-wise token and\\nsynonym replacement, and show improved perfor-\\nmance using both recurrent and transformer models.\\nZhang et al. (2020) propose a DA method based\\non MIXUP called SEQMIX for active sequence la-\\nbeling by augmenting queried samples, showing\\nimprovements on NER and Event Detection.\\n5.4\\nParsing Tasks\\nJia and Liang (2016) propose DATA RECOMBINA-\\nTION for injecting task-speciﬁc priors to neural se-\\nmantic parsers. A synchronous context-free gram-\\n\\nmar (SCFG) is induced from training data, and\\nnew \"recombinant\" examples are sampled. Yu et al.\\n(2020) introduce GRAPPA, a pretraining approach\\nfor table semantic parsing, and generate synthetic\\nquestion-SQL pairs via an SCFG. Andreas (2020)\\nuse compositionality to construct synthetic exam-\\nples for downstream tasks like semantic parsing.\\nFragments of original examples are replaced with\\nfragments from other examples in similar contexts.\\nVania et al. (2019) investigate DA for low-\\nresource dependency parsing including dependency\\ntree morphing from ¸Sahin and Steedman (2018)\\n(Figure 2) and modiﬁed nonce sentence genera-\\ntion from Gulordava et al. (2018), which replaces\\ncontent words with other words of the same POS,\\nmorphological features, and dependency labels.\\n5.5\\nGrammatical Error Correction (GEC)\\nLack of parallel data is typically a barrier for GEC.\\nVarious works have thus looked at DA methods\\nfor GEC. We discuss some here, and more can be\\nfound in Table 2 in Appendix C.\\nThere is work that makes use of additional re-\\nsources.\\nBoyd (2018) use German edits from\\nWikipedia revision history and use those relating\\nto GEC as augmented training data. Zhang et al.\\n(2019b) explore multi-task transfer, or the use of\\nannotated data from other tasks.\\nThere is also work that adds synthetic errors to\\nnoise the text. Wang et al. (2019a) investigate two\\napproaches: token-level perturbations and training\\nerror generation models with a ﬁltering strategy\\nto keep generations with sufﬁcient errors. Grund-\\nkiewicz et al. (2019) use confusion sets generated\\nby a spellchecker for noising. Choe et al. (2019)\\nlearn error patterns from small annotated samples\\nalong with POS-speciﬁc noising.\\nThere have also been approaches to improve the\\ndiversity of generated errors. Wan et al. (2020)\\ninvestigate noising through editing the latent repre-\\nsentations of grammatical sentences, and Xie et al.\\n(2018) use a neural sequence transduction model\\nand beam search noising procedures.\\n5.6\\nNeural Machine Translation (NMT)\\nThere are many works which have investigated DA\\nfor NMT. We highlighted some in §3 and §4.1,\\ne.g. (Sennrich et al., 2016; Fadaee et al., 2017; Xia\\net al., 2019). We discuss some further ones here,\\nand more can be found in Table 3 in Appendix C.\\nWang et al. (2018a) propose SWITCHOUT, a\\nDA method that randomly replaces words in both\\nsource and target sentences with other random\\nwords from their corresponding vocabularies. Gao\\net al. (2019) introduce SOFT CONTEXTUAL DA\\nthat softly augments randomly chosen words in a\\nsentence using a contextual mixture of multiple\\nrelated words over the vocabulary. Nguyen et al.\\n(2020) propose DATA DIVERSIFICATION which\\nmerges original training data with the predictions\\nof several forward and backward models.\\n5.7\\nData-to-Text NLG\\nData-to-text NLG refers to tasks which require gen-\\nerating natural language descriptions of structured\\nor semi-structured data inputs, e.g. game score\\ntables (Wiseman et al., 2017). Randomly perturb-\\ning game score values without invalidating overall\\ngame outcome is one DA strategy explored in game\\nsummary generation (Hayashi et al., 2019).\\nTwo popular recent benchmarks are E2E-NLG\\n(Dušek et al., 2018) and WebNLG (Gardent et al.,\\n2017). Both involve generation from structured\\ninputs - meaning representation (MR) sequences\\nand triple sequences, respectively. Montella et al.\\n(2020) show performance gains on WebNLG by\\nDA using Wikipedia sentences as targets and\\nparsed OpenIE triples as inputs.\\nTandon et al.\\n(2018) propose DA for E2E-NLG based on per-\\nmuting the input MR sequence. Kedzie and McK-\\neown (2019) inject Gaussian noise into a trained\\ndecoder’s hidden states and sample diverse aug-\\nmented examples from it. This sample-augment-\\nretrain loop helps performance on E2E-NLG.\\n5.8\\nOpen-Ended & Conditional Generation\\nThere has been limited work on DA for open-ended\\nand conditional text generation. Feng et al. (2020)\\nexperiment with a suite of DA methods for ﬁnetun-\\ning GPT-2 on a low-resource domain in attempts\\nto improve the quality of generated continuations,\\nwhich they call GENAUG. They ﬁnd that WN-\\nHYPERS (WordNet hypernym replacement of key-\\nwords) and SYNTHETIC NOISE (randomly perturb-\\ning non-terminal characters in words) are useful,\\nand the quality of generated text improves to a peak\\nat ≈3x the original amount of training data.\\n5.9\\nDialogue\\nMost DA approaches for dialogue focus on task-\\noriented dialogue. We outline some below, and\\nmore can be found in Table 4 in Appendix C.\\nQuan and Xiong (2019) present sentence and\\nword-level DA approaches for end-to-end task-\\n\\noriented dialogue. Louvan and Magnini (2020)\\npropose LIGHTWEIGHT AUGMENTATION, a set of\\nword-span and sentence-level DA methods for low-\\nresource slot ﬁlling and intent classiﬁcation.\\nHou et al. (2018) present a seq2seq DA frame-\\nwork to augment dialogue utterances for dialogue\\nlanguage understanding (Young et al., 2013), in-\\ncluding a diversity rank to produce diverse utter-\\nances. Zhang et al. (2019c) propose MADA to\\ngenerate diverse responses using the property that\\nseveral valid responses exist for a dialogue context.\\nThere is also DA work for spoken dialogue. Hou\\net al. (2018), Kim et al. (2019), Zhao et al. (2019),\\nand Yoo et al. (2019) investigate DA methods for di-\\nalogue and spoken language understanding (SLU),\\nincluding generative latent variable models.\\n5.10\\nMultimodal Tasks\\nDA techniques have also been proposed for multi-\\nmodal tasks where aligned data for multiple modal-\\nities is required. We look at ones that involve lan-\\nguage or text. Some are discussed below, and more\\ncan be found in Table 5 in Appendix C.\\nBeginning with speech, Wang et al. (2020) pro-\\npose a DA method to improve the robustness of\\ndownstream dialogue models to speech recognition\\nerrors. Wiesner et al. (2018) and Renduchintala\\net al. (2018) propose DA methods for end-to-end\\nautomatic speech recognition (ASR).\\nLooking at images or video, Xu et al. (2020)\\nlearn a cross-modality matching network to pro-\\nduce synthetic image-text pairs for multimodal clas-\\nsiﬁers. Atliha and Šešok (2020) explore DA meth-\\nods such as synonym replacement and contextual-\\nized word embeddings augmentation using BERT\\nfor image captioning. Kaﬂe et al. (2017), Yokota\\nand Nakayama (2018), and Tang et al. (2020) pro-\\npose methods for visual QA including question\\ngeneration and adversarial examples.\\n6\\nChallenges & Future Directions\\nLooking forward, data augmentation faces substan-\\ntial challenges, speciﬁcally for NLP, and with these\\nchallenges, new opportunities for future work arise.\\nDissonance between empirical novelties and\\ntheoretical narrative:\\nThere appears to be a con-\\nspicuous lack of research on why DA works. Most\\nstudies might show empirically that a DA technique\\nworks and provide some intuition, but it is currently\\nchallenging to measure the goodness of a technique\\nwithout resorting to a full-scale experiment. A re-\\ncent work in vision (Gontijo-Lopes et al., 2020)\\nhas proposed that afﬁnity (the distributional shift\\ncaused by DA) and diversity (the complexity of the\\naugmentation) can predict DA performance, but it\\nis unclear how these results might translate to NLP.\\nMinimal beneﬁt for pretrained models on in-\\ndomain data:\\nWith the popularization of large\\npretrained language models, it has come to light\\nthat a couple previously effective DA techniques\\nfor certain English text classiﬁcation tasks (Wei and\\nZou, 2019; Sennrich et al., 2016) provide little ben-\\neﬁt for models like BERT and RoBERTa, which\\nalready achieve high performance on in-domain\\ntext classiﬁcation (Longpre et al., 2020). One hy-\\npothesis is that using simple DA techniques pro-\\nvides little beneﬁt when ﬁnetuning large pretrained\\ntransformers on tasks for which examples are well-\\nrepresented in the pretraining data, but DA methods\\ncould still be effective when ﬁnetuning on tasks for\\nwhich examples are scarce or out-of-domain com-\\npared with the training data. Further work could\\nstudy under which scenarios data augmentation for\\nlarge pretrained models is likely to be effective.\\nMultimodal challenges:\\nWhile there has been\\nincreased work in multimodal DA, as discussed in\\n§5.10, effective DA methods for multiple modal-\\nities has been challenging. Many works focus on\\naugmenting a single modality or multiple ones sep-\\narately. For example, there is potential to further\\nexplore simultaneous image and text augmentation\\nfor image captioning, such as a combination of\\nCUTMIX (Yun et al., 2019) and caption editing.\\nSpan-based tasks\\noffer unique DA challenges\\nas there are typically many correlated classiﬁcation\\ndecisions. For example, random token replacement\\nmay be a locally acceptable DA method but possi-\\nbly disrupt coreference chains for latter sentences.\\nDA techniques here must take into account depen-\\ndencies between different locations in the text.\\nWorking in specialized domains\\nsuch as those\\nwith domain-speciﬁc vocabulary and jargon (e.g.\\nmedicine) can present challenges. Many pretrained\\nmodels and external knowledge (e.g. WordNet)\\ncannot be effectively used. Studies have shown\\nthat DA becomes less beneﬁcial when applied to\\nout-of-domain data, likely because the distribution\\nof augmented data can substantially differ from the\\noriginal data (Zhang et al., 2019a; Herzig et al.,\\n2020; Campagna et al., 2020; Zhong et al., 2020).\\n\\nWorking with low-resource languages\\nmay\\npresent similar difﬁculties as specialized domains.\\nFurther, DA techniques successful in the high-\\nresource scenario may not be effective for low-\\nresource languages that are of a different language\\nfamily or very distinctive in linguistic and typolog-\\nical terms. For example, those which are language\\nisolates or lack high-resource cognates.\\nMore\\nvision-inspired\\ntechniques:\\nAlthough\\nmany NLP DA methods have been inspired by anal-\\nogous approaches in CV, there is potential for draw-\\ning further connections. Many CV DA techniques\\nmotivated by real-world invariances (e.g. many\\nangles of looking at the same object) may have\\nsimilar NLP interpretations. For instance, grayscal-\\ning could translate to toning down aspects of the\\ntext (e.g. plural to singular, \"awesome\" →\"good\").\\nMorphing a dependency tree could be analogous\\nto rotating an image, and paraphrasing techniques\\nmay be analogous to changing perspective. For ex-\\nample, negative data augmentation (NDA) (Sinha\\net al., 2021) involves creating out-of-distribution\\nsamples. It has so far been exclusively explored for\\nCV, but could be investigated for text.\\nSelf-supervised learning:\\nMore recently, DA\\nhas been increasingly used as a key component\\nof self-supervised learning, particularly in vision\\n(Chen et al., 2020e). In NLP, BART (Lewis et al.,\\n2020) showed that predicting deleted tokens as a\\npretraining task can achieve similar performance as\\nthe masked LM, and ELECTRA (Clark et al., 2020)\\nfound that pretraining by predicting corrupted to-\\nkens outperforms BERT given the same model size,\\ndata, and compute. We expect future work will\\ncontinue exploring how to effectively manipulate\\ntext for both pretraining and downstream tasks.\\nOfﬂine versus online data augmentation:\\nIn\\nCV, standard techniques such as cropping and ro-\\ntations are typically done stochastically, allowing\\nfor DA to be incorporated elegantly into the train-\\ning pipeline. In NLP, however, it is unclear how\\nto include a lightweight code module to apply DA\\nstochastically. This is because DA techniques for\\nNLP often leverage external resources (e.g. a dic-\\ntionary for token substitution or translation model\\nfor backtranslation) that are not easily transferable\\nacross training pipelines. Thus, a common prac-\\ntice for DA in NLP is to generate augmented data\\nofﬂine and store it as additional data to be loaded\\nduring training.4 Future work on a lightweight\\nmodule for online DA in NLP could be fruitful,\\nthough another challenge will be determining when\\nsuch a module will be helpful, which—compared\\nwith CV, where invariances being imposed are well-\\naccepted—can vary substantially across NLP tasks.\\nLack of uniﬁcation\\nis a challenge for the cur-\\nrent literature on data augmentation for NLP, and\\npopular methods are often presented in an aux-\\niliary fashion. Whereas there are well-accepted\\nframeworks for DA for CV (e.g. default augmen-\\ntation libraries in PyTorch, RandAugment (Cubuk\\net al., 2020)), there are no such \"generalized\" DA\\ntechniques for NLP. Further, we believe that DA\\nresearch would beneﬁt from the establishment of\\nstandard and uniﬁed benchmark tasks and datasets\\nto compare different augmentation methods.\\nGood data augmentation practices\\nwould help\\nmake DA work more accessible and reproducible\\nto the NLP and ML communities.\\nOn top of\\nuniﬁed benchmark tasks, datasets, and frame-\\nworks/libraries mentioned above, other good prac-\\ntices include making code and augmented datasets\\npublicly available, reporting variation among re-\\nsults (e.g. standard deviation across random seeds),\\nand more standardized evaluation procedures. Fur-\\nther, transparent hyperparameter analysis, explic-\\nitly stating failure cases of proposed techniques,\\nand discussion of the intuition and theory behind\\nthem would further improve the transparency and\\ninterpretability of DA techniques.\\n7\\nConclusion\\nIn this paper, we presented a comprehensive and\\nstructured survey of data augmentation for nat-\\nural language processing (NLP). We provided a\\nbackground about data augmentation and how it\\nworks, discussed major methodologically represen-\\ntative data augmentation techniques for NLP, and\\ntouched upon data augmentation techniques for\\npopular NLP applications and tasks. Finally, we\\noutlined current challenges and directions for fu-\\nture research, and showed that there is much room\\nfor further exploration. Overall, we hope our paper\\ncan serve as a guide for NLP researchers to decide\\non which data augmentation techniques to use, and\\ninspire additional interest and work in this area.\\nPlease see the corresponding GitHub repository at\\nhttps://github.com/styfeng/DataAug4NLP.\\n4See Appendix D.\\n\\nReferences\\nDiego Alves, Askars Salimbajevs, and M¯arcis Pinnis.\\n2020. Data augmentation for pipeline-based speech\\ntranslation. In 9th International Conference on Hu-\\nman Language Technologies - the Baltic Perspective\\n(Baltic HLT 2020), Kaunas, Lithuania.\\nAteret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich,\\nAmir Kantor, George Kour, Segev Shlomov, Naama\\nTepper, and Naama Zwerdling. 2020. Do not have\\nenough data? Deep learning to the rescue! In Pro-\\nceedings of AAAI, pages 7383–7390.\\nJacob Andreas. 2020.\\nGood-enough compositional\\ndata augmentation. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics, pages 7556–7566, Online. Association\\nfor Computational Linguistics.\\nAkari Asai and Hannaneh Hajishirzi. 2020.\\nLogic-\\nguided data augmentation and regularization for con-\\nsistent question answering. In Proceedings of the\\n58th Annual Meeting of the Association for Compu-\\ntational Linguistics, pages 5642–5650, Online. As-\\nsociation for Computational Linguistics.\\nViktar Atliha and Dmitrij Šešok. 2020. Text augmen-\\ntation using BERT for image captioning. Applied\\nSciences, 10:5978.\\nChristopher Beckham, Sina Honari, Vikas Verma,\\nAlex M. Lamb, Farnoosh Ghadiri, R Devon Hjelm,\\nYoshua Bengio, and Chris Pal. 2019. On adversarial\\nmixup resynthesis. In Advances in Neural Informa-\\ntion Processing Systems, pages 4346–4357.\\nChris M. Bishop. 1995. Training with noise is equiv-\\nalent to Tikhonov regularization. Neural Computa-\\ntion, 7(1):108–116.\\nAdriane Boyd. 2018.\\nUsing Wikipedia edits in low\\nresource grammatical error correction. In Proceed-\\nings of the 2018 EMNLP Workshop W-NUT: The\\n4th Workshop on Noisy User-generated Text, pages\\n79–84, Brussels, Belgium. Association for Compu-\\ntational Linguistics.\\nBram Bulte and Arda Tezcan. 2019. Neural fuzzy re-\\npair: Integrating fuzzy matches into neural machine\\ntranslation. In Proceedings of the 57th Annual Meet-\\ning of the Association for Computational Linguistics,\\npages 1800–1809, Florence, Italy. Association for\\nComputational Linguistics.\\nHengyi Cai, Hongshen Chen, Yonghao Song, Cheng\\nZhang, Xiaofang Zhao, and Dawei Yin. 2020. Data\\nmanipulation: Towards effective instance learning\\nfor neural dialogue generation via learning to aug-\\nment and reweight. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics, pages 6334–6343, Online. Association\\nfor Computational Linguistics.\\nGiovanni Campagna, Agata Foryciarz, Mehrad Morad-\\nshahi, and Monica Lam. 2020. Zero-shot transfer\\nlearning with synthesized data for multi-domain dia-\\nlogue state tracking. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics, pages 122–132, Online. Association for\\nComputational Linguistics.\\nF. Charte, Antonio Rivera Rivas, María José Del Je-\\nsus, and Francisco Herrera. 2015.\\nMlsmote: Ap-\\nproaching imbalanced multilabel learning through\\nsynthetic instance generation.\\nKnowledge-Based\\nSystems.\\nNitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall,\\nand W. Philip Kegelmeyer. 2002.\\nSMOTE: Syn-\\nthetic minority over-sampling technique. Journal of\\nArtiﬁcial Intelligence Research, 16:321–357.\\nGuanhua Chen, Yun Chen, Yong Wang, and Victor O.K.\\nLi. 2020a. Lexical-constraint-aware neural machine\\ntranslation via data augmentation. In Proceedings of\\nthe Twenty-Ninth International Joint Conference on\\nArtiﬁcial Intelligence, IJCAI-20, pages 3587–3593.\\nInternational Joint Conferences on Artiﬁcial Intelli-\\ngence Organization. Main track.\\nHannah Chen, Yangfeng Ji, and David Evans. 2020b.\\nFinding friends and ﬂipping frenemies: Automatic\\nparaphrase dataset augmentation using graph theory.\\nIn Findings of the Association for Computational\\nLinguistics: EMNLP 2020, pages 4741–4751, On-\\nline. Association for Computational Linguistics.\\nJiaao Chen, Zichao Yang, and Diyi Yang. 2020c. Mix-\\nText: Linguistically-informed interpolation of hid-\\nden space for semi-supervised text classiﬁcation. In\\nProceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics, pages 2147–\\n2157, Online. Association for Computational Lin-\\nguistics.\\nShuxiao Chen, Edgar Dobriban, and Jane Lee. 2020d.\\nA group-theoretic framework for data augmentation.\\nAdvances in Neural Information Processing Systems,\\n33.\\nTing Chen, Simon Kornblith, Mohammad Norouzi,\\nand Geoffrey Hinton. 2020e. A simple framework\\nfor contrastive learning of visual representations. In\\nProceedings of the 37th International Conference\\non Machine Learning, volume 119 of Proceedings\\nof Machine Learning Research, pages 1597–1607.\\nPMLR.\\nYong Cheng, Lu Jiang, Wolfgang Macherey, and Ja-\\ncob Eisenstein. 2020.\\nAdvAug: Robust adversar-\\nial augmentation for neural machine translation. In\\nProceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics, pages 5961–\\n5970, Online. Association for Computational Lin-\\nguistics.\\nMara Chinea-Ríos,\\nÁlvaro Peris,\\nand Francisco\\nCasacuberta. 2017. Adapting neural machine trans-\\nlation with parallel synthetic data. In Proceedings\\nof the Second Conference on Machine Translation,\\n\\npages 138–147, Copenhagen, Denmark. Association\\nfor Computational Linguistics.\\nYo Joong Choe, Jiyeon Ham, Kyubyong Park, and\\nYeoil Yoon. 2019. A neural grammatical error cor-\\nrection system built on better pre-training and se-\\nquential transfer learning.\\nIn Proceedings of the\\nFourteenth Workshop on Innovative Use of NLP for\\nBuilding Educational Applications, pages 213–227,\\nFlorence, Italy. Association for Computational Lin-\\nguistics.\\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and\\nChristopher D. Manning. 2020.\\nELECTRA: Pre-\\ntraining text encoders as discriminators rather than\\ngenerators. Proceedings of ICLR.\\nEkin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay\\nVasudevan, and Quoc V. Le. 2018. Autoaugment:\\nLearning augmentation policies from data. Proceed-\\nings of CVPR.\\nEkin D. Cubuk, Barret Zoph, Jonathon Shlens, and\\nQuoc V. Le. 2020.\\nRandaugment: Practical au-\\ntomated data augmentation with a reduced search\\nspace. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition Work-\\nshops, pages 702–703.\\nXiang Dai and Heike Adel. 2020.\\nAn analysis of\\nsimple data augmentation for named entity recogni-\\ntion. In Proceedings of the 28th International Con-\\nference on Computational Linguistics, pages 3861–\\n3867, Barcelona, Spain (Online). International Com-\\nmittee on Computational Linguistics.\\nHao T. Dang. 2005. Overview of DUC 2005.\\nTri Dao, Albert Gu, Alexander J. Ratner, Virginia\\nSmith, Christopher De Sa, and Christopher Ré. 2019.\\nA kernel theory of modern data augmentation. Pro-\\nceedings of Machine Learning Research, 97:1528.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019.\\nBERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers), pages\\n4171–4186.\\nTerrance DeVries and Graham W Taylor. 2017.\\nIm-\\nproved regularization of convolutional neural net-\\nworks with cutout. arXiv preprint.\\nBosheng Ding, Linlin Liu, Lidong Bing, Canasai Kru-\\nengkrai, Thien Hai Nguyen, Shaﬁq Joty, Luo Si, and\\nChunyan Miao. 2020. DAGA: Data augmentation\\nwith a generation approach forLow-resource tagging\\ntasks.\\nIn Proceedings of the 2020 Conference on\\nEmpirical Methods in Natural Language Process-\\ning (EMNLP), pages 6045–6057, Online. Associa-\\ntion for Computational Linguistics.\\nTommaso Dreossi, Shromona Ghosh, Xiangyu Yue,\\nKurt Keutzer, Alberto L. Sangiovanni-Vincentelli,\\nand Sanjit A. Seshia. 2018. Counterexample-guided\\ndata augmentation. In Proceedings of IJCAI.\\nSufeng Duan, Hai Zhao, Dongdong Zhang, and Rui\\nWang. 2020. Syntax-aware data augmentation for\\nneural machine translation. arXiv preprint.\\nOndˇrej Dušek, Jekaterina Novikova, and Verena Rieser.\\n2018.\\nFindings of the E2E NLG challenge.\\nIn\\nProceedings of the 11th International Conference\\non Natural Language Generation, pages 322–328,\\nTilburg University, The Netherlands. Association for\\nComputational Linguistics.\\nAlexander R. Fabbri,\\nSimeng Han,\\nHaoyuan Li,\\nHaoran Li, Marjan Ghazvininejad, Shaﬁq Joty,\\nDragomir Radev, and Yashar Mehdad. 2020.\\nIm-\\nproving zero and few-shot abstractive summariza-\\ntion with intermediate ﬁne-tuning and data augmen-\\ntation. arXiv preprint.\\nMarzieh Fadaee, Arianna Bisazza, and Christof Monz.\\n2017.\\nData augmentation for low-resource neural\\nmachine translation. In Proceedings of the 55th An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 2: Short Papers), pages 567–\\n573, Vancouver, Canada. Association for Computa-\\ntional Linguistics.\\nAlex Falcon, Oswald Lanz, and Giuseppe Serra. 2020.\\nData augmentation techniques for the video question\\nanswering task. arXiv preprint.\\nMojtaba Faramarzi, Mohammad Amini, Akilesh Badri-\\nnaaraayanan, Vikas Verma, and Sarath Chandar.\\n2020. Patchup: A regularization technique for con-\\nvolutional neural networks. arXiv preprint.\\nMariano Felice. 2016. Artiﬁcial error generation for\\ntranslation-based grammatical error correction. Uni-\\nversity of Cambridge Technical Report.\\nSteven Y. Feng, Varun Gangal, Dongyeop Kang,\\nTeruko Mitamura, and Eduard Hovy. 2020. GenAug:\\nData augmentation for ﬁnetuning text generators. In\\nProceedings of Deep Learning Inside Out (DeeLIO):\\nThe First Workshop on Knowledge Extraction and\\nIntegration for Deep Learning Architectures, pages\\n29–42, Online. Association for Computational Lin-\\nguistics.\\nSteven Y. Feng, Aaron W. Li, and Jesse Hoey. 2019.\\nKeep calm and switch on! Preserving sentiment and\\nﬂuency in semantic text exchange. In Proceedings\\nof the 2019 Conference on Empirical Methods in\\nNatural Language Processing and the 9th Interna-\\ntional Joint Conference on Natural Language Pro-\\ncessing (EMNLP-IJCNLP), pages 2701–2711, Hong\\nKong, China. Association for Computational Lin-\\nguistics.\\nAlberto Fernández, Salvador Garcia, Francisco Her-\\nrera, and Nitesh V. Chawla. 2018. SMOTE for learn-\\ning from imbalanced data: progress and challenges,\\n\\nmarking the 15-year anniversary. Journal of Artiﬁ-\\ncial Intelligence Research, 61:863–905.\\nJennifer Foster and Oistein Andersen. 2009.\\nGen-\\nERRate: Generating errors for use in grammatical\\nerror detection. In Proceedings of the Fourth Work-\\nshop on Innovative Use of NLP for Building Edu-\\ncational Applications, pages 82–90, Boulder, Col-\\norado. Association for Computational Linguistics.\\nVarun Gangal, Steven Y. Feng, Eduard Hovy, and\\nTeruko Mitamura. 2021. Nareor: The narrative re-\\nordering problem. arXiv preprint.\\nFei Gao, Jinhua Zhu, Lijun Wu, Yingce Xia, Tao\\nQin, Xueqi Cheng, Wengang Zhou, and Tie-Yan Liu.\\n2019. Soft contextual data augmentation for neural\\nmachine translation. In Proceedings of the 57th An-\\nnual Meeting of the Association for Computational\\nLinguistics, pages 5539–5544, Florence, Italy. Asso-\\nciation for Computational Linguistics.\\nSilin Gao, Yichi Zhang, Zhijian Ou, and Zhou Yu.\\n2020.\\nParaphrase augmented task-oriented dialog\\ngeneration. In Proceedings of the 58th Annual Meet-\\ning of the Association for Computational Linguistics,\\npages 639–649, Online. Association for Computa-\\ntional Linguistics.\\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\\nand Laura Perez-Beltrachini. 2017. The WebNLG\\nchallenge: Generating text from RDF data. In Pro-\\nceedings of the 10th International Conference on\\nNatural Language Generation, pages 124–133, San-\\ntiago de Compostela, Spain. Association for Compu-\\ntational Linguistics.\\nGolnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian,\\nTsung-Yi Lin, Ekin D. Cubuk, Quoc V. Le, and\\nBarret Zoph. 2020. Simple copy-paste is a strong\\ndata augmentation method for instance segmenta-\\ntion. arXiv preprint.\\nMax Glockner, Vered Shwartz, and Yoav Goldberg.\\n2018. Breaking NLI systems with sentences that re-\\nquire simple lexical inferences. In Proceedings of\\nthe 56th Annual Meeting of the Association for Com-\\nputational Linguistics (Volume 2:\\nShort Papers),\\npages 650–655, Melbourne, Australia. Association\\nfor Computational Linguistics.\\nRaphael Gontijo-Lopes, Sylvia J. Smullin, Ekin D.\\nCubuk, and Ethan Dyer. 2020.\\nTradeoffs in data\\naugmentation: An empirical study. Proceedings of\\nICLR.\\nMiguel Graça, Yunsu Kim, Julian Schamper, Shahram\\nKhadivi, and Hermann Ney. 2019.\\nGeneralizing\\nback-translation in neural machine translation.\\nIn\\nProceedings of the Fourth Conference on Machine\\nTranslation (Volume 1: Research Papers), pages 45–\\n52, Florence, Italy. Association for Computational\\nLinguistics.\\nMilan Gritta, Gerasimos Lampouras, and Ignacio Ia-\\ncobacci. 2021. Conversation graph: Data augmen-\\ntation, training, and evaluation for non-deterministic\\ndialogue management. Transactions of the Associa-\\ntion for Computational Linguistics, 9:36–52.\\nRoman Grundkiewicz, Marcin Junczys-Dowmunt, and\\nKenneth Heaﬁeld. 2019. Neural grammatical error\\ncorrection systems with unsupervised pre-training\\non synthetic data. In Proceedings of the Fourteenth\\nWorkshop on Innovative Use of NLP for Building\\nEducational Applications, pages 252–263, Florence,\\nItaly. Association for Computational Linguistics.\\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\\nTal Linzen, and Marco Baroni. 2018.\\nColorless\\ngreen recurrent networks dream hierarchically. In\\nProceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies,\\nVolume 1 (Long Papers), pages 1195–1205, New\\nOrleans, Louisiana. Association for Computational\\nLinguistics.\\nDemi Guo, Yoon Kim, and Alexander Rush. 2020.\\nSequence-level mixed sample data augmentation. In\\nProceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP),\\npages 5547–5552, Online. Association for Computa-\\ntional Linguistics.\\nHongyu Guo. 2020.\\nNonlinear mixup:\\nOut-of-\\nmanifold data augmentation for text classiﬁcation.\\nIn Proceedings of AAAI, pages 4044–4051.\\nRowan Hall Maudslay, Hila Gonen, Ryan Cotterell,\\nand Simone Teufel. 2019. It’s all in the name: Mit-\\nigating gender bias with name-based counterfactual\\ndata substitution. In Proceedings of the 2019 Con-\\nference on Empirical Methods in Natural Language\\nProcessing and the 9th International Joint Confer-\\nence on Natural Language Processing (EMNLP-\\nIJCNLP), pages 5267–5275, Hong Kong, China. As-\\nsociation for Computational Linguistics.\\nBharath Hariharan and Ross Girshick. 2017.\\nLow-\\nshot visual recognition by shrinking and hallucinat-\\ning features. In Proceedings of the IEEE Interna-\\ntional Conference on Computer Vision, pages 3018–\\n3027.\\nHany Hassan, Mostafa Elaraby, and Ahmed Tawﬁk.\\n2017. Synthetic data for neural machine translation\\nof spoken-dialects. arXiv preprint.\\nHiroaki Hayashi, Yusuke Oda, Alexandra Birch, Ioan-\\nnis Konstas, Andrew Finch, Minh-Thang Luong,\\nGraham Neubig, and Katsuhito Sudoh. 2019. Find-\\nings of the third workshop on neural generation and\\ntranslation.\\nIn Proceedings of the 3rd Workshop\\non Neural Generation and Translation, pages 1–14,\\nHong Kong. Association for Computational Linguis-\\ntics.\\n\\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\\nand Phil Blunsom. 2015. Teaching machines to read\\nand comprehend. In NeurIPS, pages 1693–1701.\\nAlex Hernández-García and Peter König. 2020. Data\\naugmentation instead of explicit regularization.\\narXiv preprint.\\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\\nMüller, Francesco Piccinno, and Julian Eisenschlos.\\n2020. TaPas: Weakly supervised table parsing via\\npre-training.\\nIn Proceedings of the 58th Annual\\nMeeting of the Association for Computational Lin-\\nguistics, pages 4320–4333, Online. Association for\\nComputational Linguistics.\\nYutai Hou, Yijia Liu, Wanxiang Che, and Ting Liu.\\n2018. Sequence-to-sequence data augmentation for\\ndialogue language understanding. In Proceedings of\\nthe 27th International Conference on Computational\\nLinguistics, pages 1234–1245, Santa Fe, New Mex-\\nico, USA. Association for Computational Linguis-\\ntics.\\nZhiting Hu, Bowen Tan, Russ R. Salakhutdinov,\\nTom M. Mitchell, and Eric P. Xing. 2019. Learning\\ndata manipulation for augmentation and weighting.\\nIn Advances in Neural Information Processing Sys-\\ntems, volume 32. Curran Associates, Inc.\\nJian Huang, Ya Li, Jianhua Tao, Zheng Lian, Mingyue\\nNiu, and Minghao Yang. 2018. Multimodal continu-\\nous emotion recognition with data augmentation us-\\ning recurrent neural networks. In Proceedings of the\\n2018 on Audio/Visual Emotion Challenge and Work-\\nshop, AVEC’18, page 57–64, New York, NY, USA.\\nAssociation for Computing Machinery.\\nBrian Kenji Iwana and Seiichi Uchida. 2020. An em-\\npirical survey of data augmentation for time series\\nclassiﬁcation with neural networks. arXiv preprint.\\nMohit Iyyer, John Wieting, Kevin Gimpel, and Luke\\nZettlemoyer. 2018. Adversarial example generation\\nwith syntactically controlled paraphrase networks.\\nIn Proceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies,\\nVolume 1 (Long Papers), pages 1875–1885, New\\nOrleans, Louisiana. Association for Computational\\nLinguistics.\\nRobin Jia and Percy Liang. 2016. Data recombination\\nfor neural semantic parsing. In Proceedings of the\\n54th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), pages\\n12–22, Berlin, Germany. Association for Computa-\\ntional Linguistics.\\nRobin Jia, Aditi Raghunathan, Kerem Göksel, and\\nPercy Liang. 2019.\\nCertiﬁed robustness to adver-\\nsarial word substitutions.\\nIn Proceedings of the\\n2019 Conference on Empirical Methods in Natu-\\nral Language Processing and the 9th International\\nJoint Conference on Natural Language Processing\\n(EMNLP-IJCNLP), pages 4120–4133.\\nAmit Jindal, Arijit Ghosh Chowdhury, Aniket Didolkar,\\nDi Jin, Ramit Sawhney, and Rajiv Ratn Shah. 2020a.\\nAugmenting NLP models using latent feature in-\\nterpolations.\\nIn Proceedings of the 28th Inter-\\nnational Conference on Computational Linguistics,\\npages 6931–6936, Barcelona, Spain (Online). Inter-\\nnational Committee on Computational Linguistics.\\nAmit Jindal, Narayanan Elavathur Ranganatha, Aniket\\nDidolkar, Arijit Ghosh Chowdhury, Di Jin, Ramit\\nSawhney, and Rajiv Ratn Shah. 2020b. Speechmix\\n- augmenting deep sound recognition using hidden\\nspace interpolations. In INTERSPEECH, pages 861–\\n865.\\nKushal Kaﬂe, Mohammed Yousefhussien, and Christo-\\npher Kanan. 2017.\\nData augmentation for visual\\nquestion answering. In Proceedings of the 10th In-\\nternational Conference on Natural Language Gen-\\neration, pages 198–202, Santiago de Compostela,\\nSpain. Association for Computational Linguistics.\\nDongyeop Kang, Tushar Khot, Ashish Sabharwal, and\\nEduard Hovy. 2018. AdvEntuRe: Adversarial train-\\ning for textual entailment with knowledge-guided ex-\\namples. In Proceedings of the 56th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers), pages 2418–2428, Mel-\\nbourne, Australia. Association for Computational\\nLinguistics.\\nMin-Hyung Kang. 2019.\\nValar nmt : Vastly lack-\\ning resources neural machine translation. Stanford\\nCS224N.\\nOmid Kasheﬁand Rebecca Hwa. 2020. Quantifying\\nthe evaluation of heuristic methods for textual data\\naugmentation.\\nIn Proceedings of the Sixth Work-\\nshop on Noisy User-generated Text (W-NUT 2020),\\npages 200–208, Online. Association for Computa-\\ntional Linguistics.\\nDivyansh Kaushik, Eduard Hovy, and Zachary Lipton.\\n2020. Learning the difference that makes a differ-\\nence with counterfactually-augmented data. In Inter-\\nnational Conference on Learning Representations.\\nDivyansh Kaushik, Amrith Setlur, Eduard H. Hovy,\\nand Zachary Chase Lipton. 2021. Explaining the ef-\\nﬁcacy of counterfactually augmented data. In Inter-\\nnational Conference on Learning Representations.\\nChris Kedzie and Kathleen McKeown. 2019. A good\\nsample is hard to ﬁnd: Noise injection sampling and\\nself-training for neural language generation models.\\nIn Proceedings of the 12th International Conference\\non Natural Language Generation, pages 584–593,\\nTokyo, Japan. Association for Computational Lin-\\nguistics.\\nHwa-Yeon Kim, Yoon-Hyung Roh, and Young-Kil\\nKim. 2019.\\nData augmentation by data noising\\n\\nfor open-vocabulary slots in spoken language under-\\nstanding. In Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Student Research Work-\\nshop, pages 97–102, Minneapolis, Minnesota. Asso-\\nciation for Computational Linguistics.\\nAlex Kimn. 2020. A syntactic rule-based framework\\nfor parallel data synthesis in japanese gec.\\nMas-\\nsachusetts Institute of Technology.\\nSosuke Kobayashi. 2018.\\nContextual augmentation:\\nData augmentation by words with paradigmatic re-\\nlations. In Proceedings of the 2018 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, Volume 2 (Short Papers), pages 452–457,\\nNew Orleans, Louisiana. Association for Computa-\\ntional Linguistics.\\nAshutosh Kumar, Kabir Ahuja, Raghuram Vadapalli,\\nand Partha Talukdar. 2020.\\nSyntax-guided con-\\ntrolled generation of paraphrases.\\nTransactions\\nof the Association for Computational Linguistics,\\n8:329–345.\\nAshutosh Kumar, Satwik Bhattamishra, Manik Bhan-\\ndari, and Partha Talukdar. 2019a.\\nSubmodular\\noptimization-based diverse paraphrasing and its ef-\\nfectiveness in data augmentation. In Proceedings of\\nthe 2019 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies, Volume 1 (Long\\nand Short Papers), pages 3609–3619, Minneapolis,\\nMinnesota. Association for Computational Linguis-\\ntics.\\nVarun Kumar, Hadrien Glaude, Cyprien de Lichy, and\\nWlliam Campbell. 2019b. A closer look at feature\\nspace data augmentation for few-shot intent classi-\\nﬁcation.\\nIn Proceedings of the 2nd Workshop on\\nDeep Learning Approaches for Low-Resource NLP\\n(DeepLo 2019), pages 1–10, Hong Kong, China. As-\\nsociation for Computational Linguistics.\\nKenton Lee, Kelvin Guu, Luheng He, Tim Dozat, and\\nHyung Won Chung. 2021. Neural data augmenta-\\ntion via example extrapolation. arXiv preprint.\\nMike\\nLewis,\\nYinhan\\nLiu,\\nNaman\\nGoyal,\\nMar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\\n2020. BART: Denoising sequence-to-sequence pre-\\ntraining for natural language generation, translation,\\nand comprehension. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics, pages 7871–7880, Online. Association\\nfor Computational Linguistics.\\nDaniel Li, Te I, Naveen Arivazhagan, Colin Cherry,\\nand Dirk Padﬁeld. 2020a. Sentence boundary aug-\\nmentation for neural machine translation robustness.\\narXiv preprint.\\nYitong Li, Trevor Cohn, and Timothy Baldwin. 2017.\\nRobust training under linguistic adversity. In Pro-\\nceedings of the 15th Conference of the European\\nChapter of the Association for Computational Lin-\\nguistics: Volume 2, Short Papers, pages 21–27, Va-\\nlencia, Spain. Association for Computational Lin-\\nguistics.\\nYu Li, Xiao Li, Yating Yang, and Rui Dong. 2020b. A\\ndiverse data augmentation strategy for low-resource\\nneural machine translation. Information, 11(5).\\nZhenhao Li and Lucia Specia. 2019. Improving neu-\\nral machine translation robustness via data augmen-\\ntation: Beyond back-translation. In Proceedings of\\nthe 5th Workshop on Noisy User-generated Text (W-\\nNUT 2019), pages 328–336, Hong Kong, China. As-\\nsociation for Computational Linguistics.\\nJared Lichtarge, Chris Alberti, Shankar Kumar, Noam\\nShazeer, Niki Parmar, and Simon Tong. 2019. Cor-\\npora generation for grammatical error correction. In\\nProceedings of the 2019 Conference of the North\\nAmerican Chapter of the Association for Compu-\\ntational Linguistics: Human Language Technolo-\\ngies, Volume 1 (Long and Short Papers), pages\\n3291–3301, Minneapolis, Minnesota. Association\\nfor Computational Linguistics.\\nPei Liu, Xuemin Wang, Chao Xiang, and Weiye Meng.\\n2020a. A survey of text data augmentation. In 2020\\nInternational Conference on Computer Communica-\\ntion and Network Security (CCNS), pages 191–195.\\nRuibo Liu, Guangxuan Xu, Chenyan Jia, Weicheng\\nMa, Lili Wang, and Soroush Vosoughi. 2020b. Data\\nboost: Text data augmentation through reinforce-\\nment learning guided conditional generation.\\nIn\\nProceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP),\\npages 9031–9041, Online. Association for Computa-\\ntional Linguistics.\\nRuibo Liu, Guangxuan Xu, and Soroush Vosoughi.\\n2020c.\\nEnhanced offensive language detection\\nthrough data augmentation.\\nICWSM Data Chal-\\nlenge.\\nShayne Longpre, Yi Lu, Zhucheng Tu, and Chris\\nDuBois. 2019. An exploration of data augmentation\\nand sampling techniques for domain-agnostic ques-\\ntion answering. In Proceedings of the 2nd Workshop\\non Machine Reading for Question Answering, pages\\n220–227, Hong Kong, China. Association for Com-\\nputational Linguistics.\\nShayne Longpre, Yu Wang, and Christopher DuBois.\\n2020. How effective is task-agnostic data augmen-\\ntation for pretrained transformers?\\narXiv preprint\\narXiv:2010.01764.\\nSamuel Louvan and Bernardo Magnini. 2020. Simple\\nis better! lightweight data augmentation for low re-\\nsource slot ﬁlling and intent classiﬁcation. In Pro-\\nceedings of the 34th Paciﬁc Asia Conference on Lan-\\n\\nguage, Information and Computation, pages 167–\\n177, Hanoi, Vietnam. Association for Computational\\nLinguistics.\\nKaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Aman-\\ncharla, and Anupam Datta. 2020. Gender Bias in\\nNeural Natural Language Processing, pages 189–\\n202. Springer International Publishing, Cham.\\nGeorge A. Miller. 1995.\\nWordnet:\\na lexical\\ndatabase for english. Communications of the ACM,\\n38(11):39–41.\\nTomoya Mizumoto, Mamoru Komachi, Masaaki Na-\\ngata, and Yuji Matsumoto. 2011.\\nMining revi-\\nsion log of language learning SNS for automated\\nJapanese error correction of second language learn-\\ners. In Proceedings of 5th International Joint Con-\\nference on Natural Language Processing, pages\\n147–155, Chiang Mai, Thailand. Asian Federation\\nof Natural Language Processing.\\nSebastien Montella, Betty Fabre, Tanguy Urvoy, Jo-\\nhannes Heinecke, and Lina Rojas-Barahona. 2020.\\nDenoising pre-training and data augmentation strate-\\ngies for enhanced RDF verbalization with transform-\\ners. In Proceedings of the 3rd International Work-\\nshop on Natural Language Generation from the Se-\\nmantic Web (WebNLG+), pages 89–99, Dublin, Ire-\\nland (Virtual). Association for Computational Lin-\\nguistics.\\nNaﬁse Sadat Moosavi, Marcel de Boer, Prasetya Ajie\\nUtama, and Iryna Gurevych. 2020.\\nImproving\\nrobustness by augmenting training sentences with\\npredicate-argument structures. arXiv preprint.\\nXiangyang Mou, Brandyn Sigouin, Ian Steenstra, and\\nHui Su. 2020. Multimodal dialogue state tracking by\\nqa approach with data augmentation. AAAI DSTC8\\nWorkshop.\\nDiego\\nMoussallem,\\nMihael\\nArˇcan,\\nAxel-\\nCyrille\\nNgonga\\nNgomo,\\nand\\nPaul\\nBuitelaar.\\n2019. Augmenting neural machine translation with\\nknowledge graphs. arXiv preprint.\\nNathan Ng, Kyunghyun Cho, and Marzyeh Ghassemi.\\n2020. SSMBA: Self-supervised manifold based data\\naugmentation for improving out-of-domain robust-\\nness.\\nIn Proceedings of the 2020 Conference on\\nEmpirical Methods in Natural Language Process-\\ning (EMNLP), pages 1268–1283, Online. Associa-\\ntion for Computational Linguistics.\\nXuan-Phi Nguyen, Shaﬁq Joty, Kui Wu, and Ai Ti Aw.\\n2020.\\nData diversiﬁcation: A simple strategy for\\nneural machine translation. In Advances in Neural\\nInformation Processing Systems, volume 33, pages\\n10018–10029. Curran Associates, Inc.\\nYuyang Nie, Yuanhe Tian, Xiang Wan, Yan Song, and\\nBo Dai. 2020.\\nNamed entity recognition for so-\\ncial media texts with semantic augmentation.\\nIn\\nProceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP),\\npages 1383–1391, Online. Association for Computa-\\ntional Linguistics.\\nYuta Nishimura, Katsuhito Sudoh, Graham Neubig,\\nand Satoshi Nakamura. 2018. Multi-source neural\\nmachine translation with data augmentation. 15th\\nInternational Workshop on Spoken Language Trans-\\nlation 2018.\\nShantipriya Parida and Petr Motlicek. 2019. Abstract\\ntext summarization: A low resource challenge. In\\nProceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the\\n9th International Joint Conference on Natural Lan-\\nguage Processing (EMNLP-IJCNLP), pages 5994–\\n5998, Hong Kong, China. Association for Computa-\\ntional Linguistics.\\nMagdalini Paschali, Walter Simson, Abhijit Guha Roy,\\nMuhammad Ferjad Naeem, Rüdiger Göbl, Christian\\nWachinger, and Nassir Navab. 2019. Data augmen-\\ntation with manifold exploring geometric transfor-\\nmations for increased performance and robustness.\\narXiv preprint.\\nRamakanth Pasunuru, Asli Celikyilmaz, Michel Galley,\\nChenyan Xiong, Yizhe Zhang, Mohit Bansal, and\\nJianfeng Gao. 2021. Data augmentation for abstrac-\\ntive query-focused multi-document summarization.\\narXiv preprint.\\nEllie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch,\\nBenjamin Van Durme, and Chris Callison-Burch.\\n2015. PPDB 2.0: Better paraphrase ranking, ﬁne-\\ngrained entailment relations, word embeddings, and\\nstyle classiﬁcation. In Proceedings of the 53rd An-\\nnual Meeting of the Association for Computational\\nLinguistics and the 7th International Joint Confer-\\nence on Natural Language Processing (Volume 2:\\nShort Papers), pages 425–430, Beijing, China. As-\\nsociation for Computational Linguistics.\\nWei Peng, Chongxuan Huang, Tianhao Li, Yun Chen,\\nand Qun Liu. 2020. Dictionary-based data augmen-\\ntation for cross-domain neural machine translation.\\narXiv preprint.\\nLibo Qin, Minheng Ni, Yue Zhang, and Wanxiang\\nChe. 2020. Cosda-ml: Multi-lingual code-switching\\ndata augmentation for zero-shot cross-lingual nlp.\\nIn Proceedings of the Twenty-Ninth International\\nJoint Conference on Artiﬁcial Intelligence, IJCAI-\\n20, pages 3853–3860. International Joint Confer-\\nences on Artiﬁcial Intelligence Organization. Main\\ntrack.\\nJun Quan and Deyi Xiong. 2019. Effective data aug-\\nmentation approaches to end-to-end task-oriented di-\\nalogue. In 2019 International Conference on Asian\\nLanguage Processing (IALP), pages 47–52.\\nHusam Quteineh, Spyridon Samothrakis, and Richard\\nSutcliffe. 2020. Textual data augmentation for efﬁ-\\ncient active learning on tiny datasets. In Proceed-\\nings of the 2020 Conference on Empirical Methods\\n\\nin Natural Language Processing (EMNLP), pages\\n7400–7410, Online. Association for Computational\\nLinguistics.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Language\\nmodels are unsupervised multitask learners. OpenAI\\nBlog, 1(8):9.\\nShashank Rajput, Zhili Feng, Zachary Charles, Po-\\nLing Loh, and Dimitris Papailiopoulos. 2019. Does\\ndata augmentation lead to positive margin?\\nIn In-\\nternational Conference on Machine Learning, pages\\n5321–5330. PMLR.\\nAJ Ratner, HR Ehrenberg, Z Hussain, J Dunnmon, and\\nC Ré. 2017. Learning to Compose Domain-Speciﬁc\\nTransformations for Data Augmentation. Advances\\nin Neural Information Processing Systems, 30:3239–\\n3249.\\nAdithya Renduchintala, Shuoyang Ding, Matthew\\nWiesner, and Shinji Watanabe. 2018. Multi-Modal\\nData Augmentation for End-to-end ASR. Proc. In-\\nterspeech 2018, pages 2394–2398.\\nArij Riabi, Thomas Scialom, Rachel Keraron, Benoît\\nSagot, Djamé Seddah, and Jacopo Staiano. 2020.\\nSynthetic Data Augmentation for Zero-Shot Cross-\\nLingual Question Answering. arXiv preprint.\\nGözde Gül ¸Sahin and Mark Steedman. 2018. Data aug-\\nmentation via dependency tree morphing for low-\\nresource languages.\\nIn Proceedings of the 2018\\nConference on Empirical Methods in Natural Lan-\\nguage Processing, pages 5004–5009, Brussels, Bel-\\ngium. Association for Computational Linguistics.\\nEli Schwartz, Leonid Karlinsky, Joseph Shtok, Sivan\\nHarary, Mattias Marder, Abhishek Kumar, Rogerio\\nFeris, Raja Giryes, and Alex M Bronstein. 2018. δ-\\nencoder: an effective sample synthesis method for\\nfew-shot object recognition. In Proceedings of the\\n32nd International Conference on Neural Informa-\\ntion Processing Systems, pages 2850–2860.\\nRico Sennrich, Barry Haddow, and Alexandra Birch.\\n2016. Improving Neural Machine Translation Mod-\\nels with Monolingual Data. In Proceedings of the\\n54th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), pages\\n86–96, Berlin, Germany. Association for Computa-\\ntional Linguistics.\\nHaoyue Shi, Karen Livescu, and Kevin Gimpel. 2021.\\nSubstructure Substitution: Structured Data Augmen-\\ntation for NLP. arXiv preprint arXiv:2101.00411.\\nConnor Shorten and Taghi M Khoshgoftaar. 2019.\\nA survey on Image Data Augmentation for Deep\\nLearning. Journal of Big Data, 6(1):60.\\nJasdeep Singh, Bryan McCann, Nitish Shirish Keskar,\\nCaiming Xiong, and Richard Socher. 2019. Xlda:\\nCross-lingual data augmentation for natural lan-\\nguage inference and question answering.\\narXiv\\npreprint arXiv:1905.11471.\\nAbhishek Sinha, Kumar Ayush, Jiaming Song, Bu-\\nrak Uzkent, Hongxia Jin, and Stefano Ermon. 2021.\\nNegative data augmentation. In International Con-\\nference on Learning Representations.\\nXiaohui Song, Liangjun Zang, Yipeng Su, Xing Wu,\\nJizhong Han, and Songlin Hu. 2020. Data Augmen-\\ntation for Copy-Mechanism in Dialogue State Track-\\ning. arXiv preprint arXiv:2002.09634.\\nAmane Sugiyama and Naoki Yoshinaga. 2019. Data\\naugmentation using back-translation for context-\\naware neural machine translation. In Proceedings\\nof the Fourth Workshop on Discourse in Machine\\nTranslation (DiscoMT 2019), pages 35–44, Hong\\nKong, China. Association for Computational Lin-\\nguistics.\\nShubhangi Tandon, TS Sharath, Shereen Oraby, Lena\\nReed, Stephanie Lukin, and Marilyn Walker. 2018.\\nTNT-NLG, System 2: Data repetition and meaning\\nrepresentation manipulation to improve neural gen-\\neration. E2E NLG Challenge Sfystem Descriptions.\\nRuixue Tang, Chao Ma, Wei Emma Zhang, Qi Wu, and\\nXiaokang Yang. 2020. Semantic equivalent adver-\\nsarial data augmentation for visual question answer-\\ning. In European Conference on Computer Vision,\\npages 437–453. Springer.\\nNandan Thakur, Nils Reimers, Johannes Daxenberger,\\nand Iryna Gurevych. 2021. Augmented sbert: Data\\naugmentation method for improving bi-encoders for\\npairwise sentence scoring tasks.\\nProceedings of\\nNAACL.\\nVaibhav Vaibhav, Sumeet Singh, Craig Stewart, and\\nGraham Neubig. 2019.\\nImproving Robustness of\\nMachine Translation with Synthetic Noise. In Pro-\\nceedings of the 2019 Conference of the North Amer-\\nican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Vol-\\nume 1 (Long and Short Papers), pages 1916–1920,\\nMinneapolis, Minnesota. Association for Computa-\\ntional Linguistics.\\nClara Vania, Yova Kementchedjhieva, Anders Søgaard,\\nand Adam Lopez. 2019. A systematic comparison\\nof methods for low-resource dependency parsing on\\ngenuinely low-resource languages. In Proceedings\\nof the 2019 Conference on Empirical Methods in\\nNatural Language Processing and the 9th Interna-\\ntional Joint Conference on Natural Language Pro-\\ncessing (EMNLP-IJCNLP), pages 1105–1116, Hong\\nKong, China. Association for Computational Lin-\\nguistics.\\nVikas Verma, Alex Lamb, Christopher Beckham, Amir\\nNajaﬁ, Ioannis Mitliagkas, David Lopez-Paz, and\\nYoshua Bengio. 2019. Manifold mixup: Better rep-\\nresentations by interpolating hidden states.\\nIn In-\\nternational Conference on Machine Learning, pages\\n6438–6447. PMLR.\\n\\nZhaohong Wan, Xiaojun Wan, and Wenguang Wang.\\n2020.\\nImproving Grammatical Error Correction\\nwith Data Augmentation by Editing Latent Rep-\\nresentation.\\nIn Proceedings of the 28th Interna-\\ntional Conference on Computational Linguistics,\\npages 2202–2212, Barcelona, Spain (Online). Inter-\\nnational Committee on Computational Linguistics.\\nChencheng Wang, Liner Yang, Yun Chen, Yongping\\nDu, and Erhong Yang. 2019a.\\nControllable Data\\nSynthesis Method for Grammatical Error Correction.\\narXiv preprint arXiv:1909.13302.\\nLongshaokan Wang, Maryam Fazel-Zarandi, Aditya Ti-\\nwari, Spyros Matsoukas, and Lazaros Polymenakos.\\n2020. Data augmentation for training dialog models\\nrobust to speech recognition errors. arXiv preprint\\narXiv:2006.05635.\\nXiang Wang, Kai Wang, and Shiguo Lian. 2019b. A\\nsurvey on face data augmentation. arXiv preprint\\narXiv:1904.11685.\\nXinyi Wang, Hieu Pham, Zihang Dai, and Graham Neu-\\nbig. 2018a. SwitchOut: an Efﬁcient Data Augmen-\\ntation Algorithm for Neural Machine Translation.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n856–861, Brussels, Belgium. Association for Com-\\nputational Linguistics.\\nXinyi Wang, Hieu Pham, Zihang Dai, and Graham Neu-\\nbig. 2018b. SwitchOut: an Efﬁcient Data Augmen-\\ntation Algorithm for Neural Machine Translation.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n856–861, Brussels, Belgium. Association for Com-\\nputational Linguistics.\\nJason Wei,\\nChengyu Huang,\\nSoroush Vosoughi,\\nYu Cheng, and Shiqi Xu. 2021a. Few-shot text clas-\\nsiﬁcation with triplet networks, data augmentation,\\nand curriculum learning. Proceedings of NAACL.\\nJason Wei, Chengyu Huang, Shiqi Xu, and Soroush\\nVosoughi. 2021b. Text augmentation in a multi-task\\nview. In Proceedings of the 16th Conference of the\\nEuropean Chapter of the Association for Computa-\\ntional Linguistics: Main Volume, pages 2888–2894,\\nOnline. Association for Computational Linguistics.\\nJason Wei and Kai Zou. 2019. EDA: Easy data aug-\\nmentation techniques for boosting performance on\\ntext classiﬁcation tasks.\\nIn Proceedings of the\\n2019 Conference on Empirical Methods in Natu-\\nral Language Processing and the 9th International\\nJoint Conference on Natural Language Processing\\n(EMNLP-IJCNLP), pages 6382–6388, Hong Kong,\\nChina. Association for Computational Linguistics.\\nMax White and Alla Rozovskaya. 2020. A Compar-\\native Study of Synthetic Data Generation Methods\\nfor Grammatical Error Correction. In Proceedings\\nof the Fifteenth Workshop on Innovative Use of NLP\\nfor Building Educational Applications, pages 198–\\n208, Seattle, WA, USA â†’ Online. Association for\\nComputational Linguistics.\\nMatthew Wiesner,\\nAdithya Renduchintala,\\nShinji\\nWatanabe, Chunxi Liu, Najim Dehak, and Sanjeev\\nKhudanpur. 2018. Low resource multi-modal data\\naugmentation for end-to-end ASR. CoRR.\\nJohn Wieting and Kevin Gimpel. 2017. Revisiting Re-\\ncurrent Networks for Paraphrastic Sentence Embed-\\ndings. In Proceedings of the 55th Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers), pages 2078–2088, Vancouver,\\nCanada. Association for Computational Linguistics.\\nSam Wiseman, Stuart M Shieber, and Alexander M\\nRush. 2017. Challenges in Data-to-Document Gen-\\neration. In Proceedings of the 2017 Conference on\\nEmpirical Methods in Natural Language Processing,\\npages 2253–2263.\\nMengzhou Xia, Xiang Kong, Antonios Anastasopou-\\nlos, and Graham Neubig. 2019. Generalized Data\\nAugmentation for Low-Resource Translation.\\nIn\\nProceedings of the 57th Annual Meeting of the\\nAssociation for Computational Linguistics, pages\\n5786–5796, Florence, Italy. Association for Compu-\\ntational Linguistics.\\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong,\\nand Quoc Le. 2020. Unsupervised data augmenta-\\ntion for consistency training. Advances in Neural\\nInformation Processing Systems, 33.\\nZiang Xie, Guillaume Genthial, Stanley Xie, Andrew\\nNg, and Dan Jurafsky. 2018. Noising and Denois-\\ning Natural Language: Diverse Backtranslation for\\nGrammar Correction. In Proceedings of the 2018\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long Papers),\\npages 619–628, New Orleans, Louisiana. Associa-\\ntion for Computational Linguistics.\\nN. Xu, W. Mao, P. Wei, and D. Zeng. 2020. MDA: Mul-\\ntimodal Data Augmentation Framework for Boost-\\ning Performance on Image-Text Sentiment/Emotion\\nClassiﬁcation Tasks.\\nIEEE Intelligent Systems,\\npages 1–1.\\nShuyao Xu, Jiehao Zhang, Jin Chen, and Long Qin.\\n2019. Erroneous data generation for Grammatical\\nError Correction. In Proceedings of the Fourteenth\\nWorkshop on Innovative Use of NLP for Building\\nEducational Applications, pages 149–158, Florence,\\nItaly. Association for Computational Linguistics.\\nWei Yang, Yuqing Xie, Luchen Tan, Kun Xiong, Ming\\nLi, and Jimmy Lin. 2019. Data Augmentation for\\nBERT Fine-Tuning in Open-Domain Question An-\\nswering. arXiv preprint arXiv:1904.06652.\\nYiben Yang, Chaitanya Malaviya, Jared Fernandez,\\nSwabha Swayamdipta, Ronan Le Bras, Ji-Ping\\nWang, Chandra Bhagavatula, Yejin Choi, and Doug\\n\\nDowney. 2020. G-daug: Generative data augmenta-\\ntion for commonsense reasoning. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Nat-\\nural Language Processing: Findings, pages 1008–\\n1025.\\nYichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, and\\nQun Liu. 2020.\\nDialog State Tracking with Re-\\ninforced Data Augmentation.\\nIn Proceedings of\\nthe AAAI Conference on Artiﬁcial Intelligence, vol-\\nume 34, pages 9474–9481.\\nMasashi Yokota and Hideki Nakayama. 2018.\\nAug-\\nmenting Image Question Answering Dataset by Ex-\\nploiting Image Captions.\\nIn Proceedings of the\\nEleventh International Conference on Language Re-\\nsources and Evaluation (LREC-2018), Miyazaki,\\nJapan. European Languages Resources Association\\n(ELRA).\\nKang Min Yoo, Youhyun Shin, and Sang-goo Lee.\\n2019. Data Augmentation for Spoken Language Un-\\nderstanding via Joint Variational Generation. In Pro-\\nceedings of the AAAI conference on artiﬁcial intelli-\\ngence, volume 33, pages 7402–7409.\\nS. Young, M. Gaši´c, B. Thomson, and J. D. Williams.\\n2013.\\nPOMDP-Based Statistical Spoken Dialog\\nSystems:\\nA Review.\\nProceedings of the IEEE,\\n101(5):1160–1179.\\nAdams Wei Yu, David Dohan, Quoc Le, Thang Luong,\\nRui Zhao, and Kai Chen. 2018. Fast and accurate\\nreading comprehension by combining self-attention\\nand convolution.\\nIn International Conference on\\nLearning Representations.\\nTao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin\\nWang, Yi Chern Tan, Xinyi Yang, Dragomir Radev,\\nRichard Socher, and Caiming Xiong. 2020. GraPPa:\\nGrammar-Augmented Pre-Training for Table Se-\\nmantic Parsing. arXiv preprint arXiv:2009.13845.\\nSangdoo Yun,\\nDongyoon Han,\\nSeong Joon Oh,\\nSanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.\\n2019.\\nCutmix:\\nRegularization strategy to train\\nstrong classiﬁers with localizable features. In Pro-\\nceedings of the IEEE/CVF International Conference\\non Computer Vision, pages 6023–6032.\\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin,\\nand David Lopez-Paz. 2017. mixup: Beyond em-\\npirical risk minimization. Proceedings of ICLR.\\nRongzhi Zhang, Yue Yu, and Chao Zhang. 2020. Se-\\nqMix: Augmenting Active Sequence Labeling via\\nSequence Mixup. In Proceedings of the 2020 Con-\\nference on Empirical Methods in Natural Language\\nProcessing (EMNLP), pages 8566–8579, Online. As-\\nsociation for Computational Linguistics.\\nRui Zhang, Tao Yu, Heyang Er, Sungrok Shim,\\nEric Xue, Xi Victoria Lin, Tianze Shi, Caiming\\nXiong, Richard Socher, and Dragomir Radev. 2019a.\\nEditing-based SQL Query Generation for Cross-\\nDomain Context-Dependent Questions. In Proceed-\\nings of the 2019 Conference on Empirical Methods\\nin Natural Language Processing and the 9th Inter-\\nnational Joint Conference on Natural Language Pro-\\ncessing (EMNLP-IJCNLP), pages 5338–5349, Hong\\nKong, China. Association for Computational Lin-\\nguistics.\\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\\nCharacter-Level Convolutional Networks for Text\\nClassiﬁcation. In Proceedings of the 28th Interna-\\ntional Conference on Neural Information Processing\\nSystems - Volume 1, NIPS’15, page 649–657, Cam-\\nbridge, MA, USA. MIT Press.\\nYi Zhang, Tao Ge, Furu Wei, Ming Zhou, and Xu Sun.\\n2019b.\\nSequence-to-sequence Pre-training with\\nData Augmentation for Sentence Rewriting. arXiv\\npreprint arXiv:1909.06002.\\nYichi Zhang, Zhijian Ou, and Zhou Yu. 2019c. Task-\\noriented dialog systems that consider multiple appro-\\npriate responses under the same context.\\nYuan Zhang, Jason Baldridge, and Luheng He. 2019d.\\nPAWS: Paraphrase adversaries from word scram-\\nbling.\\nIn Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers), pages\\n1298–1308, Minneapolis, Minnesota. Association\\nfor Computational Linguistics.\\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\\ndonez, and Kai-Wei Chang. 2018. Gender bias in\\ncoreference resolution:\\nEvaluation and debiasing\\nmethods.\\nIn Proceedings of the 2018 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, Volume 2 (Short Papers), pages 15–20,\\nNew Orleans, Louisiana. Association for Computa-\\ntional Linguistics.\\nZijian Zhao, Su Zhu, and Kai Yu. 2019. Data augmen-\\ntation with atomic templates for spoken language un-\\nderstanding. arXiv preprint arXiv:1908.10770.\\nVictor Zhong, Mike Lewis, Sida I. Wang, and Luke\\nZettlemoyer. 2020. Grounded adaptation for zero-\\nshot executable semantic parsing. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Nat-\\nural Language Processing (EMNLP), pages 6869–\\n6882, Online. Association for Computational Lin-\\nguistics.\\nHaichao Zhu, Li Dong, Furu Wei, Bing Qin, and\\nTing Liu. 2019.\\nTransforming wikipedia into\\naugmented data for query-focused summarization.\\narXiv preprint arXiv:1911.03324.\\nRan Zmigrod, Sebastian J. Mielke, Hanna Wallach, and\\nRyan Cotterell. 2019.\\nCounterfactual Data Aug-\\nmentation for Mitigating Gender Stereotypes in Lan-\\nguages with Rich Morphology. In Proceedings of\\n\\nthe 57th Annual Meeting of the Association for Com-\\nputational Linguistics, pages 1651–1661, Florence,\\nItaly. Association for Computational Linguistics.\\nAppendices\\nA\\nUseful Blog Posts and Code\\nRepositories\\nThe following blog posts and code repositories\\ncould be helpful in addition to the information pre-\\nsented and papers/works mentioned in the body:\\n• Introduction to popular text augmentation\\ntechniques:\\nhttps://towardsdatascience.com/\\ndata-augmentation-in-nlp-2801a34dfc28\\n• Detailed\\nblog\\npost\\non\\nvarious\\ntext\\nDA\\ntechniques:\\nhttps://amitness.com/2020/05/\\ndata-augmentation-for-nlp/\\n• Lightweight library for DA on text and audio:\\nhttps://github.com/makcedward/nlpaug\\n• python framework for adversarial examples:\\nhttps://github.com/QData/TextAttack\\nB\\nDA Methods Table - Description of\\nColumns and Attributes\\nTable 1 in the main body compares a non-\\nexhaustive selection of DA methods along various\\naspects relating to their applicability, dependencies,\\nand requirements. Below, we provide a more ex-\\ntensive description of each of this table’s columns\\nand their attributes.\\n1. Ext.Know: Short for external knowledge, this\\ncolumn is \\x13 when the data augmentation pro-\\ncess requires knowledge resources which go\\nbeyond the immediate input examples and\\nthe task deﬁnition, such as WordNet (Miller,\\n1995) or PPDB (Pavlick et al., 2015). Note\\nthat we exclude the case where these resources\\nare pretrained models under a separate point\\n(next) for clarity, since these are widespread\\nenough to merit a separate category.\\n2. Pretrained: Denotes that the data augmenta-\\ntion process requires a pretrained model, such\\nas BERT (Devlin et al., 2019) or GPT-2 (Rad-\\nford et al., 2019).\\n3. Preprocess: Denotes the preprocessing steps,\\ne.g. tokenization (tok), dependency parsing\\n(dep), etc. required for the DA process. A\\nhyphen (-) means either no preprocessing is\\nrequired or that it was not explicitly stated.\\n4. Level: Denotes the depth and extent to which\\nelements of the instance/data are modiﬁed by\\nthe DA. Some primitives modify just the IN-\\nPUT (e.g. word swapping), some modify both\\nINPUT and LABEL (e.g. negation), while oth-\\ners make changes in the embedding or hidden\\nspace (EMBED/HIDDEN) or higher represen-\\ntation layers enroute to the task model.\\n5. Task-Agnostic: This is an approximate, par-\\ntially subjective column denoting the extent\\nto which a DA method can be applied to dif-\\nferent tasks. When we say \\x13 here, we don’t\\ndenote a very rigid sense of the term task-\\nagnostic, but mean that it would possibly eas-\\nily extend to most NLP tasks as understood\\nby the authors. Similarly, an × denotes being\\nrestricted to a speciﬁc task (or small group of\\nrelated tasks) only. There can be other labels,\\ndenoting applicability to broad task families.\\nFor example, SUBSTRUCTURAL denotes the\\nfamily of tasks where sub-parts of the input\\nare also valid input examples in their own\\nright, e.g. constituency parsing. SENTENCE\\nPAIRS denotes tasks which involve pairwise\\nsentence scoring such as paraphrase identiﬁ-\\ncation, duplicate question detection, and se-\\nmantic textual similarity.\\nC\\nAdditional DA Works by Task\\nSee Table 2 for additional DA works for GEC, Ta-\\nble 3 for additional DA works for neural machine\\ntranslation, Table 4 for additional DA works for\\ndialogue, and Table 5 for additional DA works for\\nmultimodal tasks. Each work is described brieﬂy.\\nD\\nAdditional Figure\\nFigure 4: Pedro Domingos’ quip about ofﬂine data aug-\\nmentation.\\n\\nPaper/Work\\nBrief Description\\nLichtarge et al. (2019)\\nGenerate synthetic noised examples of Wikipedia sentences using backtranslation through\\nvarious languages.\\nWhite and Rozovskaya (2020) Detailed comparative study of the DA for GEC systems UEdin-MS (Grundkiewicz et al., 2019)\\nand Kakao&Brain (Choe et al., 2019).\\nFoster and Andersen (2009)\\nIntroduces error generation tool called GenERRate which learns to generate ungrammatical\\ntext with various errors by using an error analysis ﬁle.\\nKimn (2020)\\nUse a set of syntactic rules for common Japanese grammatical errors to generate augmented\\nerror-correct sentence pairs for Japanese GEC.\\nFelice (2016)\\nThesis that surveys previous work on error generation and investigates some new approaches\\nusing random and probabilistic methods.\\nXu et al. (2019)\\nNoises using ﬁve error types: concatenation, misspelling, substitution, deletion, and transposi-\\ntion. Decent performance on the BEA 2019 Shared Task.\\nZhang et al. (2019b)\\nExplore backtranslation and feature discrimination for DA.\\nMizumoto et al. (2011)\\nDA by extracting Japanese GEC training data from the revision log of a language learning SNS.\\nTable 2: Additional DA works for grammatical error correction (GEC), along with a brief description of each.\\nPaper/Work\\nBrief Description\\nVaibhav et al. (2019)\\nPresent a synthetic noise induction model which heuristically adds social media noise to text,\\nand labeled backtranslation.\\nHassan et al. (2017)\\nPresent a DA method to project words from closely-related high-resource languages to low-\\nresource languages using word embedding representations.\\nCheng et al. (2020)\\nPropose AdvAug, an adversarial augmentation method for NMT, by sampling adversarial\\nexamples from a new vicinity distribution and using their embeddings to augment training.\\nGraça et al. (2019)\\nInvestigate improvements to sampling-based approaches and the synthetic data generated by\\nbacktranslation.\\nBulte and Tezcan (2019)\\nPropose DA approaches for NMT that leverage information retrieved from a Translation\\nMemory (TM) and using fuzzy TM matches.\\nMoussallem et al. (2019)\\nPropose an NMT model KG-NMT which is augmented by knowledge graphs to enhance\\nsemantic feature extraction and hence the translation of entities and terminological expressions.\\nPeng et al. (2020)\\nPropose dictionary-based DA (DDA) for cross-domain NMT by synthesizing a domain-speciﬁc\\ndictionary and automatically generating a pseudo in-domain parallel corpus.\\nLi et al. (2020a)\\nPresent a DA method using sentence boundary segmentation to improve the robustness of NMT\\non ASR transcripts.\\nNishimura et al. (2018)\\nIntroduce DA methods for multi-source NMT that ﬁlls in incomplete portions of multi-source\\ntraining data.\\nSugiyama and Yoshinaga (2019) Investigate effectiveness of DA by backtranslation for context-aware NMT.\\nLi and Specia (2019)\\nPresent DA methods to improve NMT robustness to noise while keeping models small, and\\nexplore the use of noise from external data (speech transcripts).\\nChinea-Ríos et al. (2017)\\nPropose DA method to create synthetic data by leveraging the embedding representation of\\nsentences.\\nAlves et al. (2020)\\nPropose two methods for pipeline-based speech translation through the introduction of errors\\nthrough 1. utilizing a speech processing workﬂow and 2. a rule-based method.\\nKang (2019)\\nInvestigate extremely low-resource settings for NMT and a DA approach using a noisy dictio-\\nnary and language models.\\nChen et al. (2020a)\\nInvestigate a DA method for lexically constraint-aware NMT to construct constraint-aware\\nsynthetic training data.\\nLi et al. (2020b)\\nPropose a diversity DA method for low-resource NMT by generating diverse synthetic parallel\\ndata on both source and target sides using a restricted sampling strategy during decoding.\\nDuan et al. (2020)\\nPropose syntax-aware DA methods with sentence-speciﬁc word selection probabilities using\\ndependency parsing.\\nTable 3: Additional DA works for neural machine translation (NMT), along with a brief description of each.\\n\\nPaper/Work\\nBrief Description\\nGao et al. (2020)\\nPropose a paraphrase augmented response generation (PARG) framework to improve dialogue generation by\\nautomatically constructing augmented paraphrased training examples based on dialogue state and act labels.\\nGritta et al. (2021) Introduce a graph-based representation of dialogues called Conversation Graph (ConvGraph) that can be\\nused for DA by creating new dialogue paths.\\nYin et al. (2020)\\nPropose an RL-based DA approach for dialogue state tracking (DST).\\nSong et al. (2020) Propose a simple DA algorithm to improve the training of copy-mechanism models for dialogue state\\ntracking (DST).\\nTable 4: Additional DA works for dialogue, along with a brief description of each.\\nPaper/Work\\nBrief Description\\nHuang et al. (2018) Propose a DA method for emotion recognition from a combination of audio, visual, and textual modalities.\\nMou et al. (2020)\\nIntroduce a DA method for Audio-Video Scene-Aware Dialogue, which involves dialogue containing a\\nsequence of QA pairs about a video.\\nFalcon et al. (2020) Investigate DA techniques for video QA including mirroring and horizontal ﬂipping.\\nTable 5: Additional DA works for multimodal tasks, along with a brief description of each.\\n'}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/DataAugmentationApproachesforNLP.pdf', 'text': 'A Survey of Data Augmentation Approaches for NLP\\nSteven Y. Feng∗, 1 Varun Gangal∗, 1 Jason Wei†, 2 Sarath Chandar,3\\nSoroush Vosoughi,4 Teruko Mitamura,1 Eduard Hovy1\\n1Carnegie Mellon University, 2Google Research\\n3Mila - Quebec AI Institute, 4Dartmouth College\\n{syfeng,vgangal,teruko,hovy}@cs.cmu.edu\\njasonwei@google.com\\nsarath.chandar@mila.quebec\\nsoroush@dartmouth.edu\\nAbstract\\nData augmentation has recently seen increased\\ninterest in NLP due to more work in low-\\nresource domains, new tasks, and the popu-\\nlarity of large-scale neural networks that re-\\nquire large amounts of training data.\\nDe-\\nspite this recent upsurge, this area is still rel-\\natively underexplored, perhaps due to the chal-\\nlenges posed by the discrete nature of language\\ndata. In this paper, we present a comprehen-\\nsive and unifying survey of data augmenta-\\ntion for NLP by summarizing the literature in\\na structured manner. We ﬁrst introduce and\\nmotivate data augmentation for NLP, and then\\ndiscuss major methodologically representative\\napproaches.\\nNext, we highlight techniques\\nthat are used for popular NLP applications and\\ntasks. We conclude by outlining current chal-\\nlenges and directions for future research. Over-\\nall, our paper aims to clarify the landscape\\nof existing literature in data augmentation for\\nNLP and motivate additional work in this area.\\nWe also present a GitHub repository with a pa-\\nper list that will be continuously updated at\\nhttps://github.com/styfeng/DataAug4NLP.\\n1\\nIntroduction\\nData augmentation (DA) refers to strategies for in-\\ncreasing the diversity of training examples without\\nexplicitly collecting new data. It has received active\\nattention in recent machine learning (ML) research\\nin the form of well-received, general-purpose tech-\\nniques such as UDA (Xie et al., 2020) (3.1), which\\nused backtranslation (Sennrich et al., 2016), Au-\\ntoAugment (Cubuk et al., 2018), and RandAugment\\n(Cubuk et al., 2020), and MIXUP (Zhang et al.,\\n2017) (3.2). These are often ﬁrst explored in com-\\nputer vision (CV), and DA’s adaptation for natural\\nlanguage processing (NLP) seems secondary and\\n∗Equal contribution by the two authors.\\n† AI Resident.\\ncomparatively underexplored, perhaps due to chal-\\nlenges presented by the discrete nature of language,\\nwhich rules out continuous noising and makes it\\nmore difﬁcult to maintain invariance.\\nDespite these challenges, there has been in-\\ncreased interest and demand for DA for NLP. As\\nNLP grows due to off-the-shelf availability of large\\npretrained models, there are increasingly more\\ntasks and domains to explore. Many of these are\\nlow-resource, and have a paucity of training exam-\\nples, creating many use-cases for which DA can\\nplay an important role. Particularly, for many non-\\nclassiﬁcation NLP tasks such as span-based tasks\\nand generation, DA research is relatively sparse\\ndespite their ubiquity in real-world settings.\\nOur paper aims to sensitize the NLP community\\ntowards this growing area of work, which has also\\nseen increasing interest in ML overall (as seen in\\nFigure 1). As interest and work on this topic con-\\ntinue to increase, this is an opportune time for a\\npaper of our kind to (i) give a bird’s eye view of\\nDA for NLP, and (ii) identify key challenges to\\neffectively motivate and orient interest in this area.\\nTo the best of our knowledge, this is the ﬁrst survey\\nto take a detailed look at DA methods for NLP.1\\nThis paper is structured as follows.\\nSection\\n2 discusses what DA is, its goals and trade-offs,\\nand why it works. Section 3 describes popular\\nmethodologically representative DA techniques for\\nNLP—which we categorize into rule-based (3.1),\\nexample interpolation-based (3.2), or model-based\\n(3.3). Section 4 discusses useful NLP applications\\nfor DA, including low-resource languages (4.1),\\nmitigating bias (4.2), ﬁxing class imbalance (4.3),\\nfew-shot learning (4.4), and adversarial examples\\n(4.5). Section 5 describes DA methods for common\\n1Liu et al. (2020a) present a smaller-scale text data aug-\\nmentation survey that is concise and focused. Our work serves\\nas a more comprehensive survey with larger coverage and is\\nmore up-to-date.\\narXiv:2105.03075v5  [cs.CL]  1 Dec 2021\\n\\nFigure 1: Weekly Google Trends scores for the search\\nterm \"data augmentation\", with a control, uneventful\\nML search term (\"minibatch\") for comparison.\\nNLP tasks including summarization (5.1), question\\nanswering (5.2), sequence tagging tasks (5.3), pars-\\ning tasks (5.4), grammatical error correction (5.5),\\nneural machine translation (5.6), data-to-text NLG\\n(5.7), open-ended and conditional text generation\\n(5.8), dialogue (5.9), and multimodal tasks (5.10).\\nFinally, Section 6 discusses challenges and future\\ndirections in DA for NLP. Appendix A lists useful\\nblog posts and code repositories.\\nThrough this work, we hope to emulate past pa-\\npers which have surveyed DA methods for other\\ntypes of data, such as images (Shorten and Khosh-\\ngoftaar, 2019), faces (Wang et al., 2019b), and time\\nseries (Iwana and Uchida, 2020). We hope to draw\\nfurther attention, elicit broader interest, and moti-\\nvate additional work in DA, particularly for NLP.\\n2\\nBackground\\nWhat is data augmentation? Data augmentation\\n(DA) encompasses methods of increasing training\\ndata diversity without directly collecting more data.\\nMost strategies either add slightly modiﬁed copies\\nof existing data or create synthetic data, aiming for\\nthe augmented data to act as a regularizer and re-\\nduce overﬁtting when training ML models (Shorten\\nand Khoshgoftaar, 2019; Hernández-García and\\nKönig, 2020). DA has been commonly used in\\nCV, where techniques like cropping, ﬂipping, and\\ncolor jittering are a standard component of model\\ntraining. In NLP, where the input space is discrete,\\nhow to generate effective augmented examples that\\ncapture the desired invariances is less obvious.\\nWhat are the goals and trade-offs?\\nDespite\\nchallenges associated with text, many DA tech-\\nniques for NLP have been proposed, ranging from\\nrule-based manipulations (Zhang et al., 2015) to\\nmore complicated generative approaches (Liu et al.,\\n2020b). As DA aims to provide an alternative to\\ncollecting more data, an ideal DA technique should\\nbe both easy-to-implement and improve model per-\\nformance. Most offer trade-offs between these two.\\nRule-based techniques are easy-to-implement\\nbut usually offer incremental performance improve-\\nments (Li et al., 2017; Wei and Zou, 2019; Wei\\net al., 2021b). Techniques leveraging trained mod-\\nels may be more costly to implement but introduce\\nmore data variation, leading to better performance\\nboosts. Model-based techniques customized for\\ndownstream tasks can have strong effects on per-\\nformance but be difﬁcult to develop and utilize.\\nFurther, the distribution of augmented data\\nshould neither be too similar nor too different from\\nthe original. This may lead to greater overﬁtting\\nor poor performance through training on examples\\nnot representative of the given domain, respectively.\\nEffective DA approaches should aim for a balance.\\nKasheﬁ\\nand\\nHwa\\n(2020)\\ndevise\\na\\nKL-\\nDivergence-based unsupervised procedure to pre-\\nemptively choose among DA heuristics, rather than\\na typical \"run-all-heuristics\" comparison, which\\ncan be very time and cost intensive.\\nInterpretation of DA\\nDao et al. (2019) note that\\n\"data augmentation is typically performed in an ad-\\nhoc manner with little understanding of the under-\\nlying theoretical principles\", and claim the typical\\nexplanation of DA as regularization to be insufﬁ-\\ncient. Overall, there indeed appears to be a lack of\\nresearch on why exactly DA works. Existing work\\non this topic is mainly surface-level, and rarely\\ninvestigates the theoretical underpinnings and prin-\\nciples. We discuss this challenge more in §6, and\\nhighlight some of the existing work below.\\nBishop (1995) show training with noised exam-\\nples is reducible to Tikhonov regularization (sub-\\nsumes L2). Rajput et al. (2019) show that DA can\\nincrease the positive margin for classiﬁers, but only\\nwhen augmenting exponentially many examples\\nfor common DA methods.\\nDao et al. (2019) think of DA transformations\\nas kernels, and ﬁnd two ways DA helps: averaging\\nof features and variance regularization. Chen et al.\\n(2020d) show that DA leads to variance reduction\\nby averaging over orbits of the group that keep the\\ndata distribution approximately invariant.\\n3\\nTechniques & Methods\\nWe now discuss some methodologically represen-\\ntative DA techniques which are relevant to all tasks\\n\\nvia the extensibility of their formulation.2\\n3.1\\nRule-Based Techniques\\nHere, we cover DA primitives which use easy-\\nto-compute, predetermined transforms sans model\\ncomponents. Feature space DA approaches gen-\\nerate augmented examples in the model’s feature\\nspace rather than input data. Many few-shot learn-\\ning approaches (Hariharan and Girshick, 2017;\\nSchwartz et al., 2018) leverage estimated feature\\nspace \"analogy\" transformations between exam-\\nples of known classes to augment for novel classes\\n(see §4.4).\\nPaschali et al. (2019) use iterative\\nafﬁne transformations and projections to maximally\\n\"stretch\" an example along the class-manifold.\\nWei and Zou (2019) propose EASY DATA AUG-\\nMENTATION (EDA), a set of token-level random\\nperturbation operations including random insertion,\\ndeletion, and swap. They show improved perfor-\\nmance on many text classiﬁcation tasks. UDA (Xie\\net al., 2020) show how supervised DA methods can\\nbe exploited for unsupervised data through consis-\\ntency training on (x, DA(x)) pairs.\\nFor paraphrase identiﬁcation, Chen et al. (2020b)\\nconstruct a signed graph over the data, with indi-\\nvidual sentences as nodes and pair labels as signed\\nedges. They use balance theory and transitivity\\nto infer augmented sentence pairs from this graph.\\nMotivated by image cropping and rotation, ¸Sahin\\nand Steedman (2018) propose dependency tree mor-\\nphing. For dependency-annotated sentences, chil-\\ndren of the same parent are swapped (à la rotation)\\nor some deleted (à la cropping), as seen in Figure 2.\\nThis is most beneﬁcial for language families with\\nrich case marking systems (e.g. Baltic and Slavic).\\n3.2\\nExample Interpolation Techniques\\nAnother class of DA techniques, pioneered by\\nMIXUP (Zhang et al., 2017), interpolates the in-\\nputs and labels of two or more real examples. This\\nclass of techniques is also sometimes referred to as\\nMixed Sample Data Augmentation (MSDA). Ensu-\\ning work has explored interpolating inner compo-\\nnents (Verma et al., 2019; Faramarzi et al., 2020),\\nmore general mixing schemes (Guo, 2020), and\\nadding adversaries (Beckham et al., 2019).\\nAnother class of extensions of MIXUP which has\\nbeen growing in the vision community attempts to\\nfuse raw input image pairs together into a single\\n2Table 1 compares several DA methods by various aspects\\nrelating to their applicability, dependencies, and requirements.\\nFigure 2: Dependency tree morphing DA applied to a\\nTurkish sentence, ¸Sahin and Steedman (2018)\\ninput image, rather than improve the continuous in-\\nterpolation mechanism. Examples of this paradigm\\ninclude CUTMIX (Yun et al., 2019), CUTOUT (De-\\nVries and Taylor, 2017) and COPY-PASTE (Ghiasi\\net al., 2020). For instance, CUTMIX replaces a\\nsmall sub-region of Image A with a patch sampled\\nfrom Image B, with the labels mixed in proportion\\nto sub-region sizes. There is potential to borrow\\nideas and inspiration from these works for NLP,\\ne.g. for multimodal work involving both images\\nand text (see \"Multimodal challenges\" in §6).\\nA bottleneck to using MIXUP for NLP tasks\\nwas the requirement of continuous inputs. This has\\nbeen overcome by mixing embeddings or higher\\nhidden layers (Chen et al., 2020c). Later variants\\npropose speech-tailored mixing schemes (Jindal\\net al., 2020b) and interpolation with adversarial\\nexamples (Cheng et al., 2020), among others.\\nSEQ2MIXUP (Guo et al., 2020) generalizes\\nMIXUP for sequence transduction tasks in two\\nways - the \"hard\" version samples a binary mask\\n(from a Bernoulli with a β(α, α) prior) and picks\\nfrom one of two sequences at each token position,\\nwhile the \"soft\" version softly interpolates between\\nsequences based on a coefﬁcient sampled from\\nβ(α, α). The \"soft\" version is found to outperform\\nthe \"hard\" version and earlier interpolation-based\\ntechniques like SWITCHOUT (Wang et al., 2018a).\\n3.3\\nModel-Based Techniques\\nSeq2seq and language models have also been used\\nfor DA. The popular BACKTRANSLATION method\\n(Sennrich et al., 2016) translates a sequence into\\nanother language and then back into the original\\n\\nFigure 3: Contextual Augmentation, Kobayashi (2018)\\nlanguage. Kumar et al. (2019a) train seq2seq mod-\\nels with their proposed method DiPS which learns\\nto generate diverse paraphrases of input text using\\na modiﬁed decoder with a submodular objective,\\nand show its effectiveness as DA for several classi-\\nﬁcation tasks. Pretrained language models such as\\nRNNs (Kobayashi, 2018) and transformers (Yang\\net al., 2020) have also been used for augmentation.\\nKobayashi (2018) generate augmented examples\\nby replacing words with others randomly drawn\\naccording to the recurrent language model’s dis-\\ntribution based on the current context (illustra-\\ntion in Figure 3). Yang et al. (2020) propose G-\\nDAUGc which generates synthetic examples using\\npretrained transformer language models, and se-\\nlects the most informative and diverse set for aug-\\nmentation. Gao et al. (2019) advocate retaining the\\nfull distribution through \"soft\" augmented exam-\\nples, showing gains on machine translation.\\nNie et al. (2020) augment word representations\\nwith a context-sensitive attention-based mixture of\\ntheir semantic neighbors from a pretrained embed-\\nding space, and show its effectiveness for NER\\non social media text. Inspired by denoising au-\\ntoencoders, Ng et al. (2020) use a corrupt-and-\\nreconstruct approach, with the corruption function\\nq(x′|x) masking an arbitrary number of word po-\\nsitions and the reconstruction function r(x|x′) un-\\nmasking them using BERT (Devlin et al., 2019).\\nTheir approach works well on domain-shifted test\\nsets across 9 datasets on sentiment, NLI, and NMT.\\nFeng et al. (2019) propose a task called SEMAN-\\nTIC TEXT EXCHANGE (STE) which involves ad-\\njusting the overall semantics of a text to ﬁt the\\ncontext of a new word/phrase that is inserted called\\nthe replacement entity (RE). They do so by using a\\nsystem called SMERTI and a masked LM approach.\\nWhile not proposed directly for DA, it can be used\\nas such, as investigated in Feng et al. (2020).\\nRather than starting from an existing exam-\\nple and modifying it, some model-based DA ap-\\nproaches directly estimate a generative process\\nfrom the training set and sample from it. Anaby-\\nTavor et al. (2020) learn a label-conditioned gen-\\nerator by ﬁnetuning GPT-2 (Radford et al., 2019)\\non the training data, using this to generate candi-\\ndate examples per class. A classiﬁer trained on the\\noriginal training set is then used to select top k can-\\ndidate examples which conﬁdently belong to the\\nrespective class for augmentation. Quteineh et al.\\n(2020) use a similar label-conditioned GPT-2 gen-\\neration method, and demonstrate its effectiveness\\nas a DA method in an active learning setup.\\nOther approaches include syntactic or controlled\\nparaphrasing (Iyyer et al., 2018; Kumar et al.,\\n2020), document-level paraphrasing (Gangal et al.,\\n2021), augmenting misclassiﬁed examples (Dreossi\\net al., 2018), BERT cross-encoder labeling of new\\ninputs (Thakur et al., 2021), guided generation us-\\ning large-scale generative language models (Liu\\net al., 2020b,c), and automated text augmentation\\n(Hu et al., 2019; Cai et al., 2020). Models can also\\nlearn to combine together simpler DA primitives\\n(Cubuk et al., 2018; Ratner et al., 2017) or add\\nhuman-in-the-loop (Kaushik et al., 2020, 2021).\\n4\\nApplications\\nIn this section, we discuss several DA methods for\\nsome common NLP applications.2\\n4.1\\nLow-Resource Languages\\nLow-resource languages are an important and chal-\\nlenging application for DA, typically for neural\\nmachine translation (NMT). Techniques using ex-\\nternal knowledge such as WordNet (Miller, 1995)\\nmay be difﬁcult to use effectively here.3 There\\nare ways to leverage high-resource languages for\\nlow-resource languages, particularly if they have\\nsimilar linguistic properties. Xia et al. (2019) use\\nthis approach to improve low-resource NMT.\\nLi et al. (2020b) use backtranslation and self-\\nlearning to generate augmented training data. In-\\nspired by work in CV, Fadaee et al. (2017) gener-\\nate additional training examples that contain low-\\nfrequency (rare) words in synthetically created con-\\ntexts. Qin et al. (2020) present a DA framework to\\ngenerate multi-lingual code-switching data to ﬁne-\\ntune multilingual-BERT. It encourages the align-\\n3Low-resource language challenges discussed more in §6.\\n\\nDA Method\\nExt.Know Pretrained\\nPreprocess\\nLevel\\nTask-Agnostic\\nSYNONYM REPLACEMENT (Zhang et al., 2015)\\n\\x13\\n×\\ntok\\nInput\\n\\x13\\nRANDOM DELETION (Wei and Zou, 2019)\\n×\\n×\\ntok\\nInput\\n\\x13\\nRANDOM SWAP (Wei and Zou, 2019)\\n×\\n×\\ntok\\nInput\\n\\x13\\nBACKTRANSLATION (Sennrich et al., 2016)\\n×\\n\\x13\\nDepends\\nInput\\n\\x13\\nSCPN (Wieting and Gimpel, 2017)\\n×\\n\\x13\\nconst\\nInput\\n\\x13\\nSEMANTIC TEXT EXCHANGE (Feng et al., 2019)\\n×\\n\\x13\\nconst\\nInput\\n\\x13\\nCONTEXTUALAUG (Kobayashi, 2018)\\n×\\n\\x13\\n-\\nInput\\n\\x13\\nLAMBADA (Anaby-Tavor et al., 2020)\\n×\\n\\x13\\n-\\nInput\\n×\\nGECA (Andreas, 2020)\\n×\\n×\\ntok\\nInput\\n×\\nSEQMIXUP (Guo et al., 2020)\\n×\\n×\\ntok\\nInput\\n×\\nSWITCHOUT (Wang et al., 2018b)\\n×\\n×\\ntok\\nInput\\n×\\nEMIX (Jindal et al., 2020a)\\n×\\n×\\n-\\nEmb/Hidden\\n\\x13\\nSPEECHMIX (Jindal et al., 2020b)\\n×\\n×\\n-\\nEmb/Hidden Speech/Audio\\nMIXTEXT (Chen et al., 2020c)\\n×\\n×\\n-\\nEmb/Hidden\\n\\x13\\nSIGNEDGRAPH (Chen et al., 2020b)\\n×\\n×\\n-\\nInput\\n×\\nDTREEMORPH (¸Sahin and Steedman, 2018)\\n×\\n×\\ndep\\nInput\\n\\x13\\nSub2 (Shi et al., 2021)\\n×\\n×\\ndep\\nInput\\nSubstructural\\nDAGA (Ding et al., 2020)\\n×\\n×\\ntok\\nInput+Label\\n×\\nWN-HYPERS (Feng et al., 2020)\\n\\x13\\n×\\nconst+KWE\\nInput\\n\\x13\\nSYNTHETIC NOISE (Feng et al., 2020)\\n×\\n×\\ntok\\nInput\\n\\x13\\nUEDIN-MS (DA part) (Grundkiewicz et al., 2019)\\n\\x13\\n×\\ntok\\nInput\\n\\x13\\nNONCE (Gulordava et al., 2018)\\n\\x13\\n×\\nconst\\nInput\\n\\x13\\nXLDA (Singh et al., 2019)\\n×\\n\\x13\\nDepends\\nInput\\n\\x13\\nSEQMIX (Zhang et al., 2020)\\n×\\n\\x13\\ntok\\nInput+Label\\n×\\nSLOT-SUB-LM (Louvan and Magnini, 2020)\\n×\\n\\x13\\ntok\\nInput\\n\\x13\\nUBT & TBT (Vaibhav et al., 2019)\\n×\\n\\x13\\nDepends\\nInput\\n\\x13\\nSOFT CONTEXTUAL DA (Gao et al., 2019)\\n×\\n\\x13\\ntok\\nEmb/Hidden\\n\\x13\\nDATA DIVERSIFICATION (Nguyen et al., 2020)\\n×\\n\\x13\\nDepends\\nInput\\n\\x13\\nDIPS (Kumar et al., 2019a)\\n×\\n\\x13\\ntok\\nInput\\n\\x13\\nAUGMENTED SBERT (Thakur et al., 2021)\\n×\\n\\x13\\n-\\nInput+Label Sentence Pairs\\nTable 1: Comparing a selection of DA methods by various aspects relating to their applicability, dependencies, and\\nrequirements. Ext.Know, KWE, tok, const, and dep stand for External Knowledge, keyword extraction, tokeniza-\\ntion, constituency parsing, and dependency parsing, respectively. Ext.Know refers to whether the DA method re-\\nquires external knowledge (e.g. WordNet) and Pretrained if it requires a pretrained model (e.g. BERT). Preprocess\\ndenotes preprocessing required, Level denotes the depth at which data is modiﬁed by the DA, and Task-Agnostic\\nrefers to whether the DA method can be applied to different tasks. See Appendix B for further explanation.\\nment of representations from source and multiple\\ntarget languages once by mixing their context in-\\nformation. They see improved performance across\\n5 tasks with 19 languages.\\n4.2\\nMitigating Bias\\nZhao et al. (2018) attempt to mitigate gender\\nbias in coreference resolution by creating an aug-\\nmented dataset identical to the original but biased\\ntowards the underrepresented gender (using gen-\\nder swapping of entities such as replacing \"he\"\\nwith \"she\") and train on the union of the two\\ndatasets. Lu et al. (2020) formally propose COUN-\\nTERFACTUAL DA (CDA) for gender bias mitiga-\\ntion, which involves causal interventions that break\\nassociations between gendered and gender-neutral\\nwords. Zmigrod et al. (2019) and Hall Maudslay\\net al. (2019) propose further improvements to CDA.\\nMoosavi et al. (2020) augment training sentences\\nwith their corresponding predicate-argument struc-\\ntures, improving the robustness of transformer mod-\\nels against various types of biases.\\n4.3\\nFixing Class Imbalance\\nFixing class imbalance typically involves a combi-\\nnation of undersampling and oversampling. SYN-\\nTHETIC MINORITY OVERSAMPLING TECHNIQUE\\n(SMOTE) (Chawla et al., 2002), which gener-\\nates augmented minority class examples through\\ninterpolation, still remains popular (Fernández\\net al., 2018). MULTILABEL SMOTE (MLSMOTE)\\n(Charte et al., 2015) modiﬁes SMOTE to balance\\nclasses for multi-label classiﬁcation, where classi-\\nﬁers predict more than one class at the same time.\\nOther techniques such as EDA (Wei and Zou, 2019)\\ncan possibly be used for oversampling as well.\\n4.4\\nFew-Shot Learning\\nDA methods can ease few-shot learning by adding\\nmore examples for novel classes introduced in the\\nfew-shot phase. Hariharan and Girshick (2017)\\nuse learned analogy transformations φ(z1, z2, x)\\n\\nbetween example pairs from a non-novel class\\nz1 →z2 to generate augmented examples x →x′\\nfor novel classes. Schwartz et al. (2018) generalize\\nthis to beyond just linear offsets, through their \"∆-\\nnetwork\" autoencoder which learns the distribution\\nP(z2|z1, C) from all y∗\\nz1 = y∗\\nz2 = C pairs, where\\nC is a class and y is the ground-truth labelling\\nfunction. Both these methods are applied only on\\nimage tasks, but their theoretical formulations are\\ngenerally applicable, and hence we discuss them.\\nKumar et al. (2019b) apply these and other\\nDA methods for few-shot learning of novel intent\\nclasses in task-oriented dialog. Wei et al. (2021a)\\nshow that data augmentation facilitates curriculum\\nlearning for training triplet networks for few-shot\\ntext classiﬁcation. Lee et al. (2021) use T5 to gen-\\nerate additional examples for data-scarce classes.\\n4.5\\nAdversarial Examples (AVEs)\\nAdversarial examples can be generated using\\ninnocuous label-preserving transformations (e.g.\\nparaphrasing) that fool state-of-the-art NLP mod-\\nels, as shown in Jia et al. (2019). Speciﬁcally,\\nthey add sentences with distractor spans to pas-\\nsages to construct AVEs for span-based QA. Zhang\\net al. (2019d) construct AVEs for paraphrase de-\\ntection using word swapping. Kang et al. (2018)\\nand Glockner et al. (2018) create AVEs for textual\\nentailment using WordNet relations.\\n5\\nTasks\\nIn this section, we discuss several DA works\\nfor common NLP tasks.2 We focus on non-\\nclassiﬁcation tasks as classiﬁcation is worked on\\nby default, and well covered in earlier sections (e.g.\\n§3 and §4). Numerous previously mentioned DA\\ntechniques, e.g. (Wei and Zou, 2019; Chen et al.,\\n2020b; Anaby-Tavor et al., 2020), have been used\\nor can be used for text classiﬁcation tasks.\\n5.1\\nSummarization\\nFabbri et al. (2020) investigate backtranslation as a\\nDA method for few-shot abstractive summarization\\nwith the use of a consistency loss inspired by UDA.\\nParida and Motlicek (2019) propose an iterative DA\\napproach for abstractive summarization that uses a\\nmix of synthetic and real data, where the former is\\ngenerated from Common Crawl. Zhu et al. (2019)\\nintroduce a query-focused summarization (Dang,\\n2005) dataset collected using Wikipedia called\\nWIKIREF which can be used for DA. Pasunuru et al.\\n(2021) use DA methods to construct two training\\ndatasets for Query-focused Multi-Document Sum-\\nmarization (QMDS) called QMDSCNN and QMD-\\nSIR by modifying CNN/DM (Hermann et al., 2015)\\nand mining search-query logs, respectively.\\n5.2\\nQuestion Answering (QA)\\nLongpre et al. (2019) investigate various DA and\\nsampling techniques for domain-agnostic QA in-\\ncluding paraphrasing by backtranslation. Yang\\net al. (2019) propose a DA method using distant\\nsupervision to improve BERT ﬁnetuning for open-\\ndomain QA. Riabi et al. (2020) leverage Question\\nGeneration models to produce augmented exam-\\nples for zero-shot cross-lingual QA. Singh et al.\\n(2019) propose XLDA, or CROSS-LINGUAL DA,\\nwhich substitutes a portion of the input text with\\nits translation in another language, improving per-\\nformance across multiple languages on NLI tasks\\nincluding the SQuAD QA task. Asai and Hajishirzi\\n(2020) use logical and linguistic knowledge to gen-\\nerate additional training data to improve the accu-\\nracy and consistency of QA responses by models.\\nYu et al. (2018) introduce a new QA architecture\\ncalled QANet that shows improved performance\\non SQuAD when combined with augmented data\\ngenerated using backtranslation.\\n5.3\\nSequence Tagging Tasks\\nDing et al. (2020) propose DAGA, a two-step DA\\nprocess. First, a language model over sequences of\\ntags and words linearized as per a certain scheme is\\nlearned. Second, sequences are sampled from this\\nlanguage model and de-linearized to generate new\\nexamples. ¸Sahin and Steedman (2018), discussed\\nin §3.1, use dependency tree morphing (Figure 2)\\nto generate additional training examples on the\\ndownstream task of part-of-speech (POS) tagging.\\nDai and Adel (2020) modify DA techniques pro-\\nposed for sentence-level tasks for named entity\\nrecognition (NER), including label-wise token and\\nsynonym replacement, and show improved perfor-\\nmance using both recurrent and transformer models.\\nZhang et al. (2020) propose a DA method based\\non MIXUP called SEQMIX for active sequence la-\\nbeling by augmenting queried samples, showing\\nimprovements on NER and Event Detection.\\n5.4\\nParsing Tasks\\nJia and Liang (2016) propose DATA RECOMBINA-\\nTION for injecting task-speciﬁc priors to neural se-\\nmantic parsers. A synchronous context-free gram-\\n\\nmar (SCFG) is induced from training data, and\\nnew \"recombinant\" examples are sampled. Yu et al.\\n(2020) introduce GRAPPA, a pretraining approach\\nfor table semantic parsing, and generate synthetic\\nquestion-SQL pairs via an SCFG. Andreas (2020)\\nuse compositionality to construct synthetic exam-\\nples for downstream tasks like semantic parsing.\\nFragments of original examples are replaced with\\nfragments from other examples in similar contexts.\\nVania et al. (2019) investigate DA for low-\\nresource dependency parsing including dependency\\ntree morphing from ¸Sahin and Steedman (2018)\\n(Figure 2) and modiﬁed nonce sentence genera-\\ntion from Gulordava et al. (2018), which replaces\\ncontent words with other words of the same POS,\\nmorphological features, and dependency labels.\\n5.5\\nGrammatical Error Correction (GEC)\\nLack of parallel data is typically a barrier for GEC.\\nVarious works have thus looked at DA methods\\nfor GEC. We discuss some here, and more can be\\nfound in Table 2 in Appendix C.\\nThere is work that makes use of additional re-\\nsources.\\nBoyd (2018) use German edits from\\nWikipedia revision history and use those relating\\nto GEC as augmented training data. Zhang et al.\\n(2019b) explore multi-task transfer, or the use of\\nannotated data from other tasks.\\nThere is also work that adds synthetic errors to\\nnoise the text. Wang et al. (2019a) investigate two\\napproaches: token-level perturbations and training\\nerror generation models with a ﬁltering strategy\\nto keep generations with sufﬁcient errors. Grund-\\nkiewicz et al. (2019) use confusion sets generated\\nby a spellchecker for noising. Choe et al. (2019)\\nlearn error patterns from small annotated samples\\nalong with POS-speciﬁc noising.\\nThere have also been approaches to improve the\\ndiversity of generated errors. Wan et al. (2020)\\ninvestigate noising through editing the latent repre-\\nsentations of grammatical sentences, and Xie et al.\\n(2018) use a neural sequence transduction model\\nand beam search noising procedures.\\n5.6\\nNeural Machine Translation (NMT)\\nThere are many works which have investigated DA\\nfor NMT. We highlighted some in §3 and §4.1,\\ne.g. (Sennrich et al., 2016; Fadaee et al., 2017; Xia\\net al., 2019). We discuss some further ones here,\\nand more can be found in Table 3 in Appendix C.\\nWang et al. (2018a) propose SWITCHOUT, a\\nDA method that randomly replaces words in both\\nsource and target sentences with other random\\nwords from their corresponding vocabularies. Gao\\net al. (2019) introduce SOFT CONTEXTUAL DA\\nthat softly augments randomly chosen words in a\\nsentence using a contextual mixture of multiple\\nrelated words over the vocabulary. Nguyen et al.\\n(2020) propose DATA DIVERSIFICATION which\\nmerges original training data with the predictions\\nof several forward and backward models.\\n5.7\\nData-to-Text NLG\\nData-to-text NLG refers to tasks which require gen-\\nerating natural language descriptions of structured\\nor semi-structured data inputs, e.g. game score\\ntables (Wiseman et al., 2017). Randomly perturb-\\ning game score values without invalidating overall\\ngame outcome is one DA strategy explored in game\\nsummary generation (Hayashi et al., 2019).\\nTwo popular recent benchmarks are E2E-NLG\\n(Dušek et al., 2018) and WebNLG (Gardent et al.,\\n2017). Both involve generation from structured\\ninputs - meaning representation (MR) sequences\\nand triple sequences, respectively. Montella et al.\\n(2020) show performance gains on WebNLG by\\nDA using Wikipedia sentences as targets and\\nparsed OpenIE triples as inputs.\\nTandon et al.\\n(2018) propose DA for E2E-NLG based on per-\\nmuting the input MR sequence. Kedzie and McK-\\neown (2019) inject Gaussian noise into a trained\\ndecoder’s hidden states and sample diverse aug-\\nmented examples from it. This sample-augment-\\nretrain loop helps performance on E2E-NLG.\\n5.8\\nOpen-Ended & Conditional Generation\\nThere has been limited work on DA for open-ended\\nand conditional text generation. Feng et al. (2020)\\nexperiment with a suite of DA methods for ﬁnetun-\\ning GPT-2 on a low-resource domain in attempts\\nto improve the quality of generated continuations,\\nwhich they call GENAUG. They ﬁnd that WN-\\nHYPERS (WordNet hypernym replacement of key-\\nwords) and SYNTHETIC NOISE (randomly perturb-\\ning non-terminal characters in words) are useful,\\nand the quality of generated text improves to a peak\\nat ≈3x the original amount of training data.\\n5.9\\nDialogue\\nMost DA approaches for dialogue focus on task-\\noriented dialogue. We outline some below, and\\nmore can be found in Table 4 in Appendix C.\\nQuan and Xiong (2019) present sentence and\\nword-level DA approaches for end-to-end task-\\n\\noriented dialogue. Louvan and Magnini (2020)\\npropose LIGHTWEIGHT AUGMENTATION, a set of\\nword-span and sentence-level DA methods for low-\\nresource slot ﬁlling and intent classiﬁcation.\\nHou et al. (2018) present a seq2seq DA frame-\\nwork to augment dialogue utterances for dialogue\\nlanguage understanding (Young et al., 2013), in-\\ncluding a diversity rank to produce diverse utter-\\nances. Zhang et al. (2019c) propose MADA to\\ngenerate diverse responses using the property that\\nseveral valid responses exist for a dialogue context.\\nThere is also DA work for spoken dialogue. Hou\\net al. (2018), Kim et al. (2019), Zhao et al. (2019),\\nand Yoo et al. (2019) investigate DA methods for di-\\nalogue and spoken language understanding (SLU),\\nincluding generative latent variable models.\\n5.10\\nMultimodal Tasks\\nDA techniques have also been proposed for multi-\\nmodal tasks where aligned data for multiple modal-\\nities is required. We look at ones that involve lan-\\nguage or text. Some are discussed below, and more\\ncan be found in Table 5 in Appendix C.\\nBeginning with speech, Wang et al. (2020) pro-\\npose a DA method to improve the robustness of\\ndownstream dialogue models to speech recognition\\nerrors. Wiesner et al. (2018) and Renduchintala\\net al. (2018) propose DA methods for end-to-end\\nautomatic speech recognition (ASR).\\nLooking at images or video, Xu et al. (2020)\\nlearn a cross-modality matching network to pro-\\nduce synthetic image-text pairs for multimodal clas-\\nsiﬁers. Atliha and Šešok (2020) explore DA meth-\\nods such as synonym replacement and contextual-\\nized word embeddings augmentation using BERT\\nfor image captioning. Kaﬂe et al. (2017), Yokota\\nand Nakayama (2018), and Tang et al. (2020) pro-\\npose methods for visual QA including question\\ngeneration and adversarial examples.\\n6\\nChallenges & Future Directions\\nLooking forward, data augmentation faces substan-\\ntial challenges, speciﬁcally for NLP, and with these\\nchallenges, new opportunities for future work arise.\\nDissonance between empirical novelties and\\ntheoretical narrative:\\nThere appears to be a con-\\nspicuous lack of research on why DA works. Most\\nstudies might show empirically that a DA technique\\nworks and provide some intuition, but it is currently\\nchallenging to measure the goodness of a technique\\nwithout resorting to a full-scale experiment. A re-\\ncent work in vision (Gontijo-Lopes et al., 2020)\\nhas proposed that afﬁnity (the distributional shift\\ncaused by DA) and diversity (the complexity of the\\naugmentation) can predict DA performance, but it\\nis unclear how these results might translate to NLP.\\nMinimal beneﬁt for pretrained models on in-\\ndomain data:\\nWith the popularization of large\\npretrained language models, it has come to light\\nthat a couple previously effective DA techniques\\nfor certain English text classiﬁcation tasks (Wei and\\nZou, 2019; Sennrich et al., 2016) provide little ben-\\neﬁt for models like BERT and RoBERTa, which\\nalready achieve high performance on in-domain\\ntext classiﬁcation (Longpre et al., 2020). One hy-\\npothesis is that using simple DA techniques pro-\\nvides little beneﬁt when ﬁnetuning large pretrained\\ntransformers on tasks for which examples are well-\\nrepresented in the pretraining data, but DA methods\\ncould still be effective when ﬁnetuning on tasks for\\nwhich examples are scarce or out-of-domain com-\\npared with the training data. Further work could\\nstudy under which scenarios data augmentation for\\nlarge pretrained models is likely to be effective.\\nMultimodal challenges:\\nWhile there has been\\nincreased work in multimodal DA, as discussed in\\n§5.10, effective DA methods for multiple modal-\\nities has been challenging. Many works focus on\\naugmenting a single modality or multiple ones sep-\\narately. For example, there is potential to further\\nexplore simultaneous image and text augmentation\\nfor image captioning, such as a combination of\\nCUTMIX (Yun et al., 2019) and caption editing.\\nSpan-based tasks\\noffer unique DA challenges\\nas there are typically many correlated classiﬁcation\\ndecisions. For example, random token replacement\\nmay be a locally acceptable DA method but possi-\\nbly disrupt coreference chains for latter sentences.\\nDA techniques here must take into account depen-\\ndencies between different locations in the text.\\nWorking in specialized domains\\nsuch as those\\nwith domain-speciﬁc vocabulary and jargon (e.g.\\nmedicine) can present challenges. Many pretrained\\nmodels and external knowledge (e.g. WordNet)\\ncannot be effectively used. Studies have shown\\nthat DA becomes less beneﬁcial when applied to\\nout-of-domain data, likely because the distribution\\nof augmented data can substantially differ from the\\noriginal data (Zhang et al., 2019a; Herzig et al.,\\n2020; Campagna et al., 2020; Zhong et al., 2020).\\n\\nWorking with low-resource languages\\nmay\\npresent similar difﬁculties as specialized domains.\\nFurther, DA techniques successful in the high-\\nresource scenario may not be effective for low-\\nresource languages that are of a different language\\nfamily or very distinctive in linguistic and typolog-\\nical terms. For example, those which are language\\nisolates or lack high-resource cognates.\\nMore\\nvision-inspired\\ntechniques:\\nAlthough\\nmany NLP DA methods have been inspired by anal-\\nogous approaches in CV, there is potential for draw-\\ning further connections. Many CV DA techniques\\nmotivated by real-world invariances (e.g. many\\nangles of looking at the same object) may have\\nsimilar NLP interpretations. For instance, grayscal-\\ning could translate to toning down aspects of the\\ntext (e.g. plural to singular, \"awesome\" →\"good\").\\nMorphing a dependency tree could be analogous\\nto rotating an image, and paraphrasing techniques\\nmay be analogous to changing perspective. For ex-\\nample, negative data augmentation (NDA) (Sinha\\net al., 2021) involves creating out-of-distribution\\nsamples. It has so far been exclusively explored for\\nCV, but could be investigated for text.\\nSelf-supervised learning:\\nMore recently, DA\\nhas been increasingly used as a key component\\nof self-supervised learning, particularly in vision\\n(Chen et al., 2020e). In NLP, BART (Lewis et al.,\\n2020) showed that predicting deleted tokens as a\\npretraining task can achieve similar performance as\\nthe masked LM, and ELECTRA (Clark et al., 2020)\\nfound that pretraining by predicting corrupted to-\\nkens outperforms BERT given the same model size,\\ndata, and compute. We expect future work will\\ncontinue exploring how to effectively manipulate\\ntext for both pretraining and downstream tasks.\\nOfﬂine versus online data augmentation:\\nIn\\nCV, standard techniques such as cropping and ro-\\ntations are typically done stochastically, allowing\\nfor DA to be incorporated elegantly into the train-\\ning pipeline. In NLP, however, it is unclear how\\nto include a lightweight code module to apply DA\\nstochastically. This is because DA techniques for\\nNLP often leverage external resources (e.g. a dic-\\ntionary for token substitution or translation model\\nfor backtranslation) that are not easily transferable\\nacross training pipelines. Thus, a common prac-\\ntice for DA in NLP is to generate augmented data\\nofﬂine and store it as additional data to be loaded\\nduring training.4 Future work on a lightweight\\nmodule for online DA in NLP could be fruitful,\\nthough another challenge will be determining when\\nsuch a module will be helpful, which—compared\\nwith CV, where invariances being imposed are well-\\naccepted—can vary substantially across NLP tasks.\\nLack of uniﬁcation\\nis a challenge for the cur-\\nrent literature on data augmentation for NLP, and\\npopular methods are often presented in an aux-\\niliary fashion. Whereas there are well-accepted\\nframeworks for DA for CV (e.g. default augmen-\\ntation libraries in PyTorch, RandAugment (Cubuk\\net al., 2020)), there are no such \"generalized\" DA\\ntechniques for NLP. Further, we believe that DA\\nresearch would beneﬁt from the establishment of\\nstandard and uniﬁed benchmark tasks and datasets\\nto compare different augmentation methods.\\nGood data augmentation practices\\nwould help\\nmake DA work more accessible and reproducible\\nto the NLP and ML communities.\\nOn top of\\nuniﬁed benchmark tasks, datasets, and frame-\\nworks/libraries mentioned above, other good prac-\\ntices include making code and augmented datasets\\npublicly available, reporting variation among re-\\nsults (e.g. standard deviation across random seeds),\\nand more standardized evaluation procedures. Fur-\\nther, transparent hyperparameter analysis, explic-\\nitly stating failure cases of proposed techniques,\\nand discussion of the intuition and theory behind\\nthem would further improve the transparency and\\ninterpretability of DA techniques.\\n7\\nConclusion\\nIn this paper, we presented a comprehensive and\\nstructured survey of data augmentation for nat-\\nural language processing (NLP). We provided a\\nbackground about data augmentation and how it\\nworks, discussed major methodologically represen-\\ntative data augmentation techniques for NLP, and\\ntouched upon data augmentation techniques for\\npopular NLP applications and tasks. Finally, we\\noutlined current challenges and directions for fu-\\nture research, and showed that there is much room\\nfor further exploration. Overall, we hope our paper\\ncan serve as a guide for NLP researchers to decide\\non which data augmentation techniques to use, and\\ninspire additional interest and work in this area.\\nPlease see the corresponding GitHub repository at\\nhttps://github.com/styfeng/DataAug4NLP.\\n4See Appendix D.\\n\\nReferences\\nDiego Alves, Askars Salimbajevs, and M¯arcis Pinnis.\\n2020. Data augmentation for pipeline-based speech\\ntranslation. In 9th International Conference on Hu-\\nman Language Technologies - the Baltic Perspective\\n(Baltic HLT 2020), Kaunas, Lithuania.\\nAteret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich,\\nAmir Kantor, George Kour, Segev Shlomov, Naama\\nTepper, and Naama Zwerdling. 2020. Do not have\\nenough data? Deep learning to the rescue! In Pro-\\nceedings of AAAI, pages 7383–7390.\\nJacob Andreas. 2020.\\nGood-enough compositional\\ndata augmentation. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics, pages 7556–7566, Online. Association\\nfor Computational Linguistics.\\nAkari Asai and Hannaneh Hajishirzi. 2020.\\nLogic-\\nguided data augmentation and regularization for con-\\nsistent question answering. In Proceedings of the\\n58th Annual Meeting of the Association for Compu-\\ntational Linguistics, pages 5642–5650, Online. As-\\nsociation for Computational Linguistics.\\nViktar Atliha and Dmitrij Šešok. 2020. Text augmen-\\ntation using BERT for image captioning. Applied\\nSciences, 10:5978.\\nChristopher Beckham, Sina Honari, Vikas Verma,\\nAlex M. Lamb, Farnoosh Ghadiri, R Devon Hjelm,\\nYoshua Bengio, and Chris Pal. 2019. On adversarial\\nmixup resynthesis. In Advances in Neural Informa-\\ntion Processing Systems, pages 4346–4357.\\nChris M. Bishop. 1995. Training with noise is equiv-\\nalent to Tikhonov regularization. Neural Computa-\\ntion, 7(1):108–116.\\nAdriane Boyd. 2018.\\nUsing Wikipedia edits in low\\nresource grammatical error correction. In Proceed-\\nings of the 2018 EMNLP Workshop W-NUT: The\\n4th Workshop on Noisy User-generated Text, pages\\n79–84, Brussels, Belgium. Association for Compu-\\ntational Linguistics.\\nBram Bulte and Arda Tezcan. 2019. Neural fuzzy re-\\npair: Integrating fuzzy matches into neural machine\\ntranslation. In Proceedings of the 57th Annual Meet-\\ning of the Association for Computational Linguistics,\\npages 1800–1809, Florence, Italy. Association for\\nComputational Linguistics.\\nHengyi Cai, Hongshen Chen, Yonghao Song, Cheng\\nZhang, Xiaofang Zhao, and Dawei Yin. 2020. Data\\nmanipulation: Towards effective instance learning\\nfor neural dialogue generation via learning to aug-\\nment and reweight. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics, pages 6334–6343, Online. Association\\nfor Computational Linguistics.\\nGiovanni Campagna, Agata Foryciarz, Mehrad Morad-\\nshahi, and Monica Lam. 2020. Zero-shot transfer\\nlearning with synthesized data for multi-domain dia-\\nlogue state tracking. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics, pages 122–132, Online. Association for\\nComputational Linguistics.\\nF. Charte, Antonio Rivera Rivas, María José Del Je-\\nsus, and Francisco Herrera. 2015.\\nMlsmote: Ap-\\nproaching imbalanced multilabel learning through\\nsynthetic instance generation.\\nKnowledge-Based\\nSystems.\\nNitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall,\\nand W. Philip Kegelmeyer. 2002.\\nSMOTE: Syn-\\nthetic minority over-sampling technique. Journal of\\nArtiﬁcial Intelligence Research, 16:321–357.\\nGuanhua Chen, Yun Chen, Yong Wang, and Victor O.K.\\nLi. 2020a. Lexical-constraint-aware neural machine\\ntranslation via data augmentation. In Proceedings of\\nthe Twenty-Ninth International Joint Conference on\\nArtiﬁcial Intelligence, IJCAI-20, pages 3587–3593.\\nInternational Joint Conferences on Artiﬁcial Intelli-\\ngence Organization. Main track.\\nHannah Chen, Yangfeng Ji, and David Evans. 2020b.\\nFinding friends and ﬂipping frenemies: Automatic\\nparaphrase dataset augmentation using graph theory.\\nIn Findings of the Association for Computational\\nLinguistics: EMNLP 2020, pages 4741–4751, On-\\nline. Association for Computational Linguistics.\\nJiaao Chen, Zichao Yang, and Diyi Yang. 2020c. Mix-\\nText: Linguistically-informed interpolation of hid-\\nden space for semi-supervised text classiﬁcation. In\\nProceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics, pages 2147–\\n2157, Online. Association for Computational Lin-\\nguistics.\\nShuxiao Chen, Edgar Dobriban, and Jane Lee. 2020d.\\nA group-theoretic framework for data augmentation.\\nAdvances in Neural Information Processing Systems,\\n33.\\nTing Chen, Simon Kornblith, Mohammad Norouzi,\\nand Geoffrey Hinton. 2020e. A simple framework\\nfor contrastive learning of visual representations. In\\nProceedings of the 37th International Conference\\non Machine Learning, volume 119 of Proceedings\\nof Machine Learning Research, pages 1597–1607.\\nPMLR.\\nYong Cheng, Lu Jiang, Wolfgang Macherey, and Ja-\\ncob Eisenstein. 2020.\\nAdvAug: Robust adversar-\\nial augmentation for neural machine translation. In\\nProceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics, pages 5961–\\n5970, Online. Association for Computational Lin-\\nguistics.\\nMara Chinea-Ríos,\\nÁlvaro Peris,\\nand Francisco\\nCasacuberta. 2017. Adapting neural machine trans-\\nlation with parallel synthetic data. In Proceedings\\nof the Second Conference on Machine Translation,\\n\\npages 138–147, Copenhagen, Denmark. Association\\nfor Computational Linguistics.\\nYo Joong Choe, Jiyeon Ham, Kyubyong Park, and\\nYeoil Yoon. 2019. A neural grammatical error cor-\\nrection system built on better pre-training and se-\\nquential transfer learning.\\nIn Proceedings of the\\nFourteenth Workshop on Innovative Use of NLP for\\nBuilding Educational Applications, pages 213–227,\\nFlorence, Italy. Association for Computational Lin-\\nguistics.\\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and\\nChristopher D. Manning. 2020.\\nELECTRA: Pre-\\ntraining text encoders as discriminators rather than\\ngenerators. Proceedings of ICLR.\\nEkin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay\\nVasudevan, and Quoc V. Le. 2018. Autoaugment:\\nLearning augmentation policies from data. Proceed-\\nings of CVPR.\\nEkin D. Cubuk, Barret Zoph, Jonathon Shlens, and\\nQuoc V. Le. 2020.\\nRandaugment: Practical au-\\ntomated data augmentation with a reduced search\\nspace. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition Work-\\nshops, pages 702–703.\\nXiang Dai and Heike Adel. 2020.\\nAn analysis of\\nsimple data augmentation for named entity recogni-\\ntion. In Proceedings of the 28th International Con-\\nference on Computational Linguistics, pages 3861–\\n3867, Barcelona, Spain (Online). International Com-\\nmittee on Computational Linguistics.\\nHao T. Dang. 2005. Overview of DUC 2005.\\nTri Dao, Albert Gu, Alexander J. Ratner, Virginia\\nSmith, Christopher De Sa, and Christopher Ré. 2019.\\nA kernel theory of modern data augmentation. Pro-\\nceedings of Machine Learning Research, 97:1528.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019.\\nBERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers), pages\\n4171–4186.\\nTerrance DeVries and Graham W Taylor. 2017.\\nIm-\\nproved regularization of convolutional neural net-\\nworks with cutout. arXiv preprint.\\nBosheng Ding, Linlin Liu, Lidong Bing, Canasai Kru-\\nengkrai, Thien Hai Nguyen, Shaﬁq Joty, Luo Si, and\\nChunyan Miao. 2020. DAGA: Data augmentation\\nwith a generation approach forLow-resource tagging\\ntasks.\\nIn Proceedings of the 2020 Conference on\\nEmpirical Methods in Natural Language Process-\\ning (EMNLP), pages 6045–6057, Online. Associa-\\ntion for Computational Linguistics.\\nTommaso Dreossi, Shromona Ghosh, Xiangyu Yue,\\nKurt Keutzer, Alberto L. Sangiovanni-Vincentelli,\\nand Sanjit A. Seshia. 2018. Counterexample-guided\\ndata augmentation. In Proceedings of IJCAI.\\nSufeng Duan, Hai Zhao, Dongdong Zhang, and Rui\\nWang. 2020. Syntax-aware data augmentation for\\nneural machine translation. arXiv preprint.\\nOndˇrej Dušek, Jekaterina Novikova, and Verena Rieser.\\n2018.\\nFindings of the E2E NLG challenge.\\nIn\\nProceedings of the 11th International Conference\\non Natural Language Generation, pages 322–328,\\nTilburg University, The Netherlands. Association for\\nComputational Linguistics.\\nAlexander R. Fabbri,\\nSimeng Han,\\nHaoyuan Li,\\nHaoran Li, Marjan Ghazvininejad, Shaﬁq Joty,\\nDragomir Radev, and Yashar Mehdad. 2020.\\nIm-\\nproving zero and few-shot abstractive summariza-\\ntion with intermediate ﬁne-tuning and data augmen-\\ntation. arXiv preprint.\\nMarzieh Fadaee, Arianna Bisazza, and Christof Monz.\\n2017.\\nData augmentation for low-resource neural\\nmachine translation. In Proceedings of the 55th An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 2: Short Papers), pages 567–\\n573, Vancouver, Canada. Association for Computa-\\ntional Linguistics.\\nAlex Falcon, Oswald Lanz, and Giuseppe Serra. 2020.\\nData augmentation techniques for the video question\\nanswering task. arXiv preprint.\\nMojtaba Faramarzi, Mohammad Amini, Akilesh Badri-\\nnaaraayanan, Vikas Verma, and Sarath Chandar.\\n2020. Patchup: A regularization technique for con-\\nvolutional neural networks. arXiv preprint.\\nMariano Felice. 2016. Artiﬁcial error generation for\\ntranslation-based grammatical error correction. Uni-\\nversity of Cambridge Technical Report.\\nSteven Y. Feng, Varun Gangal, Dongyeop Kang,\\nTeruko Mitamura, and Eduard Hovy. 2020. GenAug:\\nData augmentation for ﬁnetuning text generators. In\\nProceedings of Deep Learning Inside Out (DeeLIO):\\nThe First Workshop on Knowledge Extraction and\\nIntegration for Deep Learning Architectures, pages\\n29–42, Online. Association for Computational Lin-\\nguistics.\\nSteven Y. Feng, Aaron W. Li, and Jesse Hoey. 2019.\\nKeep calm and switch on! Preserving sentiment and\\nﬂuency in semantic text exchange. In Proceedings\\nof the 2019 Conference on Empirical Methods in\\nNatural Language Processing and the 9th Interna-\\ntional Joint Conference on Natural Language Pro-\\ncessing (EMNLP-IJCNLP), pages 2701–2711, Hong\\nKong, China. Association for Computational Lin-\\nguistics.\\nAlberto Fernández, Salvador Garcia, Francisco Her-\\nrera, and Nitesh V. Chawla. 2018. SMOTE for learn-\\ning from imbalanced data: progress and challenges,\\n\\nmarking the 15-year anniversary. Journal of Artiﬁ-\\ncial Intelligence Research, 61:863–905.\\nJennifer Foster and Oistein Andersen. 2009.\\nGen-\\nERRate: Generating errors for use in grammatical\\nerror detection. In Proceedings of the Fourth Work-\\nshop on Innovative Use of NLP for Building Edu-\\ncational Applications, pages 82–90, Boulder, Col-\\norado. Association for Computational Linguistics.\\nVarun Gangal, Steven Y. Feng, Eduard Hovy, and\\nTeruko Mitamura. 2021. Nareor: The narrative re-\\nordering problem. arXiv preprint.\\nFei Gao, Jinhua Zhu, Lijun Wu, Yingce Xia, Tao\\nQin, Xueqi Cheng, Wengang Zhou, and Tie-Yan Liu.\\n2019. Soft contextual data augmentation for neural\\nmachine translation. In Proceedings of the 57th An-\\nnual Meeting of the Association for Computational\\nLinguistics, pages 5539–5544, Florence, Italy. Asso-\\nciation for Computational Linguistics.\\nSilin Gao, Yichi Zhang, Zhijian Ou, and Zhou Yu.\\n2020.\\nParaphrase augmented task-oriented dialog\\ngeneration. In Proceedings of the 58th Annual Meet-\\ning of the Association for Computational Linguistics,\\npages 639–649, Online. Association for Computa-\\ntional Linguistics.\\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\\nand Laura Perez-Beltrachini. 2017. The WebNLG\\nchallenge: Generating text from RDF data. In Pro-\\nceedings of the 10th International Conference on\\nNatural Language Generation, pages 124–133, San-\\ntiago de Compostela, Spain. Association for Compu-\\ntational Linguistics.\\nGolnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian,\\nTsung-Yi Lin, Ekin D. Cubuk, Quoc V. Le, and\\nBarret Zoph. 2020. Simple copy-paste is a strong\\ndata augmentation method for instance segmenta-\\ntion. arXiv preprint.\\nMax Glockner, Vered Shwartz, and Yoav Goldberg.\\n2018. Breaking NLI systems with sentences that re-\\nquire simple lexical inferences. In Proceedings of\\nthe 56th Annual Meeting of the Association for Com-\\nputational Linguistics (Volume 2:\\nShort Papers),\\npages 650–655, Melbourne, Australia. Association\\nfor Computational Linguistics.\\nRaphael Gontijo-Lopes, Sylvia J. Smullin, Ekin D.\\nCubuk, and Ethan Dyer. 2020.\\nTradeoffs in data\\naugmentation: An empirical study. Proceedings of\\nICLR.\\nMiguel Graça, Yunsu Kim, Julian Schamper, Shahram\\nKhadivi, and Hermann Ney. 2019.\\nGeneralizing\\nback-translation in neural machine translation.\\nIn\\nProceedings of the Fourth Conference on Machine\\nTranslation (Volume 1: Research Papers), pages 45–\\n52, Florence, Italy. Association for Computational\\nLinguistics.\\nMilan Gritta, Gerasimos Lampouras, and Ignacio Ia-\\ncobacci. 2021. Conversation graph: Data augmen-\\ntation, training, and evaluation for non-deterministic\\ndialogue management. Transactions of the Associa-\\ntion for Computational Linguistics, 9:36–52.\\nRoman Grundkiewicz, Marcin Junczys-Dowmunt, and\\nKenneth Heaﬁeld. 2019. Neural grammatical error\\ncorrection systems with unsupervised pre-training\\non synthetic data. In Proceedings of the Fourteenth\\nWorkshop on Innovative Use of NLP for Building\\nEducational Applications, pages 252–263, Florence,\\nItaly. Association for Computational Linguistics.\\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\\nTal Linzen, and Marco Baroni. 2018.\\nColorless\\ngreen recurrent networks dream hierarchically. In\\nProceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies,\\nVolume 1 (Long Papers), pages 1195–1205, New\\nOrleans, Louisiana. Association for Computational\\nLinguistics.\\nDemi Guo, Yoon Kim, and Alexander Rush. 2020.\\nSequence-level mixed sample data augmentation. In\\nProceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP),\\npages 5547–5552, Online. Association for Computa-\\ntional Linguistics.\\nHongyu Guo. 2020.\\nNonlinear mixup:\\nOut-of-\\nmanifold data augmentation for text classiﬁcation.\\nIn Proceedings of AAAI, pages 4044–4051.\\nRowan Hall Maudslay, Hila Gonen, Ryan Cotterell,\\nand Simone Teufel. 2019. It’s all in the name: Mit-\\nigating gender bias with name-based counterfactual\\ndata substitution. In Proceedings of the 2019 Con-\\nference on Empirical Methods in Natural Language\\nProcessing and the 9th International Joint Confer-\\nence on Natural Language Processing (EMNLP-\\nIJCNLP), pages 5267–5275, Hong Kong, China. As-\\nsociation for Computational Linguistics.\\nBharath Hariharan and Ross Girshick. 2017.\\nLow-\\nshot visual recognition by shrinking and hallucinat-\\ning features. In Proceedings of the IEEE Interna-\\ntional Conference on Computer Vision, pages 3018–\\n3027.\\nHany Hassan, Mostafa Elaraby, and Ahmed Tawﬁk.\\n2017. Synthetic data for neural machine translation\\nof spoken-dialects. arXiv preprint.\\nHiroaki Hayashi, Yusuke Oda, Alexandra Birch, Ioan-\\nnis Konstas, Andrew Finch, Minh-Thang Luong,\\nGraham Neubig, and Katsuhito Sudoh. 2019. Find-\\nings of the third workshop on neural generation and\\ntranslation.\\nIn Proceedings of the 3rd Workshop\\non Neural Generation and Translation, pages 1–14,\\nHong Kong. Association for Computational Linguis-\\ntics.\\n\\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\\nand Phil Blunsom. 2015. Teaching machines to read\\nand comprehend. In NeurIPS, pages 1693–1701.\\nAlex Hernández-García and Peter König. 2020. Data\\naugmentation instead of explicit regularization.\\narXiv preprint.\\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\\nMüller, Francesco Piccinno, and Julian Eisenschlos.\\n2020. TaPas: Weakly supervised table parsing via\\npre-training.\\nIn Proceedings of the 58th Annual\\nMeeting of the Association for Computational Lin-\\nguistics, pages 4320–4333, Online. Association for\\nComputational Linguistics.\\nYutai Hou, Yijia Liu, Wanxiang Che, and Ting Liu.\\n2018. Sequence-to-sequence data augmentation for\\ndialogue language understanding. In Proceedings of\\nthe 27th International Conference on Computational\\nLinguistics, pages 1234–1245, Santa Fe, New Mex-\\nico, USA. Association for Computational Linguis-\\ntics.\\nZhiting Hu, Bowen Tan, Russ R. Salakhutdinov,\\nTom M. Mitchell, and Eric P. Xing. 2019. Learning\\ndata manipulation for augmentation and weighting.\\nIn Advances in Neural Information Processing Sys-\\ntems, volume 32. Curran Associates, Inc.\\nJian Huang, Ya Li, Jianhua Tao, Zheng Lian, Mingyue\\nNiu, and Minghao Yang. 2018. Multimodal continu-\\nous emotion recognition with data augmentation us-\\ning recurrent neural networks. In Proceedings of the\\n2018 on Audio/Visual Emotion Challenge and Work-\\nshop, AVEC’18, page 57–64, New York, NY, USA.\\nAssociation for Computing Machinery.\\nBrian Kenji Iwana and Seiichi Uchida. 2020. An em-\\npirical survey of data augmentation for time series\\nclassiﬁcation with neural networks. arXiv preprint.\\nMohit Iyyer, John Wieting, Kevin Gimpel, and Luke\\nZettlemoyer. 2018. Adversarial example generation\\nwith syntactically controlled paraphrase networks.\\nIn Proceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies,\\nVolume 1 (Long Papers), pages 1875–1885, New\\nOrleans, Louisiana. Association for Computational\\nLinguistics.\\nRobin Jia and Percy Liang. 2016. Data recombination\\nfor neural semantic parsing. In Proceedings of the\\n54th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), pages\\n12–22, Berlin, Germany. Association for Computa-\\ntional Linguistics.\\nRobin Jia, Aditi Raghunathan, Kerem Göksel, and\\nPercy Liang. 2019.\\nCertiﬁed robustness to adver-\\nsarial word substitutions.\\nIn Proceedings of the\\n2019 Conference on Empirical Methods in Natu-\\nral Language Processing and the 9th International\\nJoint Conference on Natural Language Processing\\n(EMNLP-IJCNLP), pages 4120–4133.\\nAmit Jindal, Arijit Ghosh Chowdhury, Aniket Didolkar,\\nDi Jin, Ramit Sawhney, and Rajiv Ratn Shah. 2020a.\\nAugmenting NLP models using latent feature in-\\nterpolations.\\nIn Proceedings of the 28th Inter-\\nnational Conference on Computational Linguistics,\\npages 6931–6936, Barcelona, Spain (Online). Inter-\\nnational Committee on Computational Linguistics.\\nAmit Jindal, Narayanan Elavathur Ranganatha, Aniket\\nDidolkar, Arijit Ghosh Chowdhury, Di Jin, Ramit\\nSawhney, and Rajiv Ratn Shah. 2020b. Speechmix\\n- augmenting deep sound recognition using hidden\\nspace interpolations. In INTERSPEECH, pages 861–\\n865.\\nKushal Kaﬂe, Mohammed Yousefhussien, and Christo-\\npher Kanan. 2017.\\nData augmentation for visual\\nquestion answering. In Proceedings of the 10th In-\\nternational Conference on Natural Language Gen-\\neration, pages 198–202, Santiago de Compostela,\\nSpain. Association for Computational Linguistics.\\nDongyeop Kang, Tushar Khot, Ashish Sabharwal, and\\nEduard Hovy. 2018. AdvEntuRe: Adversarial train-\\ning for textual entailment with knowledge-guided ex-\\namples. In Proceedings of the 56th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers), pages 2418–2428, Mel-\\nbourne, Australia. Association for Computational\\nLinguistics.\\nMin-Hyung Kang. 2019.\\nValar nmt : Vastly lack-\\ning resources neural machine translation. Stanford\\nCS224N.\\nOmid Kasheﬁand Rebecca Hwa. 2020. Quantifying\\nthe evaluation of heuristic methods for textual data\\naugmentation.\\nIn Proceedings of the Sixth Work-\\nshop on Noisy User-generated Text (W-NUT 2020),\\npages 200–208, Online. Association for Computa-\\ntional Linguistics.\\nDivyansh Kaushik, Eduard Hovy, and Zachary Lipton.\\n2020. Learning the difference that makes a differ-\\nence with counterfactually-augmented data. In Inter-\\nnational Conference on Learning Representations.\\nDivyansh Kaushik, Amrith Setlur, Eduard H. Hovy,\\nand Zachary Chase Lipton. 2021. Explaining the ef-\\nﬁcacy of counterfactually augmented data. In Inter-\\nnational Conference on Learning Representations.\\nChris Kedzie and Kathleen McKeown. 2019. A good\\nsample is hard to ﬁnd: Noise injection sampling and\\nself-training for neural language generation models.\\nIn Proceedings of the 12th International Conference\\non Natural Language Generation, pages 584–593,\\nTokyo, Japan. Association for Computational Lin-\\nguistics.\\nHwa-Yeon Kim, Yoon-Hyung Roh, and Young-Kil\\nKim. 2019.\\nData augmentation by data noising\\n\\nfor open-vocabulary slots in spoken language under-\\nstanding. In Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Student Research Work-\\nshop, pages 97–102, Minneapolis, Minnesota. Asso-\\nciation for Computational Linguistics.\\nAlex Kimn. 2020. A syntactic rule-based framework\\nfor parallel data synthesis in japanese gec.\\nMas-\\nsachusetts Institute of Technology.\\nSosuke Kobayashi. 2018.\\nContextual augmentation:\\nData augmentation by words with paradigmatic re-\\nlations. In Proceedings of the 2018 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, Volume 2 (Short Papers), pages 452–457,\\nNew Orleans, Louisiana. Association for Computa-\\ntional Linguistics.\\nAshutosh Kumar, Kabir Ahuja, Raghuram Vadapalli,\\nand Partha Talukdar. 2020.\\nSyntax-guided con-\\ntrolled generation of paraphrases.\\nTransactions\\nof the Association for Computational Linguistics,\\n8:329–345.\\nAshutosh Kumar, Satwik Bhattamishra, Manik Bhan-\\ndari, and Partha Talukdar. 2019a.\\nSubmodular\\noptimization-based diverse paraphrasing and its ef-\\nfectiveness in data augmentation. In Proceedings of\\nthe 2019 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies, Volume 1 (Long\\nand Short Papers), pages 3609–3619, Minneapolis,\\nMinnesota. Association for Computational Linguis-\\ntics.\\nVarun Kumar, Hadrien Glaude, Cyprien de Lichy, and\\nWlliam Campbell. 2019b. A closer look at feature\\nspace data augmentation for few-shot intent classi-\\nﬁcation.\\nIn Proceedings of the 2nd Workshop on\\nDeep Learning Approaches for Low-Resource NLP\\n(DeepLo 2019), pages 1–10, Hong Kong, China. As-\\nsociation for Computational Linguistics.\\nKenton Lee, Kelvin Guu, Luheng He, Tim Dozat, and\\nHyung Won Chung. 2021. Neural data augmenta-\\ntion via example extrapolation. arXiv preprint.\\nMike\\nLewis,\\nYinhan\\nLiu,\\nNaman\\nGoyal,\\nMar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\\n2020. BART: Denoising sequence-to-sequence pre-\\ntraining for natural language generation, translation,\\nand comprehension. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics, pages 7871–7880, Online. Association\\nfor Computational Linguistics.\\nDaniel Li, Te I, Naveen Arivazhagan, Colin Cherry,\\nand Dirk Padﬁeld. 2020a. Sentence boundary aug-\\nmentation for neural machine translation robustness.\\narXiv preprint.\\nYitong Li, Trevor Cohn, and Timothy Baldwin. 2017.\\nRobust training under linguistic adversity. In Pro-\\nceedings of the 15th Conference of the European\\nChapter of the Association for Computational Lin-\\nguistics: Volume 2, Short Papers, pages 21–27, Va-\\nlencia, Spain. Association for Computational Lin-\\nguistics.\\nYu Li, Xiao Li, Yating Yang, and Rui Dong. 2020b. A\\ndiverse data augmentation strategy for low-resource\\nneural machine translation. Information, 11(5).\\nZhenhao Li and Lucia Specia. 2019. Improving neu-\\nral machine translation robustness via data augmen-\\ntation: Beyond back-translation. In Proceedings of\\nthe 5th Workshop on Noisy User-generated Text (W-\\nNUT 2019), pages 328–336, Hong Kong, China. As-\\nsociation for Computational Linguistics.\\nJared Lichtarge, Chris Alberti, Shankar Kumar, Noam\\nShazeer, Niki Parmar, and Simon Tong. 2019. Cor-\\npora generation for grammatical error correction. In\\nProceedings of the 2019 Conference of the North\\nAmerican Chapter of the Association for Compu-\\ntational Linguistics: Human Language Technolo-\\ngies, Volume 1 (Long and Short Papers), pages\\n3291–3301, Minneapolis, Minnesota. Association\\nfor Computational Linguistics.\\nPei Liu, Xuemin Wang, Chao Xiang, and Weiye Meng.\\n2020a. A survey of text data augmentation. In 2020\\nInternational Conference on Computer Communica-\\ntion and Network Security (CCNS), pages 191–195.\\nRuibo Liu, Guangxuan Xu, Chenyan Jia, Weicheng\\nMa, Lili Wang, and Soroush Vosoughi. 2020b. Data\\nboost: Text data augmentation through reinforce-\\nment learning guided conditional generation.\\nIn\\nProceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP),\\npages 9031–9041, Online. Association for Computa-\\ntional Linguistics.\\nRuibo Liu, Guangxuan Xu, and Soroush Vosoughi.\\n2020c.\\nEnhanced offensive language detection\\nthrough data augmentation.\\nICWSM Data Chal-\\nlenge.\\nShayne Longpre, Yi Lu, Zhucheng Tu, and Chris\\nDuBois. 2019. An exploration of data augmentation\\nand sampling techniques for domain-agnostic ques-\\ntion answering. In Proceedings of the 2nd Workshop\\non Machine Reading for Question Answering, pages\\n220–227, Hong Kong, China. Association for Com-\\nputational Linguistics.\\nShayne Longpre, Yu Wang, and Christopher DuBois.\\n2020. How effective is task-agnostic data augmen-\\ntation for pretrained transformers?\\narXiv preprint\\narXiv:2010.01764.\\nSamuel Louvan and Bernardo Magnini. 2020. Simple\\nis better! lightweight data augmentation for low re-\\nsource slot ﬁlling and intent classiﬁcation. In Pro-\\nceedings of the 34th Paciﬁc Asia Conference on Lan-\\n\\nguage, Information and Computation, pages 167–\\n177, Hanoi, Vietnam. Association for Computational\\nLinguistics.\\nKaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Aman-\\ncharla, and Anupam Datta. 2020. Gender Bias in\\nNeural Natural Language Processing, pages 189–\\n202. Springer International Publishing, Cham.\\nGeorge A. Miller. 1995.\\nWordnet:\\na lexical\\ndatabase for english. Communications of the ACM,\\n38(11):39–41.\\nTomoya Mizumoto, Mamoru Komachi, Masaaki Na-\\ngata, and Yuji Matsumoto. 2011.\\nMining revi-\\nsion log of language learning SNS for automated\\nJapanese error correction of second language learn-\\ners. In Proceedings of 5th International Joint Con-\\nference on Natural Language Processing, pages\\n147–155, Chiang Mai, Thailand. Asian Federation\\nof Natural Language Processing.\\nSebastien Montella, Betty Fabre, Tanguy Urvoy, Jo-\\nhannes Heinecke, and Lina Rojas-Barahona. 2020.\\nDenoising pre-training and data augmentation strate-\\ngies for enhanced RDF verbalization with transform-\\ners. In Proceedings of the 3rd International Work-\\nshop on Natural Language Generation from the Se-\\nmantic Web (WebNLG+), pages 89–99, Dublin, Ire-\\nland (Virtual). Association for Computational Lin-\\nguistics.\\nNaﬁse Sadat Moosavi, Marcel de Boer, Prasetya Ajie\\nUtama, and Iryna Gurevych. 2020.\\nImproving\\nrobustness by augmenting training sentences with\\npredicate-argument structures. arXiv preprint.\\nXiangyang Mou, Brandyn Sigouin, Ian Steenstra, and\\nHui Su. 2020. Multimodal dialogue state tracking by\\nqa approach with data augmentation. AAAI DSTC8\\nWorkshop.\\nDiego\\nMoussallem,\\nMihael\\nArˇcan,\\nAxel-\\nCyrille\\nNgonga\\nNgomo,\\nand\\nPaul\\nBuitelaar.\\n2019. Augmenting neural machine translation with\\nknowledge graphs. arXiv preprint.\\nNathan Ng, Kyunghyun Cho, and Marzyeh Ghassemi.\\n2020. SSMBA: Self-supervised manifold based data\\naugmentation for improving out-of-domain robust-\\nness.\\nIn Proceedings of the 2020 Conference on\\nEmpirical Methods in Natural Language Process-\\ning (EMNLP), pages 1268–1283, Online. Associa-\\ntion for Computational Linguistics.\\nXuan-Phi Nguyen, Shaﬁq Joty, Kui Wu, and Ai Ti Aw.\\n2020.\\nData diversiﬁcation: A simple strategy for\\nneural machine translation. In Advances in Neural\\nInformation Processing Systems, volume 33, pages\\n10018–10029. Curran Associates, Inc.\\nYuyang Nie, Yuanhe Tian, Xiang Wan, Yan Song, and\\nBo Dai. 2020.\\nNamed entity recognition for so-\\ncial media texts with semantic augmentation.\\nIn\\nProceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP),\\npages 1383–1391, Online. Association for Computa-\\ntional Linguistics.\\nYuta Nishimura, Katsuhito Sudoh, Graham Neubig,\\nand Satoshi Nakamura. 2018. Multi-source neural\\nmachine translation with data augmentation. 15th\\nInternational Workshop on Spoken Language Trans-\\nlation 2018.\\nShantipriya Parida and Petr Motlicek. 2019. Abstract\\ntext summarization: A low resource challenge. In\\nProceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the\\n9th International Joint Conference on Natural Lan-\\nguage Processing (EMNLP-IJCNLP), pages 5994–\\n5998, Hong Kong, China. Association for Computa-\\ntional Linguistics.\\nMagdalini Paschali, Walter Simson, Abhijit Guha Roy,\\nMuhammad Ferjad Naeem, Rüdiger Göbl, Christian\\nWachinger, and Nassir Navab. 2019. Data augmen-\\ntation with manifold exploring geometric transfor-\\nmations for increased performance and robustness.\\narXiv preprint.\\nRamakanth Pasunuru, Asli Celikyilmaz, Michel Galley,\\nChenyan Xiong, Yizhe Zhang, Mohit Bansal, and\\nJianfeng Gao. 2021. Data augmentation for abstrac-\\ntive query-focused multi-document summarization.\\narXiv preprint.\\nEllie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch,\\nBenjamin Van Durme, and Chris Callison-Burch.\\n2015. PPDB 2.0: Better paraphrase ranking, ﬁne-\\ngrained entailment relations, word embeddings, and\\nstyle classiﬁcation. In Proceedings of the 53rd An-\\nnual Meeting of the Association for Computational\\nLinguistics and the 7th International Joint Confer-\\nence on Natural Language Processing (Volume 2:\\nShort Papers), pages 425–430, Beijing, China. As-\\nsociation for Computational Linguistics.\\nWei Peng, Chongxuan Huang, Tianhao Li, Yun Chen,\\nand Qun Liu. 2020. Dictionary-based data augmen-\\ntation for cross-domain neural machine translation.\\narXiv preprint.\\nLibo Qin, Minheng Ni, Yue Zhang, and Wanxiang\\nChe. 2020. Cosda-ml: Multi-lingual code-switching\\ndata augmentation for zero-shot cross-lingual nlp.\\nIn Proceedings of the Twenty-Ninth International\\nJoint Conference on Artiﬁcial Intelligence, IJCAI-\\n20, pages 3853–3860. International Joint Confer-\\nences on Artiﬁcial Intelligence Organization. Main\\ntrack.\\nJun Quan and Deyi Xiong. 2019. Effective data aug-\\nmentation approaches to end-to-end task-oriented di-\\nalogue. In 2019 International Conference on Asian\\nLanguage Processing (IALP), pages 47–52.\\nHusam Quteineh, Spyridon Samothrakis, and Richard\\nSutcliffe. 2020. Textual data augmentation for efﬁ-\\ncient active learning on tiny datasets. In Proceed-\\nings of the 2020 Conference on Empirical Methods\\n\\nin Natural Language Processing (EMNLP), pages\\n7400–7410, Online. Association for Computational\\nLinguistics.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Language\\nmodels are unsupervised multitask learners. OpenAI\\nBlog, 1(8):9.\\nShashank Rajput, Zhili Feng, Zachary Charles, Po-\\nLing Loh, and Dimitris Papailiopoulos. 2019. Does\\ndata augmentation lead to positive margin?\\nIn In-\\nternational Conference on Machine Learning, pages\\n5321–5330. PMLR.\\nAJ Ratner, HR Ehrenberg, Z Hussain, J Dunnmon, and\\nC Ré. 2017. Learning to Compose Domain-Speciﬁc\\nTransformations for Data Augmentation. Advances\\nin Neural Information Processing Systems, 30:3239–\\n3249.\\nAdithya Renduchintala, Shuoyang Ding, Matthew\\nWiesner, and Shinji Watanabe. 2018. Multi-Modal\\nData Augmentation for End-to-end ASR. Proc. In-\\nterspeech 2018, pages 2394–2398.\\nArij Riabi, Thomas Scialom, Rachel Keraron, Benoît\\nSagot, Djamé Seddah, and Jacopo Staiano. 2020.\\nSynthetic Data Augmentation for Zero-Shot Cross-\\nLingual Question Answering. arXiv preprint.\\nGözde Gül ¸Sahin and Mark Steedman. 2018. Data aug-\\nmentation via dependency tree morphing for low-\\nresource languages.\\nIn Proceedings of the 2018\\nConference on Empirical Methods in Natural Lan-\\nguage Processing, pages 5004–5009, Brussels, Bel-\\ngium. Association for Computational Linguistics.\\nEli Schwartz, Leonid Karlinsky, Joseph Shtok, Sivan\\nHarary, Mattias Marder, Abhishek Kumar, Rogerio\\nFeris, Raja Giryes, and Alex M Bronstein. 2018. δ-\\nencoder: an effective sample synthesis method for\\nfew-shot object recognition. In Proceedings of the\\n32nd International Conference on Neural Informa-\\ntion Processing Systems, pages 2850–2860.\\nRico Sennrich, Barry Haddow, and Alexandra Birch.\\n2016. Improving Neural Machine Translation Mod-\\nels with Monolingual Data. In Proceedings of the\\n54th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), pages\\n86–96, Berlin, Germany. Association for Computa-\\ntional Linguistics.\\nHaoyue Shi, Karen Livescu, and Kevin Gimpel. 2021.\\nSubstructure Substitution: Structured Data Augmen-\\ntation for NLP. arXiv preprint arXiv:2101.00411.\\nConnor Shorten and Taghi M Khoshgoftaar. 2019.\\nA survey on Image Data Augmentation for Deep\\nLearning. Journal of Big Data, 6(1):60.\\nJasdeep Singh, Bryan McCann, Nitish Shirish Keskar,\\nCaiming Xiong, and Richard Socher. 2019. Xlda:\\nCross-lingual data augmentation for natural lan-\\nguage inference and question answering.\\narXiv\\npreprint arXiv:1905.11471.\\nAbhishek Sinha, Kumar Ayush, Jiaming Song, Bu-\\nrak Uzkent, Hongxia Jin, and Stefano Ermon. 2021.\\nNegative data augmentation. In International Con-\\nference on Learning Representations.\\nXiaohui Song, Liangjun Zang, Yipeng Su, Xing Wu,\\nJizhong Han, and Songlin Hu. 2020. Data Augmen-\\ntation for Copy-Mechanism in Dialogue State Track-\\ning. arXiv preprint arXiv:2002.09634.\\nAmane Sugiyama and Naoki Yoshinaga. 2019. Data\\naugmentation using back-translation for context-\\naware neural machine translation. In Proceedings\\nof the Fourth Workshop on Discourse in Machine\\nTranslation (DiscoMT 2019), pages 35–44, Hong\\nKong, China. Association for Computational Lin-\\nguistics.\\nShubhangi Tandon, TS Sharath, Shereen Oraby, Lena\\nReed, Stephanie Lukin, and Marilyn Walker. 2018.\\nTNT-NLG, System 2: Data repetition and meaning\\nrepresentation manipulation to improve neural gen-\\neration. E2E NLG Challenge Sfystem Descriptions.\\nRuixue Tang, Chao Ma, Wei Emma Zhang, Qi Wu, and\\nXiaokang Yang. 2020. Semantic equivalent adver-\\nsarial data augmentation for visual question answer-\\ning. In European Conference on Computer Vision,\\npages 437–453. Springer.\\nNandan Thakur, Nils Reimers, Johannes Daxenberger,\\nand Iryna Gurevych. 2021. Augmented sbert: Data\\naugmentation method for improving bi-encoders for\\npairwise sentence scoring tasks.\\nProceedings of\\nNAACL.\\nVaibhav Vaibhav, Sumeet Singh, Craig Stewart, and\\nGraham Neubig. 2019.\\nImproving Robustness of\\nMachine Translation with Synthetic Noise. In Pro-\\nceedings of the 2019 Conference of the North Amer-\\nican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Vol-\\nume 1 (Long and Short Papers), pages 1916–1920,\\nMinneapolis, Minnesota. Association for Computa-\\ntional Linguistics.\\nClara Vania, Yova Kementchedjhieva, Anders Søgaard,\\nand Adam Lopez. 2019. A systematic comparison\\nof methods for low-resource dependency parsing on\\ngenuinely low-resource languages. In Proceedings\\nof the 2019 Conference on Empirical Methods in\\nNatural Language Processing and the 9th Interna-\\ntional Joint Conference on Natural Language Pro-\\ncessing (EMNLP-IJCNLP), pages 1105–1116, Hong\\nKong, China. Association for Computational Lin-\\nguistics.\\nVikas Verma, Alex Lamb, Christopher Beckham, Amir\\nNajaﬁ, Ioannis Mitliagkas, David Lopez-Paz, and\\nYoshua Bengio. 2019. Manifold mixup: Better rep-\\nresentations by interpolating hidden states.\\nIn In-\\nternational Conference on Machine Learning, pages\\n6438–6447. PMLR.\\n\\nZhaohong Wan, Xiaojun Wan, and Wenguang Wang.\\n2020.\\nImproving Grammatical Error Correction\\nwith Data Augmentation by Editing Latent Rep-\\nresentation.\\nIn Proceedings of the 28th Interna-\\ntional Conference on Computational Linguistics,\\npages 2202–2212, Barcelona, Spain (Online). Inter-\\nnational Committee on Computational Linguistics.\\nChencheng Wang, Liner Yang, Yun Chen, Yongping\\nDu, and Erhong Yang. 2019a.\\nControllable Data\\nSynthesis Method for Grammatical Error Correction.\\narXiv preprint arXiv:1909.13302.\\nLongshaokan Wang, Maryam Fazel-Zarandi, Aditya Ti-\\nwari, Spyros Matsoukas, and Lazaros Polymenakos.\\n2020. Data augmentation for training dialog models\\nrobust to speech recognition errors. arXiv preprint\\narXiv:2006.05635.\\nXiang Wang, Kai Wang, and Shiguo Lian. 2019b. A\\nsurvey on face data augmentation. arXiv preprint\\narXiv:1904.11685.\\nXinyi Wang, Hieu Pham, Zihang Dai, and Graham Neu-\\nbig. 2018a. SwitchOut: an Efﬁcient Data Augmen-\\ntation Algorithm for Neural Machine Translation.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n856–861, Brussels, Belgium. Association for Com-\\nputational Linguistics.\\nXinyi Wang, Hieu Pham, Zihang Dai, and Graham Neu-\\nbig. 2018b. SwitchOut: an Efﬁcient Data Augmen-\\ntation Algorithm for Neural Machine Translation.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n856–861, Brussels, Belgium. Association for Com-\\nputational Linguistics.\\nJason Wei,\\nChengyu Huang,\\nSoroush Vosoughi,\\nYu Cheng, and Shiqi Xu. 2021a. Few-shot text clas-\\nsiﬁcation with triplet networks, data augmentation,\\nand curriculum learning. Proceedings of NAACL.\\nJason Wei, Chengyu Huang, Shiqi Xu, and Soroush\\nVosoughi. 2021b. Text augmentation in a multi-task\\nview. In Proceedings of the 16th Conference of the\\nEuropean Chapter of the Association for Computa-\\ntional Linguistics: Main Volume, pages 2888–2894,\\nOnline. Association for Computational Linguistics.\\nJason Wei and Kai Zou. 2019. EDA: Easy data aug-\\nmentation techniques for boosting performance on\\ntext classiﬁcation tasks.\\nIn Proceedings of the\\n2019 Conference on Empirical Methods in Natu-\\nral Language Processing and the 9th International\\nJoint Conference on Natural Language Processing\\n(EMNLP-IJCNLP), pages 6382–6388, Hong Kong,\\nChina. Association for Computational Linguistics.\\nMax White and Alla Rozovskaya. 2020. A Compar-\\native Study of Synthetic Data Generation Methods\\nfor Grammatical Error Correction. In Proceedings\\nof the Fifteenth Workshop on Innovative Use of NLP\\nfor Building Educational Applications, pages 198–\\n208, Seattle, WA, USA â†’ Online. Association for\\nComputational Linguistics.\\nMatthew Wiesner,\\nAdithya Renduchintala,\\nShinji\\nWatanabe, Chunxi Liu, Najim Dehak, and Sanjeev\\nKhudanpur. 2018. Low resource multi-modal data\\naugmentation for end-to-end ASR. CoRR.\\nJohn Wieting and Kevin Gimpel. 2017. Revisiting Re-\\ncurrent Networks for Paraphrastic Sentence Embed-\\ndings. In Proceedings of the 55th Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers), pages 2078–2088, Vancouver,\\nCanada. Association for Computational Linguistics.\\nSam Wiseman, Stuart M Shieber, and Alexander M\\nRush. 2017. Challenges in Data-to-Document Gen-\\neration. In Proceedings of the 2017 Conference on\\nEmpirical Methods in Natural Language Processing,\\npages 2253–2263.\\nMengzhou Xia, Xiang Kong, Antonios Anastasopou-\\nlos, and Graham Neubig. 2019. Generalized Data\\nAugmentation for Low-Resource Translation.\\nIn\\nProceedings of the 57th Annual Meeting of the\\nAssociation for Computational Linguistics, pages\\n5786–5796, Florence, Italy. Association for Compu-\\ntational Linguistics.\\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong,\\nand Quoc Le. 2020. Unsupervised data augmenta-\\ntion for consistency training. Advances in Neural\\nInformation Processing Systems, 33.\\nZiang Xie, Guillaume Genthial, Stanley Xie, Andrew\\nNg, and Dan Jurafsky. 2018. Noising and Denois-\\ning Natural Language: Diverse Backtranslation for\\nGrammar Correction. In Proceedings of the 2018\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long Papers),\\npages 619–628, New Orleans, Louisiana. Associa-\\ntion for Computational Linguistics.\\nN. Xu, W. Mao, P. Wei, and D. Zeng. 2020. MDA: Mul-\\ntimodal Data Augmentation Framework for Boost-\\ning Performance on Image-Text Sentiment/Emotion\\nClassiﬁcation Tasks.\\nIEEE Intelligent Systems,\\npages 1–1.\\nShuyao Xu, Jiehao Zhang, Jin Chen, and Long Qin.\\n2019. Erroneous data generation for Grammatical\\nError Correction. In Proceedings of the Fourteenth\\nWorkshop on Innovative Use of NLP for Building\\nEducational Applications, pages 149–158, Florence,\\nItaly. Association for Computational Linguistics.\\nWei Yang, Yuqing Xie, Luchen Tan, Kun Xiong, Ming\\nLi, and Jimmy Lin. 2019. Data Augmentation for\\nBERT Fine-Tuning in Open-Domain Question An-\\nswering. arXiv preprint arXiv:1904.06652.\\nYiben Yang, Chaitanya Malaviya, Jared Fernandez,\\nSwabha Swayamdipta, Ronan Le Bras, Ji-Ping\\nWang, Chandra Bhagavatula, Yejin Choi, and Doug\\n\\nDowney. 2020. G-daug: Generative data augmenta-\\ntion for commonsense reasoning. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Nat-\\nural Language Processing: Findings, pages 1008–\\n1025.\\nYichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, and\\nQun Liu. 2020.\\nDialog State Tracking with Re-\\ninforced Data Augmentation.\\nIn Proceedings of\\nthe AAAI Conference on Artiﬁcial Intelligence, vol-\\nume 34, pages 9474–9481.\\nMasashi Yokota and Hideki Nakayama. 2018.\\nAug-\\nmenting Image Question Answering Dataset by Ex-\\nploiting Image Captions.\\nIn Proceedings of the\\nEleventh International Conference on Language Re-\\nsources and Evaluation (LREC-2018), Miyazaki,\\nJapan. European Languages Resources Association\\n(ELRA).\\nKang Min Yoo, Youhyun Shin, and Sang-goo Lee.\\n2019. Data Augmentation for Spoken Language Un-\\nderstanding via Joint Variational Generation. In Pro-\\nceedings of the AAAI conference on artiﬁcial intelli-\\ngence, volume 33, pages 7402–7409.\\nS. Young, M. Gaši´c, B. Thomson, and J. D. Williams.\\n2013.\\nPOMDP-Based Statistical Spoken Dialog\\nSystems:\\nA Review.\\nProceedings of the IEEE,\\n101(5):1160–1179.\\nAdams Wei Yu, David Dohan, Quoc Le, Thang Luong,\\nRui Zhao, and Kai Chen. 2018. Fast and accurate\\nreading comprehension by combining self-attention\\nand convolution.\\nIn International Conference on\\nLearning Representations.\\nTao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin\\nWang, Yi Chern Tan, Xinyi Yang, Dragomir Radev,\\nRichard Socher, and Caiming Xiong. 2020. GraPPa:\\nGrammar-Augmented Pre-Training for Table Se-\\nmantic Parsing. arXiv preprint arXiv:2009.13845.\\nSangdoo Yun,\\nDongyoon Han,\\nSeong Joon Oh,\\nSanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.\\n2019.\\nCutmix:\\nRegularization strategy to train\\nstrong classiﬁers with localizable features. In Pro-\\nceedings of the IEEE/CVF International Conference\\non Computer Vision, pages 6023–6032.\\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin,\\nand David Lopez-Paz. 2017. mixup: Beyond em-\\npirical risk minimization. Proceedings of ICLR.\\nRongzhi Zhang, Yue Yu, and Chao Zhang. 2020. Se-\\nqMix: Augmenting Active Sequence Labeling via\\nSequence Mixup. In Proceedings of the 2020 Con-\\nference on Empirical Methods in Natural Language\\nProcessing (EMNLP), pages 8566–8579, Online. As-\\nsociation for Computational Linguistics.\\nRui Zhang, Tao Yu, Heyang Er, Sungrok Shim,\\nEric Xue, Xi Victoria Lin, Tianze Shi, Caiming\\nXiong, Richard Socher, and Dragomir Radev. 2019a.\\nEditing-based SQL Query Generation for Cross-\\nDomain Context-Dependent Questions. In Proceed-\\nings of the 2019 Conference on Empirical Methods\\nin Natural Language Processing and the 9th Inter-\\nnational Joint Conference on Natural Language Pro-\\ncessing (EMNLP-IJCNLP), pages 5338–5349, Hong\\nKong, China. Association for Computational Lin-\\nguistics.\\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\\nCharacter-Level Convolutional Networks for Text\\nClassiﬁcation. In Proceedings of the 28th Interna-\\ntional Conference on Neural Information Processing\\nSystems - Volume 1, NIPS’15, page 649–657, Cam-\\nbridge, MA, USA. MIT Press.\\nYi Zhang, Tao Ge, Furu Wei, Ming Zhou, and Xu Sun.\\n2019b.\\nSequence-to-sequence Pre-training with\\nData Augmentation for Sentence Rewriting. arXiv\\npreprint arXiv:1909.06002.\\nYichi Zhang, Zhijian Ou, and Zhou Yu. 2019c. Task-\\noriented dialog systems that consider multiple appro-\\npriate responses under the same context.\\nYuan Zhang, Jason Baldridge, and Luheng He. 2019d.\\nPAWS: Paraphrase adversaries from word scram-\\nbling.\\nIn Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers), pages\\n1298–1308, Minneapolis, Minnesota. Association\\nfor Computational Linguistics.\\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\\ndonez, and Kai-Wei Chang. 2018. Gender bias in\\ncoreference resolution:\\nEvaluation and debiasing\\nmethods.\\nIn Proceedings of the 2018 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, Volume 2 (Short Papers), pages 15–20,\\nNew Orleans, Louisiana. Association for Computa-\\ntional Linguistics.\\nZijian Zhao, Su Zhu, and Kai Yu. 2019. Data augmen-\\ntation with atomic templates for spoken language un-\\nderstanding. arXiv preprint arXiv:1908.10770.\\nVictor Zhong, Mike Lewis, Sida I. Wang, and Luke\\nZettlemoyer. 2020. Grounded adaptation for zero-\\nshot executable semantic parsing. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Nat-\\nural Language Processing (EMNLP), pages 6869–\\n6882, Online. Association for Computational Lin-\\nguistics.\\nHaichao Zhu, Li Dong, Furu Wei, Bing Qin, and\\nTing Liu. 2019.\\nTransforming wikipedia into\\naugmented data for query-focused summarization.\\narXiv preprint arXiv:1911.03324.\\nRan Zmigrod, Sebastian J. Mielke, Hanna Wallach, and\\nRyan Cotterell. 2019.\\nCounterfactual Data Aug-\\nmentation for Mitigating Gender Stereotypes in Lan-\\nguages with Rich Morphology. In Proceedings of\\n\\nthe 57th Annual Meeting of the Association for Com-\\nputational Linguistics, pages 1651–1661, Florence,\\nItaly. Association for Computational Linguistics.\\nAppendices\\nA\\nUseful Blog Posts and Code\\nRepositories\\nThe following blog posts and code repositories\\ncould be helpful in addition to the information pre-\\nsented and papers/works mentioned in the body:\\n• Introduction to popular text augmentation\\ntechniques:\\nhttps://towardsdatascience.com/\\ndata-augmentation-in-nlp-2801a34dfc28\\n• Detailed\\nblog\\npost\\non\\nvarious\\ntext\\nDA\\ntechniques:\\nhttps://amitness.com/2020/05/\\ndata-augmentation-for-nlp/\\n• Lightweight library for DA on text and audio:\\nhttps://github.com/makcedward/nlpaug\\n• python framework for adversarial examples:\\nhttps://github.com/QData/TextAttack\\nB\\nDA Methods Table - Description of\\nColumns and Attributes\\nTable 1 in the main body compares a non-\\nexhaustive selection of DA methods along various\\naspects relating to their applicability, dependencies,\\nand requirements. Below, we provide a more ex-\\ntensive description of each of this table’s columns\\nand their attributes.\\n1. Ext.Know: Short for external knowledge, this\\ncolumn is \\x13 when the data augmentation pro-\\ncess requires knowledge resources which go\\nbeyond the immediate input examples and\\nthe task deﬁnition, such as WordNet (Miller,\\n1995) or PPDB (Pavlick et al., 2015). Note\\nthat we exclude the case where these resources\\nare pretrained models under a separate point\\n(next) for clarity, since these are widespread\\nenough to merit a separate category.\\n2. Pretrained: Denotes that the data augmenta-\\ntion process requires a pretrained model, such\\nas BERT (Devlin et al., 2019) or GPT-2 (Rad-\\nford et al., 2019).\\n3. Preprocess: Denotes the preprocessing steps,\\ne.g. tokenization (tok), dependency parsing\\n(dep), etc. required for the DA process. A\\nhyphen (-) means either no preprocessing is\\nrequired or that it was not explicitly stated.\\n4. Level: Denotes the depth and extent to which\\nelements of the instance/data are modiﬁed by\\nthe DA. Some primitives modify just the IN-\\nPUT (e.g. word swapping), some modify both\\nINPUT and LABEL (e.g. negation), while oth-\\ners make changes in the embedding or hidden\\nspace (EMBED/HIDDEN) or higher represen-\\ntation layers enroute to the task model.\\n5. Task-Agnostic: This is an approximate, par-\\ntially subjective column denoting the extent\\nto which a DA method can be applied to dif-\\nferent tasks. When we say \\x13 here, we don’t\\ndenote a very rigid sense of the term task-\\nagnostic, but mean that it would possibly eas-\\nily extend to most NLP tasks as understood\\nby the authors. Similarly, an × denotes being\\nrestricted to a speciﬁc task (or small group of\\nrelated tasks) only. There can be other labels,\\ndenoting applicability to broad task families.\\nFor example, SUBSTRUCTURAL denotes the\\nfamily of tasks where sub-parts of the input\\nare also valid input examples in their own\\nright, e.g. constituency parsing. SENTENCE\\nPAIRS denotes tasks which involve pairwise\\nsentence scoring such as paraphrase identiﬁ-\\ncation, duplicate question detection, and se-\\nmantic textual similarity.\\nC\\nAdditional DA Works by Task\\nSee Table 2 for additional DA works for GEC, Ta-\\nble 3 for additional DA works for neural machine\\ntranslation, Table 4 for additional DA works for\\ndialogue, and Table 5 for additional DA works for\\nmultimodal tasks. Each work is described brieﬂy.\\nD\\nAdditional Figure\\nFigure 4: Pedro Domingos’ quip about ofﬂine data aug-\\nmentation.\\n\\nPaper/Work\\nBrief Description\\nLichtarge et al. (2019)\\nGenerate synthetic noised examples of Wikipedia sentences using backtranslation through\\nvarious languages.\\nWhite and Rozovskaya (2020) Detailed comparative study of the DA for GEC systems UEdin-MS (Grundkiewicz et al., 2019)\\nand Kakao&Brain (Choe et al., 2019).\\nFoster and Andersen (2009)\\nIntroduces error generation tool called GenERRate which learns to generate ungrammatical\\ntext with various errors by using an error analysis ﬁle.\\nKimn (2020)\\nUse a set of syntactic rules for common Japanese grammatical errors to generate augmented\\nerror-correct sentence pairs for Japanese GEC.\\nFelice (2016)\\nThesis that surveys previous work on error generation and investigates some new approaches\\nusing random and probabilistic methods.\\nXu et al. (2019)\\nNoises using ﬁve error types: concatenation, misspelling, substitution, deletion, and transposi-\\ntion. Decent performance on the BEA 2019 Shared Task.\\nZhang et al. (2019b)\\nExplore backtranslation and feature discrimination for DA.\\nMizumoto et al. (2011)\\nDA by extracting Japanese GEC training data from the revision log of a language learning SNS.\\nTable 2: Additional DA works for grammatical error correction (GEC), along with a brief description of each.\\nPaper/Work\\nBrief Description\\nVaibhav et al. (2019)\\nPresent a synthetic noise induction model which heuristically adds social media noise to text,\\nand labeled backtranslation.\\nHassan et al. (2017)\\nPresent a DA method to project words from closely-related high-resource languages to low-\\nresource languages using word embedding representations.\\nCheng et al. (2020)\\nPropose AdvAug, an adversarial augmentation method for NMT, by sampling adversarial\\nexamples from a new vicinity distribution and using their embeddings to augment training.\\nGraça et al. (2019)\\nInvestigate improvements to sampling-based approaches and the synthetic data generated by\\nbacktranslation.\\nBulte and Tezcan (2019)\\nPropose DA approaches for NMT that leverage information retrieved from a Translation\\nMemory (TM) and using fuzzy TM matches.\\nMoussallem et al. (2019)\\nPropose an NMT model KG-NMT which is augmented by knowledge graphs to enhance\\nsemantic feature extraction and hence the translation of entities and terminological expressions.\\nPeng et al. (2020)\\nPropose dictionary-based DA (DDA) for cross-domain NMT by synthesizing a domain-speciﬁc\\ndictionary and automatically generating a pseudo in-domain parallel corpus.\\nLi et al. (2020a)\\nPresent a DA method using sentence boundary segmentation to improve the robustness of NMT\\non ASR transcripts.\\nNishimura et al. (2018)\\nIntroduce DA methods for multi-source NMT that ﬁlls in incomplete portions of multi-source\\ntraining data.\\nSugiyama and Yoshinaga (2019) Investigate effectiveness of DA by backtranslation for context-aware NMT.\\nLi and Specia (2019)\\nPresent DA methods to improve NMT robustness to noise while keeping models small, and\\nexplore the use of noise from external data (speech transcripts).\\nChinea-Ríos et al. (2017)\\nPropose DA method to create synthetic data by leveraging the embedding representation of\\nsentences.\\nAlves et al. (2020)\\nPropose two methods for pipeline-based speech translation through the introduction of errors\\nthrough 1. utilizing a speech processing workﬂow and 2. a rule-based method.\\nKang (2019)\\nInvestigate extremely low-resource settings for NMT and a DA approach using a noisy dictio-\\nnary and language models.\\nChen et al. (2020a)\\nInvestigate a DA method for lexically constraint-aware NMT to construct constraint-aware\\nsynthetic training data.\\nLi et al. (2020b)\\nPropose a diversity DA method for low-resource NMT by generating diverse synthetic parallel\\ndata on both source and target sides using a restricted sampling strategy during decoding.\\nDuan et al. (2020)\\nPropose syntax-aware DA methods with sentence-speciﬁc word selection probabilities using\\ndependency parsing.\\nTable 3: Additional DA works for neural machine translation (NMT), along with a brief description of each.\\n\\nPaper/Work\\nBrief Description\\nGao et al. (2020)\\nPropose a paraphrase augmented response generation (PARG) framework to improve dialogue generation by\\nautomatically constructing augmented paraphrased training examples based on dialogue state and act labels.\\nGritta et al. (2021) Introduce a graph-based representation of dialogues called Conversation Graph (ConvGraph) that can be\\nused for DA by creating new dialogue paths.\\nYin et al. (2020)\\nPropose an RL-based DA approach for dialogue state tracking (DST).\\nSong et al. (2020) Propose a simple DA algorithm to improve the training of copy-mechanism models for dialogue state\\ntracking (DST).\\nTable 4: Additional DA works for dialogue, along with a brief description of each.\\nPaper/Work\\nBrief Description\\nHuang et al. (2018) Propose a DA method for emotion recognition from a combination of audio, visual, and textual modalities.\\nMou et al. (2020)\\nIntroduce a DA method for Audio-Video Scene-Aware Dialogue, which involves dialogue containing a\\nsequence of QA pairs about a video.\\nFalcon et al. (2020) Investigate DA techniques for video QA including mirroring and horizontal ﬂipping.\\nTable 5: Additional DA works for multimodal tasks, along with a brief description of each.\\n'}, {'source': '/Users/sir/Downloads/Data/PDF/Heart Transplant Outcome Prediction using UNOS Data.pdf', 'text': 'Heart Transplant Outcome Prediction using UNOS Data\\nAnkit Agrawal\\nDept. of Electrical Engineering and Computer\\nScience\\nNorthwestern University\\n2145 Sheridan Rd\\nEvanston, IL 60201\\nUSA\\nankitag@eecs.northwestern.edu\\nMark J. Russo\\nDepartment of Cardiothoracic Surgery\\nCenter for Aortic Diseases\\nBarnabas Health Heart Centers\\nLivingston, NJ 07039\\nUSA\\nmrusso@barnabashealth.org\\nJaishankar Raman\\nSection of Cardiothoracic & Vascular Surgery\\nDepartment of Surgery\\nRush University Medical Center\\nChicago, IL\\nUSA\\njai_raman@rush.edu\\nAlok Choudhary\\nDept. of Electrical Engineering and Computer\\nScience\\nNorthwestern University\\n2145 Sheridan Rd\\nEvanston, IL 60201\\nUSA\\nchoudhar@eecs.northwestern.edu\\nABSTRACT\\nWe analyze heart transplant data from the United Network\\nfor Organ Sharing (UNOS) program with the aim of devel-\\noping accurate risk prediction models for mortality within 1\\nyear of heart transplant using data mining techniques. The\\ndata used in this study is de-identiﬁed and consists of 50\\npredictor attributes, and 1-year posttranplant survial out-\\ncome for patients who underwent heart transplant between\\nthe years 2000 and 2009. Our dataset had 19,429 such pa-\\ntient instances. Several data mining classiﬁcation techniques\\nwere used on this data along with various data mining op-\\ntimizations and validations to build predictive models for\\nthe abovementioned outcome. Prediction results were eval-\\nuated using c-statistic metric, and the highest c-statistic ob-\\ntained was 0.656. Further, we also applied feature selection\\ntechniques to reduce the number of attributes in the model\\nfrom 50 to 8, while trying to have minimal degradation in\\nc-statistic (0.645). We believe the resulting predictive model\\non the reduced dataset can be quite useful to integrate in\\na risk calculator to aid both physicians and patients in risk\\nassessment.\\nCategories and Subject Descriptors\\nH.2.8 [Database Applications]: Data mining; J.3 [Life\\nand Medical Sciences]: Medical information systems\\nPermission to make digital or hard copies of all or part of this work for\\npersonal or classroom use is granted without fee provided that copies are not\\nmade or distributed for proﬁt or commercial advantage and that copies bear\\nthis notice and the full citation on the ﬁrst page. Copyrights for components\\nof this work owned by others than ACM must be honored. Abstracting with\\ncredit is permitted. To copy otherwise, or republish, to post on servers or to\\nredistribute to lists, requires prior speciﬁc permission and/or a fee. Request\\npermissions from permissions@acm.org..\\nKDD-DMH’13, August 11, 2013, Chicago, Illinois, USA.\\nCopyright c⃝2013 ACM 978-1-4503-2174-7/13/08 ...$15.00.\\nKeywords\\nBiomedical informatics, Heart transplant, Decision making,\\nPredictive modeling\\n1.\\nINTRODUCTION\\nA heart transplant, or a cardiac transplant, is a surgical\\ntransplant procedure performed on patients with end-stage\\nheart failure or severe coronary artery disease [7].\\nAs of\\n2009, the survival rates for heart transplant after 1 year were\\n88.0% in males and 86.2% in females [30]. Typical expenses\\nduring the ﬁrst year (everything including surgery, hospital-\\nization, lab testing, medications) average around $800,000\\n[2, 5].\\nGiven the critical scarcity of organs available for trans-\\nplant (about 2,500 available every year [1] compared to 60,000\\npotential recepients [35]), achieving maximal beneﬁt from\\nheart transplantation depends upon improved recipient and\\ndonor selection [31]. Thus accurate estimation of heart trans-\\nplant outcomes can improve both informed patient consent\\nby helping patients better understand its risks and beneﬁts,\\nand also aid the physicians in decision making by assessing\\nthe true patient-speciﬁc risks of the operation, rather than\\nrelying on population-wide risk assessments. To this end,\\naccuarate outcome prediction of performing transplantation\\nis extremely important.\\nThe United Network for Organ Sharing (UNOS) is a pri-\\nvate, non-proﬁt organization that manages the nation’s or-\\ngan transplant system under contract with the federal gov-\\nernment [12]. UNOS is involved in many aspects of the organ\\ntransplant and donation process, including maintaining the\\ndatabase that contains all organ transplant data for every\\ntransplant event that occurs in the US.\\nApplying data mining techniques to heart transplantation\\ndata can be useful to rank and link pretransplantation at-\\ntributes to the outcome. Here we use data mining techniques\\non UNOS heart transplantation data to estimate 1-year sur-\\nvival of heart transplant patients, based on pretransplant\\n\\ncharacteristics. Experiments with nearly 50 modeling tech-\\nniques were conducted and the results compared to ﬁnd the\\nbest model for the data used in this study. It was found\\nthat rotation forest ensembles of alternation decision trees\\nresulted in the best discrimination (c-statistic) between sur-\\nvived and non-survived heart recepients. Further, feature\\nselection was used to ﬁnd a smaller subset of attributes that\\ncan potentially achieve similar prediction performance, but\\ncan result in a simpler model.\\nThe rest of the paper is organized as follows: Section 2\\ndescribes the data mining techniques used in this study fol-\\nlowed by a brief description of the UNOS data used in this\\nstudy in Section 3. Experiments and results are presented in\\nSection 4, and the conclusion and future work is presented\\nin Section 5.\\n2.\\nDATA MINING TECHNIQUES\\n2.1\\nModeling\\nWe used 47 classiﬁcation schemes in this study, includ-\\ning both direct application of classiﬁcation techniques and\\nalso constructing their ensembles using various ensembling\\ntechniques.\\nDue to space limitations, here we brieﬂy de-\\nscribe only those classiﬁcation/ensembling techniques whose\\nresults we present in the next section.\\n1. Support vector machines: SVMs are based on the\\nStructural Risk Minimization(SRM) principle from sta-\\ntistical learning theory. A detailed description of SVMs\\nand SRM is available in [33]. In their basic form, SVMs\\nattempt to perform classiﬁcation by constructing hy-\\nperplanes in a multidimensional space that separates\\nthe cases of diﬀerent class labels. It supports both clas-\\nsiﬁcation and regression tasks and can handle multiple\\ncontinuous and nominal variables.\\n2. Artiﬁcial neural networks: ANNs are networks of\\ninterconnected artiﬁcial neurons, and are commonly\\nused for non-linear statistical data modeling to model\\ncomplex relationships between inputs and outputs. The\\nnetwork includes a hidden layer of multiple artiﬁcial\\nneurons connected to the inputs and outputs with dif-\\nferent edge weights.\\nThe internal edge weights are\\n’learnt’ during the training process using techniques\\nlike back propagation.\\nSeveral good descriptions of\\nneural networks are available [14, 18].\\n3. Decision Table: Decision table typically constructs\\nrules involving diﬀerent combinations of attributes, which\\nare selected using an attribute selection search method.\\nSimple decision table majority classiﬁer [28] has been\\nshown to sometimes outperform state-of-the-art clas-\\nsiﬁers.\\n4. KStar: KStar [17] is a lazy instance-based classiﬁer,\\ni.e., the class of a test instance is based upon the\\nclass of those training instances similar to it, as de-\\ntermined by some similarity function. It diﬀers from\\nother instance-based learners in that it uses an entropy-\\nbased distance function.\\n5. Reduced error pruning tree: Commonly known as\\nREPTree [34], it is a implementation of a fast decision\\ntree learner, which builds a decision/regression tree\\nusing information gain/variance and prunes it using\\nreduced-error pruning.\\n6. Random forest: The Random Forest [16] classiﬁer\\nconsists of multiple decision trees. The ﬁnal class of an\\ninstance in a Random Forest is assigned by outputting\\nthe class that is the mode of the outputs of individual\\ntrees, which can produce robust and accurate classiﬁ-\\ncation, and ability to handle a very large number of\\ninput variables.\\nIt is relatively robust to overﬁtting\\nand can handle datasets with highly imbalanced class\\ndistributions.\\n7. Alternating decision tree: ADTree [19] is decision\\ntree classiﬁer which supports only binary classiﬁcation.\\nIt consists of two types of nodes: decision nodes (spec-\\nifying a predicate condition, like ’age’ > 45) and pre-\\ndiction nodes (containing a single real-value number).\\nAn instance is classiﬁed by following all paths for which\\nall decision nodes are true and summing the values of\\nany prediction nodes that are traversed. This is diﬀer-\\nent from the J48 decision tree algorithm in which an\\ninstance follows only one path through the tree.\\n8. Decision stump: A decision stump [34] is a weak\\ntree-based machine learning model consisting of a single-\\nlevel decision tree with a categorical or numeric class\\nlabel. Decision stumps are usually used in ensemble\\nmachine learning techniques.\\n9. Naive Bayes:\\nThe naive bayes classiﬁer [22] is a\\nsimple probabilistic classiﬁer that is based upon the\\nBayes theorem. This classiﬁer makes strong assump-\\ntions about the independence of the input features,\\nwhich may not always be true. It makes use of the\\nvariables contained in the data sample, by observing\\nand relating them individually to the target class, in-\\ndependent of each other. Despite this assumption, the\\nnaive bayes classiﬁer works well in practice for a wide\\nvariety of datasets and often outperforms other com-\\nplex classiﬁers.\\n10. Bayesian Network: A Bayesian network is a graphi-\\ncal model that encodes probabilistic relationships among\\na set of variables, representing a set of random vari-\\nables and their conditional dependencies via a directed\\nacyclic graph (DAG). Bayesian network learning can\\nbe used with various search algorithms for searching\\nthe network structures, and estimator algorithms for\\nﬁnding the conditional probability tables of the net-\\nwork.\\n11. Logistic Regression: Logistic Regression [26] is used\\nfor prediction of the probability of occurrence of an\\nevent by ﬁtting data to a sigmoidal S-shaped logistic\\ncurve. Logistic regression is often used with ridge es-\\ntimators [29] to improve the parameter estimates and\\nto reduce the error made by further predictions.\\n12. AdaBoost: AdaBoost [20] is a commonly used en-\\nsembling technique for boosting a nominal class clas-\\nsiﬁer. In general, boosting can be used to signiﬁcantly\\nreduce the error of any weak learning algorithm that\\nconsistently generates classiﬁers which need only be a\\n\\nlittle bit better than random guessing. It usually dra-\\nmatically improves performance, but is also prone to\\noverﬁtting.\\n13. LogitBoost: The LogitBoost algorithm is an ensem-\\nbling technique implementation of additive logistic re-\\ngression which performs classiﬁcation using a regres-\\nsion scheme as the base learner, and can handle multi-\\nclass problems. In [21], the authors explain the theo-\\nretical connection between Boosting and additive mod-\\nels.\\n14. Bagging: Bagging [15] is a meta-algorithm to improve\\nthe stability of classiﬁcation and regression algorithms\\nby reducing variance.\\nBagging is usually applied to\\ndecision tree models to boost their performance. It in-\\nvolves generating a number of new training sets (called\\nbootstrap modules) from the original set by sampling\\nuniformly with replacement. The bootstrap modules\\nare then used to generate models whose predictions are\\naveraged to generate the ﬁnal prediction. Bagging has\\nbeen shown work better with decision trees than with\\nlinear models.\\n15. Random subspace: The Random Subspace classi-\\nﬁer [25] constructs a decision tree based classiﬁer con-\\nsisting of multiple trees, which are constructed system-\\natically by pseudo-randomly selecting subsets of fea-\\ntures, trying to achieve a balance between overﬁtting\\nand achieving maximum accuracy. It maintains high-\\nest accuracy on training data and improves on gener-\\nalization accuracy as it grows in complexity.\\n16. Rotation Forest: Rotation forest [32] is a method\\nfor generating classiﬁer ensembles based on feature ex-\\ntraction, which can work both with classiﬁcation and\\nregression base learners. The training data for a the\\nbase classiﬁer is created by applying Principal Compo-\\nnent Analysis (PCA) [27] to K subsets of the feature\\nset, followed by K axis rotations to form the new fea-\\ntures for the base learner, to encourage simultaneously\\nindividual accuracy and diversity within the ensemble.\\n2.2\\nFeature Selection\\nFeature selection techniques are typically used to select a\\nsubset of relevant features for use in a model. It is based on\\nthe assumption that data contains many redundant or irrel-\\nevant attributes that do not add much to the information\\nprovided by other existing attributes.\\nWe used 2 feature\\nselection techniques in this study:\\n1. Correlation Feature Selection (CFS): CFS is used\\nto identify a subset of features highly correlated with\\nthe class variable and weakly correlated amongst them\\n[23]. CFS was used in conjunction with a greedy step-\\nwise search to ﬁnd a subset S with best average merit,\\nwhich is given by:\\nMeritS =\\nn.rfo\\np\\nn + n(n −1).rff\\nwhere n is the number of features in S, rfo is the av-\\nerage value of feature-outcome correlations, and rff is\\nthe average value of all feature-feature correlations.\\n2. Information Gain: This is used to assess the rela-\\ntive predictive power of the predictor attributes, which\\nevaluates the worth of an attribute by measuring the\\ninformation gain with respect to the outcome status:\\nIG(Class, Attrib) = H(Class) −H(Class|Attrib)\\nwhere H(.) denotes the information entropy.\\nThe CFS technique evaluates subsets rather than individ-\\nual attributes, so it was ﬁrst used to ﬁnd a smaller subset of\\nattributes. Subsequently, information gain was used on the\\nreduced set of attributes to get a ranking of the attributes in\\nthe order of their predictive potential, as information gain\\nevaluates each attribute independently.\\n3.\\nUNOS DATA\\nAll individuals aged 18 years and older undergoing heart\\ntransplantation between 2000 and 2009 in the United States\\nwere part of the study population, with a total of 19,429 pa-\\ntients, and 50 attributes were assessed. The outcome vari-\\nable was 1-year survival.\\nWe omit the details of all the\\ninput 50 attributes here due to space constraints. A brief\\ndescription of the selected subset 8 attributes used in the\\nﬁnal model is presented later. Out of 19,429 patients, 2,615\\npatients (13.46%) did not survive more than 1 year after\\ntransplant.\\n4.\\nEXPERIMENTS AND RESULTS\\nWe used the WEKA toolkit 3.6.7 for the implementation\\nof data mining techniques described earlier [24]. 3-fold cross-\\nvalidation was used for evaluation. Cross-validation is rou-\\ntinely used to evaluate the prediction performance of data\\nmining models to eliminate any chances of over-ﬁtting. In\\nk-fold cross-validation, the input data is randomly divided\\ninto k segments. k −1 segments are used to build the model\\nand the remaining 1 segment unseen by the model is used to\\ntest/evaluate it. This is repeated k times with diﬀerent test\\nsegments, and the results are aggregrated. In this way, each\\ninstance of the dataset is run through a model that has not\\nseen it during the training phase. Running a test instance\\nthrough a trained model generates a probability distribu-\\ntion for that instance to belong to diﬀerent possible class\\nvalues (here, binary 1-year survival). Area under the ROC\\ncurve, or c-statistic was used as the metric for model evalu-\\nation, as it measures the ability of the model to eﬀectively\\ndiscriminate between cases and non-cases.\\nAs mentioned before, we used 47 classiﬁcation schemes\\non this data. Fig. 1 present the results on 15 classiﬁcation\\nschemes for 1-year survival, consisting of most of the popu-\\nlar classiﬁers. For each of the ensembling techniques, many\\nunderlying classﬁers were tried in the experiments but only\\nthe one with the best c-statistic is preented in the ﬁgure.\\nBlue bars represent the c-statistic with the entire set of 50\\nattributes, and red bars represent the results with the re-\\nduced set after feature selection.\\nUsing correlation based\\nfeature selection (CFS) technique yielded a subset of only 8\\nfeatures for the given outcome of 1-year survival.\\nIn Fig. 1, the technique that resulted in the best c-statistic\\nis placed at extreme right. The number on top of the each\\nbar is the corresponding c-statistic. The numbers in brown\\nrepresent that the c-statistic is signiﬁcantly lower than the\\nbest model at p=0.05. Other numbers in black indicate that\\n\\nFigure 1:\\nPrediction performance comparison for 1-year survival in terms of area under the ROC curve\\n(c-statistic).\\nFigure 2: Relative information gain of features resulting from the CFS technique for 1-year survival.\\n\\nthe performance is not statistically distinguishable from the\\nbest model at p=0.05. The ﬁgures clearly show that many\\nof the evaluated classiﬁcation schemes perform comparably\\nwell for 1-year survival. Of all the models used in this study,\\nRotation Forest with Alternate Decision Trees as the un-\\nderlying classiﬁer gave the best c-statistic of 0.656 with 50\\nattributes, and of 0.645 with 8 attributes.\\nThus, feature\\nselection techniques were able to identify a much smaller\\nsubset without a signiﬁcant loss in c-statistic.\\nFigure 2 presents the relative predictive power of the re-\\nsulting smaller subset of attributes identiﬁed by CFS for\\n1-year survival. Following is a brief description of these 8\\nattributes:\\n1. Estimated GFR at time of aortic cross clamp:\\nGlomerular ﬁltration rate (GFR) is a test used to check\\nhow well the kidneys are working. Speciﬁcally, it esti-\\nmates how much blood passes through the tiny ﬁlters\\nin the kidneys (glomeruli) per minute [6]. An aortic\\ncross-clamp is a surgical instrument used in cardiac\\nsurgery to clamp the aorta and separate the systemic\\ncirculation from the outﬂow of the heart [3].\\n2. Intubated at transplant: Intubation refers to the\\ninsertion of a tube into an external or internal oriﬁce of\\nthe body for the purpose of adding or removing ﬂuids\\n[9]. This is a binary attribute.\\n3. Hemodialysis at transplant: Hemodialysis is a method\\nthat is used for extracorporeal removal of waste prod-\\nucts such as creatinine and urea and free water from\\nthe blood when the kidneys are in a state of renal fail-\\nure [8]. This is also a binary variable.\\n4. Time spent on the list before transplantation:\\nPatients who are determined to be eligible for a heart\\ntransplant are placed on a waiting list. This waiting\\nlist is part of a national allocation system for donor\\norgans run by the Organ Procurement and Transplan-\\ntation Network (OPTN) [13].\\n5. Age of donor: The age of the donor heart (contiu-\\nnous attribute).\\n6. MCS, explant of VAD at transplant: Mechani-\\ncal circulatory support (MCS) therapy boosts hemo-\\ndynamic function in failing hearts via ventricular as-\\nsist devices (VADs).\\nFor cardiac transplant-eligible\\npatients waiting for a donor heart, MCS can provide\\nBridge-to-Transplantation Therapy, both for interme-\\ndiate support and for optimization of long-term trans-\\nplant outcomes [11]. This is a binary attribute.\\n7. Ischemic time of the organ: The time that an or-\\ngan is outside the body when the heart is not beating\\nor supplied with oxygen by the coronary arteries [10].\\nThis is a contiunous attribute.\\n8. ECMO at transplant:\\nExtracorporeal membrane\\noxygenation (ECMO) is an extracorporeal technique of\\nproviding both cardiac and respiratory support oxygen\\nto patients whose heart and lungs are so severely dis-\\neased or damaged that they can no longer serve their\\nfunction [4]. This is a binary attribute.\\nWe believe that the preliminary results obtained in this\\nwork are quite encouraging and the fact that we can signiﬁ-\\ncantly reduce the number of attrbutes in the model without\\nsacriﬁcing much on the front of accuracy motivates integra-\\ntion of such models in clinical decision making.\\n5.\\nCONCLUSION AND FUTURE WORK\\nIn this workshop paper, we present our preliminary results\\nof data mining on UNOS data on heart transplantation out-\\ncome. We evaluated nearly 50 classiﬁcation schemes for pre-\\ndicting 1-year survival after the transplant. c-statistic of up\\nto 0.656 was achieved. Further, feature selection techniques\\nwere able to signiﬁcantly reduce the number of attributes\\nin the model, incurring a minimal cost in c-statistic (0.645).\\nWe believe that the resulting models can be very useful to\\naid physicians in decision making by providing them with\\npatient-speciﬁc risk estimations.\\nFuture work includes developing more sophisticated mod-\\nels for the studied outcome, and also exploring conditional\\noutcome models using some post-transplant information (e.g.\\nrisk of 2-year mortality, given that the patient has already\\nsurvived 1 year after transplant), and exploring the use of\\nundersampling/oversampling to deal with unbalanced data.\\nWe also plan to do similar analysis for other types of trans-\\nplants, and integrate the current and future work into health-\\ncare and clinical decision making in practice, in the form of\\nrisk calculators, for example.\\n6.\\nACKNOWLEDGMENTS\\nThis work is supported in part by the following grants:\\nNSF awards CCF-0833131, CNS-0830927, IIS-0905205, CCF-\\n0938000, CCF-1029166, and OCI-1144061; DOE awards DE-\\nFG02-08ER25848, DE-SC0001283, DE-SC0005309, DESC0005340,\\nand DESC0007456; AFOSR award FA9550-12-1-0458.\\n7.\\nREFERENCES\\n[1] Optn/drtr 2008 annual report. available at\\nhttp://www.ustransplant.org/annual_reports/\\ncurrent/313_ord.htm, accessed january 27, 2010.\\n[2] Url: 2008 u.s. organ and tissue transplant cost\\nestimates and discussion, millman inc., http://\\npublications.milliman.com/research/health-rr/\\npdfs/2008-us-organ-tisse-RR4-1-08.pdf, accessed\\nmay 13, 2013.\\n[3] Url: Aortic cross-clamp, wikipedia, http:\\n//en.wikipedia.org/wiki/Aortic_cross-clamp,\\naccessed may 15, 2013.\\n[4] Url: Extracorporeal membrane oxygenation,\\nwikipedia, http://en.wikipedia.org/wiki/\\nExtracorporeal_membrane_oxygenation, accessed\\nmay 15, 2013.\\n[5] Url: Financing a transplant, tranplant living,\\nhttp://www.transplantliving.org/\\nbefore-the-transplant/financing-a-transplant/\\nthe-costs/, accessed may 13, 2013.\\n[6] Url: Glomerular ﬁltration rate, medlineplus,\\nhttp://www.nlm.nih.gov/medlineplus/ency/\\narticle/007305.htm, accessed may 15, 2013.\\n[7] Url: Heart transplantation, wikipedia, https:\\n//en.wikipedia.org/wiki/Heart_transplantation,\\naccessed may 13, 2013.\\n\\n[8] Url: Hemodialysis, wikipedia,\\nhttp://en.wikipedia.org/wiki/Hemodialysis,\\naccessed may 15, 2013.\\n[9] Url: Intubation, wikipedia,\\nhttp://en.wikipedia.org/wiki/Intubation,\\naccessed may 15, 2013.\\n[10] Url: Ischemic time, the free dictionary,\\nhttp://medical-dictionary.thefreedictionary.\\ncom/ischemic+time, accessed may 15, 2013.\\n[11] Url: Mechanical circulatory support therapy, thoratec\\ncorporation,\\nhttp://www.thoratec.com/medical-professionals/\\ntreating-advanced-heart-failure/mcs-therapy.\\naspx, accessed may 15, 2013.\\n[12] Url: United network for organ sharing, unos,\\nhttp://www.unos.org/about/index.php, accessed\\nmay 13, 2013.\\n[13] Url: What to expect before a heart transplant,\\nnational heart lung and blood institute,\\nhttp://www.nhlbi.nih.gov/health//dci/Diseases/\\nht/ht_before.html, accessed may 15, 2013.\\n[14] C. Bishop. Neural Networks for Pattern Recognition.\\nOxford: University Press, 1995.\\n[15] L. Breiman. Bagging predictors. Machine Learning,\\n24(2):123–140, 1996.\\n[16] L. Breiman. Random forests. Machine learning,\\n45(1):5–32, 2001.\\n[17] J. G. Cleary and L. E. Trigg. K*: An instance-based\\nlearner using an entropic distance measure. In In\\nProceedings of the 12th International Conference on\\nMachine Learning, pages 108–114. Morgan Kaufmann,\\n1995.\\n[18] L. Fausett. Fundamentals of Neural Networks. New\\nYork, Prentice Hall, 1994.\\n[19] Y. Freund and L. Mason. The alternating decision tree\\nlearning algorithm. In Proceeding of the Sixteenth\\nInternational Conference on Machine Learning, pages\\n124–133. Citeseer, 1999.\\n[20] Y. Freund and R. E. Schapire. Experiments with a\\nnew boosting algorithm. 1996.\\n[21] J. Friedman, T. Hastie, and R. Tibshirani. Special\\ninvited paper. additive logistic regression: A statistical\\nview of boosting. Annals of statistics, 28(2):337–374,\\n2000.\\n[22] H. George. John and Pat Langley. Estimating\\ncontinuous distributions in bayesian classiﬁers. In\\nProceedings of the Eleventh Conference on Uncertainty\\nin Artiﬁcial Intelligence, pages 338–345, 1995.\\n[23] M. Hall. Correlation-based feature selection for\\nmachine learning. PhD thesis, Citeseer, 1999.\\n[24] M. Hall, E. Frank, G. Holmes, B. Pfahringer,\\nP. Reutemann, and I. H. Witten. The weka data\\nmining software: An update. SIGKDD Explorations,\\n11(1), 2009.\\n[25] T. Ho. The random subspace method for constructing\\ndecision forests. IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence, 20(8):832–844,\\n1998.\\n[26] D. Hosmer and S. Lemeshow. Applied Logistic\\nRegression. John Wiley and Sons, Inc., 1989.\\n[27] I. T. Jolliﬀe. Principal Component Analysis. Springer,\\nsecond edition, 2002.\\n[28] R. Kohavi. The power of decision tables. In\\nProceedings of the 8th European Conference on\\nMachine Learning, ECML ’95, pages 174–189, 1995.\\n[29] S. le Cessie and J. van Houwelingen. Ridge estimators\\nin logistic regression. Applied Statistics, 41(1):191–201,\\n1992.\\n[30] W. G. Members, V. L. Roger, A. S. Go, D. M.\\nLloyd-Jones, E. J. Benjamin, J. D. Berry, W. B.\\nBorden, D. M. Bravata, S. Dai, E. S. Ford, C. S. Fox,\\nH. J. Fullerton, C. Gillespie, S. M. Hailpern, J. A.\\nHeit, V. J. Howard, B. M. Kissela, S. J. Kittner, D. T.\\nLackland, J. H. Lichtman, L. D. Lisabeth, D. M.\\nMakuc, G. M. Marcus, A. Marelli, D. B. Matchar,\\nC. S. Moy, D. Mozaﬀarian, M. E. Mussolino,\\nG. Nichol, N. P. Paynter, E. Z. Soliman, P. D. Sorlie,\\nN. Sotoodehnia, T. N. Turan, S. S. Virani, N. D.\\nWong, D. Woo, and M. B. Turner. Heart disease and\\nstroke statistics-2012 update: A report from the\\namerican heart association. Circulation,\\n125(1):e2–e220, 2012.\\n[31] J. Orens, M. Estenne, S. Arcasoy, J. Conte, P. Corris,\\nJ. Egan, T. Egan, S. Keshavjee, C. Knoop, R. Kotloﬀ,\\nF. Martinez, S. Nathan, S. Palmer, A. Patterson,\\nL. Singer, G. Snell, S. Studer, J. Vachiery, and\\nA. Glanville. International guidelines for the selection\\nof lung transplant candidates: 2006 update–a\\nconsensus report from the pulmonary scientiﬁc council\\nof the international society for heart and lung\\ntransplantation. J Heart Lung Transplant,\\n25(7):745–55, 2006.\\n[32] J. Rodriguez, L. Kuncheva, and C. Alonso. Rotation\\nforest: A new classiﬁer ensemble method. Pattern\\nAnalysis and Machine Intelligence, IEEE Transactions\\non, 28(10):1619 –1630, oct. 2006.\\n[33] V. N. Vapnik. The nature of statistical learning\\ntheory. Springer, 1995.\\n[34] I. Witten and E. Frank. Data Mining: Practical\\nmachine learning tools and techniques. Morgan\\nKaufmann Pub, 2005.\\n[35] J. Zaroﬀ, B. Rosengard, W. Armstrong, W. Babcock,\\nA. D’Alessandro, G. Dec, N. Edwards, R. Higgins,\\nV. Jeevanandum, M. Kauﬀman, J. Kirklin, S. Large,\\nD. Marelli, T. Peterson, W. Ring, R. Robbins,\\nS. Russell, D. Taylor, A. V. Bakel, J. Wallwork, and\\nJ. Young. Consensus conference report: maximizing\\nuse of organs recovered from the cadaver donor:\\ncardiac recommendations. Circulation, 106:836–41,\\n2002.\\n'}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/Heart Transplant Outcome Prediction using UNOS Data.pdf', 'text': 'Heart Transplant Outcome Prediction using UNOS Data\\nAnkit Agrawal\\nDept. of Electrical Engineering and Computer\\nScience\\nNorthwestern University\\n2145 Sheridan Rd\\nEvanston, IL 60201\\nUSA\\nankitag@eecs.northwestern.edu\\nMark J. Russo\\nDepartment of Cardiothoracic Surgery\\nCenter for Aortic Diseases\\nBarnabas Health Heart Centers\\nLivingston, NJ 07039\\nUSA\\nmrusso@barnabashealth.org\\nJaishankar Raman\\nSection of Cardiothoracic & Vascular Surgery\\nDepartment of Surgery\\nRush University Medical Center\\nChicago, IL\\nUSA\\njai_raman@rush.edu\\nAlok Choudhary\\nDept. of Electrical Engineering and Computer\\nScience\\nNorthwestern University\\n2145 Sheridan Rd\\nEvanston, IL 60201\\nUSA\\nchoudhar@eecs.northwestern.edu\\nABSTRACT\\nWe analyze heart transplant data from the United Network\\nfor Organ Sharing (UNOS) program with the aim of devel-\\noping accurate risk prediction models for mortality within 1\\nyear of heart transplant using data mining techniques. The\\ndata used in this study is de-identiﬁed and consists of 50\\npredictor attributes, and 1-year posttranplant survial out-\\ncome for patients who underwent heart transplant between\\nthe years 2000 and 2009. Our dataset had 19,429 such pa-\\ntient instances. Several data mining classiﬁcation techniques\\nwere used on this data along with various data mining op-\\ntimizations and validations to build predictive models for\\nthe abovementioned outcome. Prediction results were eval-\\nuated using c-statistic metric, and the highest c-statistic ob-\\ntained was 0.656. Further, we also applied feature selection\\ntechniques to reduce the number of attributes in the model\\nfrom 50 to 8, while trying to have minimal degradation in\\nc-statistic (0.645). We believe the resulting predictive model\\non the reduced dataset can be quite useful to integrate in\\na risk calculator to aid both physicians and patients in risk\\nassessment.\\nCategories and Subject Descriptors\\nH.2.8 [Database Applications]: Data mining; J.3 [Life\\nand Medical Sciences]: Medical information systems\\nPermission to make digital or hard copies of all or part of this work for\\npersonal or classroom use is granted without fee provided that copies are not\\nmade or distributed for proﬁt or commercial advantage and that copies bear\\nthis notice and the full citation on the ﬁrst page. Copyrights for components\\nof this work owned by others than ACM must be honored. Abstracting with\\ncredit is permitted. To copy otherwise, or republish, to post on servers or to\\nredistribute to lists, requires prior speciﬁc permission and/or a fee. Request\\npermissions from permissions@acm.org..\\nKDD-DMH’13, August 11, 2013, Chicago, Illinois, USA.\\nCopyright c⃝2013 ACM 978-1-4503-2174-7/13/08 ...$15.00.\\nKeywords\\nBiomedical informatics, Heart transplant, Decision making,\\nPredictive modeling\\n1.\\nINTRODUCTION\\nA heart transplant, or a cardiac transplant, is a surgical\\ntransplant procedure performed on patients with end-stage\\nheart failure or severe coronary artery disease [7].\\nAs of\\n2009, the survival rates for heart transplant after 1 year were\\n88.0% in males and 86.2% in females [30]. Typical expenses\\nduring the ﬁrst year (everything including surgery, hospital-\\nization, lab testing, medications) average around $800,000\\n[2, 5].\\nGiven the critical scarcity of organs available for trans-\\nplant (about 2,500 available every year [1] compared to 60,000\\npotential recepients [35]), achieving maximal beneﬁt from\\nheart transplantation depends upon improved recipient and\\ndonor selection [31]. Thus accurate estimation of heart trans-\\nplant outcomes can improve both informed patient consent\\nby helping patients better understand its risks and beneﬁts,\\nand also aid the physicians in decision making by assessing\\nthe true patient-speciﬁc risks of the operation, rather than\\nrelying on population-wide risk assessments. To this end,\\naccuarate outcome prediction of performing transplantation\\nis extremely important.\\nThe United Network for Organ Sharing (UNOS) is a pri-\\nvate, non-proﬁt organization that manages the nation’s or-\\ngan transplant system under contract with the federal gov-\\nernment [12]. UNOS is involved in many aspects of the organ\\ntransplant and donation process, including maintaining the\\ndatabase that contains all organ transplant data for every\\ntransplant event that occurs in the US.\\nApplying data mining techniques to heart transplantation\\ndata can be useful to rank and link pretransplantation at-\\ntributes to the outcome. Here we use data mining techniques\\non UNOS heart transplantation data to estimate 1-year sur-\\nvival of heart transplant patients, based on pretransplant\\n\\ncharacteristics. Experiments with nearly 50 modeling tech-\\nniques were conducted and the results compared to ﬁnd the\\nbest model for the data used in this study. It was found\\nthat rotation forest ensembles of alternation decision trees\\nresulted in the best discrimination (c-statistic) between sur-\\nvived and non-survived heart recepients. Further, feature\\nselection was used to ﬁnd a smaller subset of attributes that\\ncan potentially achieve similar prediction performance, but\\ncan result in a simpler model.\\nThe rest of the paper is organized as follows: Section 2\\ndescribes the data mining techniques used in this study fol-\\nlowed by a brief description of the UNOS data used in this\\nstudy in Section 3. Experiments and results are presented in\\nSection 4, and the conclusion and future work is presented\\nin Section 5.\\n2.\\nDATA MINING TECHNIQUES\\n2.1\\nModeling\\nWe used 47 classiﬁcation schemes in this study, includ-\\ning both direct application of classiﬁcation techniques and\\nalso constructing their ensembles using various ensembling\\ntechniques.\\nDue to space limitations, here we brieﬂy de-\\nscribe only those classiﬁcation/ensembling techniques whose\\nresults we present in the next section.\\n1. Support vector machines: SVMs are based on the\\nStructural Risk Minimization(SRM) principle from sta-\\ntistical learning theory. A detailed description of SVMs\\nand SRM is available in [33]. In their basic form, SVMs\\nattempt to perform classiﬁcation by constructing hy-\\nperplanes in a multidimensional space that separates\\nthe cases of diﬀerent class labels. It supports both clas-\\nsiﬁcation and regression tasks and can handle multiple\\ncontinuous and nominal variables.\\n2. Artiﬁcial neural networks: ANNs are networks of\\ninterconnected artiﬁcial neurons, and are commonly\\nused for non-linear statistical data modeling to model\\ncomplex relationships between inputs and outputs. The\\nnetwork includes a hidden layer of multiple artiﬁcial\\nneurons connected to the inputs and outputs with dif-\\nferent edge weights.\\nThe internal edge weights are\\n’learnt’ during the training process using techniques\\nlike back propagation.\\nSeveral good descriptions of\\nneural networks are available [14, 18].\\n3. Decision Table: Decision table typically constructs\\nrules involving diﬀerent combinations of attributes, which\\nare selected using an attribute selection search method.\\nSimple decision table majority classiﬁer [28] has been\\nshown to sometimes outperform state-of-the-art clas-\\nsiﬁers.\\n4. KStar: KStar [17] is a lazy instance-based classiﬁer,\\ni.e., the class of a test instance is based upon the\\nclass of those training instances similar to it, as de-\\ntermined by some similarity function. It diﬀers from\\nother instance-based learners in that it uses an entropy-\\nbased distance function.\\n5. Reduced error pruning tree: Commonly known as\\nREPTree [34], it is a implementation of a fast decision\\ntree learner, which builds a decision/regression tree\\nusing information gain/variance and prunes it using\\nreduced-error pruning.\\n6. Random forest: The Random Forest [16] classiﬁer\\nconsists of multiple decision trees. The ﬁnal class of an\\ninstance in a Random Forest is assigned by outputting\\nthe class that is the mode of the outputs of individual\\ntrees, which can produce robust and accurate classiﬁ-\\ncation, and ability to handle a very large number of\\ninput variables.\\nIt is relatively robust to overﬁtting\\nand can handle datasets with highly imbalanced class\\ndistributions.\\n7. Alternating decision tree: ADTree [19] is decision\\ntree classiﬁer which supports only binary classiﬁcation.\\nIt consists of two types of nodes: decision nodes (spec-\\nifying a predicate condition, like ’age’ > 45) and pre-\\ndiction nodes (containing a single real-value number).\\nAn instance is classiﬁed by following all paths for which\\nall decision nodes are true and summing the values of\\nany prediction nodes that are traversed. This is diﬀer-\\nent from the J48 decision tree algorithm in which an\\ninstance follows only one path through the tree.\\n8. Decision stump: A decision stump [34] is a weak\\ntree-based machine learning model consisting of a single-\\nlevel decision tree with a categorical or numeric class\\nlabel. Decision stumps are usually used in ensemble\\nmachine learning techniques.\\n9. Naive Bayes:\\nThe naive bayes classiﬁer [22] is a\\nsimple probabilistic classiﬁer that is based upon the\\nBayes theorem. This classiﬁer makes strong assump-\\ntions about the independence of the input features,\\nwhich may not always be true. It makes use of the\\nvariables contained in the data sample, by observing\\nand relating them individually to the target class, in-\\ndependent of each other. Despite this assumption, the\\nnaive bayes classiﬁer works well in practice for a wide\\nvariety of datasets and often outperforms other com-\\nplex classiﬁers.\\n10. Bayesian Network: A Bayesian network is a graphi-\\ncal model that encodes probabilistic relationships among\\na set of variables, representing a set of random vari-\\nables and their conditional dependencies via a directed\\nacyclic graph (DAG). Bayesian network learning can\\nbe used with various search algorithms for searching\\nthe network structures, and estimator algorithms for\\nﬁnding the conditional probability tables of the net-\\nwork.\\n11. Logistic Regression: Logistic Regression [26] is used\\nfor prediction of the probability of occurrence of an\\nevent by ﬁtting data to a sigmoidal S-shaped logistic\\ncurve. Logistic regression is often used with ridge es-\\ntimators [29] to improve the parameter estimates and\\nto reduce the error made by further predictions.\\n12. AdaBoost: AdaBoost [20] is a commonly used en-\\nsembling technique for boosting a nominal class clas-\\nsiﬁer. In general, boosting can be used to signiﬁcantly\\nreduce the error of any weak learning algorithm that\\nconsistently generates classiﬁers which need only be a\\n\\nlittle bit better than random guessing. It usually dra-\\nmatically improves performance, but is also prone to\\noverﬁtting.\\n13. LogitBoost: The LogitBoost algorithm is an ensem-\\nbling technique implementation of additive logistic re-\\ngression which performs classiﬁcation using a regres-\\nsion scheme as the base learner, and can handle multi-\\nclass problems. In [21], the authors explain the theo-\\nretical connection between Boosting and additive mod-\\nels.\\n14. Bagging: Bagging [15] is a meta-algorithm to improve\\nthe stability of classiﬁcation and regression algorithms\\nby reducing variance.\\nBagging is usually applied to\\ndecision tree models to boost their performance. It in-\\nvolves generating a number of new training sets (called\\nbootstrap modules) from the original set by sampling\\nuniformly with replacement. The bootstrap modules\\nare then used to generate models whose predictions are\\naveraged to generate the ﬁnal prediction. Bagging has\\nbeen shown work better with decision trees than with\\nlinear models.\\n15. Random subspace: The Random Subspace classi-\\nﬁer [25] constructs a decision tree based classiﬁer con-\\nsisting of multiple trees, which are constructed system-\\natically by pseudo-randomly selecting subsets of fea-\\ntures, trying to achieve a balance between overﬁtting\\nand achieving maximum accuracy. It maintains high-\\nest accuracy on training data and improves on gener-\\nalization accuracy as it grows in complexity.\\n16. Rotation Forest: Rotation forest [32] is a method\\nfor generating classiﬁer ensembles based on feature ex-\\ntraction, which can work both with classiﬁcation and\\nregression base learners. The training data for a the\\nbase classiﬁer is created by applying Principal Compo-\\nnent Analysis (PCA) [27] to K subsets of the feature\\nset, followed by K axis rotations to form the new fea-\\ntures for the base learner, to encourage simultaneously\\nindividual accuracy and diversity within the ensemble.\\n2.2\\nFeature Selection\\nFeature selection techniques are typically used to select a\\nsubset of relevant features for use in a model. It is based on\\nthe assumption that data contains many redundant or irrel-\\nevant attributes that do not add much to the information\\nprovided by other existing attributes.\\nWe used 2 feature\\nselection techniques in this study:\\n1. Correlation Feature Selection (CFS): CFS is used\\nto identify a subset of features highly correlated with\\nthe class variable and weakly correlated amongst them\\n[23]. CFS was used in conjunction with a greedy step-\\nwise search to ﬁnd a subset S with best average merit,\\nwhich is given by:\\nMeritS =\\nn.rfo\\np\\nn + n(n −1).rff\\nwhere n is the number of features in S, rfo is the av-\\nerage value of feature-outcome correlations, and rff is\\nthe average value of all feature-feature correlations.\\n2. Information Gain: This is used to assess the rela-\\ntive predictive power of the predictor attributes, which\\nevaluates the worth of an attribute by measuring the\\ninformation gain with respect to the outcome status:\\nIG(Class, Attrib) = H(Class) −H(Class|Attrib)\\nwhere H(.) denotes the information entropy.\\nThe CFS technique evaluates subsets rather than individ-\\nual attributes, so it was ﬁrst used to ﬁnd a smaller subset of\\nattributes. Subsequently, information gain was used on the\\nreduced set of attributes to get a ranking of the attributes in\\nthe order of their predictive potential, as information gain\\nevaluates each attribute independently.\\n3.\\nUNOS DATA\\nAll individuals aged 18 years and older undergoing heart\\ntransplantation between 2000 and 2009 in the United States\\nwere part of the study population, with a total of 19,429 pa-\\ntients, and 50 attributes were assessed. The outcome vari-\\nable was 1-year survival.\\nWe omit the details of all the\\ninput 50 attributes here due to space constraints. A brief\\ndescription of the selected subset 8 attributes used in the\\nﬁnal model is presented later. Out of 19,429 patients, 2,615\\npatients (13.46%) did not survive more than 1 year after\\ntransplant.\\n4.\\nEXPERIMENTS AND RESULTS\\nWe used the WEKA toolkit 3.6.7 for the implementation\\nof data mining techniques described earlier [24]. 3-fold cross-\\nvalidation was used for evaluation. Cross-validation is rou-\\ntinely used to evaluate the prediction performance of data\\nmining models to eliminate any chances of over-ﬁtting. In\\nk-fold cross-validation, the input data is randomly divided\\ninto k segments. k −1 segments are used to build the model\\nand the remaining 1 segment unseen by the model is used to\\ntest/evaluate it. This is repeated k times with diﬀerent test\\nsegments, and the results are aggregrated. In this way, each\\ninstance of the dataset is run through a model that has not\\nseen it during the training phase. Running a test instance\\nthrough a trained model generates a probability distribu-\\ntion for that instance to belong to diﬀerent possible class\\nvalues (here, binary 1-year survival). Area under the ROC\\ncurve, or c-statistic was used as the metric for model evalu-\\nation, as it measures the ability of the model to eﬀectively\\ndiscriminate between cases and non-cases.\\nAs mentioned before, we used 47 classiﬁcation schemes\\non this data. Fig. 1 present the results on 15 classiﬁcation\\nschemes for 1-year survival, consisting of most of the popu-\\nlar classiﬁers. For each of the ensembling techniques, many\\nunderlying classﬁers were tried in the experiments but only\\nthe one with the best c-statistic is preented in the ﬁgure.\\nBlue bars represent the c-statistic with the entire set of 50\\nattributes, and red bars represent the results with the re-\\nduced set after feature selection.\\nUsing correlation based\\nfeature selection (CFS) technique yielded a subset of only 8\\nfeatures for the given outcome of 1-year survival.\\nIn Fig. 1, the technique that resulted in the best c-statistic\\nis placed at extreme right. The number on top of the each\\nbar is the corresponding c-statistic. The numbers in brown\\nrepresent that the c-statistic is signiﬁcantly lower than the\\nbest model at p=0.05. Other numbers in black indicate that\\n\\nFigure 1:\\nPrediction performance comparison for 1-year survival in terms of area under the ROC curve\\n(c-statistic).\\nFigure 2: Relative information gain of features resulting from the CFS technique for 1-year survival.\\n\\nthe performance is not statistically distinguishable from the\\nbest model at p=0.05. The ﬁgures clearly show that many\\nof the evaluated classiﬁcation schemes perform comparably\\nwell for 1-year survival. Of all the models used in this study,\\nRotation Forest with Alternate Decision Trees as the un-\\nderlying classiﬁer gave the best c-statistic of 0.656 with 50\\nattributes, and of 0.645 with 8 attributes.\\nThus, feature\\nselection techniques were able to identify a much smaller\\nsubset without a signiﬁcant loss in c-statistic.\\nFigure 2 presents the relative predictive power of the re-\\nsulting smaller subset of attributes identiﬁed by CFS for\\n1-year survival. Following is a brief description of these 8\\nattributes:\\n1. Estimated GFR at time of aortic cross clamp:\\nGlomerular ﬁltration rate (GFR) is a test used to check\\nhow well the kidneys are working. Speciﬁcally, it esti-\\nmates how much blood passes through the tiny ﬁlters\\nin the kidneys (glomeruli) per minute [6]. An aortic\\ncross-clamp is a surgical instrument used in cardiac\\nsurgery to clamp the aorta and separate the systemic\\ncirculation from the outﬂow of the heart [3].\\n2. Intubated at transplant: Intubation refers to the\\ninsertion of a tube into an external or internal oriﬁce of\\nthe body for the purpose of adding or removing ﬂuids\\n[9]. This is a binary attribute.\\n3. Hemodialysis at transplant: Hemodialysis is a method\\nthat is used for extracorporeal removal of waste prod-\\nucts such as creatinine and urea and free water from\\nthe blood when the kidneys are in a state of renal fail-\\nure [8]. This is also a binary variable.\\n4. Time spent on the list before transplantation:\\nPatients who are determined to be eligible for a heart\\ntransplant are placed on a waiting list. This waiting\\nlist is part of a national allocation system for donor\\norgans run by the Organ Procurement and Transplan-\\ntation Network (OPTN) [13].\\n5. Age of donor: The age of the donor heart (contiu-\\nnous attribute).\\n6. MCS, explant of VAD at transplant: Mechani-\\ncal circulatory support (MCS) therapy boosts hemo-\\ndynamic function in failing hearts via ventricular as-\\nsist devices (VADs).\\nFor cardiac transplant-eligible\\npatients waiting for a donor heart, MCS can provide\\nBridge-to-Transplantation Therapy, both for interme-\\ndiate support and for optimization of long-term trans-\\nplant outcomes [11]. This is a binary attribute.\\n7. Ischemic time of the organ: The time that an or-\\ngan is outside the body when the heart is not beating\\nor supplied with oxygen by the coronary arteries [10].\\nThis is a contiunous attribute.\\n8. ECMO at transplant:\\nExtracorporeal membrane\\noxygenation (ECMO) is an extracorporeal technique of\\nproviding both cardiac and respiratory support oxygen\\nto patients whose heart and lungs are so severely dis-\\neased or damaged that they can no longer serve their\\nfunction [4]. This is a binary attribute.\\nWe believe that the preliminary results obtained in this\\nwork are quite encouraging and the fact that we can signiﬁ-\\ncantly reduce the number of attrbutes in the model without\\nsacriﬁcing much on the front of accuracy motivates integra-\\ntion of such models in clinical decision making.\\n5.\\nCONCLUSION AND FUTURE WORK\\nIn this workshop paper, we present our preliminary results\\nof data mining on UNOS data on heart transplantation out-\\ncome. We evaluated nearly 50 classiﬁcation schemes for pre-\\ndicting 1-year survival after the transplant. c-statistic of up\\nto 0.656 was achieved. Further, feature selection techniques\\nwere able to signiﬁcantly reduce the number of attributes\\nin the model, incurring a minimal cost in c-statistic (0.645).\\nWe believe that the resulting models can be very useful to\\naid physicians in decision making by providing them with\\npatient-speciﬁc risk estimations.\\nFuture work includes developing more sophisticated mod-\\nels for the studied outcome, and also exploring conditional\\noutcome models using some post-transplant information (e.g.\\nrisk of 2-year mortality, given that the patient has already\\nsurvived 1 year after transplant), and exploring the use of\\nundersampling/oversampling to deal with unbalanced data.\\nWe also plan to do similar analysis for other types of trans-\\nplants, and integrate the current and future work into health-\\ncare and clinical decision making in practice, in the form of\\nrisk calculators, for example.\\n6.\\nACKNOWLEDGMENTS\\nThis work is supported in part by the following grants:\\nNSF awards CCF-0833131, CNS-0830927, IIS-0905205, CCF-\\n0938000, CCF-1029166, and OCI-1144061; DOE awards DE-\\nFG02-08ER25848, DE-SC0001283, DE-SC0005309, DESC0005340,\\nand DESC0007456; AFOSR award FA9550-12-1-0458.\\n7.\\nREFERENCES\\n[1] Optn/drtr 2008 annual report. available at\\nhttp://www.ustransplant.org/annual_reports/\\ncurrent/313_ord.htm, accessed january 27, 2010.\\n[2] Url: 2008 u.s. organ and tissue transplant cost\\nestimates and discussion, millman inc., http://\\npublications.milliman.com/research/health-rr/\\npdfs/2008-us-organ-tisse-RR4-1-08.pdf, accessed\\nmay 13, 2013.\\n[3] Url: Aortic cross-clamp, wikipedia, http:\\n//en.wikipedia.org/wiki/Aortic_cross-clamp,\\naccessed may 15, 2013.\\n[4] Url: Extracorporeal membrane oxygenation,\\nwikipedia, http://en.wikipedia.org/wiki/\\nExtracorporeal_membrane_oxygenation, accessed\\nmay 15, 2013.\\n[5] Url: Financing a transplant, tranplant living,\\nhttp://www.transplantliving.org/\\nbefore-the-transplant/financing-a-transplant/\\nthe-costs/, accessed may 13, 2013.\\n[6] Url: Glomerular ﬁltration rate, medlineplus,\\nhttp://www.nlm.nih.gov/medlineplus/ency/\\narticle/007305.htm, accessed may 15, 2013.\\n[7] Url: Heart transplantation, wikipedia, https:\\n//en.wikipedia.org/wiki/Heart_transplantation,\\naccessed may 13, 2013.\\n\\n[8] Url: Hemodialysis, wikipedia,\\nhttp://en.wikipedia.org/wiki/Hemodialysis,\\naccessed may 15, 2013.\\n[9] Url: Intubation, wikipedia,\\nhttp://en.wikipedia.org/wiki/Intubation,\\naccessed may 15, 2013.\\n[10] Url: Ischemic time, the free dictionary,\\nhttp://medical-dictionary.thefreedictionary.\\ncom/ischemic+time, accessed may 15, 2013.\\n[11] Url: Mechanical circulatory support therapy, thoratec\\ncorporation,\\nhttp://www.thoratec.com/medical-professionals/\\ntreating-advanced-heart-failure/mcs-therapy.\\naspx, accessed may 15, 2013.\\n[12] Url: United network for organ sharing, unos,\\nhttp://www.unos.org/about/index.php, accessed\\nmay 13, 2013.\\n[13] Url: What to expect before a heart transplant,\\nnational heart lung and blood institute,\\nhttp://www.nhlbi.nih.gov/health//dci/Diseases/\\nht/ht_before.html, accessed may 15, 2013.\\n[14] C. Bishop. Neural Networks for Pattern Recognition.\\nOxford: University Press, 1995.\\n[15] L. Breiman. Bagging predictors. Machine Learning,\\n24(2):123–140, 1996.\\n[16] L. Breiman. Random forests. Machine learning,\\n45(1):5–32, 2001.\\n[17] J. G. Cleary and L. E. Trigg. K*: An instance-based\\nlearner using an entropic distance measure. In In\\nProceedings of the 12th International Conference on\\nMachine Learning, pages 108–114. Morgan Kaufmann,\\n1995.\\n[18] L. Fausett. Fundamentals of Neural Networks. New\\nYork, Prentice Hall, 1994.\\n[19] Y. Freund and L. Mason. The alternating decision tree\\nlearning algorithm. In Proceeding of the Sixteenth\\nInternational Conference on Machine Learning, pages\\n124–133. Citeseer, 1999.\\n[20] Y. Freund and R. E. Schapire. Experiments with a\\nnew boosting algorithm. 1996.\\n[21] J. Friedman, T. Hastie, and R. Tibshirani. Special\\ninvited paper. additive logistic regression: A statistical\\nview of boosting. Annals of statistics, 28(2):337–374,\\n2000.\\n[22] H. George. John and Pat Langley. Estimating\\ncontinuous distributions in bayesian classiﬁers. In\\nProceedings of the Eleventh Conference on Uncertainty\\nin Artiﬁcial Intelligence, pages 338–345, 1995.\\n[23] M. Hall. Correlation-based feature selection for\\nmachine learning. PhD thesis, Citeseer, 1999.\\n[24] M. Hall, E. Frank, G. Holmes, B. Pfahringer,\\nP. Reutemann, and I. H. Witten. The weka data\\nmining software: An update. SIGKDD Explorations,\\n11(1), 2009.\\n[25] T. Ho. The random subspace method for constructing\\ndecision forests. IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence, 20(8):832–844,\\n1998.\\n[26] D. Hosmer and S. Lemeshow. Applied Logistic\\nRegression. John Wiley and Sons, Inc., 1989.\\n[27] I. T. Jolliﬀe. Principal Component Analysis. Springer,\\nsecond edition, 2002.\\n[28] R. Kohavi. The power of decision tables. In\\nProceedings of the 8th European Conference on\\nMachine Learning, ECML ’95, pages 174–189, 1995.\\n[29] S. le Cessie and J. van Houwelingen. Ridge estimators\\nin logistic regression. Applied Statistics, 41(1):191–201,\\n1992.\\n[30] W. G. Members, V. L. Roger, A. S. Go, D. M.\\nLloyd-Jones, E. J. Benjamin, J. D. Berry, W. B.\\nBorden, D. M. Bravata, S. Dai, E. S. Ford, C. S. Fox,\\nH. J. Fullerton, C. Gillespie, S. M. Hailpern, J. A.\\nHeit, V. J. Howard, B. M. Kissela, S. J. Kittner, D. T.\\nLackland, J. H. Lichtman, L. D. Lisabeth, D. M.\\nMakuc, G. M. Marcus, A. Marelli, D. B. Matchar,\\nC. S. Moy, D. Mozaﬀarian, M. E. Mussolino,\\nG. Nichol, N. P. Paynter, E. Z. Soliman, P. D. Sorlie,\\nN. Sotoodehnia, T. N. Turan, S. S. Virani, N. D.\\nWong, D. Woo, and M. B. Turner. Heart disease and\\nstroke statistics-2012 update: A report from the\\namerican heart association. Circulation,\\n125(1):e2–e220, 2012.\\n[31] J. Orens, M. Estenne, S. Arcasoy, J. Conte, P. Corris,\\nJ. Egan, T. Egan, S. Keshavjee, C. Knoop, R. Kotloﬀ,\\nF. Martinez, S. Nathan, S. Palmer, A. Patterson,\\nL. Singer, G. Snell, S. Studer, J. Vachiery, and\\nA. Glanville. International guidelines for the selection\\nof lung transplant candidates: 2006 update–a\\nconsensus report from the pulmonary scientiﬁc council\\nof the international society for heart and lung\\ntransplantation. J Heart Lung Transplant,\\n25(7):745–55, 2006.\\n[32] J. Rodriguez, L. Kuncheva, and C. Alonso. Rotation\\nforest: A new classiﬁer ensemble method. Pattern\\nAnalysis and Machine Intelligence, IEEE Transactions\\non, 28(10):1619 –1630, oct. 2006.\\n[33] V. N. Vapnik. The nature of statistical learning\\ntheory. Springer, 1995.\\n[34] I. Witten and E. Frank. Data Mining: Practical\\nmachine learning tools and techniques. Morgan\\nKaufmann Pub, 2005.\\n[35] J. Zaroﬀ, B. Rosengard, W. Armstrong, W. Babcock,\\nA. D’Alessandro, G. Dec, N. Edwards, R. Higgins,\\nV. Jeevanandum, M. Kauﬀman, J. Kirklin, S. Large,\\nD. Marelli, T. Peterson, W. Ring, R. Robbins,\\nS. Russell, D. Taylor, A. V. Bakel, J. Wallwork, and\\nJ. Young. Consensus conference report: maximizing\\nuse of organs recovered from the cadaver donor:\\ncardiac recommendations. Circulation, 106:836–41,\\n2002.\\n'}, {'source': '/Users/sir/Downloads/Data/PDF/Classification-of-functional-voice-disorders-ba_2010_Artificial-Intelligence.pdf', 'text': 'Classiﬁcation of functional voice disorders based on phonovibrograms\\nDaniel Voigt a,*, Michael Do¨llinger a, Thomas Braunschweig b, Anxiong Yang a,\\nUlrich Eysholdt a, Jo¨rg Lohscheller c\\na Department of Phoniatrics and Pediatric Audiology, University Hospital Erlangen, Bohlenplatz 21, D-91054 Erlangen, Germany\\nb Department of Phoniatrics and Pediatric Audiology, University Hospital Jena, Stoystraße 3, D-07743 Jena, Germany\\nc University of Applied Sciences Trier, Department of Computer Science, Medical Informatics, Schneidershof, D-54293 Trier, Germany\\n1. Introduction\\nDiscriminating healthy and pathological vocal fold vibration\\npatterns is essential to the clinical diagnosis of voice functioning,\\nwhich\\nis\\nusually\\ncarried\\nout\\nby\\nspeech/voice\\npathologists,\\nphoniatricians, and vocologists. A common quality criterion for\\na normal voice is the degree of symmetry and regularity of the\\noscillating vocal folds [1,2]. In order to clinically assess these\\ndynamic aspects, the vocal folds’ movement patterns need to be\\ncaptured during phonation. As the fundamental frequency of\\noscillating vocal folds ranges from approximately 80 to 300 Hz\\n(given the habitual pitch speaking level of an adult), the temporal\\nresolution of conventional visual recording systems does not\\nsufﬁce to capture the details of the underlying vibratory patterns.\\nHence, a variety of specialized technologies have been developed\\nto allow for the observation of the rapidly moving vocal folds [3–7].\\nHowever, the most common clinically used examination approach,\\nnamely stroboscopy [8], shows serious diagnostic deﬁcits, as only\\nperiodic laryngeal movements can be adequately investigated due\\nto sampling rate restrictions [9].\\nEndoscopic high-speed (HS) camera systems are a state-of-the-\\nart examination technique for the visual inspection of a patient’s\\nlaryngeal\\ndynamics\\n[10].\\nIn\\ndoing\\nso,\\nthe\\nvoice\\nclinician\\nsubjectively assesses the occurring mode of vocal fold movement\\nand the symmetry between left and right vocal fold side. The HS\\ntechnology even allows for capturing irregular movement patterns\\nArtiﬁcial Intelligence in Medicine 49 (2010) 51–59\\nA R T I C L E\\nI N F O\\nArticle history:\\nReceived 12 November 2008\\nReceived in revised form 20 August 2009\\nAccepted 10 January 2010\\nKeywords:\\nVoice pathology classiﬁcation\\nLaryngeal high-speed video\\nDiagnosis support system\\nPhonovibrogram\\nPVG\\nFeature extraction\\nPattern recognition\\nMachine learning\\nA B S T R A C T\\nObjective: This work presents a computer-aided method for automatically and objectively classifying\\nindividuals with healthy and dysfunctional vocal fold vibration patterns as depicted in clinical high-\\nspeed (HS) videos of the larynx.\\nMethods: By employing a specialized image segmentation and vocal fold movement visualization\\ntechnique – namely phonovibrography – a novel set of numerical features is derived from laryngeal HS\\nvideos capturing the dynamic behavior and the symmetry of oscillating vocal folds. In order to assess the\\ndiscriminatory power of the features, a support vector machine is applied to the preprocessed data with\\nregard to clinically relevant diagnostic tasks. Finally, the classiﬁcation performance of the learned\\nnonlinear models is evaluated to allow for conclusions to be drawn about suitability of features and data\\nresulting from different examination paradigms. As a reference, a second feature set is determined which\\ncorresponds to more traditional voice analysis approaches.\\nResults: For the ﬁrst time an automatic classiﬁcation of healthy and pathological voices could be\\nobtained by analyzing the vibratory patterns of vocal folds using phonovibrograms (PVGs). An average\\nclassiﬁcation accuracy of approximately 81% was achieved for 2-class discrimination with PVG features.\\nThis exceeds the results obtained through traditional voice analysis features. Furthermore, a relevant\\ninﬂuence of phonation frequency on classiﬁcation accuracy was substantiated by the clinical HS data.\\nConclusion: The PVG feature extraction and classiﬁcation approach can be assessed as being promising\\nwith regard to the diagnosis of functional voice disorders. The obtained results indicate that an objective\\nanalysis of dysfunctional vocal fold vibration can be achieved with considerably high accuracy.\\nMoreover, the PVG classiﬁcation method holds a lot of potential when it comes to the clinical assessment\\nof voice pathologies in general, as the diagnostic support can be provided to the voice clinician in a timely\\nand reliable manner. Due to the observed interdependency between phonation frequency and\\nclassiﬁcation accuracy, in future comparative studies of HS recordings of oscillating vocal folds\\nhomogeneous frequencies should be taken into account during examination.\\n\\x02 2010 Elsevier B.V. All rights reserved.\\n* Corresponding author. Tel.: +49 9131 85 32602; fax: +49 9131 85 32687.\\nE-mail address: daniel.voigt@uk-erlangen.de (D. Voigt).\\nContents lists available at ScienceDirect\\nArtificial Intelligence in Medicine\\njournal homepage: www.elsevier.com/locate/aiim\\n0933-3657/$ – see front matter \\x02 2010 Elsevier B.V. All rights reserved.\\ndoi:10.1016/j.artmed.2010.01.001\\n\\n[11], as the vocal fold oscillations are recorded with a frame rate of\\n4000 frames/s and more. However, the amount of recorded image\\ndata rapidly exceeds the limit of what can be evaluated in a usual\\nclinical time-frame. Additionally, plenty of experience regarding\\nthe analysis of HS videos is needed on the part of the examiner. This\\nis due to the fact that the human eye is much more adapted to the\\nprocessing of static visual information than to moving images.\\nConsequently, the clinical assessment of vocal fold movement as\\ncaptured in HS recordings is inherently imprecise and exhibits a\\nrather low inter- and intra-rater reliability. To overcome the\\nlimitations of subjective evaluation, a computerized video analysis\\nis required.\\nThe focus of this work was on HS recordings of patients with\\nfunctional voice disorders. The clinical picture of this particular\\nkind of dysphonia is quite diffuse, and as a consequence, its rating\\nis highly subjective [12,13]. Unlike organic dysphonias (e.g.\\nReinke’s edema, polyp) where an appropriate diagnosis can be\\nmade based almost solely on a single image of a patient’s vocal\\nfolds [14], in case of functional voice disorders the diagnostic\\nprocess is much more complex. This is because the corresponding\\nvocal fold movement can only be diagnosed in the context of\\noverall vibratory behavior, which, to date, is only captured in an\\nadequate manner by HS examination. Moreover, during the\\ndiagnostic process other factors like muscle tension and mental\\ncondition of the patient should be regarded as well [15–17].\\nAccordingly, there is signiﬁcant demand for an objective method to\\ndifferentiate between functional voice disorders and healthy\\nmovement patterns.\\nAs yet, a lot of promising approaches have been introduced to\\nfacilitate the objective analysis of HS recordings [18–21]. For the\\nmost part, these methods focus on the segmentation and analysis\\nof the one-dimensional glottal area signal over time. Another\\napproach consists in extracting the position of individual vocal fold\\npoints and observing their displacements in regard to a ﬁxed line.\\nFurthermore, to quantify asymmetries and irregularities of the\\nvibrations, the parameters of biomechanical multi-mass-models\\nare automatically ﬁtted to detected vocal fold deﬂections and used\\nas an indicator for pathological behavior. Lately, Nyquist plots [22],\\nHilbert transform-based approaches [23], and methods from\\nnonlinear\\nsystems\\nanalysis\\n[24]\\nare\\nalso\\napplied.\\nStill,\\nall\\nmentioned methods lack the ability to analyze the complete\\noscillation pattern of the vocal folds at once.\\nPhonovibrography, a recently developed visualization tech-\\nnique, is a fast and clinically evaluated method for capturing the\\nwhole spatio-temporal pattern of activity along the entire length of\\nthe vocal folds [25]. The deﬂections of the vocal folds contained in\\nthe recorded HS videos are extracted and can be compactly\\ndepicted in a single color-coded image, denoted as phonovibro-\\ngram (PVG) [26]. The PVG gives insight into the vibratory\\ninformation of both vocal folds simultaneously. Thus, occurring\\nvocal\\nfold\\nmovement\\nirregularities\\ncan\\nbe\\nidentiﬁed\\nquite\\nintuitively by visual inspection. The PVG allows a comprehensive\\nanalysis of the underlying two-dimensional laryngeal dynamics\\n[27]. Besides being a valuable diagnostic tool, a PVG can also be\\ntaken as a basis for extracting a set of numerical features. These\\nfeatures objectively describe the characteristics of the vocal fold\\nvibration patterns. Hence, they can be used for automatically\\ndistinguishing between pathological and healthy behavior. In\\ncontrast to the more traditional approaches, where features are\\nderived from the one-dimensional glottal signal, the PVG allows for\\nthe\\nextraction\\nof\\nmore\\nextensive\\ntwo-dimensional\\nfeature\\ndescriptions.\\nIn this work, the HS recordings of a collective of 75 healthy and\\npathological female subjects has been analyzed with a novel\\nmethod for describing the spatio-temporal PVG dynamics and\\nclassifying them according to normal and dysfunctional vocal fold\\nmovement patterns. The obtained PVG features were analyzed\\nusing a nonlinear support vector machine (SVM) approach [28–30]\\nin combination with an evolutionary parameter optimization.\\nSubsequent to building a model of the data, new examples were\\nclassiﬁed according to different binary classiﬁcation tasks which\\nare relevant to the identiﬁcation of functional disorders. As a\\nreference, the same classiﬁcation tasks were also carried out on a\\nset of traditional glottal features. With the resulting cross-\\nvalidated classiﬁcation accuracies, the different feature sets were\\ncompared to each other regarding their ability to describe vocal\\nfold movement.\\n2. Data\\nThe vocal fold movements of n ¼ 75 patients were captured\\nwith state-of-the-art HS recording technique. The diagnoses that\\nsubsequently served as a gold standard for classiﬁcation and\\nevaluation were made by clinically experienced physicians and\\nspeech therapists according to the basic protocol of voice\\npathology assessment of the European Laryngological Society\\n[2]. At this, ﬁve different examination steps were accomplished\\nconsecutively for each individual: auditory-perceptual assess-\\nment, videolaryngoscopic examination, aerodynamic and acoustic\\nanalysis, and not least, self-rating of the patient.\\nIn this manner, a population of n ¼ 50 women with a diagnosed\\nfunctional voice disorder was obtained. This clinical picture is also\\nreferred to as primary muscle tension dysphonia, and is diagnosed\\nin case of dysphonia given normal vocal fold morphology and\\nmotion, and the absence of organic pathological conditions [15].\\nThe considered population included n ¼ 25 cases with a hyper-\\nfunctional and n ¼ 25 cases with a hypofunctional disorder. The\\ndistinction between these two dysfunctional types is clinically\\nmade based on the patient’s overall muscle tension status, the\\namount of laryngeal muscle tension applied during phonation, the\\nvarying degree of hoarseness during crescendo [16] and abnormal\\nlaryngeal posture during connected speech [15]. Furthermore, as a\\nreference population for normal voices, the laryngeal dynamics of\\nn ¼ 25 female candidate speech therapists were recorded. These\\nhealthy individuals exhibited no voice irregularities. Table 1 sums\\nup the age distribution and two acoustic perturbation measures of\\nthe considered population.\\n3. Methods\\n3.1. High-speed videos\\nThe laryngeal images were recorded with a digital HS camera\\nsystem, model Wolf High Speed Endocam 5542. The camera sensor\\ntakes images at a frame rate of 4000 frames/s and a spatial\\nresolution of 256 \\x02 256 image points with 8-bit grayscale (see\\nFig. 1 for example pictures). The sensor receives the optical images\\nof a patient’s vibrating vocal folds through a rigid 90\\x03 endoscope\\n(Wolf Endoscope 8454) mounted in front of the camera. A typical\\nTable 1\\nAge distribution and acoustic perturbation measures of the normal and dysphonic\\npopulation.\\nDiagnoses\\nHealthy\\nHyper\\nHypo\\nNumber of individuals\\n25\\n25\\n25\\nAverage age (in years)\\n19:9 \\x04 1:3\\n42:7 \\x04 14:8\\n40:4 \\x04 20:8\\nJitter a (%)\\n0:30 \\x04 0:15\\n0:34 \\x04 0:23\\n0:36 \\x04 0:22\\nShimmer a (%)\\n2:60 \\x04 1:15\\n2:77 \\x04 1:29\\n3:07 \\x04 1:17\\na For the determination of jitter and shimmer only reliable values < 5% were\\nconsidered [31].\\nD. Voigt et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 51–59\\n52\\n\\nrecord is a sequence of 2000–6000 frames which corresponds to a\\ntotal duration of 0.5–1.5 s. During examination the patient was\\nasked to phonate the sustained vowel /a/.\\n3.2. Image segmentation and visualization\\nIn order to identify the position of the vocal folds over time, the\\nrecorded HS videos were segmented to detect the glottal area aðtÞ.\\nIn Fig. 1 the glottis can be perceived as the expanding and\\ncontracting opening in the center of the images. As a result, the\\nposition of the left and right vocal fold was obtained for each frame\\nof the video using the glottal axis gðtÞ as a splitting point for the\\nsegmented edge [26].\\nTo obtain a compact representation of the determined vocal\\nfold edges over time, for each HS recording the according PVG was\\ncomputed. A PVG encodes the deﬂections of both vocal folds in\\nregard to the glottal axis as graded color intensities—the brighter\\nthe color of a certain PVG pixel, the farther away the corresponding\\nvocal fold point from the glottal axis gðtÞ. For a detailed description\\nof the PVG generation process refer to [26].\\nThe vocal folds’ spatio-temporal movement patterns are\\ntransformed into geometrical shapes using the PVG (see Fig. 2\\nfor an example). Thus, they can be readily employed by a voice\\nclinician to quickly gain information on the overall laryngeal\\ndynamics of a patient. So, for example, the PVG shown in Fig. 2\\nreveals a triangular vocal fold movement pattern which is quite\\nstable and symmetrical—indicating healthy laryngeal dynamics.\\nMoreover, the PVG provides the opportunity to describe vocal fold\\nmovement in a quantitative manner. To this end, the derived PVG\\ndata matrix was subsequently analyzed to capture the underlying\\nvocal fold movement patterns by extracting a set of descriptive\\nfeatures.\\n3.3. Feature extraction\\n3.3.1. Cycle detection\\nVocal fold vibration comprises recurring movement patterns\\nwhich bear a certain degree of similarity to each other (see the\\nshape of the opening and closing phases in Fig. 2). Hence, the ﬁrst\\nstep\\ntowards\\nfeature\\nextraction\\nconsisted\\nin\\nautomatically\\ndetecting the individual oscillation cycles within the continuous\\nPVG (see dashed white lines in Fig. 2). The boundaries of all\\ncaptured cycles were determined by applying a peak-picking\\napproach in the image domain [26]. Because the ﬁrst and the last\\nvocal fold oscillation cycle may have already been truncated in the\\noriginal HS recording (e.g. rightmost cycle in Fig. 2), the two\\noutermost cycles found by the algorithm were withheld from\\nfurther analysis. As a result, a robust approximation of the points in\\ntime when an individual oscillation cycle starts and ends was\\nobtained. The amount of frames used for cycle detection was set to\\nK ¼ 1000 for all included PVGs, respectively.\\nAfter boundary detection, all found cycles were normalized to a\\nstandard width of L ¼ 256 frames to reduce the effects of differing\\nphonation frequencies and varying endoscope positioning in the\\noral cavity (see Fig. 3). Thus, for the PVG of a single vocal fold side a\\nset of normalized oscillation cycles Ca;i (with a 2 ½Left; Right\\x05;\\ni 2 f1; . . . ; Iag and Ia denoting the total amount of cycles found) was\\nobtained. The individual deﬂection values of the cycle are accessed\\nvia function ca;iðx; yÞ with x; y 2 f1; . . . ; 256g. Henceforth, a single\\nrow ca;i\\ny ðxÞ with x 2 f1; . . . ; 256g of a normalized cycle will be\\nreferred to as a trajectory.\\nFig. 1. Excerpt of four single HS video frames illustrating the vocal fold movement and the segmented vocal fold edges. In the ﬁrst frame the position of the vocal folds and the\\nresulting glottis is shown. The second and the third frame illustrate the glottal axis and the edges of the left and right vocal fold side, respectively. In the last frame the position\\nof the anterior and posterior end is depicted.\\nFig. 2. PVG representation of vocal fold dynamics of a healthy individual\\n( f 0 ¼ 144:9 Hz). The movement of both vocal fold sides can be compared to\\neach other, as the deﬂections of a certain frame are depicted as color-coded pixels in\\na single PVG column. Usually a PVG consists of three distinct colors (red, black,\\nblue), but the black-and-white representation shown here sufﬁces to give an idea of\\nits basic structure: while bright sections visualize large distances from the glottal\\naxis, dark pixels represent proximity to the midline.\\nFig. 3. Normalization of a detected PVG cycle. The width alignment procedure is\\nperformed for all PVG cycles of both vocal fold sides. These normalized cycles\\nprovide a basis for extracting quantitative shape information from the data.\\nD. Voigt et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 51–59\\n53\\n\\n3.3.2. PVG contour features\\nThe shape information contained in the oscillation cycles was\\ncaptured by means of novel PVG contour features. Firstly, the\\npoints in time with maximum and minimum deﬂection were\\nidentiﬁed for all trajectories of the oscillation cycles:\\ndy;max :¼ arg max\\nx\\ncyðxÞ\\ndy;min :¼ arg min\\nx\\ncyðxÞ\\nfor Ca;i;\\n8a; i; y:\\n(1)\\nIn this manner, a relative contour threshold\\nty;h ¼ cyðdy;minÞ þ h \\x06 ðcyðdy;maxÞ \\x07 cyðdy;minÞÞ;\\nwith h 2 f0; . . . ; 1g\\n(2)\\nwas derived from the deﬂection values of the two detected points\\nin time.\\nSecondly, starting from position dy;max, the deﬂection values of\\neach trajectory were traced along both temporal directions\\ntowards the adjacent closed states of the vocal folds (see\\nFig. 4c). This bilateral descent step was performed until the ﬁrst\\npoint in time was reached whose deﬂection value was equal or\\nbelow the relative contour threshold, respectively. Parameter h of\\nthe contour threshold was set to 0.5 which corresponds to the\\npoint in time when a vocal fold point has reached 50% of its\\ndisplacement between minimum and maximum deﬂection:\\ndy;0:5;O :¼ arg\\nx\\nðcyðxÞ \\x08 ty;0:5Þ;\\nwith x < dy;max\\ndy;0:5;C :¼ arg\\nx\\nðcyðxÞ \\x08 ty;0:5Þ;\\nwith x > dy;max:\\n(3)\\nThus, the points in time when the vocal folds are located at the\\nstate of half deﬂection were determined, yielding two distinct\\ncontour lines in the opening and closing phase of the cycle. Some\\nexample PVG cycles and the corresponding contours are shown in\\nFig. 4.\\nThe resulting 256 individual points of a contour line were\\nsubsumed by averaging over predeﬁned contour intervals [26]. For\\nthis purpose the opening and closing contour line were divided\\ninto 16 intervals, yielding two averaged contours Oa;i\\n0:5 and Ca;i\\n0:5. So\\nan individual contour point included an x-position along the\\ntimeline and a z-position quantifying the interval’s mean deﬂec-\\ntion. Based on the two-dimensional PVG signal, the spatio-\\ntemporal behavior along the entire vocal fold length was described\\nin terms of numerical features.\\nFurthermore, to relate the vibration characteristics of both\\nvocal folds to each other, proportions between contour features of\\nthe left and the right side were computed:\\nPi\\nO;0:5 ¼ OLeft;i\\n0:5 =ORight;i\\n0:5\\nPi\\nC;0:5 ¼ CLeft;i\\n0:5 =CRight;i\\n0:5\\n8 i:\\n(4)\\nAdditionally, as another characterization of symmetries be-\\ntween left and right vocal fold side, the contours’ Euclidian\\ndistances were computed as follows:\\nDi\\nO;0:5 ¼\\nOLeft;i\\n0:5\\n\\x07 CRight;i\\n0:5\\n\\x02\\x02\\x02\\n\\x02\\x02\\x02\\n2\\nDi\\nC;0:5 ¼\\nCLeft;i\\n0:5\\n\\x07 CRight;i\\n0:5\\n\\x02\\x02\\x02\\n\\x02\\x02\\x02\\n2\\n8 i:\\n(5)\\nThus, a set of supplementary PVG features was obtained\\ndescribing bilateral properties of the vocal folds.\\n3.3.3. Reference glottal features\\nIn order to assess the descriptive power of the new PVG-based\\nfeatures, an additional reference feature set was computed for all\\nHS movies. Employing the one-dimensional glottal signal aðtÞ [22]\\nand\\nthe\\ncorresponding\\nmovement\\ncycles,\\na\\nset\\nof\\nglottal\\nparameters was derived which until now is part of the standard\\nrepertoire for describing vocal fold movement. The features\\ncapture the length of the cycles’ individual phases, the stability\\nof their glottal deﬂection modes and their total duration over time.\\nThus, the vocal folds’ movement patterns were described at the\\nlevel of occurring glottal changes. As these glottal parameters have\\nalready been widely used in the voice analysis literature (e.g.\\n[19,32,33]), in the following, they will be referred to as traditional\\nfeatures.\\nThe following parameters were computed from the glottal\\nsignal:\\n\\t the open quotient Qo, which quantiﬁes the proportion of time the\\nglottis is open during an oscillation cycle [34],\\n\\t the speed quotient Qs, which represents the temporal proportion\\nbetween the opening and the closing phase of a cycle [32],\\n\\t the glottal insufﬁciency Qg, which captures the relation between\\na cycle’s minimum and maximum glottal opening [33],\\n\\t the time periodicity index It p, which describes the temporal\\nstability of the cycle duration [19],\\n\\t and the amplitude periodicity index Ia p, which measures a vocal\\nfold’s deﬂection stability [19].\\nTo determine the two indices It p and Ia p, pairs of consecutive\\noscillation cycles were related to each other. Thus, they can be\\nregarded as being equivalent to the voice quality measures jitter\\nand shimmer which are commonly used in the quantitative\\nevaluation of speech (e.g. in [35]).\\n3.3.4. Feature aggregation\\nTo integrate the individual cycle descriptions and to achieve\\ntemporal abstraction, for each PVG and glottal feature the mean\\nand the standard deviation were computed over all cycles. The\\nmean was primarily used for aggregating the individual cycles’\\nfeature values and capturing the average vocal fold movement. The\\nstandard deviation measured a feature’s variability over time, and\\nthus, represented the vocal fold’s dynamic changes. An overview of\\nthe feature sets used in this study is given in Table 2.\\n3.4. Data analysis\\nThe computed features subsume the deﬂections of a subject’s\\nvocal folds and their positional changes over time at an abstract\\nlevel. While feature set F1 describes both vocal folds’ behavior in\\nterms of the two-dimensional PVG signal, feature set F2 captures\\nthe changes of the one-dimensional glottal signal. The different\\nFig. 4. Contour lines of different normalized vocal fold oscillation cycles of three\\nindividuals. While the ﬁrst cycle (a) is derived from a healthy case, the remaining\\ntwo represent functional voice disorders (b: hyper-, c: hypofunctional). In addition,\\nfor each trajectory the point in time with maximum deﬂection is shown (dashed\\nline) which serves as a starting point for contour detection.\\nD. Voigt et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 51–59\\n54\\n\\nquantitative description approaches of the underlying vibratory\\npatterns were used to identify disease-speciﬁc particularities in\\nfeature space. The discovered structures allow a potential mapping\\nfrom HS recordings of newly examined patients to certain vocal\\nfold disease classes.\\n3.4.1. Class structure\\nIn order to analyze the features in terms of class membership\\nand to build models of vocal fold dysfunction, they were integrated\\ninto describing feature vectors as shown in Table 2. In addition, a\\ndistinct class label indicating the clinical diagnosis of the HS video\\nwas attached to all feature vectors. The obtained HS data\\ndescriptions were pooled in different combinations to provide\\nadequate training sets S1\\x073 for the considered classiﬁcation tasks\\nC1\\x075. The overall set of 25 healthy, 25 hyper-, and 25 hypofunc-\\ntional examples was partitioned as shown in Table 3.\\nIn standard clinical examination situations phonation frequen-\\ncy is usually determined by the patient’s individual voice\\ncharacteristics. Moreover, it is oftentimes inﬂuenced by the vocal\\ndysfunction in question. Hence, an unbalanced frequency distri-\\nbution of classes will be obtained. The frequency distribution of the\\nthree classes included in training set S1 is shown in Fig. 5. As\\nexpected, the underlying frequencies are unbalanced: while the\\nhealthy examples show a bias towards the upper frequency\\nspectrum ( ¯fhealthy ¼ 275:6 \\x04 42:3 Hz), the functional examples are\\nlocated mostly in the lower spectrum ( ¯fhyper ¼ 243:0 \\x04 54:6 Hz\\nand ¯fhypo ¼ 242:8 \\x04 39:0 Hz).\\nAs indicated in the literature [36,37], oscillation frequency of\\nthe vocal folds during voice production affects the outcome of\\nthe perturbation measurement process. This suggests that the\\nfeatures derived from the HS videos may also be inﬂuenced by\\nphonation frequency, and as a consequence, may have an effect\\non the classiﬁcation tasks at hand. To verify this assumption,\\nfrom the available set of 75 learning examples a subset S2\\nconsisting of 15 healthy, 15 hyper-, and 15 hypofunctional\\nexamples was selected. All examples were located in the\\nhomogeneous frequency interval I ¼ ½199; 281\\x05 Hz. Additionally,\\na complementary training set S3 was drawn from the data,\\ncomprising the remaining examples outside interval I plus a\\nsmall overlap to retain class balance. In doing so, for S2 and S3\\nthe potential inﬂuence of oscillation frequency was minimized\\nand maximized, respectively. Moreover, as an additional method\\nto\\nassess\\nthe\\nrelation\\nbetween\\nphonation\\nfrequency\\nand\\nclassiﬁcation outcome further class structures were examined\\nincorporating different frequency intervals (see Table 4). For this\\npurpose, the overall training set S1 was subdivided into n ¼\\n2; . . . ; 4\\nfrequency\\nclasses,\\nwhile\\nentirely\\ndisregarding\\nthe\\nunderlying diagnoses. Hence, the resulting classiﬁcation tasks\\nconsisted in building models of frequency membership. With\\nthe two training sets S2;3 and the classiﬁcation tasks C6\\x078 the\\ninﬂuence of the frequency range on classiﬁcation accuracy was\\nevaluated.\\nFurthermore, to obtain balanced class distributions for classiﬁ-\\ncation task C1 undersampling was performed (marked by \\n) [38].\\nBy randomly selecting 50% of the data of the two remaining classes\\n(i.e. hyper- and hypofunctional examples), respectively, and\\nmerging them into one counter-class (i.e. pathological) equal\\nclass sizes were established. To compensate for random side effects\\ncaused by undersampling multiple sampling runs were performed,\\neach resulting in a different training set conﬁguration. The\\nindividual results were averaged to obtain reliable estimates of\\nclassiﬁcation accuracy.\\nTable 3\\nConsidered training sets, classiﬁcation tasks and their respective class distributions. In classiﬁcation task C1 the * indicates merged and undersampled classes.\\nClassiﬁcation tasks\\nTraining sets\\nS1: mixed frequency data\\nS2: homogeneous frequency interval\\nS3: inhomogeneous frequency interval\\nC1: healthy vs. pathological (hyper [ hypo)\\n25–25*\\n15–15*\\n15–15*\\nC2: hyper vs. hypo\\n25–25\\n15–15\\n15–15\\nC3: healthy vs. hyper\\n25–25\\n15–15\\n15–15\\nC4: healthy vs. hypo\\n25–25\\n15–15\\n15–15\\nC5: healthy vs. hyper vs. hypo\\n25–25–25\\n15–15–15\\n15–15–15\\nTable 2\\nFeature sets derived from the HS recordings of the patients’ vocal fold movements. While F1 contains all the features capturing shape and symmetry of the normalized PVG\\ncycles, F2 comprises the reference features derived from the glottal signal. Feature set F3 is obtained by joining F1 and F2, yielding another reference description for the\\nidentiﬁcation of beneﬁcial feature combinations.\\nFeature set\\nContained features\\nUnderlying signal\\nF1: PVG features\\n¯Oa\\n0:5;sðOa\\n0:5Þ; ¯Ca\\n0:5;sðCa\\n0:5Þ; ¯PO;0:5;sðPO;0:5Þ; ¯PC;0:5;sðPC;0:5Þ; ¯DO;0:5;sðDO;0:5Þ; ¯DC;0:5;sðDC;0:5Þ\\nPVG (2d)\\nF2: traditional features\\n¯Qo;sðQoÞ; ¯Qs;sðQsÞ; ¯Qg;sðQgÞ; ¯It p;sðIt pÞ; ¯Ia p;sð¯Ia pÞ\\nGlottis (1d)\\nF3: combined features\\nF1 [ F2\\nPVG (2d) + glottis (1d)\\nTable 4\\nDifferent frequency interval classes derived from the 75 examples of training set S1\\nand the resulting class distributions.\\nClassiﬁcation tasks\\nS1: all frequency data\\nC6: high vs. low\\n37–38\\nC7: high vs. medium vs. low\\n25–25–25\\nC8: high vs. upper medium vs.\\nlower medium vs. low\\n18–19–19–19\\nFig. 5. Distribution of the 75 training examples of dataset S1 regarding frequency\\nand class membership. The boxes’ position indicates the respective dataset’s\\nfundamental frequency.\\nD. Voigt et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 51–59\\n55\\n\\n3.4.2. Machine learning and evaluation\\nThe derived datasets were subsequently used as an input to an\\ninductive learning scheme which built model descriptions of the\\ndata. To this end, a SVM with a Gaussian radial basis function\\nkernel was applied to the training sets [28,39]. On the basis of the\\nresulting nonlinear SVM models the most likely class label was\\nassigned to an unseen feature vector which was withheld from the\\nprocess of model building.\\nAppropriate SVM parameters were determined by an evolu-\\ntionary strategy optimization procedure [29,40]. The parameter\\nspace of SVM cost parameter C and the width g of the radial basis\\nfunction kernel was automatically searched in order to obtain best\\nclassiﬁcation results [41]. The models’ classiﬁcation accuracy was\\nevaluated via 10-fold cross-validation with stratiﬁcation [42]. In\\nthis manner, the individual results were compared to each other,\\nyielding the best performing classiﬁcation task and feature set. The\\napplied learning scheme is illustrated in Fig. 6.\\n4. Results\\n4.1. Mixed frequency data\\nIn Fig. 7 the results of classiﬁcation tasks C1\\x075 are shown for\\nfeature sets F1\\x073. The respective models were trained using the\\noverall training set S1\\n(without considering the underlying\\nfrequency distribution).\\nThe average classiﬁcation result obtained by PVG features F1\\nexceed the one of glottal features F2 with high signiﬁcance\\n(78:5 \\x04 15:8% vs. 73:5 \\x04 15:7%,\\np ¼ 0:009). Hence, vocal fold\\nmovement patterns described by PVG features can be distin-\\nguished more readily in terms of healthy and pathological behavior\\nthan through the use of glottal features. A combination of both\\nfeature description approaches does not improve classiﬁcation\\nperformance signiﬁcantly (F3: 77:6 \\x04 14:6%, p > 0:05), and hence,\\nis not further considered in the results. Classiﬁcation tasks C3 and\\nC4 yielded better average accuracies than C1 (77.5% and 76.9% vs.\\n74.4%). Thus, for classiﬁcation purposes, it is beneﬁcial to consider\\nthe different types of functional voice disorders individually than\\nto merge them into one class. The results’ high standard deviations\\ncan be ascribed to the relatively small amount of evaluation data\\navailable in the individual folds of cross-validation (n ¼ 7:5). So,\\ndepending on the performed split of the training data, classiﬁca-\\ntion results with high variability are obtained.\\n4.2. Frequency classiﬁcation\\nPhonation\\nfrequency\\nexerts\\na\\ndistinct\\ninﬂuence\\non\\nthe\\nmeasurement of vocal fold movement [36,37]. Therefore, the\\noscillation patterns are possibly subject to change due to frequency\\nalterations. According to this, a subject’s vocal fold movement\\npattern ought to be automatically assigned to its appropriate\\nfrequency interval with high accuracy solely based on the\\ninformation of its spatio-temporal shape. This means that side\\neffects caused by differing phonation frequencies may have a\\nstronger inﬂuence on the classiﬁcation results obtained via\\ntraining set S1 than the disturbed movement pattern of the\\ndisease itself. Thus, it must be assumed that the results presented\\nin Fig. 7 are potentially biased by frequency outcomes. In order to\\nassess the actual frequency effect on the data, the examples were\\narranged into a new class structure by considering only the\\nmembership to certain frequency intervals as class information. In\\nFig. 8 the classiﬁcation results obtained for the according\\nclassiﬁcation tasks C6\\x078 are shown.\\nThe classiﬁcation results of the frequency classes outperform\\nthe results of the healthy/dysfunctional classiﬁcation presented in\\nFig. 7 (2-class average: 81:1 \\x04 12:0% vs. 76:0 \\x04 23:6%). Conse-\\nquently, the data of the healthy and pathological examples need to\\nbe analyzed explicitly taking into consideration the frequency\\nFig. 6. Workﬂow diagram of the overall classiﬁcation process employed to assess\\nthe performance of the different feature sets.\\nFig. 7. Classiﬁcation results obtained through the total amount of 75 datasets for\\ntraining and evaluation. The two dashed horizontal lines at 50% and 33.3% denote\\nthe accuracy levels which can be reached by classifying all examples as the majority\\nclass, respectively. The error bars indicate the results’ standard deviations arising\\nfrom cross-validation.\\nFig. 8. Classiﬁcation results obtained by grouping the total amount of 75 training\\nexamples into classes educed from different frequency intervals. The dashed\\nhorizontal lines mark the baseline accuracy achievable by classifying all cases as the\\nmajority class.\\nD. Voigt et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 51–59\\n56\\n\\neffect outlined above. Furthermore, the PVG feature set F1 yields\\nbetter results than the glottal features F2 throughout all frequency\\nclassiﬁcation tasks.\\n4.3. Different frequency intervals\\nTo assess the inﬂuence of varying phonation frequencies on the\\ndiscrimination of healthy and pathological examples, the classiﬁ-\\ncation performance of F1 and F2 were examined individually for\\nthe following training sets: all data without regarding frequency at\\nall (S1), a subset with weakened frequency inﬂuence (S2), and a\\nsubset with intensiﬁed frequency inﬂuence (S3). In Fig. 9 the\\naveraged 2-class results of the different frequency intervals are\\ncontrasted with each other. For averaging only the results of C2\\x074\\nwere considered, as the pathological class underlying C1 contains\\nexamples from both functional classes and its inclusion would\\nyield an overoptimistic performance estimate.\\nClassiﬁcation results of training set S1 in Fig. 9 are signiﬁcantly\\nexceeded by the ones obtained through subset S2 (F1: 78.5% vs.\\n80.9%;\\nF2:\\n73.5%\\nvs. 81.7%;\\np ¼ 0:015).\\nThus, healthy and\\npathological vocal fold movements can be differentiated with\\nhigher reliability if homogeneous frequency intervals are exam-\\nined. Classiﬁcation results obtained through feature set F1 are\\nrelatively stable for all three examined training sets, resulting in a\\nstandard deviation of 1.3% across frequency intervals. In homoge-\\nnized subset S2 PVG features F1 and glottal features F2 perform\\nequally well in average. Despite exhibiting a particular improve-\\nment between training sets S1 and S2 (+8.2%), the relative decline of\\nF2 in the S3 results is much stronger than for F1 (\\x0711.0% vs. \\x072.2%).\\nAt this, the error bar for F2 using inhomogeneous training set S3\\neven falls below the baseline accuracy of 50%. Thus, in total the\\nclassiﬁcation results of glottal features F2 are more sensitive to\\noccurring frequency inﬂuences than PVG features F1 (standard\\ndeviation across training sets: 5.7%). Due to this fact, PVG features\\nwere found to be more suitable to describe laryngeal dynamics\\nunder varying frequency distributions than glottal features.\\n4.4. Discussion\\nAn important ﬁnding of this study was that PVG feature set F1\\noutperforms glottal feature set F2. Thus, the beneﬁt of considering\\nthe overall vocal fold movement pattern over time as represented\\nin PVGs could be shown with high statistical signiﬁcance. To focus\\nonly on feature descriptions of the changing glottal area yields\\nsuboptimal classiﬁcation results. The reason for this can be seen in\\nthe fact that F2 describes the laryngeal dynamics only at a rather\\ncoarse level, and as a consequence, does not capture the necessary\\ndetails of vocal fold vibration. Moreover, it lacks the ability to\\ndistinguish the two vocal fold sides, and thus, to measure left-right\\nasymmetries. Due to this, classiﬁcation results obtained through\\nglottal features F2 are clearly surpassed by PVG features F1 in\\ntraining sets S1 and S3 (see Fig. 9). In addition, F2 possesses a\\ndistinct sensitivity to phonation frequency, as the quite large\\nvariability of the obtained results reveals. The combination of both\\nfeature\\nset\\napproaches\\nas\\nrepresented\\nin\\nF3\\nachieves\\nno\\nclassiﬁcation improvement.\\nThe subsumption of the two types of functional voice disorders\\ninto one class (classiﬁcation task C1) has been shown to be\\nobstructive in terms of differentiating between healthy and\\npathological vocal fold movement. Therefore, it is beneﬁcial to\\nanalyze and model hyper- and hypofunctional diagnoses individ-\\nually (classiﬁcation tasks C3 and C4), as the corresponding\\nclassiﬁcation results are better than in the class pooling approach.\\nIn the hyperfunctional case the patient’s muscular tension tends to\\nbe increased, whereas in the hypofunctional case it is considerably\\nreduced [16]. Since healthy laryngeal dynamics are essentially\\nsituated between these two diseased states, the merging of both\\ndiagnoses into one class results in an increased overlap of\\npathological and healthy examples. Accordingly, the identiﬁcation\\nof adequate class boundaries is aggravated by merging functional\\ndiagnoses.\\nFrom the fact that classiﬁcation accuracy could be improved by\\nfocusing on homogeneous frequency data it may be concluded that\\na constant phonation frequency should be seeked for during\\nexamination. However, a mixture of phonation frequencies is a\\nmore realistic assumption in a standard clinical setting, as habitual\\npitch phonation plays an important part in making proper\\ndiagnoses of voice disorders. Hence, under these clinical con-\\nsiderations the proposed PVG features have been shown to be more\\nreliable for the identiﬁcation of dysfunctional behavior than the set\\nof glottal parameters. The frequency-dependent classiﬁcation\\nperformance can be accounted for in practice by including\\nsupplementary features capturing relevant phonation frequency\\ninformation which facilitates the discrimination of vocal fold\\nmovement patterns. So the presented approach shows a lot of\\npromise in regard to the characterization and classiﬁcation of\\nfunctional voice disorders. For the analysis of organic disorders, the\\nPVG features are actually expected to perform at least equally well\\nin terms of distinguishing healthy and pathological cases. So, for\\ninstance, for the identiﬁcation of vocal fold paresis it is of particular\\nimportance to capture the left-right asymmetries—a property\\nwhich cannot be described with the glottal signal. Notwithstand-\\ning, in future studies the HS recordings of the patients should be\\nmade under more controlled phonation conditions in order to\\nallow for a systematic analysis of the frequency inﬂuence.\\nAs the PVG method presented in this work is a very novel\\napproach to the classiﬁcation of moving vocal folds, only relatively\\nfew comparable results can be found in the literature. Some works\\nfocus on the extraction of features from the glottal signal and the\\nsubsequent identiﬁcation of normal and dysphonic feature ranges\\n(e.g. [19]). But most commonly, the voice signal of a patient is\\nrecorded and analyzed using acoustic features. So, for example, in\\n[43] a classiﬁcation accuracy of 80% is reported by applying\\nartiﬁcial neural networks to training data derived from the audio\\nsignal of 120 individuals. By analyzing voice signal features Awan\\net al. achieved an accuracy of 74.6% for the classiﬁcation of healthy\\nand functional examples into voice quality classes using stepwise\\ndiscriminant analysis [44]. A sophisticated method from nonlinear\\ndynamical systems theory combined with quadratic discriminant\\nFig. 9. Average 2-class classiﬁcation results obtained with PVG feature set F1 and\\nglottal feature set F2, respectively. In building the models different training sets\\nwere applied varying in terms of considered frequency distribution.\\nD. Voigt et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 51–59\\n57\\n\\nanalysis yields results as good as 91.8% [45]. As a matter of fact,\\nother studies employing logistic regression analysis [46] and\\nkernel principal component analysis [47] report on errorless\\nclassiﬁcation.\\nIn the context of these works, the following facts should be kept\\nin mind. Firstly, in contrast to organic voice disorders, the clinical\\npicture of functional voice disorders is pretty vague and cannot be\\neasily covered by a distinct set of diagnostic rules which apply\\nunder any clinical circumstances (see [16,12,13]). As a result, the\\nsubjective assessment of the symptoms can lead to quite low inter-\\nrater reliability. So the PVG description approach presented here is\\na promising step towards the objectiﬁcation of clinical criteria\\nunderlying the diagnostic process of functional voice disorders. By\\nmeans of numerical PVG features extracted from laryngeal HS\\nvideos a level of inter-individual comparability is achieved which\\neffectively enables the realization of evidence-based medicine in\\nthe ﬁeld of clinical voice diagnosis.\\nSecondly, only vibratory vocal fold information was utilized.\\nThe results obtained from the classiﬁcation of PVG cycle shape\\nfeatures may possibly be improved by incorporating additional\\ninformation characterizing a patient’s vocal fold behavior, phona-\\ntion frequency or acoustic outcome. So, for example, in further\\nstudies the PVG features can be combined with parameters derived\\nfrom the recorded voice signal [48] or features capturing the\\ntransient oscillations during phonation onset [49].\\nThirdly, another essential aspect that needs to be regarded is the\\nexistence of the superimposed phonation frequency effect. Its\\ninﬂuence on classiﬁcation accuracy of vocal fold movement patterns\\nwas substantiated through the improving results shown in Fig. 9.\\nEven though the frequency inﬂuence on the data could be reduced to\\na certain extent by narrowing down the bandwidth of the analyzed\\nHS videos to 82 Hz in the homogeneous frequency interval, its effect\\nis still existent in the data. As a consequence, the results obtained\\nfrom training set S2 may still be affected by the frequency inﬂuence.\\n5. Conclusion\\nA novel method for capturing the movement of vocal folds was\\npresented which can be used to discriminate healthy and\\npathological vibration modes. To this end, the laryngeal dynamics\\nof a collective of individuals with normal voices and diagnosed\\nfunctional voice disorders were recorded with state-of-the-art HS\\nexamination technique. The resulting videos were automatically\\nanalyzed and transformed into PVGs. Subsequently, the movement\\npatterns contained in these PVG representation were described by\\na set of PVG features capturing spatio-temporal shape and\\nsymmetry of vocal fold oscillation. As a way to evaluate the\\ndescriptiveness of the derived PVG features, another set of\\ntraditional glottal parameters was determined for the HS data.\\nThese two feature sets were used as a basis for building nonlinear\\nmodels of the healthy and pathological examples by employing a\\nSVM. Thus, different classiﬁcation tasks that are relevant to the\\ndiagnosis of functional voice disorders were analyzed. This allowed\\nto draw conclusions with respect to the adequacy of the feature\\nsets and the general classiﬁcation accuracy.\\nThe features computed from PVGs are more suitable for the\\ndifferentiation of voice disorders than the traditional glottal\\nparameters. The average classiﬁcation results based on PVG\\nfeatures yield considerably better results throughout all consid-\\nered learning tasks and exhibit higher stability under different\\nclinical conditions. In average, a classiﬁcation accuracy of 81% was\\nobtained\\nfor\\n2-class\\ntasks\\nconcerning\\nthe\\nidentiﬁcation\\nof\\nfunctional dysphonia. This is very promising given the vague\\nclinical picture of the disease and its difﬁcult subjective diagnosis.\\nA further ﬁnding of this study is that the choice of the phonation\\nfrequency plays an important role in the process of discriminating\\nhealthy and pathological behavior, and thus, needs to be especially\\nconsidered during analysis. By and large, the presented approach\\nto combine knowledge-based feature extraction techniques with\\nmethods from machine learning in order to develop objective\\nmedical decision support systems can be regarded as being\\nsuccessful and should be reﬁned in the future.\\nAcknowledgments\\nThis work was supported by Deutsche Forschungsgemeinschaft\\n(DFG) grants no. LO1413/2 1-3 and EY15/11 3-4. The authors\\nwould like to thank the High Performance Computing Group at\\nRegionales Rechenzentrum Erlangen (RRZE) for providing compu-\\ntational resources.\\nReferences\\n[1] Hoppe U. Mechanisms of hoarseness—visualization and interpretation by\\nmeans of nonlinear dynamics. Aachen, Germany: Shaker; 2001.\\n[2] Dejonckere PH, Bradley P, Clemente P, Cornut G, Crevier-Buchman L, Friedrich\\nG. A basic protocol for functional assessment of voice pathology, especially for\\ninvestigating the efﬁcacy of (phonosurgical) treatments and evaluating new\\nassessment techniques. guideline elaborated by the committee on phoniatrics\\nof the European laryngological society (els). Eur Arch Otorhinolaryngol\\n2001;258(February 2):77–82.\\n[3] van Michel C, Pﬁster KA, Luchsinger R. Electroglottography and slow-motion\\nﬁlms of the larynx, comparison of results. Folia Phoniatr 1970;22(2):81–91.\\n[4] Childers DG, Larar JN. Electroglottography for laryngeal function assessment\\nand speech analysis. IEEE Trans Biomed Eng 1984;31(December 12):807–17.\\n[5] Raes J, Lebrun Y, Clement P. Videostroboscopy of the larynx. Acta Otorhino-\\nlaryngol Belg 1986;40(2):421–5.\\n[6] Sˇvec JG, Schutte HK. Videokymography: high-speed line scanning of vocal fold\\nvibration. J Voice 1996;10(June 2):201–5.\\n[7] Wittenberg T, Tigges M, Mergell P, Eysholdt U. Functional imaging of vocal fold\\nvibration: digital multislice high-speed kymography. J Voice 2000;14(Septem-\\nber 3):422–42.\\n[8] Olthoff A, Woywod C, Kruse E. Stroboscopy versus high-speed glottography: a\\ncomparative study. Laryngoscope 2007;117(June 6):1123–6.\\n[9] Yumoto E. Aerodynamics, voice quality, and laryngeal image analysis of\\nnormal and pathologic voices. Curr Opin Otolaryngol Head Neck Surg\\n2004;12(June 3):166–73.\\n[10] Deliyski DD, Petrushev PP, Bonilha HS, Gerlach TT, Martin-Harris B, Hillman\\nRE. Clinical implementation of laryngeal high-speed videoendoscopy: chal-\\nlenges and evolution. Folia Phoniatr Logop 2008;60(1):33–44.\\n[11] Eysholdt U, Rosanowski F, Hoppe U. Vocal fold vibration irregularities caused\\nby different types of laryngeal asymmetry. Eur Arch Otorhinolaryngol\\n2003;260(September 8):412–7.\\n[12] Morrison MD, Nichol H, Rammage LA. Diagnostic criteria in functional dys-\\nphonia. Laryngoscope 1986;96(January 1):1–8.\\n[13] Altman KW, Atkinson C, Lazarus C. Current and emerging concepts in muscle\\ntension dysphonia: a 30-month review. J Voice 2005;19(June 2):261–7.\\n[14] Verikas A, Gelzinis A, Bacauskiene M, Uloza V. Towards a computer-aided\\ndiagnosis system for vocal cord diseases. Artif Intell Med 2006;36(January 1):\\n71–84.\\n[15] Rosen CA, Murry T. Nomenclature of voice disorders and vocal pathology.\\nOtolaryngol Clin North Am 2000;33(October 5):1035–46.\\n[16] Wendler J, Seidner W, Eysholdt U. Lehrbuch der Phoniatrie und Pa¨daudiologie,\\n4th ed., Stuttgart, Germany: Thieme; 2005.\\n[17] Seifert E, Kollbrunner J. Stress and distress in non-organic voice disorder. Swiss\\nMed Wkly 2005;135(July 27/28):387–97.\\n[18] Sˇvec JG, Sram F, Schutte HK. Videokymography in voice disorders: what to look\\nfor? Ann Otol Rhinol Laryngol 2007;116(March 3):172–80.\\n[19] Qiu Q, Schutte HK, Gu L, Yu Q. An automatic method to quantify the vibration\\nproperties of human vocal folds via videokymography. Folia Phoniatr Logop\\n2003;55(3):128–36.\\n[20] Wurzbacher T, Do¨llinger M, Schwarz R, Hoppe U, Eysholdt U, Lohscheller J.\\nSpatiotemporal classiﬁcation of vocal fold dynamics by a multimass model\\ncomprising time-dependent parameters. J Acoust Soc Am 2008;123(April\\n4):2324–34.\\n[21] Mergell P, Herzel H, Titze IR. Irregular vocal-fold vibration—high-speed ob-\\nservation and modeling. J Acoust Soc Am 2000;108(December 6):2996–3002.\\n[22] Yan Y, Ahmad K, Kunduk M, Bless D. Analysis of vocal-fold vibrations from\\nhigh-speed laryngeal images using a hilbert transform-based methodology. J\\nVoice 2005;19(June 2):161–75.\\n[23] Yan Y, Damrose E, Bless D. Functional analysis of voice using simultaneous high-\\nspeed imaging and acoustic recordings. J Voice 2007;21(September 5):604–16.\\n[24] Zhang Y, Tao C, Jiang JJ. Parameter estimation of an asymmetric vocal-fold\\nsystem from glottal area time series using chaos synchronization. Chaos\\n2006;16(June 2):023118.\\n[25] Lohscheller J, Toy H, Rosanowski F, Eysholdt U, Do¨llinger M. Clinically\\nevaluated procedure for the reconstruction of vocal fold vibrations from\\nD. Voigt et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 51–59\\n58\\n\\nendoscopic digital high-speed videos. Med Image Anal 2007;11(August 4):\\n400–13.\\n[26] Lohscheller J, Eysholdt U, Toy H, Do¨llinger M. Phonovibrography: mapping\\nhigh-speed movies of vocal fold vibrations into 2-d diagrams for visualizing\\nand analyzing the underlying laryngeal dynamics. IEEE Trans Med Imaging\\n2008;27(March 3):300–9.\\n[27] Lohscheller J, Eysholdt U. Phonovibrogram visualization of entire vocal fold\\ndynamics. Laryngoscope 2008;118(April 4):753–8.\\n[28] Vapnik VN. The nature of statistical learning theory. New York, NY, USA:\\nSpringer-Verlag New York, Inc.; 1995.\\n[29] Yang SY, Huang Q, Li LL, Ma CY, Zhang H, Bai R. An integrated scheme for\\nfeature selection and parameter setting in the support vector machine model-\\ning and its application to the prediction of pharmacokinetic properties of\\ndrugs. Artif Intell Med 2009;46(June 2):155–63.\\n[30] Asl BM, Setarehdan SK, Mohebbi M. Support vector machine-based arrhythmia\\nclassiﬁcation using reduced features of heart rate variability signal. Artif Intell\\nMed 2008;44(September 1):51–64.\\n[31] Titze IR. Workshop on acoustic voice analysis: summary statement, 1995.\\n[32] Sapienza CM, Stathopoulos ET, Dromey C. Approximations of open quotient\\nand speed quotient from glottal airﬂow and egg waveforms: effects of mea-\\nsurement criteria and sound pressure level. J Voice 1998;12(March 1):31–43.\\n[33] Bielamowicz S, Kapoor R, Schwartz J, Stager SV. Relationship among glottal\\narea, static supraglottic compression, and laryngeal function studies in uni-\\nlateral vocal fold paresis and paralysis. J Voice 2004;18(March 1):138–45.\\n[34] Henrich N, D’Alessandro C, Doval B, Castellengo M. Glottal open quotient in\\nsinging: measurements and correlation with laryngeal mechanisms, vocal\\nintensity, and fundamental frequency. J Acoust Soc Am 2005;117(March 3 Pt\\n1):1417–30.\\n[35] Deliyski DD, Shaw HS, Evans MK, Vesselinov R. Regression tree approach to\\nstudying factors inﬂuencing acoustic voice analysis. Folia Phoniatr Logop\\n2006;58(4):274–88.\\n[36] Orlikoff RF, Baken RJ. Consideration of the relationship between the funda-\\nmental frequency of phonation and vocal jitter. Folia Phoniatr (Basel)\\n1990;42(1):31–40.\\n[37] Rasp O, Lohscheller J, Do¨llinger M, Eysholdt U, Hoppe U. The pitch rise\\nparadigm: a new task for real-time endoscopy of non-stationary phonation.\\nFolia Phoniatr Logop 2006;58(3):175–85.\\n[38] Japkowicz N, Stephen S. The class imbalance problem: a systematic study.\\nIntell Data Anal 2002;6(5):429–49.\\n[39] Burges CJC. A tutorial on support vector machines for pattern recognition. Data\\nMin Knowl Discov 1998;2(2):121–67.\\n[40] Beyer HG, Schwefel HP. Evolution strategies—a comprehensive introduction.\\nNat Comput 2002;1(May):3–52.\\n[41] Hsu CW, Chang CC, Lin CJ. A practical guide to support vector classiﬁcation.\\nTechnical report, Department of Computer Science and Information Engineer-\\ning, National Taiwan University, 2003.\\n[42] Kohavi R. A study of cross-validation and bootstrap for accuracy estimation\\nand model selection. In: IJCAI. 1995. p. 1137–45.\\n[43] Linder R, Albers AE, Hess M, Po¨ppl SJ, Scho¨nweiler R. Artiﬁcial neural network-\\nbased classiﬁcation to screen for dysphonia using psychoacoustic scaling of\\nacoustic voice features. J Voice 2008;22(March 2):155–63.\\n[44] Awan SN, Roy N. Acoustic prediction of voice type in women with functional\\ndysphonia. J Voice 2005;19(June 2):268–82.\\n[45] Little MA, McSharry PE, Roberts SJ, Costello DAE, Moroz IM. Exploiting non-\\nlinear recurrence and fractal scaling properties for voice disorder detection.\\nBiomed Eng Online 2007;6:23.\\n[46] Eadie TL, Doyle PC. Classiﬁcation of dysphonic voice: acoustic and auditory-\\nperceptual measures. J Voice 2005;19(March 1):1–14.\\n[47] Alvarez M, Henao R, Castellanos G, Godino JI, Orozco A. Kernel principal\\ncomponent analysis through time for voice disorder classiﬁcation. Conf Proc\\nIEEE Eng Med Biol Soc 2006;1:5511–4.\\n[48] Do¨llinger M, Lohscheller J, McWhorter A, Kunduk M. Variability of normal\\nvocal fold dynamics for different vocal loading in one healthy subject\\ninvestigated by phonovibrograms. J Voice 2009;23(March 2):175–81.\\n[49] Braunschweig T, Flaschka J, Schelhorn-Neise P, Do¨llinger M. High-speed\\nvideo\\nanalysis of\\nthe\\nphonation\\nonset,\\nwith\\nan\\napplication to\\nthe\\ndiagnosis of functional dysphonias. Med Eng Phys 2008;30(January 1):\\n59–66.\\nD. Voigt et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 51–59\\n59\\n'}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/Classification-of-functional-voice-disorders-ba_2010_Artificial-Intelligence.pdf', 'text': 'Classiﬁcation of functional voice disorders based on phonovibrograms\\nDaniel Voigt a,*, Michael Do¨llinger a, Thomas Braunschweig b, Anxiong Yang a,\\nUlrich Eysholdt a, Jo¨rg Lohscheller c\\na Department of Phoniatrics and Pediatric Audiology, University Hospital Erlangen, Bohlenplatz 21, D-91054 Erlangen, Germany\\nb Department of Phoniatrics and Pediatric Audiology, University Hospital Jena, Stoystraße 3, D-07743 Jena, Germany\\nc University of Applied Sciences Trier, Department of Computer Science, Medical Informatics, Schneidershof, D-54293 Trier, Germany\\n1. Introduction\\nDiscriminating healthy and pathological vocal fold vibration\\npatterns is essential to the clinical diagnosis of voice functioning,\\nwhich\\nis\\nusually\\ncarried\\nout\\nby\\nspeech/voice\\npathologists,\\nphoniatricians, and vocologists. A common quality criterion for\\na normal voice is the degree of symmetry and regularity of the\\noscillating vocal folds [1,2]. In order to clinically assess these\\ndynamic aspects, the vocal folds’ movement patterns need to be\\ncaptured during phonation. As the fundamental frequency of\\noscillating vocal folds ranges from approximately 80 to 300 Hz\\n(given the habitual pitch speaking level of an adult), the temporal\\nresolution of conventional visual recording systems does not\\nsufﬁce to capture the details of the underlying vibratory patterns.\\nHence, a variety of specialized technologies have been developed\\nto allow for the observation of the rapidly moving vocal folds [3–7].\\nHowever, the most common clinically used examination approach,\\nnamely stroboscopy [8], shows serious diagnostic deﬁcits, as only\\nperiodic laryngeal movements can be adequately investigated due\\nto sampling rate restrictions [9].\\nEndoscopic high-speed (HS) camera systems are a state-of-the-\\nart examination technique for the visual inspection of a patient’s\\nlaryngeal\\ndynamics\\n[10].\\nIn\\ndoing\\nso,\\nthe\\nvoice\\nclinician\\nsubjectively assesses the occurring mode of vocal fold movement\\nand the symmetry between left and right vocal fold side. The HS\\ntechnology even allows for capturing irregular movement patterns\\nArtiﬁcial Intelligence in Medicine 49 (2010) 51–59\\nA R T I C L E\\nI N F O\\nArticle history:\\nReceived 12 November 2008\\nReceived in revised form 20 August 2009\\nAccepted 10 January 2010\\nKeywords:\\nVoice pathology classiﬁcation\\nLaryngeal high-speed video\\nDiagnosis support system\\nPhonovibrogram\\nPVG\\nFeature extraction\\nPattern recognition\\nMachine learning\\nA B S T R A C T\\nObjective: This work presents a computer-aided method for automatically and objectively classifying\\nindividuals with healthy and dysfunctional vocal fold vibration patterns as depicted in clinical high-\\nspeed (HS) videos of the larynx.\\nMethods: By employing a specialized image segmentation and vocal fold movement visualization\\ntechnique – namely phonovibrography – a novel set of numerical features is derived from laryngeal HS\\nvideos capturing the dynamic behavior and the symmetry of oscillating vocal folds. In order to assess the\\ndiscriminatory power of the features, a support vector machine is applied to the preprocessed data with\\nregard to clinically relevant diagnostic tasks. Finally, the classiﬁcation performance of the learned\\nnonlinear models is evaluated to allow for conclusions to be drawn about suitability of features and data\\nresulting from different examination paradigms. As a reference, a second feature set is determined which\\ncorresponds to more traditional voice analysis approaches.\\nResults: For the ﬁrst time an automatic classiﬁcation of healthy and pathological voices could be\\nobtained by analyzing the vibratory patterns of vocal folds using phonovibrograms (PVGs). An average\\nclassiﬁcation accuracy of approximately 81% was achieved for 2-class discrimination with PVG features.\\nThis exceeds the results obtained through traditional voice analysis features. Furthermore, a relevant\\ninﬂuence of phonation frequency on classiﬁcation accuracy was substantiated by the clinical HS data.\\nConclusion: The PVG feature extraction and classiﬁcation approach can be assessed as being promising\\nwith regard to the diagnosis of functional voice disorders. The obtained results indicate that an objective\\nanalysis of dysfunctional vocal fold vibration can be achieved with considerably high accuracy.\\nMoreover, the PVG classiﬁcation method holds a lot of potential when it comes to the clinical assessment\\nof voice pathologies in general, as the diagnostic support can be provided to the voice clinician in a timely\\nand reliable manner. Due to the observed interdependency between phonation frequency and\\nclassiﬁcation accuracy, in future comparative studies of HS recordings of oscillating vocal folds\\nhomogeneous frequencies should be taken into account during examination.\\n\\x02 2010 Elsevier B.V. All rights reserved.\\n* Corresponding author. Tel.: +49 9131 85 32602; fax: +49 9131 85 32687.\\nE-mail address: daniel.voigt@uk-erlangen.de (D. Voigt).\\nContents lists available at ScienceDirect\\nArtificial Intelligence in Medicine\\njournal homepage: www.elsevier.com/locate/aiim\\n0933-3657/$ – see front matter \\x02 2010 Elsevier B.V. All rights reserved.\\ndoi:10.1016/j.artmed.2010.01.001\\n\\n[11], as the vocal fold oscillations are recorded with a frame rate of\\n4000 frames/s and more. However, the amount of recorded image\\ndata rapidly exceeds the limit of what can be evaluated in a usual\\nclinical time-frame. Additionally, plenty of experience regarding\\nthe analysis of HS videos is needed on the part of the examiner. This\\nis due to the fact that the human eye is much more adapted to the\\nprocessing of static visual information than to moving images.\\nConsequently, the clinical assessment of vocal fold movement as\\ncaptured in HS recordings is inherently imprecise and exhibits a\\nrather low inter- and intra-rater reliability. To overcome the\\nlimitations of subjective evaluation, a computerized video analysis\\nis required.\\nThe focus of this work was on HS recordings of patients with\\nfunctional voice disorders. The clinical picture of this particular\\nkind of dysphonia is quite diffuse, and as a consequence, its rating\\nis highly subjective [12,13]. Unlike organic dysphonias (e.g.\\nReinke’s edema, polyp) where an appropriate diagnosis can be\\nmade based almost solely on a single image of a patient’s vocal\\nfolds [14], in case of functional voice disorders the diagnostic\\nprocess is much more complex. This is because the corresponding\\nvocal fold movement can only be diagnosed in the context of\\noverall vibratory behavior, which, to date, is only captured in an\\nadequate manner by HS examination. Moreover, during the\\ndiagnostic process other factors like muscle tension and mental\\ncondition of the patient should be regarded as well [15–17].\\nAccordingly, there is signiﬁcant demand for an objective method to\\ndifferentiate between functional voice disorders and healthy\\nmovement patterns.\\nAs yet, a lot of promising approaches have been introduced to\\nfacilitate the objective analysis of HS recordings [18–21]. For the\\nmost part, these methods focus on the segmentation and analysis\\nof the one-dimensional glottal area signal over time. Another\\napproach consists in extracting the position of individual vocal fold\\npoints and observing their displacements in regard to a ﬁxed line.\\nFurthermore, to quantify asymmetries and irregularities of the\\nvibrations, the parameters of biomechanical multi-mass-models\\nare automatically ﬁtted to detected vocal fold deﬂections and used\\nas an indicator for pathological behavior. Lately, Nyquist plots [22],\\nHilbert transform-based approaches [23], and methods from\\nnonlinear\\nsystems\\nanalysis\\n[24]\\nare\\nalso\\napplied.\\nStill,\\nall\\nmentioned methods lack the ability to analyze the complete\\noscillation pattern of the vocal folds at once.\\nPhonovibrography, a recently developed visualization tech-\\nnique, is a fast and clinically evaluated method for capturing the\\nwhole spatio-temporal pattern of activity along the entire length of\\nthe vocal folds [25]. The deﬂections of the vocal folds contained in\\nthe recorded HS videos are extracted and can be compactly\\ndepicted in a single color-coded image, denoted as phonovibro-\\ngram (PVG) [26]. The PVG gives insight into the vibratory\\ninformation of both vocal folds simultaneously. Thus, occurring\\nvocal\\nfold\\nmovement\\nirregularities\\ncan\\nbe\\nidentiﬁed\\nquite\\nintuitively by visual inspection. The PVG allows a comprehensive\\nanalysis of the underlying two-dimensional laryngeal dynamics\\n[27]. Besides being a valuable diagnostic tool, a PVG can also be\\ntaken as a basis for extracting a set of numerical features. These\\nfeatures objectively describe the characteristics of the vocal fold\\nvibration patterns. Hence, they can be used for automatically\\ndistinguishing between pathological and healthy behavior. In\\ncontrast to the more traditional approaches, where features are\\nderived from the one-dimensional glottal signal, the PVG allows for\\nthe\\nextraction\\nof\\nmore\\nextensive\\ntwo-dimensional\\nfeature\\ndescriptions.\\nIn this work, the HS recordings of a collective of 75 healthy and\\npathological female subjects has been analyzed with a novel\\nmethod for describing the spatio-temporal PVG dynamics and\\nclassifying them according to normal and dysfunctional vocal fold\\nmovement patterns. The obtained PVG features were analyzed\\nusing a nonlinear support vector machine (SVM) approach [28–30]\\nin combination with an evolutionary parameter optimization.\\nSubsequent to building a model of the data, new examples were\\nclassiﬁed according to different binary classiﬁcation tasks which\\nare relevant to the identiﬁcation of functional disorders. As a\\nreference, the same classiﬁcation tasks were also carried out on a\\nset of traditional glottal features. With the resulting cross-\\nvalidated classiﬁcation accuracies, the different feature sets were\\ncompared to each other regarding their ability to describe vocal\\nfold movement.\\n2. Data\\nThe vocal fold movements of n ¼ 75 patients were captured\\nwith state-of-the-art HS recording technique. The diagnoses that\\nsubsequently served as a gold standard for classiﬁcation and\\nevaluation were made by clinically experienced physicians and\\nspeech therapists according to the basic protocol of voice\\npathology assessment of the European Laryngological Society\\n[2]. At this, ﬁve different examination steps were accomplished\\nconsecutively for each individual: auditory-perceptual assess-\\nment, videolaryngoscopic examination, aerodynamic and acoustic\\nanalysis, and not least, self-rating of the patient.\\nIn this manner, a population of n ¼ 50 women with a diagnosed\\nfunctional voice disorder was obtained. This clinical picture is also\\nreferred to as primary muscle tension dysphonia, and is diagnosed\\nin case of dysphonia given normal vocal fold morphology and\\nmotion, and the absence of organic pathological conditions [15].\\nThe considered population included n ¼ 25 cases with a hyper-\\nfunctional and n ¼ 25 cases with a hypofunctional disorder. The\\ndistinction between these two dysfunctional types is clinically\\nmade based on the patient’s overall muscle tension status, the\\namount of laryngeal muscle tension applied during phonation, the\\nvarying degree of hoarseness during crescendo [16] and abnormal\\nlaryngeal posture during connected speech [15]. Furthermore, as a\\nreference population for normal voices, the laryngeal dynamics of\\nn ¼ 25 female candidate speech therapists were recorded. These\\nhealthy individuals exhibited no voice irregularities. Table 1 sums\\nup the age distribution and two acoustic perturbation measures of\\nthe considered population.\\n3. Methods\\n3.1. High-speed videos\\nThe laryngeal images were recorded with a digital HS camera\\nsystem, model Wolf High Speed Endocam 5542. The camera sensor\\ntakes images at a frame rate of 4000 frames/s and a spatial\\nresolution of 256 \\x02 256 image points with 8-bit grayscale (see\\nFig. 1 for example pictures). The sensor receives the optical images\\nof a patient’s vibrating vocal folds through a rigid 90\\x03 endoscope\\n(Wolf Endoscope 8454) mounted in front of the camera. A typical\\nTable 1\\nAge distribution and acoustic perturbation measures of the normal and dysphonic\\npopulation.\\nDiagnoses\\nHealthy\\nHyper\\nHypo\\nNumber of individuals\\n25\\n25\\n25\\nAverage age (in years)\\n19:9 \\x04 1:3\\n42:7 \\x04 14:8\\n40:4 \\x04 20:8\\nJitter a (%)\\n0:30 \\x04 0:15\\n0:34 \\x04 0:23\\n0:36 \\x04 0:22\\nShimmer a (%)\\n2:60 \\x04 1:15\\n2:77 \\x04 1:29\\n3:07 \\x04 1:17\\na For the determination of jitter and shimmer only reliable values < 5% were\\nconsidered [31].\\nD. Voigt et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 51–59\\n52\\n\\nrecord is a sequence of 2000–6000 frames which corresponds to a\\ntotal duration of 0.5–1.5 s. During examination the patient was\\nasked to phonate the sustained vowel /a/.\\n3.2. Image segmentation and visualization\\nIn order to identify the position of the vocal folds over time, the\\nrecorded HS videos were segmented to detect the glottal area aðtÞ.\\nIn Fig. 1 the glottis can be perceived as the expanding and\\ncontracting opening in the center of the images. As a result, the\\nposition of the left and right vocal fold was obtained for each frame\\nof the video using the glottal axis gðtÞ as a splitting point for the\\nsegmented edge [26].\\nTo obtain a compact representation of the determined vocal\\nfold edges over time, for each HS recording the according PVG was\\ncomputed. A PVG encodes the deﬂections of both vocal folds in\\nregard to the glottal axis as graded color intensities—the brighter\\nthe color of a certain PVG pixel, the farther away the corresponding\\nvocal fold point from the glottal axis gðtÞ. For a detailed description\\nof the PVG generation process refer to [26].\\nThe vocal folds’ spatio-temporal movement patterns are\\ntransformed into geometrical shapes using the PVG (see Fig. 2\\nfor an example). Thus, they can be readily employed by a voice\\nclinician to quickly gain information on the overall laryngeal\\ndynamics of a patient. So, for example, the PVG shown in Fig. 2\\nreveals a triangular vocal fold movement pattern which is quite\\nstable and symmetrical—indicating healthy laryngeal dynamics.\\nMoreover, the PVG provides the opportunity to describe vocal fold\\nmovement in a quantitative manner. To this end, the derived PVG\\ndata matrix was subsequently analyzed to capture the underlying\\nvocal fold movement patterns by extracting a set of descriptive\\nfeatures.\\n3.3. Feature extraction\\n3.3.1. Cycle detection\\nVocal fold vibration comprises recurring movement patterns\\nwhich bear a certain degree of similarity to each other (see the\\nshape of the opening and closing phases in Fig. 2). Hence, the ﬁrst\\nstep\\ntowards\\nfeature\\nextraction\\nconsisted\\nin\\nautomatically\\ndetecting the individual oscillation cycles within the continuous\\nPVG (see dashed white lines in Fig. 2). The boundaries of all\\ncaptured cycles were determined by applying a peak-picking\\napproach in the image domain [26]. Because the ﬁrst and the last\\nvocal fold oscillation cycle may have already been truncated in the\\noriginal HS recording (e.g. rightmost cycle in Fig. 2), the two\\noutermost cycles found by the algorithm were withheld from\\nfurther analysis. As a result, a robust approximation of the points in\\ntime when an individual oscillation cycle starts and ends was\\nobtained. The amount of frames used for cycle detection was set to\\nK ¼ 1000 for all included PVGs, respectively.\\nAfter boundary detection, all found cycles were normalized to a\\nstandard width of L ¼ 256 frames to reduce the effects of differing\\nphonation frequencies and varying endoscope positioning in the\\noral cavity (see Fig. 3). Thus, for the PVG of a single vocal fold side a\\nset of normalized oscillation cycles Ca;i (with a 2 ½Left; Right\\x05;\\ni 2 f1; . . . ; Iag and Ia denoting the total amount of cycles found) was\\nobtained. The individual deﬂection values of the cycle are accessed\\nvia function ca;iðx; yÞ with x; y 2 f1; . . . ; 256g. Henceforth, a single\\nrow ca;i\\ny ðxÞ with x 2 f1; . . . ; 256g of a normalized cycle will be\\nreferred to as a trajectory.\\nFig. 1. Excerpt of four single HS video frames illustrating the vocal fold movement and the segmented vocal fold edges. In the ﬁrst frame the position of the vocal folds and the\\nresulting glottis is shown. The second and the third frame illustrate the glottal axis and the edges of the left and right vocal fold side, respectively. In the last frame the position\\nof the anterior and posterior end is depicted.\\nFig. 2. PVG representation of vocal fold dynamics of a healthy individual\\n( f 0 ¼ 144:9 Hz). The movement of both vocal fold sides can be compared to\\neach other, as the deﬂections of a certain frame are depicted as color-coded pixels in\\na single PVG column. Usually a PVG consists of three distinct colors (red, black,\\nblue), but the black-and-white representation shown here sufﬁces to give an idea of\\nits basic structure: while bright sections visualize large distances from the glottal\\naxis, dark pixels represent proximity to the midline.\\nFig. 3. Normalization of a detected PVG cycle. The width alignment procedure is\\nperformed for all PVG cycles of both vocal fold sides. These normalized cycles\\nprovide a basis for extracting quantitative shape information from the data.\\nD. Voigt et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 51–59\\n53\\n\\n3.3.2. PVG contour features\\nThe shape information contained in the oscillation cycles was\\ncaptured by means of novel PVG contour features. Firstly, the\\npoints in time with maximum and minimum deﬂection were\\nidentiﬁed for all trajectories of the oscillation cycles:\\ndy;max :¼ arg max\\nx\\ncyðxÞ\\ndy;min :¼ arg min\\nx\\ncyðxÞ\\nfor Ca;i;\\n8a; i; y:\\n(1)\\nIn this manner, a relative contour threshold\\nty;h ¼ cyðdy;minÞ þ h \\x06 ðcyðdy;maxÞ \\x07 cyðdy;minÞÞ;\\nwith h 2 f0; . . . ; 1g\\n(2)\\nwas derived from the deﬂection values of the two detected points\\nin time.\\nSecondly, starting from position dy;max, the deﬂection values of\\neach trajectory were traced along both temporal directions\\ntowards the adjacent closed states of the vocal folds (see\\nFig. 4c). This bilateral descent step was performed until the ﬁrst\\npoint in time was reached whose deﬂection value was equal or\\nbelow the relative contour threshold, respectively. Parameter h of\\nthe contour threshold was set to 0.5 which corresponds to the\\npoint in time when a vocal fold point has reached 50% of its\\ndisplacement between minimum and maximum deﬂection:\\ndy;0:5;O :¼ arg\\nx\\nðcyðxÞ \\x08 ty;0:5Þ;\\nwith x < dy;max\\ndy;0:5;C :¼ arg\\nx\\nðcyðxÞ \\x08 ty;0:5Þ;\\nwith x > dy;max:\\n(3)\\nThus, the points in time when the vocal folds are located at the\\nstate of half deﬂection were determined, yielding two distinct\\ncontour lines in the opening and closing phase of the cycle. Some\\nexample PVG cycles and the corresponding contours are shown in\\nFig. 4.\\nThe resulting 256 individual points of a contour line were\\nsubsumed by averaging over predeﬁned contour intervals [26]. For\\nthis purpose the opening and closing contour line were divided\\ninto 16 intervals, yielding two averaged contours Oa;i\\n0:5 and Ca;i\\n0:5. So\\nan individual contour point included an x-position along the\\ntimeline and a z-position quantifying the interval’s mean deﬂec-\\ntion. Based on the two-dimensional PVG signal, the spatio-\\ntemporal behavior along the entire vocal fold length was described\\nin terms of numerical features.\\nFurthermore, to relate the vibration characteristics of both\\nvocal folds to each other, proportions between contour features of\\nthe left and the right side were computed:\\nPi\\nO;0:5 ¼ OLeft;i\\n0:5 =ORight;i\\n0:5\\nPi\\nC;0:5 ¼ CLeft;i\\n0:5 =CRight;i\\n0:5\\n8 i:\\n(4)\\nAdditionally, as another characterization of symmetries be-\\ntween left and right vocal fold side, the contours’ Euclidian\\ndistances were computed as follows:\\nDi\\nO;0:5 ¼\\nOLeft;i\\n0:5\\n\\x07 CRight;i\\n0:5\\n\\x02\\x02\\x02\\n\\x02\\x02\\x02\\n2\\nDi\\nC;0:5 ¼\\nCLeft;i\\n0:5\\n\\x07 CRight;i\\n0:5\\n\\x02\\x02\\x02\\n\\x02\\x02\\x02\\n2\\n8 i:\\n(5)\\nThus, a set of supplementary PVG features was obtained\\ndescribing bilateral properties of the vocal folds.\\n3.3.3. Reference glottal features\\nIn order to assess the descriptive power of the new PVG-based\\nfeatures, an additional reference feature set was computed for all\\nHS movies. Employing the one-dimensional glottal signal aðtÞ [22]\\nand\\nthe\\ncorresponding\\nmovement\\ncycles,\\na\\nset\\nof\\nglottal\\nparameters was derived which until now is part of the standard\\nrepertoire for describing vocal fold movement. The features\\ncapture the length of the cycles’ individual phases, the stability\\nof their glottal deﬂection modes and their total duration over time.\\nThus, the vocal folds’ movement patterns were described at the\\nlevel of occurring glottal changes. As these glottal parameters have\\nalready been widely used in the voice analysis literature (e.g.\\n[19,32,33]), in the following, they will be referred to as traditional\\nfeatures.\\nThe following parameters were computed from the glottal\\nsignal:\\n\\t the open quotient Qo, which quantiﬁes the proportion of time the\\nglottis is open during an oscillation cycle [34],\\n\\t the speed quotient Qs, which represents the temporal proportion\\nbetween the opening and the closing phase of a cycle [32],\\n\\t the glottal insufﬁciency Qg, which captures the relation between\\na cycle’s minimum and maximum glottal opening [33],\\n\\t the time periodicity index It p, which describes the temporal\\nstability of the cycle duration [19],\\n\\t and the amplitude periodicity index Ia p, which measures a vocal\\nfold’s deﬂection stability [19].\\nTo determine the two indices It p and Ia p, pairs of consecutive\\noscillation cycles were related to each other. Thus, they can be\\nregarded as being equivalent to the voice quality measures jitter\\nand shimmer which are commonly used in the quantitative\\nevaluation of speech (e.g. in [35]).\\n3.3.4. Feature aggregation\\nTo integrate the individual cycle descriptions and to achieve\\ntemporal abstraction, for each PVG and glottal feature the mean\\nand the standard deviation were computed over all cycles. The\\nmean was primarily used for aggregating the individual cycles’\\nfeature values and capturing the average vocal fold movement. The\\nstandard deviation measured a feature’s variability over time, and\\nthus, represented the vocal fold’s dynamic changes. An overview of\\nthe feature sets used in this study is given in Table 2.\\n3.4. Data analysis\\nThe computed features subsume the deﬂections of a subject’s\\nvocal folds and their positional changes over time at an abstract\\nlevel. While feature set F1 describes both vocal folds’ behavior in\\nterms of the two-dimensional PVG signal, feature set F2 captures\\nthe changes of the one-dimensional glottal signal. The different\\nFig. 4. Contour lines of different normalized vocal fold oscillation cycles of three\\nindividuals. While the ﬁrst cycle (a) is derived from a healthy case, the remaining\\ntwo represent functional voice disorders (b: hyper-, c: hypofunctional). In addition,\\nfor each trajectory the point in time with maximum deﬂection is shown (dashed\\nline) which serves as a starting point for contour detection.\\nD. Voigt et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 51–59\\n54\\n\\nquantitative description approaches of the underlying vibratory\\npatterns were used to identify disease-speciﬁc particularities in\\nfeature space. The discovered structures allow a potential mapping\\nfrom HS recordings of newly examined patients to certain vocal\\nfold disease classes.\\n3.4.1. Class structure\\nIn order to analyze the features in terms of class membership\\nand to build models of vocal fold dysfunction, they were integrated\\ninto describing feature vectors as shown in Table 2. In addition, a\\ndistinct class label indicating the clinical diagnosis of the HS video\\nwas attached to all feature vectors. The obtained HS data\\ndescriptions were pooled in different combinations to provide\\nadequate training sets S1\\x073 for the considered classiﬁcation tasks\\nC1\\x075. The overall set of 25 healthy, 25 hyper-, and 25 hypofunc-\\ntional examples was partitioned as shown in Table 3.\\nIn standard clinical examination situations phonation frequen-\\ncy is usually determined by the patient’s individual voice\\ncharacteristics. Moreover, it is oftentimes inﬂuenced by the vocal\\ndysfunction in question. Hence, an unbalanced frequency distri-\\nbution of classes will be obtained. The frequency distribution of the\\nthree classes included in training set S1 is shown in Fig. 5. As\\nexpected, the underlying frequencies are unbalanced: while the\\nhealthy examples show a bias towards the upper frequency\\nspectrum ( ¯fhealthy ¼ 275:6 \\x04 42:3 Hz), the functional examples are\\nlocated mostly in the lower spectrum ( ¯fhyper ¼ 243:0 \\x04 54:6 Hz\\nand ¯fhypo ¼ 242:8 \\x04 39:0 Hz).\\nAs indicated in the literature [36,37], oscillation frequency of\\nthe vocal folds during voice production affects the outcome of\\nthe perturbation measurement process. This suggests that the\\nfeatures derived from the HS videos may also be inﬂuenced by\\nphonation frequency, and as a consequence, may have an effect\\non the classiﬁcation tasks at hand. To verify this assumption,\\nfrom the available set of 75 learning examples a subset S2\\nconsisting of 15 healthy, 15 hyper-, and 15 hypofunctional\\nexamples was selected. All examples were located in the\\nhomogeneous frequency interval I ¼ ½199; 281\\x05 Hz. Additionally,\\na complementary training set S3 was drawn from the data,\\ncomprising the remaining examples outside interval I plus a\\nsmall overlap to retain class balance. In doing so, for S2 and S3\\nthe potential inﬂuence of oscillation frequency was minimized\\nand maximized, respectively. Moreover, as an additional method\\nto\\nassess\\nthe\\nrelation\\nbetween\\nphonation\\nfrequency\\nand\\nclassiﬁcation outcome further class structures were examined\\nincorporating different frequency intervals (see Table 4). For this\\npurpose, the overall training set S1 was subdivided into n ¼\\n2; . . . ; 4\\nfrequency\\nclasses,\\nwhile\\nentirely\\ndisregarding\\nthe\\nunderlying diagnoses. Hence, the resulting classiﬁcation tasks\\nconsisted in building models of frequency membership. With\\nthe two training sets S2;3 and the classiﬁcation tasks C6\\x078 the\\ninﬂuence of the frequency range on classiﬁcation accuracy was\\nevaluated.\\nFurthermore, to obtain balanced class distributions for classiﬁ-\\ncation task C1 undersampling was performed (marked by \\n) [38].\\nBy randomly selecting 50% of the data of the two remaining classes\\n(i.e. hyper- and hypofunctional examples), respectively, and\\nmerging them into one counter-class (i.e. pathological) equal\\nclass sizes were established. To compensate for random side effects\\ncaused by undersampling multiple sampling runs were performed,\\neach resulting in a different training set conﬁguration. The\\nindividual results were averaged to obtain reliable estimates of\\nclassiﬁcation accuracy.\\nTable 3\\nConsidered training sets, classiﬁcation tasks and their respective class distributions. In classiﬁcation task C1 the * indicates merged and undersampled classes.\\nClassiﬁcation tasks\\nTraining sets\\nS1: mixed frequency data\\nS2: homogeneous frequency interval\\nS3: inhomogeneous frequency interval\\nC1: healthy vs. pathological (hyper [ hypo)\\n25–25*\\n15–15*\\n15–15*\\nC2: hyper vs. hypo\\n25–25\\n15–15\\n15–15\\nC3: healthy vs. hyper\\n25–25\\n15–15\\n15–15\\nC4: healthy vs. hypo\\n25–25\\n15–15\\n15–15\\nC5: healthy vs. hyper vs. hypo\\n25–25–25\\n15–15–15\\n15–15–15\\nTable 2\\nFeature sets derived from the HS recordings of the patients’ vocal fold movements. While F1 contains all the features capturing shape and symmetry of the normalized PVG\\ncycles, F2 comprises the reference features derived from the glottal signal. Feature set F3 is obtained by joining F1 and F2, yielding another reference description for the\\nidentiﬁcation of beneﬁcial feature combinations.\\nFeature set\\nContained features\\nUnderlying signal\\nF1: PVG features\\n¯Oa\\n0:5;sðOa\\n0:5Þ; ¯Ca\\n0:5;sðCa\\n0:5Þ; ¯PO;0:5;sðPO;0:5Þ; ¯PC;0:5;sðPC;0:5Þ; ¯DO;0:5;sðDO;0:5Þ; ¯DC;0:5;sðDC;0:5Þ\\nPVG (2d)\\nF2: traditional features\\n¯Qo;sðQoÞ; ¯Qs;sðQsÞ; ¯Qg;sðQgÞ; ¯It p;sðIt pÞ; ¯Ia p;sð¯Ia pÞ\\nGlottis (1d)\\nF3: combined features\\nF1 [ F2\\nPVG (2d) + glottis (1d)\\nTable 4\\nDifferent frequency interval classes derived from the 75 examples of training set S1\\nand the resulting class distributions.\\nClassiﬁcation tasks\\nS1: all frequency data\\nC6: high vs. low\\n37–38\\nC7: high vs. medium vs. low\\n25–25–25\\nC8: high vs. upper medium vs.\\nlower medium vs. low\\n18–19–19–19\\nFig. 5. Distribution of the 75 training examples of dataset S1 regarding frequency\\nand class membership. The boxes’ position indicates the respective dataset’s\\nfundamental frequency.\\nD. Voigt et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 51–59\\n55\\n\\n3.4.2. Machine learning and evaluation\\nThe derived datasets were subsequently used as an input to an\\ninductive learning scheme which built model descriptions of the\\ndata. To this end, a SVM with a Gaussian radial basis function\\nkernel was applied to the training sets [28,39]. On the basis of the\\nresulting nonlinear SVM models the most likely class label was\\nassigned to an unseen feature vector which was withheld from the\\nprocess of model building.\\nAppropriate SVM parameters were determined by an evolu-\\ntionary strategy optimization procedure [29,40]. The parameter\\nspace of SVM cost parameter C and the width g of the radial basis\\nfunction kernel was automatically searched in order to obtain best\\nclassiﬁcation results [41]. The models’ classiﬁcation accuracy was\\nevaluated via 10-fold cross-validation with stratiﬁcation [42]. In\\nthis manner, the individual results were compared to each other,\\nyielding the best performing classiﬁcation task and feature set. The\\napplied learning scheme is illustrated in Fig. 6.\\n4. Results\\n4.1. Mixed frequency data\\nIn Fig. 7 the results of classiﬁcation tasks C1\\x075 are shown for\\nfeature sets F1\\x073. The respective models were trained using the\\noverall training set S1\\n(without considering the underlying\\nfrequency distribution).\\nThe average classiﬁcation result obtained by PVG features F1\\nexceed the one of glottal features F2 with high signiﬁcance\\n(78:5 \\x04 15:8% vs. 73:5 \\x04 15:7%,\\np ¼ 0:009). Hence, vocal fold\\nmovement patterns described by PVG features can be distin-\\nguished more readily in terms of healthy and pathological behavior\\nthan through the use of glottal features. A combination of both\\nfeature description approaches does not improve classiﬁcation\\nperformance signiﬁcantly (F3: 77:6 \\x04 14:6%, p > 0:05), and hence,\\nis not further considered in the results. Classiﬁcation tasks C3 and\\nC4 yielded better average accuracies than C1 (77.5% and 76.9% vs.\\n74.4%). Thus, for classiﬁcation purposes, it is beneﬁcial to consider\\nthe different types of functional voice disorders individually than\\nto merge them into one class. The results’ high standard deviations\\ncan be ascribed to the relatively small amount of evaluation data\\navailable in the individual folds of cross-validation (n ¼ 7:5). So,\\ndepending on the performed split of the training data, classiﬁca-\\ntion results with high variability are obtained.\\n4.2. Frequency classiﬁcation\\nPhonation\\nfrequency\\nexerts\\na\\ndistinct\\ninﬂuence\\non\\nthe\\nmeasurement of vocal fold movement [36,37]. Therefore, the\\noscillation patterns are possibly subject to change due to frequency\\nalterations. According to this, a subject’s vocal fold movement\\npattern ought to be automatically assigned to its appropriate\\nfrequency interval with high accuracy solely based on the\\ninformation of its spatio-temporal shape. This means that side\\neffects caused by differing phonation frequencies may have a\\nstronger inﬂuence on the classiﬁcation results obtained via\\ntraining set S1 than the disturbed movement pattern of the\\ndisease itself. Thus, it must be assumed that the results presented\\nin Fig. 7 are potentially biased by frequency outcomes. In order to\\nassess the actual frequency effect on the data, the examples were\\narranged into a new class structure by considering only the\\nmembership to certain frequency intervals as class information. In\\nFig. 8 the classiﬁcation results obtained for the according\\nclassiﬁcation tasks C6\\x078 are shown.\\nThe classiﬁcation results of the frequency classes outperform\\nthe results of the healthy/dysfunctional classiﬁcation presented in\\nFig. 7 (2-class average: 81:1 \\x04 12:0% vs. 76:0 \\x04 23:6%). Conse-\\nquently, the data of the healthy and pathological examples need to\\nbe analyzed explicitly taking into consideration the frequency\\nFig. 6. Workﬂow diagram of the overall classiﬁcation process employed to assess\\nthe performance of the different feature sets.\\nFig. 7. Classiﬁcation results obtained through the total amount of 75 datasets for\\ntraining and evaluation. The two dashed horizontal lines at 50% and 33.3% denote\\nthe accuracy levels which can be reached by classifying all examples as the majority\\nclass, respectively. The error bars indicate the results’ standard deviations arising\\nfrom cross-validation.\\nFig. 8. Classiﬁcation results obtained by grouping the total amount of 75 training\\nexamples into classes educed from different frequency intervals. The dashed\\nhorizontal lines mark the baseline accuracy achievable by classifying all cases as the\\nmajority class.\\nD. Voigt et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 51–59\\n56\\n\\neffect outlined above. Furthermore, the PVG feature set F1 yields\\nbetter results than the glottal features F2 throughout all frequency\\nclassiﬁcation tasks.\\n4.3. Different frequency intervals\\nTo assess the inﬂuence of varying phonation frequencies on the\\ndiscrimination of healthy and pathological examples, the classiﬁ-\\ncation performance of F1 and F2 were examined individually for\\nthe following training sets: all data without regarding frequency at\\nall (S1), a subset with weakened frequency inﬂuence (S2), and a\\nsubset with intensiﬁed frequency inﬂuence (S3). In Fig. 9 the\\naveraged 2-class results of the different frequency intervals are\\ncontrasted with each other. For averaging only the results of C2\\x074\\nwere considered, as the pathological class underlying C1 contains\\nexamples from both functional classes and its inclusion would\\nyield an overoptimistic performance estimate.\\nClassiﬁcation results of training set S1 in Fig. 9 are signiﬁcantly\\nexceeded by the ones obtained through subset S2 (F1: 78.5% vs.\\n80.9%;\\nF2:\\n73.5%\\nvs. 81.7%;\\np ¼ 0:015).\\nThus, healthy and\\npathological vocal fold movements can be differentiated with\\nhigher reliability if homogeneous frequency intervals are exam-\\nined. Classiﬁcation results obtained through feature set F1 are\\nrelatively stable for all three examined training sets, resulting in a\\nstandard deviation of 1.3% across frequency intervals. In homoge-\\nnized subset S2 PVG features F1 and glottal features F2 perform\\nequally well in average. Despite exhibiting a particular improve-\\nment between training sets S1 and S2 (+8.2%), the relative decline of\\nF2 in the S3 results is much stronger than for F1 (\\x0711.0% vs. \\x072.2%).\\nAt this, the error bar for F2 using inhomogeneous training set S3\\neven falls below the baseline accuracy of 50%. Thus, in total the\\nclassiﬁcation results of glottal features F2 are more sensitive to\\noccurring frequency inﬂuences than PVG features F1 (standard\\ndeviation across training sets: 5.7%). Due to this fact, PVG features\\nwere found to be more suitable to describe laryngeal dynamics\\nunder varying frequency distributions than glottal features.\\n4.4. Discussion\\nAn important ﬁnding of this study was that PVG feature set F1\\noutperforms glottal feature set F2. Thus, the beneﬁt of considering\\nthe overall vocal fold movement pattern over time as represented\\nin PVGs could be shown with high statistical signiﬁcance. To focus\\nonly on feature descriptions of the changing glottal area yields\\nsuboptimal classiﬁcation results. The reason for this can be seen in\\nthe fact that F2 describes the laryngeal dynamics only at a rather\\ncoarse level, and as a consequence, does not capture the necessary\\ndetails of vocal fold vibration. Moreover, it lacks the ability to\\ndistinguish the two vocal fold sides, and thus, to measure left-right\\nasymmetries. Due to this, classiﬁcation results obtained through\\nglottal features F2 are clearly surpassed by PVG features F1 in\\ntraining sets S1 and S3 (see Fig. 9). In addition, F2 possesses a\\ndistinct sensitivity to phonation frequency, as the quite large\\nvariability of the obtained results reveals. The combination of both\\nfeature\\nset\\napproaches\\nas\\nrepresented\\nin\\nF3\\nachieves\\nno\\nclassiﬁcation improvement.\\nThe subsumption of the two types of functional voice disorders\\ninto one class (classiﬁcation task C1) has been shown to be\\nobstructive in terms of differentiating between healthy and\\npathological vocal fold movement. Therefore, it is beneﬁcial to\\nanalyze and model hyper- and hypofunctional diagnoses individ-\\nually (classiﬁcation tasks C3 and C4), as the corresponding\\nclassiﬁcation results are better than in the class pooling approach.\\nIn the hyperfunctional case the patient’s muscular tension tends to\\nbe increased, whereas in the hypofunctional case it is considerably\\nreduced [16]. Since healthy laryngeal dynamics are essentially\\nsituated between these two diseased states, the merging of both\\ndiagnoses into one class results in an increased overlap of\\npathological and healthy examples. Accordingly, the identiﬁcation\\nof adequate class boundaries is aggravated by merging functional\\ndiagnoses.\\nFrom the fact that classiﬁcation accuracy could be improved by\\nfocusing on homogeneous frequency data it may be concluded that\\na constant phonation frequency should be seeked for during\\nexamination. However, a mixture of phonation frequencies is a\\nmore realistic assumption in a standard clinical setting, as habitual\\npitch phonation plays an important part in making proper\\ndiagnoses of voice disorders. Hence, under these clinical con-\\nsiderations the proposed PVG features have been shown to be more\\nreliable for the identiﬁcation of dysfunctional behavior than the set\\nof glottal parameters. The frequency-dependent classiﬁcation\\nperformance can be accounted for in practice by including\\nsupplementary features capturing relevant phonation frequency\\ninformation which facilitates the discrimination of vocal fold\\nmovement patterns. So the presented approach shows a lot of\\npromise in regard to the characterization and classiﬁcation of\\nfunctional voice disorders. For the analysis of organic disorders, the\\nPVG features are actually expected to perform at least equally well\\nin terms of distinguishing healthy and pathological cases. So, for\\ninstance, for the identiﬁcation of vocal fold paresis it is of particular\\nimportance to capture the left-right asymmetries—a property\\nwhich cannot be described with the glottal signal. Notwithstand-\\ning, in future studies the HS recordings of the patients should be\\nmade under more controlled phonation conditions in order to\\nallow for a systematic analysis of the frequency inﬂuence.\\nAs the PVG method presented in this work is a very novel\\napproach to the classiﬁcation of moving vocal folds, only relatively\\nfew comparable results can be found in the literature. Some works\\nfocus on the extraction of features from the glottal signal and the\\nsubsequent identiﬁcation of normal and dysphonic feature ranges\\n(e.g. [19]). But most commonly, the voice signal of a patient is\\nrecorded and analyzed using acoustic features. So, for example, in\\n[43] a classiﬁcation accuracy of 80% is reported by applying\\nartiﬁcial neural networks to training data derived from the audio\\nsignal of 120 individuals. By analyzing voice signal features Awan\\net al. achieved an accuracy of 74.6% for the classiﬁcation of healthy\\nand functional examples into voice quality classes using stepwise\\ndiscriminant analysis [44]. A sophisticated method from nonlinear\\ndynamical systems theory combined with quadratic discriminant\\nFig. 9. Average 2-class classiﬁcation results obtained with PVG feature set F1 and\\nglottal feature set F2, respectively. In building the models different training sets\\nwere applied varying in terms of considered frequency distribution.\\nD. Voigt et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 51–59\\n57\\n\\nanalysis yields results as good as 91.8% [45]. As a matter of fact,\\nother studies employing logistic regression analysis [46] and\\nkernel principal component analysis [47] report on errorless\\nclassiﬁcation.\\nIn the context of these works, the following facts should be kept\\nin mind. Firstly, in contrast to organic voice disorders, the clinical\\npicture of functional voice disorders is pretty vague and cannot be\\neasily covered by a distinct set of diagnostic rules which apply\\nunder any clinical circumstances (see [16,12,13]). As a result, the\\nsubjective assessment of the symptoms can lead to quite low inter-\\nrater reliability. So the PVG description approach presented here is\\na promising step towards the objectiﬁcation of clinical criteria\\nunderlying the diagnostic process of functional voice disorders. By\\nmeans of numerical PVG features extracted from laryngeal HS\\nvideos a level of inter-individual comparability is achieved which\\neffectively enables the realization of evidence-based medicine in\\nthe ﬁeld of clinical voice diagnosis.\\nSecondly, only vibratory vocal fold information was utilized.\\nThe results obtained from the classiﬁcation of PVG cycle shape\\nfeatures may possibly be improved by incorporating additional\\ninformation characterizing a patient’s vocal fold behavior, phona-\\ntion frequency or acoustic outcome. So, for example, in further\\nstudies the PVG features can be combined with parameters derived\\nfrom the recorded voice signal [48] or features capturing the\\ntransient oscillations during phonation onset [49].\\nThirdly, another essential aspect that needs to be regarded is the\\nexistence of the superimposed phonation frequency effect. Its\\ninﬂuence on classiﬁcation accuracy of vocal fold movement patterns\\nwas substantiated through the improving results shown in Fig. 9.\\nEven though the frequency inﬂuence on the data could be reduced to\\na certain extent by narrowing down the bandwidth of the analyzed\\nHS videos to 82 Hz in the homogeneous frequency interval, its effect\\nis still existent in the data. As a consequence, the results obtained\\nfrom training set S2 may still be affected by the frequency inﬂuence.\\n5. Conclusion\\nA novel method for capturing the movement of vocal folds was\\npresented which can be used to discriminate healthy and\\npathological vibration modes. To this end, the laryngeal dynamics\\nof a collective of individuals with normal voices and diagnosed\\nfunctional voice disorders were recorded with state-of-the-art HS\\nexamination technique. The resulting videos were automatically\\nanalyzed and transformed into PVGs. Subsequently, the movement\\npatterns contained in these PVG representation were described by\\na set of PVG features capturing spatio-temporal shape and\\nsymmetry of vocal fold oscillation. As a way to evaluate the\\ndescriptiveness of the derived PVG features, another set of\\ntraditional glottal parameters was determined for the HS data.\\nThese two feature sets were used as a basis for building nonlinear\\nmodels of the healthy and pathological examples by employing a\\nSVM. Thus, different classiﬁcation tasks that are relevant to the\\ndiagnosis of functional voice disorders were analyzed. This allowed\\nto draw conclusions with respect to the adequacy of the feature\\nsets and the general classiﬁcation accuracy.\\nThe features computed from PVGs are more suitable for the\\ndifferentiation of voice disorders than the traditional glottal\\nparameters. The average classiﬁcation results based on PVG\\nfeatures yield considerably better results throughout all consid-\\nered learning tasks and exhibit higher stability under different\\nclinical conditions. In average, a classiﬁcation accuracy of 81% was\\nobtained\\nfor\\n2-class\\ntasks\\nconcerning\\nthe\\nidentiﬁcation\\nof\\nfunctional dysphonia. This is very promising given the vague\\nclinical picture of the disease and its difﬁcult subjective diagnosis.\\nA further ﬁnding of this study is that the choice of the phonation\\nfrequency plays an important role in the process of discriminating\\nhealthy and pathological behavior, and thus, needs to be especially\\nconsidered during analysis. By and large, the presented approach\\nto combine knowledge-based feature extraction techniques with\\nmethods from machine learning in order to develop objective\\nmedical decision support systems can be regarded as being\\nsuccessful and should be reﬁned in the future.\\nAcknowledgments\\nThis work was supported by Deutsche Forschungsgemeinschaft\\n(DFG) grants no. LO1413/2 1-3 and EY15/11 3-4. The authors\\nwould like to thank the High Performance Computing Group at\\nRegionales Rechenzentrum Erlangen (RRZE) for providing compu-\\ntational resources.\\nReferences\\n[1] Hoppe U. Mechanisms of hoarseness—visualization and interpretation by\\nmeans of nonlinear dynamics. Aachen, Germany: Shaker; 2001.\\n[2] Dejonckere PH, Bradley P, Clemente P, Cornut G, Crevier-Buchman L, Friedrich\\nG. A basic protocol for functional assessment of voice pathology, especially for\\ninvestigating the efﬁcacy of (phonosurgical) treatments and evaluating new\\nassessment techniques. guideline elaborated by the committee on phoniatrics\\nof the European laryngological society (els). Eur Arch Otorhinolaryngol\\n2001;258(February 2):77–82.\\n[3] van Michel C, Pﬁster KA, Luchsinger R. Electroglottography and slow-motion\\nﬁlms of the larynx, comparison of results. Folia Phoniatr 1970;22(2):81–91.\\n[4] Childers DG, Larar JN. Electroglottography for laryngeal function assessment\\nand speech analysis. IEEE Trans Biomed Eng 1984;31(December 12):807–17.\\n[5] Raes J, Lebrun Y, Clement P. Videostroboscopy of the larynx. Acta Otorhino-\\nlaryngol Belg 1986;40(2):421–5.\\n[6] Sˇvec JG, Schutte HK. Videokymography: high-speed line scanning of vocal fold\\nvibration. J Voice 1996;10(June 2):201–5.\\n[7] Wittenberg T, Tigges M, Mergell P, Eysholdt U. Functional imaging of vocal fold\\nvibration: digital multislice high-speed kymography. J Voice 2000;14(Septem-\\nber 3):422–42.\\n[8] Olthoff A, Woywod C, Kruse E. Stroboscopy versus high-speed glottography: a\\ncomparative study. Laryngoscope 2007;117(June 6):1123–6.\\n[9] Yumoto E. Aerodynamics, voice quality, and laryngeal image analysis of\\nnormal and pathologic voices. Curr Opin Otolaryngol Head Neck Surg\\n2004;12(June 3):166–73.\\n[10] Deliyski DD, Petrushev PP, Bonilha HS, Gerlach TT, Martin-Harris B, Hillman\\nRE. Clinical implementation of laryngeal high-speed videoendoscopy: chal-\\nlenges and evolution. Folia Phoniatr Logop 2008;60(1):33–44.\\n[11] Eysholdt U, Rosanowski F, Hoppe U. Vocal fold vibration irregularities caused\\nby different types of laryngeal asymmetry. Eur Arch Otorhinolaryngol\\n2003;260(September 8):412–7.\\n[12] Morrison MD, Nichol H, Rammage LA. Diagnostic criteria in functional dys-\\nphonia. Laryngoscope 1986;96(January 1):1–8.\\n[13] Altman KW, Atkinson C, Lazarus C. Current and emerging concepts in muscle\\ntension dysphonia: a 30-month review. J Voice 2005;19(June 2):261–7.\\n[14] Verikas A, Gelzinis A, Bacauskiene M, Uloza V. Towards a computer-aided\\ndiagnosis system for vocal cord diseases. Artif Intell Med 2006;36(January 1):\\n71–84.\\n[15] Rosen CA, Murry T. Nomenclature of voice disorders and vocal pathology.\\nOtolaryngol Clin North Am 2000;33(October 5):1035–46.\\n[16] Wendler J, Seidner W, Eysholdt U. Lehrbuch der Phoniatrie und Pa¨daudiologie,\\n4th ed., Stuttgart, Germany: Thieme; 2005.\\n[17] Seifert E, Kollbrunner J. Stress and distress in non-organic voice disorder. Swiss\\nMed Wkly 2005;135(July 27/28):387–97.\\n[18] Sˇvec JG, Sram F, Schutte HK. Videokymography in voice disorders: what to look\\nfor? Ann Otol Rhinol Laryngol 2007;116(March 3):172–80.\\n[19] Qiu Q, Schutte HK, Gu L, Yu Q. An automatic method to quantify the vibration\\nproperties of human vocal folds via videokymography. Folia Phoniatr Logop\\n2003;55(3):128–36.\\n[20] Wurzbacher T, Do¨llinger M, Schwarz R, Hoppe U, Eysholdt U, Lohscheller J.\\nSpatiotemporal classiﬁcation of vocal fold dynamics by a multimass model\\ncomprising time-dependent parameters. J Acoust Soc Am 2008;123(April\\n4):2324–34.\\n[21] Mergell P, Herzel H, Titze IR. Irregular vocal-fold vibration—high-speed ob-\\nservation and modeling. J Acoust Soc Am 2000;108(December 6):2996–3002.\\n[22] Yan Y, Ahmad K, Kunduk M, Bless D. Analysis of vocal-fold vibrations from\\nhigh-speed laryngeal images using a hilbert transform-based methodology. J\\nVoice 2005;19(June 2):161–75.\\n[23] Yan Y, Damrose E, Bless D. Functional analysis of voice using simultaneous high-\\nspeed imaging and acoustic recordings. J Voice 2007;21(September 5):604–16.\\n[24] Zhang Y, Tao C, Jiang JJ. Parameter estimation of an asymmetric vocal-fold\\nsystem from glottal area time series using chaos synchronization. Chaos\\n2006;16(June 2):023118.\\n[25] Lohscheller J, Toy H, Rosanowski F, Eysholdt U, Do¨llinger M. Clinically\\nevaluated procedure for the reconstruction of vocal fold vibrations from\\nD. Voigt et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 51–59\\n58\\n\\nendoscopic digital high-speed videos. Med Image Anal 2007;11(August 4):\\n400–13.\\n[26] Lohscheller J, Eysholdt U, Toy H, Do¨llinger M. Phonovibrography: mapping\\nhigh-speed movies of vocal fold vibrations into 2-d diagrams for visualizing\\nand analyzing the underlying laryngeal dynamics. IEEE Trans Med Imaging\\n2008;27(March 3):300–9.\\n[27] Lohscheller J, Eysholdt U. Phonovibrogram visualization of entire vocal fold\\ndynamics. Laryngoscope 2008;118(April 4):753–8.\\n[28] Vapnik VN. The nature of statistical learning theory. New York, NY, USA:\\nSpringer-Verlag New York, Inc.; 1995.\\n[29] Yang SY, Huang Q, Li LL, Ma CY, Zhang H, Bai R. An integrated scheme for\\nfeature selection and parameter setting in the support vector machine model-\\ning and its application to the prediction of pharmacokinetic properties of\\ndrugs. Artif Intell Med 2009;46(June 2):155–63.\\n[30] Asl BM, Setarehdan SK, Mohebbi M. Support vector machine-based arrhythmia\\nclassiﬁcation using reduced features of heart rate variability signal. Artif Intell\\nMed 2008;44(September 1):51–64.\\n[31] Titze IR. Workshop on acoustic voice analysis: summary statement, 1995.\\n[32] Sapienza CM, Stathopoulos ET, Dromey C. Approximations of open quotient\\nand speed quotient from glottal airﬂow and egg waveforms: effects of mea-\\nsurement criteria and sound pressure level. J Voice 1998;12(March 1):31–43.\\n[33] Bielamowicz S, Kapoor R, Schwartz J, Stager SV. Relationship among glottal\\narea, static supraglottic compression, and laryngeal function studies in uni-\\nlateral vocal fold paresis and paralysis. J Voice 2004;18(March 1):138–45.\\n[34] Henrich N, D’Alessandro C, Doval B, Castellengo M. Glottal open quotient in\\nsinging: measurements and correlation with laryngeal mechanisms, vocal\\nintensity, and fundamental frequency. J Acoust Soc Am 2005;117(March 3 Pt\\n1):1417–30.\\n[35] Deliyski DD, Shaw HS, Evans MK, Vesselinov R. Regression tree approach to\\nstudying factors inﬂuencing acoustic voice analysis. Folia Phoniatr Logop\\n2006;58(4):274–88.\\n[36] Orlikoff RF, Baken RJ. Consideration of the relationship between the funda-\\nmental frequency of phonation and vocal jitter. Folia Phoniatr (Basel)\\n1990;42(1):31–40.\\n[37] Rasp O, Lohscheller J, Do¨llinger M, Eysholdt U, Hoppe U. The pitch rise\\nparadigm: a new task for real-time endoscopy of non-stationary phonation.\\nFolia Phoniatr Logop 2006;58(3):175–85.\\n[38] Japkowicz N, Stephen S. The class imbalance problem: a systematic study.\\nIntell Data Anal 2002;6(5):429–49.\\n[39] Burges CJC. A tutorial on support vector machines for pattern recognition. Data\\nMin Knowl Discov 1998;2(2):121–67.\\n[40] Beyer HG, Schwefel HP. Evolution strategies—a comprehensive introduction.\\nNat Comput 2002;1(May):3–52.\\n[41] Hsu CW, Chang CC, Lin CJ. A practical guide to support vector classiﬁcation.\\nTechnical report, Department of Computer Science and Information Engineer-\\ning, National Taiwan University, 2003.\\n[42] Kohavi R. A study of cross-validation and bootstrap for accuracy estimation\\nand model selection. In: IJCAI. 1995. p. 1137–45.\\n[43] Linder R, Albers AE, Hess M, Po¨ppl SJ, Scho¨nweiler R. Artiﬁcial neural network-\\nbased classiﬁcation to screen for dysphonia using psychoacoustic scaling of\\nacoustic voice features. J Voice 2008;22(March 2):155–63.\\n[44] Awan SN, Roy N. Acoustic prediction of voice type in women with functional\\ndysphonia. J Voice 2005;19(June 2):268–82.\\n[45] Little MA, McSharry PE, Roberts SJ, Costello DAE, Moroz IM. Exploiting non-\\nlinear recurrence and fractal scaling properties for voice disorder detection.\\nBiomed Eng Online 2007;6:23.\\n[46] Eadie TL, Doyle PC. Classiﬁcation of dysphonic voice: acoustic and auditory-\\nperceptual measures. J Voice 2005;19(March 1):1–14.\\n[47] Alvarez M, Henao R, Castellanos G, Godino JI, Orozco A. Kernel principal\\ncomponent analysis through time for voice disorder classiﬁcation. Conf Proc\\nIEEE Eng Med Biol Soc 2006;1:5511–4.\\n[48] Do¨llinger M, Lohscheller J, McWhorter A, Kunduk M. Variability of normal\\nvocal fold dynamics for different vocal loading in one healthy subject\\ninvestigated by phonovibrograms. J Voice 2009;23(March 2):175–81.\\n[49] Braunschweig T, Flaschka J, Schelhorn-Neise P, Do¨llinger M. High-speed\\nvideo\\nanalysis of\\nthe\\nphonation\\nonset,\\nwith\\nan\\napplication to\\nthe\\ndiagnosis of functional dysphonias. Med Eng Phys 2008;30(January 1):\\n59–66.\\nD. Voigt et al. / Artiﬁcial Intelligence in Medicine 49 (2010) 51–59\\n59\\n'}, {'source': '/Users/sir/Downloads/Data/PDF/A machine learning-based approach to prognostic analysis of thoracic transplantations - ScienceDirec.pdf', 'text': 'Artiﬁcial Intelligence in Medicine\\nVolume 49, Issue 1, May 2010, Pages 33-42\\nA machine learning-based approach to\\nprognostic analysis of thoracic\\ntransplantations\\nDursun Delen \\n, Asil Oztekin  , Zhenyu (James) Kong \\nShow more\\nAdd to Mendeley\\nhttps://doi.org/10.1016/j.artmed.2010.01.002\\nGet rights and content\\nAbstract\\nObjective\\nThe prediction of survival time after organ transplantations and prognosis analysis of\\ndifferent risk groups of transplant patients are not only clinically important but also\\ntechnically challenging. The current studies, which are mostly linear modeling-based\\nstatistical analyses, have focused on small sets of disparate predictive factors where\\nmany potentially important variables are neglected in their analyses. Data mining\\nmethods, such as machine learning-based approaches, are capable of providing an\\neffective way of overcoming these limitations by utilizing sufficiently large data sets with\\nmany predictive factors to identify not only linear associations but also highly complex,\\nnon-linear relationships. Therefore, this study is aimed at exploring risk groups of\\nthoracic recipients through machine learning-based methods.\\nMethods and material\\nA large, feature-rich, nation-wide thoracic transplantation dataset (obtained from the\\nUnited Network for Organ Sharingâ€”UNOS) is used to develop predictive models for the\\nsurvival time estimation. The predictive factors that are most relevant to the survival\\ntime identified via, (1) conducting sensitivity analysis on models developed by the\\nmachine learning methods, (2) extraction of variables from the published literature, and\\n(3) eliciting variables from the medical experts and other domain specific knowledge\\nbases. A unified set of predictors is then used to develop a Cox regression model and the\\nrelated prognosis indices. A comparison of clustering algorithm-based and conventional\\nrisk grouping techniques is conducted based on the outcome of the Cox regression model\\nin order to identify optimal number of risk groups of thoracic recipients. Finally, the\\nKaplanâ€“Meier survival analysis is performed to validate the discrimination among the\\nidentified various risk groups.\\nResults\\nThe machine learning models performed very effectively in predicting the survival time:\\nthe support vector machine model with a radial basis Kernel function produced the best\\nfit with an R  value of 0.879, the artificial neural network (multilayer perceptron-MLP-\\nmodel) came the second with an R  value of 0.847, and the M5 algorithm-based\\nregression tree model came last with an R  value of 0.785. Following the proposed\\nmethod, a consolidated set of predictive variables are determined and used to build the\\nCox survival model. Using the prognosis indices revealed by the Cox survival model along\\nwith a k-means clustering algorithm, an optimal number of â€œthreeâ€ risk groups is\\nidentified. The significance of differences among these risk groups are also validated\\nusing the Kaplanâ€“Meier survival analysis.\\nConclusions\\nThis study demonstrated that the integrated machine learning method to select the\\npredictor variables is more effective in developing the Cox survival models than the\\ntraditional methods commonly found in the literature. The significant distinction among\\nthe risk groups of thoracic patients also validates the effectiveness of the methodology\\nproposed herein. We anticipate that this study (and other AI based analytic studies like\\nthis one) will lead to more effective analyses of thoracic transplant procedures to better\\nunderstand the prognosis of thoracic organ recipients. It would potentially lead to new\\nmedical and biological advances and more effective allocation policies in the field of\\norgan transplantation.\\nIntroduction\\nThoracic (heart and lung) transplantation has been accepted as a viable treatment for\\nend-stage cardiac and pulmonary failure. The increased experience in cardiac and\\npulmonary transplantation, improvements in patient selection, organ preservation, and\\npreoperative support have significantly reduced the early threats to patient survival [1].\\nOver the past decade, the thoracic transplant waiting time for a listed patient has\\nmarkedly increased, but the number of transplants performed has declined. In addition,\\nthe research also found that there is a perceived inequity in access to organs. The organ\\nallocation system needs to be improved since it may become a major factor negatively\\ninfluencing the survivability of thoracic transplant [2].\\nThe survivability prediction is becoming increasingly more important in medicine. When\\na resource is scarce, the need for accurate prediction becomes acute [3]. Especially\\nprediction of survival time and prognosis prediction of medical treatments are clinically\\nimportant and challenging problems [4]. Scarceness of organs necessitates the\\ndevelopment of effective and efficient procedures to select the most optimal organ\\nreceiver since demand for organs of all patients might not be satisfied. To achieve this,\\none critical step is to reveal the knowledge underlying huge amount of data collected and\\nstored from organ transplantation procedures performed in the past. The objectives are\\n(1) to maximize the patientsâ€™ survival time after the organ transplantation surgery,\\nand (2) to optimize the prognosis for the organ recipients. These can be potentially\\nachieved by discovering the knowledge that may be contained in large dataset consisting\\nof more than hundreds of determinative variables regarding the donors, the potential\\nrecipients, and transplantation procedures. Therefore, in this study a data mining method\\nis proposed to process large amount of transplantation data obtained from UNOS to\\nidentify the important factors as well as their relationships to the survival of the graft\\nand the patient. Thereafter, a prognostic index [5], [6] is developed to classify the\\npatients into different risk groups for better understanding of the transplantation\\nphenomenon. In short, this study will address the following questions: (1) what are the\\nmost important variables to be included in an effective prognostic index related to\\nthoracic organ transplantations? (2) what are the most coherent risk groups that can be\\nformed based on the prognostic index? Predicting the thoracic survivability and\\nclassifying the patients (potential thoracic organ receivers) into different classes of risks\\nwould help decision makers in determining patientsâ€™ priority for transplantation\\nsource assignment.\\nIn the recent past, a number of studies were conducted using data-driven analytics on\\nvarious organ transplantation datasets. Closely related to the study reported herein,\\nHariharan et al. [7] focused on the analysis of improved graft survival rate using\\ncyclosporine after renal transplantation in both short-term (less than 1 year) and long-\\nterm (more than 1 year). A regression analysis was used to predict the probability of the\\ngraft failure after kidney transplantation in both short-term and long-term period in the\\nlight of demographic characteristics, transplant-related variables, and post-\\ntransplantation variables. The study performed by Herrero et al. [8] included 116 patients\\nwho received a liver transplant between the years 1994 and 2000. Statistical tests are\\nused to compare the demographic and characteristic variables, pretransplant, and intra-\\noperative variables between the two groups, namely younger and older than 60. The\\nresults indicate that there is a clear trend showing that older patients have lower survival\\nafter liver transplantation. Hong et al. [9] presented a survival analysis of liver transplant\\npatients in Canada by considering some factors such as age, blood type, donor type\\n(cadaveric or alive), race, and gender of recipient and donors. However, having limited\\nthe variables with this scope, they also admitted that the clinical information lacks of\\nmany potential details.\\nTaking a data mining approach, Kusiak et al. [10] compared two rule-based data mining\\ntechniques, i.e. decision trees and rough sets, to predict survival time of kidney dialysis\\npatients. This study achieved satisfactorily high prediction accuracy. The main limitation\\nof the study was the utilization of a small dataset with only 188 patients in total and also\\nmany patient-related parameters were neglected in the problem formulation. Using\\nmore traditional methods, and specifically having focused on thoracic transplantation,\\nJenkins et al. [11] and Fernandez-Yanez et al. [12] had a rich pool of independent\\nvariables for survivability prediction. Their studies used popular statistical techniques\\nsuch as Kaplanâ€“Meier method of survival analysis with Mantelâ€“Haenszel log-rank\\ntest. However, both of these techniques have been criticized with two major limitations:\\n(1) linear relationships are assumed, which hence cannot capture the nonlinearity among\\nthe variables, and (2) the independent variables were selected solely based on the\\nexperiences and intuitions of the analysts who conducted these studies. Thus, many\\npotentially significant variables might be left outside the scope of this study. Tjang et al.\\n[13] added more explanatory variables to determine the survivability in heart\\ntransplantation, such as body mass index, waiting time on the list, and previous cardiac\\nsurgery, their study also ignored the non-linear relationships among the pool of\\nsurvivability-related variables. Similar limitations exist in some other studies focused\\ndirectly or indirectly on thoracic transplantation [14], [15], [16].\\nThe existing studies implicitly assume that the relationships among the predictive\\nvariables and output variable are linear and the predictor variables are independent of\\neach other, which may not be valid in reality. Moreover, the abovementioned studies\\nfocus on small datasets with limited number of predictors for survivability of patients\\nafter transplantation. This limitation may cause incomprehensive modeling due to the\\ninsufficient information contents (i.e., omission of a number of potentially important\\npredictor variables).\\nPrognostic index (PI) provides compact prognosis information regarding a specific\\npatient based on the results of a Cox proportional hazards model [5]. Cox proportional\\nhazards model helps identify variables of prognostic importance and hence prognostic\\nindex can be used to define groups of individuals at different risk categories. Even though\\nprognostic index is a convenient tool to measure how well the patients are doing after\\nthe transplantation, its use in the organ transplantation area has been limited mostly due\\nto the lack of follow-up data. Some existing studies related to devising a PI in transplant\\narea are summarized as follows.\\nIn the study conducted by Christensen et al. [17], it is mentioned that primary biliary\\ncirrhosis requires a liver transplantation operation at the end stage. Based on the\\nprognosis analysis with as well as without transplantation, it is decided whether or not\\nthe transplantation is required, if so when. To achieve this goal, corresponding PIs and\\nprobabilities of surviving are computed for transplantation and non-transplantation\\ncases. Yoo et al. [18] developed a similar index and revealed that socioeconomic status\\ndoes not influence patient or graft survival that undergoes liver transplantation at the\\ninstitute where they performed their study. Deng et al. [19] conducted a study with a\\nnational dataset in Germany, which discovers the effect of receiving a heart transplant for\\nthe patients in a waiting list. The results indicate that cardiac transplant is associated\\nwith survival benefit only for patients with a predicted high risk of dying on the waiting\\nlist. Ghobrial et al. [20] performed a study to determine prognostic factors for overall\\nsurvival in 107 adult patients with post-transplantation lymphoproliferative disorders\\n(PTLDs). It is validated that in discriminating the low and high scored patients the\\nproposed prognostic scoring significantly performs better than the International\\nPrognostic Index for the subset of the patients (56 out of 107) with lactate\\ndehydrogenase.\\nThe common limitation in all of these studies is similar to the limitations of the studies\\nsummarized in Section 1.2.1. Namely, they directly devise a prognostic index without\\ndetermining if the variables used in prognostic index devising phase are necessary and\\nsufficient. This motivates a machine learning-based initial step of variable selection\\nprocedure. Because, if the critical predictive factors are not captured effectively due to\\nthe intuition- and experience-based selection, the resulting prognostic indices developed\\nbased on the selected variables would be inaccurate and, in turn, related risk groups of\\npatients would be deviated from the real classes. This may cause mistakes for decision\\nmaker in making organ transplantation policies.\\nSection snippets\\nProposed method\\nSection 1.2 shows that the most of the existing studies for organ transplantation\\nprocedures utilize conventional statistical approaches such as Kaplanâ€“Meier function\\nand log-rank test along with expert-selected variables to predict the survivability.\\nHowever, organ transplantation procedures consist of a large number of variables\\n(several hundred) that may have nontrivial impact on modeling the prognosis of the\\ngrafts/patients. Using a somewhat comprehensive variable list may help discriminate â€¦\\nThe case study and discussion\\nIn order to demonstrate and validate the proposed methodology in Section 2, two most\\npopular data mining toolkit are used, namely SPSS PASW Modeler\\n [48] and SAS 9.1.3\\n[49] statistical software package. Using the UNOS data set, Sections 3.1 Predictive model\\nresults, 3.2 Determination of the candidate covariates for Cox regression model, 3.3\\nDeployment of Cox regression model and devising the prognostic indices, 3.4 Clustering\\nthe prognostic indices, 3.5 Validation of risk groups byâ€¦\\nConclusions and future research directions\\nThis study demonstrates that machine learning-based methodology for selecting\\npredictor variables in survivability and prognostic modeling of thoracic organ\\ntransplantation is superior to the approaches adopting only expert-selected variables.\\nThe study showed that of the comprehensive list of predictors, some have been included\\nin the previous studies (such as gender and age of the recipient, his/her medical\\ncondition at registration) while some others (which are found to be critical) haveâ€¦\\nReferences (53)\\nR.N. Pierson et al.\\nThoracic organ transplantation\\nAmerican Journal of Transplantation (2004)\\nD. Sheppard et al.\\nPredicting cytomegalovirus disease after renal transplantation: an artificial\\nneural network approach\\nInternational Journal of Medical Informatics (1999)\\nR.S. Lin et al.\\nSingle and multiple time-point prediction models in kidney transplant\\noutcomes\\nJournal of Biomedical Informatics (2008)\\nJ.I. Herrero et al.\\nLiver transplant recipients older than 60 years have lower survival and higher\\nincidence of malignancy\\nAmerican Journal of Transplantation (2003)\\nZ. Hong et al.\\nSurvival analysis of liver transplant patients in Canada\\nTransplantation Proceedings (2006)\\nA. Kusiak et al.\\nPredicting survival time for kidney dialysis patients: a data mining approach\\nComputers in Biology and Medicine (2005)\\nP.C. Jenkins et al.\\nSurvival analysis and risk factors for mortality in transplantation and staged\\nsurgery for hypoplastic left heart syndrome\\nJournal of the American College of Cardiology (2000)\\nJ. Fernandez-Yanez et al.\\nPrognosis of heart transplant candidates stabilized on medical therapy\\nRevista Espanola de Cardiologia (2005)\\nJ.T. Cope et al.\\nA cost comparison of heart transplantation versus alternative operations for\\ncardiomyopathy\\nAnnual thoracic Surgery (2001)\\nJ. Aguero et al.\\nDifferences in clinical profile and survival after heart transplantation according\\nto prior heart disease\\nTransplantation Proceedings (2007)\\nView more references\\nCited by (58)\\nSurvival analysis for pediatric heart transplant patients using a novel machine\\nlearning algorithm: A UNOS analysis\\n2023, Journal of Heart and Lung Transplantation\\nShow abstract\\nUnmanned aerial vehicle (UAV) imaging and machine learning applications for\\nplant phenotyping\\n2023, Computers and Electronics in Agriculture\\nShow abstract\\nArtificial intelligence, big data and heart transplantation: Actualities\\n2023, International Journal of Medical Informatics\\nShow abstract\\nMachine learning using institution-specific multi-modal electronic health\\nrecords improves mortality risk prediction for cardiac surgery patients\\n2023, JTCVS Open\\nShow abstract\\nLong-term mortality risk stratification of liver transplant recipients: real-time\\napplication of deep learning algorithms on longitudinal data\\n2021, The Lancet Digital Health\\nShow abstract\\nModern Internet Search Analytics and Total Joint Arthroplasty: What Are\\nPatients Asking and Reading Online?\\n2021, Journal of Arthroplasty\\nShow abstract\\nView all citing articles on Scopus\\nView full text\\nCopyright Â© 2010 Elsevier B.V. All rights reserved.\\nFLAS: Fuzzy lung allocation system for US-\\nbased transplantations\\nEuropean Journal of Operational Research, Volume 24…\\nLina Al-Ebbini, â€¦, Yao Chen\\nPredicting heart transplantation outcomes\\nthrough data analytics\\nDecision Support Systems, Volume 94, 2017, pp. 42-52\\nAli Dag, â€¦, Fadel M. Megahed\\nThe lung allocation score and other\\navailable models lack predictive accuracy…\\nThe Journal of Heart and Lung Transplantation, Volu…\\nJay M. Brahmbhatt, â€¦, Kathleen J. Ramos\\nShow 3 more articles\\nAbout ScienceDirect\\nRemote access\\nShopping cart\\nAdvertise\\nContact and support\\nTerms and conditions\\nPrivacy policy\\nWe use cookies to help provide and enhance our service and tailor content and ads. By continuing you agree to the use of cookies.\\nAll content on this site: Copyright Â© 2023 Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access\\ncontent, the Creative Commons licensing terms apply.\\nArticle preview\\nAbstract\\nIntroduction\\nSection snippets\\nReferences (53)\\nCited by (58)\\na\\nb c\\nb\\nShare\\nCite\\n2\\n2\\n2\\nÂ®\\nÂ®\\nRecommended articles\\nPurchase PDF\\nAccess throughÂ\\xa0DePaul University\\nOther access options\\nRegister\\nSign in\\nJournals & Books\\nSearch ScienceDirect\\n'}], Label: 1.0\n",
      "Texts: [{'source': '/Users/sir/Downloads/Data/PDF/A machine learning-based approach to prognostic analysis of thoracic transplantations - ScienceDirec.pdf', 'text': 'Artiﬁcial Intelligence in Medicine\\nVolume 49, Issue 1, May 2010, Pages 33-42\\nA machine learning-based approach to\\nprognostic analysis of thoracic\\ntransplantations\\nDursun Delen \\n, Asil Oztekin  , Zhenyu (James) Kong \\nShow more\\nAdd to Mendeley\\nhttps://doi.org/10.1016/j.artmed.2010.01.002\\nGet rights and content\\nAbstract\\nObjective\\nThe prediction of survival time after organ transplantations and prognosis analysis of\\ndifferent risk groups of transplant patients are not only clinically important but also\\ntechnically challenging. The current studies, which are mostly linear modeling-based\\nstatistical analyses, have focused on small sets of disparate predictive factors where\\nmany potentially important variables are neglected in their analyses. Data mining\\nmethods, such as machine learning-based approaches, are capable of providing an\\neffective way of overcoming these limitations by utilizing sufficiently large data sets with\\nmany predictive factors to identify not only linear associations but also highly complex,\\nnon-linear relationships. Therefore, this study is aimed at exploring risk groups of\\nthoracic recipients through machine learning-based methods.\\nMethods and material\\nA large, feature-rich, nation-wide thoracic transplantation dataset (obtained from the\\nUnited Network for Organ Sharingâ€”UNOS) is used to develop predictive models for the\\nsurvival time estimation. The predictive factors that are most relevant to the survival\\ntime identified via, (1) conducting sensitivity analysis on models developed by the\\nmachine learning methods, (2) extraction of variables from the published literature, and\\n(3) eliciting variables from the medical experts and other domain specific knowledge\\nbases. A unified set of predictors is then used to develop a Cox regression model and the\\nrelated prognosis indices. A comparison of clustering algorithm-based and conventional\\nrisk grouping techniques is conducted based on the outcome of the Cox regression model\\nin order to identify optimal number of risk groups of thoracic recipients. Finally, the\\nKaplanâ€“Meier survival analysis is performed to validate the discrimination among the\\nidentified various risk groups.\\nResults\\nThe machine learning models performed very effectively in predicting the survival time:\\nthe support vector machine model with a radial basis Kernel function produced the best\\nfit with an R  value of 0.879, the artificial neural network (multilayer perceptron-MLP-\\nmodel) came the second with an R  value of 0.847, and the M5 algorithm-based\\nregression tree model came last with an R  value of 0.785. Following the proposed\\nmethod, a consolidated set of predictive variables are determined and used to build the\\nCox survival model. Using the prognosis indices revealed by the Cox survival model along\\nwith a k-means clustering algorithm, an optimal number of â€œthreeâ€ risk groups is\\nidentified. The significance of differences among these risk groups are also validated\\nusing the Kaplanâ€“Meier survival analysis.\\nConclusions\\nThis study demonstrated that the integrated machine learning method to select the\\npredictor variables is more effective in developing the Cox survival models than the\\ntraditional methods commonly found in the literature. The significant distinction among\\nthe risk groups of thoracic patients also validates the effectiveness of the methodology\\nproposed herein. We anticipate that this study (and other AI based analytic studies like\\nthis one) will lead to more effective analyses of thoracic transplant procedures to better\\nunderstand the prognosis of thoracic organ recipients. It would potentially lead to new\\nmedical and biological advances and more effective allocation policies in the field of\\norgan transplantation.\\nIntroduction\\nThoracic (heart and lung) transplantation has been accepted as a viable treatment for\\nend-stage cardiac and pulmonary failure. The increased experience in cardiac and\\npulmonary transplantation, improvements in patient selection, organ preservation, and\\npreoperative support have significantly reduced the early threats to patient survival [1].\\nOver the past decade, the thoracic transplant waiting time for a listed patient has\\nmarkedly increased, but the number of transplants performed has declined. In addition,\\nthe research also found that there is a perceived inequity in access to organs. The organ\\nallocation system needs to be improved since it may become a major factor negatively\\ninfluencing the survivability of thoracic transplant [2].\\nThe survivability prediction is becoming increasingly more important in medicine. When\\na resource is scarce, the need for accurate prediction becomes acute [3]. Especially\\nprediction of survival time and prognosis prediction of medical treatments are clinically\\nimportant and challenging problems [4]. Scarceness of organs necessitates the\\ndevelopment of effective and efficient procedures to select the most optimal organ\\nreceiver since demand for organs of all patients might not be satisfied. To achieve this,\\none critical step is to reveal the knowledge underlying huge amount of data collected and\\nstored from organ transplantation procedures performed in the past. The objectives are\\n(1) to maximize the patientsâ€™ survival time after the organ transplantation surgery,\\nand (2) to optimize the prognosis for the organ recipients. These can be potentially\\nachieved by discovering the knowledge that may be contained in large dataset consisting\\nof more than hundreds of determinative variables regarding the donors, the potential\\nrecipients, and transplantation procedures. Therefore, in this study a data mining method\\nis proposed to process large amount of transplantation data obtained from UNOS to\\nidentify the important factors as well as their relationships to the survival of the graft\\nand the patient. Thereafter, a prognostic index [5], [6] is developed to classify the\\npatients into different risk groups for better understanding of the transplantation\\nphenomenon. In short, this study will address the following questions: (1) what are the\\nmost important variables to be included in an effective prognostic index related to\\nthoracic organ transplantations? (2) what are the most coherent risk groups that can be\\nformed based on the prognostic index? Predicting the thoracic survivability and\\nclassifying the patients (potential thoracic organ receivers) into different classes of risks\\nwould help decision makers in determining patientsâ€™ priority for transplantation\\nsource assignment.\\nIn the recent past, a number of studies were conducted using data-driven analytics on\\nvarious organ transplantation datasets. Closely related to the study reported herein,\\nHariharan et al. [7] focused on the analysis of improved graft survival rate using\\ncyclosporine after renal transplantation in both short-term (less than 1 year) and long-\\nterm (more than 1 year). A regression analysis was used to predict the probability of the\\ngraft failure after kidney transplantation in both short-term and long-term period in the\\nlight of demographic characteristics, transplant-related variables, and post-\\ntransplantation variables. The study performed by Herrero et al. [8] included 116 patients\\nwho received a liver transplant between the years 1994 and 2000. Statistical tests are\\nused to compare the demographic and characteristic variables, pretransplant, and intra-\\noperative variables between the two groups, namely younger and older than 60. The\\nresults indicate that there is a clear trend showing that older patients have lower survival\\nafter liver transplantation. Hong et al. [9] presented a survival analysis of liver transplant\\npatients in Canada by considering some factors such as age, blood type, donor type\\n(cadaveric or alive), race, and gender of recipient and donors. However, having limited\\nthe variables with this scope, they also admitted that the clinical information lacks of\\nmany potential details.\\nTaking a data mining approach, Kusiak et al. [10] compared two rule-based data mining\\ntechniques, i.e. decision trees and rough sets, to predict survival time of kidney dialysis\\npatients. This study achieved satisfactorily high prediction accuracy. The main limitation\\nof the study was the utilization of a small dataset with only 188 patients in total and also\\nmany patient-related parameters were neglected in the problem formulation. Using\\nmore traditional methods, and specifically having focused on thoracic transplantation,\\nJenkins et al. [11] and Fernandez-Yanez et al. [12] had a rich pool of independent\\nvariables for survivability prediction. Their studies used popular statistical techniques\\nsuch as Kaplanâ€“Meier method of survival analysis with Mantelâ€“Haenszel log-rank\\ntest. However, both of these techniques have been criticized with two major limitations:\\n(1) linear relationships are assumed, which hence cannot capture the nonlinearity among\\nthe variables, and (2) the independent variables were selected solely based on the\\nexperiences and intuitions of the analysts who conducted these studies. Thus, many\\npotentially significant variables might be left outside the scope of this study. Tjang et al.\\n[13] added more explanatory variables to determine the survivability in heart\\ntransplantation, such as body mass index, waiting time on the list, and previous cardiac\\nsurgery, their study also ignored the non-linear relationships among the pool of\\nsurvivability-related variables. Similar limitations exist in some other studies focused\\ndirectly or indirectly on thoracic transplantation [14], [15], [16].\\nThe existing studies implicitly assume that the relationships among the predictive\\nvariables and output variable are linear and the predictor variables are independent of\\neach other, which may not be valid in reality. Moreover, the abovementioned studies\\nfocus on small datasets with limited number of predictors for survivability of patients\\nafter transplantation. This limitation may cause incomprehensive modeling due to the\\ninsufficient information contents (i.e., omission of a number of potentially important\\npredictor variables).\\nPrognostic index (PI) provides compact prognosis information regarding a specific\\npatient based on the results of a Cox proportional hazards model [5]. Cox proportional\\nhazards model helps identify variables of prognostic importance and hence prognostic\\nindex can be used to define groups of individuals at different risk categories. Even though\\nprognostic index is a convenient tool to measure how well the patients are doing after\\nthe transplantation, its use in the organ transplantation area has been limited mostly due\\nto the lack of follow-up data. Some existing studies related to devising a PI in transplant\\narea are summarized as follows.\\nIn the study conducted by Christensen et al. [17], it is mentioned that primary biliary\\ncirrhosis requires a liver transplantation operation at the end stage. Based on the\\nprognosis analysis with as well as without transplantation, it is decided whether or not\\nthe transplantation is required, if so when. To achieve this goal, corresponding PIs and\\nprobabilities of surviving are computed for transplantation and non-transplantation\\ncases. Yoo et al. [18] developed a similar index and revealed that socioeconomic status\\ndoes not influence patient or graft survival that undergoes liver transplantation at the\\ninstitute where they performed their study. Deng et al. [19] conducted a study with a\\nnational dataset in Germany, which discovers the effect of receiving a heart transplant for\\nthe patients in a waiting list. The results indicate that cardiac transplant is associated\\nwith survival benefit only for patients with a predicted high risk of dying on the waiting\\nlist. Ghobrial et al. [20] performed a study to determine prognostic factors for overall\\nsurvival in 107 adult patients with post-transplantation lymphoproliferative disorders\\n(PTLDs). It is validated that in discriminating the low and high scored patients the\\nproposed prognostic scoring significantly performs better than the International\\nPrognostic Index for the subset of the patients (56 out of 107) with lactate\\ndehydrogenase.\\nThe common limitation in all of these studies is similar to the limitations of the studies\\nsummarized in Section 1.2.1. Namely, they directly devise a prognostic index without\\ndetermining if the variables used in prognostic index devising phase are necessary and\\nsufficient. This motivates a machine learning-based initial step of variable selection\\nprocedure. Because, if the critical predictive factors are not captured effectively due to\\nthe intuition- and experience-based selection, the resulting prognostic indices developed\\nbased on the selected variables would be inaccurate and, in turn, related risk groups of\\npatients would be deviated from the real classes. This may cause mistakes for decision\\nmaker in making organ transplantation policies.\\nSection snippets\\nProposed method\\nSection 1.2 shows that the most of the existing studies for organ transplantation\\nprocedures utilize conventional statistical approaches such as Kaplanâ€“Meier function\\nand log-rank test along with expert-selected variables to predict the survivability.\\nHowever, organ transplantation procedures consist of a large number of variables\\n(several hundred) that may have nontrivial impact on modeling the prognosis of the\\ngrafts/patients. Using a somewhat comprehensive variable list may help discriminate â€¦\\nThe case study and discussion\\nIn order to demonstrate and validate the proposed methodology in Section 2, two most\\npopular data mining toolkit are used, namely SPSS PASW Modeler\\n [48] and SAS 9.1.3\\n[49] statistical software package. Using the UNOS data set, Sections 3.1 Predictive model\\nresults, 3.2 Determination of the candidate covariates for Cox regression model, 3.3\\nDeployment of Cox regression model and devising the prognostic indices, 3.4 Clustering\\nthe prognostic indices, 3.5 Validation of risk groups byâ€¦\\nConclusions and future research directions\\nThis study demonstrates that machine learning-based methodology for selecting\\npredictor variables in survivability and prognostic modeling of thoracic organ\\ntransplantation is superior to the approaches adopting only expert-selected variables.\\nThe study showed that of the comprehensive list of predictors, some have been included\\nin the previous studies (such as gender and age of the recipient, his/her medical\\ncondition at registration) while some others (which are found to be critical) haveâ€¦\\nReferences (53)\\nR.N. Pierson et al.\\nThoracic organ transplantation\\nAmerican Journal of Transplantation (2004)\\nD. Sheppard et al.\\nPredicting cytomegalovirus disease after renal transplantation: an artificial\\nneural network approach\\nInternational Journal of Medical Informatics (1999)\\nR.S. Lin et al.\\nSingle and multiple time-point prediction models in kidney transplant\\noutcomes\\nJournal of Biomedical Informatics (2008)\\nJ.I. Herrero et al.\\nLiver transplant recipients older than 60 years have lower survival and higher\\nincidence of malignancy\\nAmerican Journal of Transplantation (2003)\\nZ. Hong et al.\\nSurvival analysis of liver transplant patients in Canada\\nTransplantation Proceedings (2006)\\nA. Kusiak et al.\\nPredicting survival time for kidney dialysis patients: a data mining approach\\nComputers in Biology and Medicine (2005)\\nP.C. Jenkins et al.\\nSurvival analysis and risk factors for mortality in transplantation and staged\\nsurgery for hypoplastic left heart syndrome\\nJournal of the American College of Cardiology (2000)\\nJ. Fernandez-Yanez et al.\\nPrognosis of heart transplant candidates stabilized on medical therapy\\nRevista Espanola de Cardiologia (2005)\\nJ.T. Cope et al.\\nA cost comparison of heart transplantation versus alternative operations for\\ncardiomyopathy\\nAnnual thoracic Surgery (2001)\\nJ. Aguero et al.\\nDifferences in clinical profile and survival after heart transplantation according\\nto prior heart disease\\nTransplantation Proceedings (2007)\\nView more references\\nCited by (58)\\nSurvival analysis for pediatric heart transplant patients using a novel machine\\nlearning algorithm: A UNOS analysis\\n2023, Journal of Heart and Lung Transplantation\\nShow abstract\\nUnmanned aerial vehicle (UAV) imaging and machine learning applications for\\nplant phenotyping\\n2023, Computers and Electronics in Agriculture\\nShow abstract\\nArtificial intelligence, big data and heart transplantation: Actualities\\n2023, International Journal of Medical Informatics\\nShow abstract\\nMachine learning using institution-specific multi-modal electronic health\\nrecords improves mortality risk prediction for cardiac surgery patients\\n2023, JTCVS Open\\nShow abstract\\nLong-term mortality risk stratification of liver transplant recipients: real-time\\napplication of deep learning algorithms on longitudinal data\\n2021, The Lancet Digital Health\\nShow abstract\\nModern Internet Search Analytics and Total Joint Arthroplasty: What Are\\nPatients Asking and Reading Online?\\n2021, Journal of Arthroplasty\\nShow abstract\\nView all citing articles on Scopus\\nView full text\\nCopyright Â© 2010 Elsevier B.V. All rights reserved.\\nFLAS: Fuzzy lung allocation system for US-\\nbased transplantations\\nEuropean Journal of Operational Research, Volume 24…\\nLina Al-Ebbini, â€¦, Yao Chen\\nPredicting heart transplantation outcomes\\nthrough data analytics\\nDecision Support Systems, Volume 94, 2017, pp. 42-52\\nAli Dag, â€¦, Fadel M. Megahed\\nThe lung allocation score and other\\navailable models lack predictive accuracy…\\nThe Journal of Heart and Lung Transplantation, Volu…\\nJay M. Brahmbhatt, â€¦, Kathleen J. Ramos\\nShow 3 more articles\\nAbout ScienceDirect\\nRemote access\\nShopping cart\\nAdvertise\\nContact and support\\nTerms and conditions\\nPrivacy policy\\nWe use cookies to help provide and enhance our service and tailor content and ads. By continuing you agree to the use of cookies.\\nAll content on this site: Copyright Â© 2023 Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access\\ncontent, the Creative Commons licensing terms apply.\\nArticle preview\\nAbstract\\nIntroduction\\nSection snippets\\nReferences (53)\\nCited by (58)\\na\\nb c\\nb\\nShare\\nCite\\n2\\n2\\n2\\nÂ®\\nÂ®\\nRecommended articles\\nPurchase PDF\\nAccess throughÂ\\xa0DePaul University\\nOther access options\\nRegister\\nSign in\\nJournals & Books\\nSearch ScienceDirect\\n'}, {'source': '/Users/sir/Downloads/Data/PDF/DeepBoost.pdf', 'text': 'Deep Boosting\\nCorinna Cortes\\nCORINNA@GOOGLE.COM\\nGoogle Research, 111 8th Avenue, New York, NY 10011\\nMehryar Mohri\\nMOHRI@CIMS.NYU.EDU\\nCourant Institute and Google Research, 251 Mercer Street, New York, NY 10012\\nUmar Syed\\nUSYED@GOOGLE.COM\\nGoogle Research, 111 8th Avenue, New York, NY 10011\\nAbstract\\nWe present a new ensemble learning algorithm,\\nDeepBoost, which can use as base classiﬁers a\\nhypothesis set containing deep decision trees, or\\nmembers of other rich or complex families, and\\nsucceed in achieving high accuracy without over-\\nﬁtting the data. The key to the success of the al-\\ngorithm is a capacity-conscious criterion for the\\nselection of the hypotheses. We give new data-\\ndependent learning bounds for convex ensembles\\nexpressed in terms of the Rademacher complexi-\\nties of the sub-families composing the base clas-\\nsiﬁer set, and the mixture weight assigned to each\\nsub-family. Our algorithm directly beneﬁts from\\nthese guarantees since it seeks to minimize the\\ncorresponding learning bound. We give a full de-\\nscription of our algorithm, including the details\\nof its derivation, and report the results of several\\nexperiments showing that its performance com-\\npares favorably to that of AdaBoost and Logistic\\nRegression and their L1-regularized variants.\\n1. Introduction\\nEnsemble methods are general techniques in machine\\nlearning for combining several predictors or experts to\\ncreate a more accurate one.\\nIn the batch learning set-\\nting, techniques such as bagging, boosting, stacking, error-\\ncorrection techniques, Bayesian averaging, or other av-\\neraging schemes are prominent instances of these meth-\\nods (Breiman, 1996; Freund & Schapire, 1997; Smyth &\\nWolpert, 1999; MacKay, 1991; Freund et al., 2004). En-\\nsemble methods often signiﬁcantly improve performance\\nin practice (Quinlan, 1996; Bauer & Kohavi, 1999; Caru-\\nana et al., 2004; Dietterich, 2000; Schapire, 2003) and ben-\\nProceedings of the 31 st International Conference on Machine\\nLearning, Beijing, China, 2014. JMLR: W&CP volume 32. Copy-\\nright 2014 by the author(s).\\neﬁt from favorable learning guarantees. In particular, Ad-\\naBoost and its variants are based on a rich theoretical anal-\\nysis, with performance guarantees in terms of the margins\\nof the training samples (Schapire et al., 1997; Koltchinskii\\n& Panchenko, 2002).\\nStandard ensemble algorithms such as AdaBoost combine\\nfunctions selected from a base classiﬁer hypothesis set H.\\nIn many successful applications of AdaBoost, H is reduced\\nto the so-called boosting stumps, that is decision trees of\\ndepth one.\\nFor some difﬁcult tasks in speech or image\\nprocessing, simple boosting stumps are not sufﬁcient to\\nachieve a high level of accuracy. It is tempting then to use\\na more complex hypothesis set, for example the set of all\\ndecision trees with depth bounded by some relatively large\\nnumber. But, existing learning guarantees for AdaBoost\\ndepend not only on the margin and the number of the\\ntraining examples, but also on the complexity of H mea-\\nsured in terms of its VC-dimension or its Rademacher com-\\nplexity (Schapire et al., 1997; Koltchinskii & Panchenko,\\n2002).\\nThese learning bounds become looser when us-\\ning too complex base classiﬁer sets H. They suggest a\\nrisk of overﬁtting which indeed can be observed in some\\nexperiments with AdaBoost (Grove & Schuurmans, 1998;\\nSchapire, 1999; Dietterich, 2000; R¨atsch et al., 2001b).\\nThis paper explores the design of alternative ensemble al-\\ngorithms using as base classiﬁers a hypothesis set H that\\nmay contain very deep decision trees, or members of some\\nother very rich or complex families, and that can yet suc-\\nceed in achieving a higher performance level. Assume that\\nthe set of base classiﬁers H can be decomposed as the\\nunion of p disjoint families H1, . . . , Hp ordered by increas-\\ning complexity, where Hk, k 2 [1, p], could be for example\\nthe set of decision trees of depth k, or a set of functions\\nbased on monomials of degree k. Figure 1 shows a pictorial\\nillustration. Of course, if we strictly conﬁne ourselves to\\nusing hypotheses belonging only to families Hk with small\\nk, then we are effectively using a smaller base classiﬁer set\\nH with favorable guarantees. But, to succeed in some chal-\\n\\nDeep Boosting\\nH1\\nH2\\nH3\\nH4\\nH5\\nH1\\nH1[H2\\n· · ·\\nH1[· · · [Hp\\nFigure 1. Base classiﬁer set H decomposed in terms of sub-\\nfamilies H1, . . . , Hp or their unions.\\nlenging tasks, the use of a few more complex hypotheses\\ncould be needed. The main idea behind the design of our\\nalgorithms is that an ensemble based on hypotheses drawn\\nfrom H1, . . . , Hp can achieve a higher accuracy by making\\nuse of hypotheses drawn from Hks with large k if it allo-\\ncates more weights to hypotheses drawn from Hks with a\\nsmall k. But, can we determine quantitatively the amounts\\nof mixture weights apportioned to different families? Can\\nwe provide learning guarantees for such algorithms?\\nNote that our objective is somewhat reminiscent of that of\\nmodel selection, in particular Structural Risk Minimization\\n(SRM) (Vapnik, 1998), but it differs from that in that we\\ndo not wish to limit our base classiﬁer set to some optimal\\nHq = Sq\\nk=1 Hk. Rather, we seek the freedom of using as\\nbase hypotheses even relatively deep trees from rich Hks,\\nwith the promise of doing so infrequently, or that of re-\\nserving them a somewhat small weight contribution. This\\nprovides the ﬂexibility of learning with deep hypotheses.\\nWe present a new algorithm, DeepBoost, whose design is\\nprecisely guided by the ideas just discussed. Our algorithm\\nis grounded in a solid theoretical analysis that we present\\nin Section 2. We give new data-dependent learning bounds\\nfor convex ensembles. These guarantees are expressed in\\nterms of the Rademacher complexities of the sub-families\\nHk and the mixture weight assigned to each Hk, in ad-\\ndition to the familiar margin terms and sample size. Our\\ncapacity-conscious algorithm is derived via the application\\nof a coordinate descent technique seeking to minimize such\\nlearning bounds. We give a full description of our algo-\\nrithm, including the details of its derivation and its pseu-\\ndocode (Section 3) and discuss its connection with previ-\\nous boosting-style algorithms. We also report the results of\\nseveral experiments (Section 4) demonstrating that its per-\\nformance compares favorably to that of AdaBoost, which\\nis known to be one of the most competitive binary classiﬁ-\\ncation algorithms.\\n2. Data-dependent learning guarantees for\\nconvex ensembles with multiple hypothesis\\nsets\\nNon-negative linear combination ensembles such as boost-\\ning or bagging typically assume that base functions are se-\\nlected from the same hypothesis set H. Margin-based gen-\\neralization bounds were given for ensembles of base func-\\ntions taking values in {−1, +1} by Schapire et al. (1997) in\\nterms of the VC-dimension of H. Tighter margin bounds\\nwith simpler proofs were later given by Koltchinskii &\\nPanchenko (2002), see also (Bartlett & Mendelson, 2002),\\nfor the more general case of a family H taking arbitrary\\nreal values, in terms of the Rademacher complexity of H.\\nHere, we also consider base hypotheses taking arbitrary\\nreal values but assume that they can be selected from sev-\\neral distinct hypothesis sets H1, . . . , Hp with p ≥1 and\\npresent margin-based learning in terms of the Rademacher\\ncomplexity of these sets. Remarkably, the complexity term\\nof these bounds admits an explicit dependency in terms of\\nthe mixture coefﬁcients deﬁning the ensembles. Thus, the\\nensemble family we consider is F = conv(Sp\\nk=1 Hk), that\\nis the family of functions f of the form f = PT\\nt=1 ↵tht,\\nwhere ↵= (↵1, . . . , ↵T ) is in the simplex ∆and where,\\nfor each t 2 [1, T], ht is in Hkt for some kt 2 [1, p].\\nLet X denote the input space. H1, . . . , Hp are thus fam-\\nilies of functions mapping from X to R.\\nWe consider\\nthe familiar supervised learning scenario and assume that\\ntraining and test points are drawn i.i.d. according to some\\ndistribution D over X ⇥{−1, +1} and denote by S =\\n((x1, y1), . . . , (xm, ym)) a training sample of size m drawn\\naccording to Dm.\\nLet ⇢> 0. For a function f taking values in R, we de-\\nnote by R(f) its binary classiﬁcation error, by R⇢(f) its\\n⇢-margin error, and by bRS,⇢(f) its empirical margin error:\\nR(f) =\\nE\\n(x,y)⇠D[1yf(x)\\uf8ff0],\\nR⇢(f) =\\nE\\n(x,y)⇠D[1yf(x)\\uf8ff⇢],\\nbR⇢(f) =\\nE\\n(x,y)⇠S[1yf(x)\\uf8ff⇢],\\nwhere the notation (x, y) ⇠S indicates that (x, y) is drawn\\naccording to the empirical distribution deﬁned by S.\\nThe following theorem gives a margin-based Rademacher\\ncomplexity bound for learning with such functions in the\\nbinary classiﬁcation case. As with other Rademacher com-\\nplexity learning guarantees, our bound is data-dependent,\\nwhich is an important and favorable characteristic of our\\nresults. For p = 1, that is for the special case of a single\\nhypothesis set, the analysis coincides with that of the stan-\\ndard ensemble margin bounds (Koltchinskii & Panchenko,\\n2002).\\nTheorem 1. Assume p > 1. Fix ⇢> 0. Then, for any\\nδ > 0, with probability at least 1 −δ over the choice of\\na sample S of size m drawn i.i.d. according to Dm, the\\nfollowing inequality holds for all f = PT\\nt=1 ↵tht 2 F:\\nR(f) \\uf8ffbRS,⇢(f) + 4\\n⇢\\nT\\nX\\nt=1\\n↵tRm(Hkt)\\n+ 2\\n⇢\\nr\\nlog p\\nm\\n+\\ns⇠4\\n⇢2 log\\nh ⇢2m\\nlog p\\ni⇡log p\\nm\\n+ log 2\\nδ\\n2m .\\n\\nDeep Boosting\\nThus, R(f) \\uf8ffbRS,⇢(f) + 4\\n⇢\\nPT\\nt=1 ↵tRm(Hkt) + C(m, p)\\nwith C(m, p) = O\\n⇣q\\nlog p\\n⇢2m log\\n⇥⇢2m\\nlog p\\n⇤⌘\\n.\\nThis result is remarkable since the complexity term in the\\nright-hand side of the bound admits an explicit depen-\\ndency on the mixture coefﬁcients ↵t. It is a weighted aver-\\nage of Rademacher complexities with mixture weights ↵t,\\nt 2 [1, T]. Thus, the second term of the bound suggests\\nthat, while some hypothesis sets Hk used for learning could\\nhave a large Rademacher complexity, this may not be detri-\\nmental to generalization if the corresponding total mixture\\nweight (sum of ↵ts corresponding to that hypothesis set) is\\nrelatively small. Such complex families offer the potential\\nof achieving a better margin on the training sample.\\nThe theorem cannot be proven via a standard Rademacher\\ncomplexity analysis such as that of Koltchinskii &\\nPanchenko (2002) since the complexity term of the bound\\nwould then be the Rademacher complexity of the family\\nof hypotheses F = conv(Sp\\nk=1 Hk) and would not de-\\npend on the speciﬁc weights ↵t deﬁning a given func-\\ntion f. Furthermore, the complexity term of a standard\\nRademacher complexity analysis is always lower bounded\\nby the complexity term appearing in our bound. Indeed,\\nsince Rm(conv([p\\nk=1Hk)) = Rm([p\\nk=1Hk), the follow-\\ning lower bound holds for any choice of the non-negative\\nmixtures weights ↵t summing to one:\\nRm(F) ≥\\nm\\nmax\\nk=1 Rm(Hk) ≥\\nT\\nX\\nt=1\\n↵tRm(Hkt).\\n(1)\\nThus, Theorem 1 provides a ﬁner learning bound than the\\none obtained via a standard Rademacher complexity anal-\\nysis. The full proof of the theorem is given in Appendix A.\\nOur proof technique exploits standard tools used to de-\\nrive Rademacher complexity learning bounds (Koltchin-\\nskii & Panchenko, 2002) as well as a technique used by\\nSchapire, Freund, Bartlett, and Lee (1997) to derive early\\nVC-dimension margin bounds. Using other standard tech-\\nniques as in (Koltchinskii & Panchenko, 2002; Mohri et al.,\\n2012), Theorem 1 can be straightforwardly generalized to\\nhold uniformly for all ⇢> 0 at the price of an additional\\nterm that is in O\\n⇣q\\nlog log 2\\n⇢\\nm\\n⌘\\n.\\n3. Algorithm\\nIn this section, we will use the learning guarantees of Sec-\\ntion 2 to derive a capacity-conscious ensemble algorithm\\nfor binary classiﬁcation.\\n3.1. Optimization problem\\nLet H1, . . . , Hp be p disjoint families of functions taking\\nvalues in [−1, +1] with increasing Rademacher complex-\\nities Rm(Hk), k 2 [1, p]. We will assume that the hy-\\npothesis sets Hk are symmetric, that is, for any h 2 Hk,\\nwe also have (−h) 2 Hk, which holds for most hypothe-\\nsis sets typically considered in practice. This assumption\\nis not necessary but it helps simplifying the presentation of\\nour algorithm. For any hypothesis h 2 [p\\nk=1Hk, we denote\\nby d(h) the index of the hypothesis set it belongs to, that is\\nh 2 Hd(h). The bound of Theorem 1 holds uniformly for\\nall ⇢> 0 and functions f 2 conv(Sp\\nk=1 Hk).1 Since the\\nlast term of the bound does not depend on ↵, it suggests\\nselecting ↵to minimize\\nG(↵) = 1\\nm\\nm\\nX\\ni=1\\n1yi\\nPT\\nt=1 ↵tht(xi)\\uf8ff⇢+ 4\\n⇢\\nT\\nX\\nt=1\\n↵trt,\\nwhere rt = Rm(Hd(ht)). Since for any ⇢> 0, f and f/⇢\\nadmit the same generalization error, we can instead search\\nfor ↵≥0 with PT\\nt=1 ↵t \\uf8ff1/⇢which leads to\\nmin\\n↵≥0\\n1\\nm\\nm\\nX\\ni=1\\n1yi\\nPT\\nt=1↵tht(xi)\\uf8ff1+4\\nT\\nX\\nt=1\\n↵trt s.t.\\nT\\nX\\nt=1\\n↵t \\uf8ff1\\n⇢.\\nThe ﬁrst term of the objective is not a convex function\\nof ↵and its minimization is known to be computation-\\nally hard. Thus, we will consider instead a convex upper\\nbound. Let u 7! Φ(−u) be a non-increasing convex func-\\ntion upper bounding u 7! 1u\\uf8ff0 with Φ differentiable over\\nR and Φ0(u) 6= 0 for all u. Φ may be selected to be for\\nexample the exponential function as in AdaBoost (Freund\\n& Schapire, 1997) or the logistic function. Using such an\\nupper bound, we obtain the following convex optimization\\nproblem:\\nmin\\n↵≥0\\n1\\nm\\nm\\nX\\ni=1\\nΦ\\n⇣\\n1 −yi\\nT\\nX\\nt=1\\n↵tht(xi)\\n⌘\\n+ λ\\nT\\nX\\nt=1\\n↵trt\\n(2)\\ns.t.\\nT\\nX\\nt=1\\n↵t \\uf8ff1\\n⇢,\\nwhere we introduced a parameter λ ≥0 controlling the bal-\\nance between the magnitude of the values taken by function\\nΦ and the second term. Introducing a Lagrange variable\\nβ ≥0 associated to the constraint in (2), the problem can\\nbe equivalently written as\\nmin\\n↵≥0\\n1\\nm\\nm\\nX\\ni=1\\nΦ\\n⇣\\n1 −yi\\nT\\nX\\nt=1\\n↵tht(xi)\\n⌘\\n+\\nT\\nX\\nt=1\\n(λrt + β)↵t.\\nHere, β is a parameter that can be freely selected by the\\nalgorithm since any choice of its value is equivalent to a\\n1The condition PT\\nt=1 ↵t = 1 of Theorem 1 can be relaxed\\nto PT\\nt=1 ↵t \\uf8ff1. To see this, use for example a null hypothesis\\n(ht = 0 for some t).\\n\\nDeep Boosting\\nchoice of ⇢in (2). Let {h1, . . . , hN} be the set of distinct\\nbase functions, and let G be the objective function based\\non that collection:\\nG(↵)= 1\\nm\\nm\\nX\\ni=1\\nΦ\\n⇣\\n1−yi\\nN\\nX\\nj=1\\n↵jhj(xi)\\n⌘\\n+\\nN\\nX\\nt=1\\n(λrj+β)↵j,\\nwith ↵= (↵1, . . . , ↵N) 2 RN. Note that we can drop the\\nrequirement ↵≥0 since the hypothesis sets are symmetric\\nand ↵tht = (−↵t)(−ht). For each hypothesis h, we keep\\neither h or −h in {h1, . . . , hN}. Using the notation\\n⇤j = λrj + β,\\n(3)\\nfor all j 2 [1, N], our optimization problem can then be\\nrewritten as min↵F(↵) with\\nF(↵)= 1\\nm\\nm\\nX\\ni=1\\nΦ\\n⇣\\n1−yi\\nN\\nX\\nj=1\\n↵jhj(xi)\\n⌘\\n+\\nN\\nX\\nt=1\\n⇤j|↵j|, (4)\\nwith no non-negativity constraint on ↵. The function F\\nis convex as a sum of convex functions and admits a sub-\\ndifferential at all ↵2 R. We can design a boosting-style\\nalgorithm by applying coordinate descent to F(↵).\\nLet ↵t = (↵t,1, . . . , ↵t,N)> denote the vector obtained af-\\nter t ≥1 iterations and let ↵0 = 0. Let ek denote the\\nkth unit vector in RN, k 2 [1, N]. The direction ek and\\nthe step ⌘selected at the tth round are those minimizing\\nF(↵t−1 + ⌘ek), that is\\nF(↵t−1 + ⌘ek)= 1\\nm\\nm\\nX\\ni=1\\nΦ\\n⇣\\n1 −yift−1(xi)−⌘yihk(xi)\\n⌘\\n+\\nX\\nj6=k\\n⇤j|↵t−1,j| + ⇤k|↵t−1,k + ⌘|,\\nwhere ft−1 = PN\\nj=1 ↵t−1,jhj. For any t 2 [1, T], we\\ndenote by Dt the distribution deﬁned by\\nDt(i) = Φ00\\n1 −yift−1(xi)\\n1\\nSt\\n,\\n(5)\\nwhere St is a normalization factor, St = Pm\\ni=1 Φ0(1 −\\nyift−1(xi)). For any s 2 [1, T] and j 2 [1, N], we denote\\nby ✏s,j the weighted error of hypothesis hj for the distribu-\\ntion Ds, for s 2 [1, T]:\\n✏s,j = 1\\n2\\nh\\n1 −\\nE\\ni⇠Ds[yihj(xi)]\\ni\\n.\\n(6)\\n3.2. DeepBoost\\nFigure 2 shows the pseudocode of the algorithm DeepBoost\\nderived by applying coordinate descent to the objective\\nfunction (4). The details of the derivation of the expres-\\nsion are given in Appendix B. In the special cases of the\\nDEEPBOOST(S = ((x1, y1), . . . , (xm, ym)))\\n1\\nfor i  1 to m do\\n2\\nD1(i)  1\\nm\\n3\\nfor t  1 to T do\\n4\\nfor j  1 to N do\\n5\\nif (↵t−1,j 6= 0) then\\n6\\ndj  \\n0\\n✏t,j −1\\n2\\n1\\n+ sgn(↵t−1,j) ⇤jm\\n2St\\n7\\nelseif\\n022✏t,j −1\\n2\\n22 \\uf8ff⇤jm\\n2St\\n1\\nthen\\n8\\ndj  0\\n9\\nelse dj  \\n0\\n✏t,j −1\\n2\\n1\\n−sgn(✏t,j −1\\n2) ⇤jm\\n2St\\n10\\nk  argmax\\nj2[1,N]\\n|dj|\\n11\\n✏t  ✏t,k\\n12\\nif\\n0\\n|(1 −✏t)e↵t−1,k −✏te−↵t−1,k|\\uf8ff⇤km\\nSt\\n1\\nthen\\n13\\n⌘t  −↵t−1,k\\n14\\nelseif\\n0\\n(1 −✏t)e↵t−1,k −✏te−↵t−1,k > ⇤km\\nSt\\n1\\nthen\\n15\\n⌘t  log\\nh\\n−⇤km\\n2✏tSt +\\nq⇥⇤km\\n2✏tSt\\n⇤2+ 1−✏t\\n✏t\\ni\\n16\\nelse ⌘t  log\\nh\\n+ ⇤km\\n2✏tSt +\\nq⇥⇤km\\n2✏tSt\\n⇤2+ 1−✏t\\n✏t\\ni\\n17\\n↵t  ↵t−1 + ⌘tek\\n18\\nSt+1  Pm\\ni=1 Φ00\\n1 −yi\\nPN\\nj=1 ↵t,jhj(xi)\\n1\\n19\\nfor i  1 to m do\\n20\\nDt+1(i)  \\nΦ00\\n1−yi\\nPN\\nj=1 ↵t,jhj(xi)\\n1\\nSt+1\\n21\\nf  PN\\nj=1 ↵T,jhj\\n22\\nreturn f\\nFigure 2. Pseudocode of the DeepBoost algorithm for both the\\nexponential loss and the logistic loss.\\nThe expression of the\\nweighted error ✏t,j is given in (6). In the generic case of a sur-\\nrogate loss Φ different from the exponential or logistic losses, ⌘t\\nis found instead via a line search or other numerical methods from\\n⌘t = argmax⌘F(↵t−1 + ⌘ek).\\nexponential loss (Φ(−u) = exp(−u)) or the logistic loss\\n(Φ(−u) = log2(1 + exp(−u))), a closed-form expression\\nis given for the step size (lines 12-16), which is the same in\\nboth cases (see Sections B.4 and B.5). In the generic case,\\nthe step size ⌘t can be found using a line search or other\\nnumerical methods. Note that when the condition of line\\n12 is satisﬁed, the step taken by the algorithm cancels out\\nthe coordinate along the direction k, thereby leading to a\\nsparser result. This is consistent with the fact that the ob-\\njective function contains a second term based on (weighted)\\nL1-norm, which is favoring sparsity.\\nOur algorithm is related to several other boosting-type al-\\ngorithms devised in the past. For λ = 0 and β = 0 and\\nusing the exponential surrogate loss, it coincides with Ada-\\nBoost (Freund & Schapire, 1997) with precisely the same\\ndirection and same step log\\nhq\\n1−✏t\\n✏t\\ni\\nusing H = Sp\\nk=1 Hk\\nas the hypothesis set for base learners. This corresponds to\\n\\nDeep Boosting\\nignoring the complexity term of our bound as well as the\\ncontrol of the sum of the mixture weights via β. For λ = 0\\nand β = 0 and using the logistic surrogate loss, our algo-\\nrithm also coincides with additive logistic loss (Friedman\\net al., 1998).\\nIn the special case where λ = 0 and β 6= 0 and for\\nthe exponential surrogate loss, our algorithm matches the\\nL1-norm regularized AdaBoost (e.g., see (R¨atsch et al.,\\n2001a)). For the same choice of the parameters and for\\nthe logistic surrogate loss, our algorithm matches the L1-\\nnorm regularized additive Logistic Regression studied by\\nDuchi & Singer (2009) using the base learner hypothesis\\nset H = Sp\\nk=1 Hk. H may in general be very rich. The key\\nfoundation of our algorithm and analysis is instead to take\\ninto account the relative complexity of the sub-families Hk.\\nAlso, note that L1-norm regularized AdaBoost and Logis-\\ntic Regression can be viewed as algorithms minimizing the\\nlearning bound obtained via the standard Rademacher com-\\nplexity analysis (Koltchinskii & Panchenko, 2002), using\\nthe exponential or logistic surrogate losses. Instead, the\\nobjective function minimized by our algorithm is based on\\nthe generalization bound of Theorem 1, which as discussed\\nearlier is a ﬁner bound (see (1)). For λ = 0 but β 6= 0, our\\nalgorithm is also close to the so-called unnormalized Arc-\\ning (Breiman, 1999) or AdaBoost⇢(R¨atsch & Warmuth,\\n2002) using H as a hypothesis set. AdaBoost⇢coincides\\nwith AdaBoost modulo the step size, which is more con-\\nservative than that of AdaBoost and depends on ⇢. R¨atsch\\n& Warmuth (2005) give another variant of the algorithm\\nthat does not require knowing the best ⇢, see also the re-\\nlated work of Kivinen & Warmuth (1999); Warmuth et al.\\n(2006).\\nOur algorithm directly beneﬁts from the learning guaran-\\ntees given in Section 2 since it seeks to minimize the bound\\nof Theorem 1. In the next section, we report the results of\\nour experiments with DeepBoost. Let us mention that we\\nhave also designed an alternative deep boosting algorithm\\nthat we brieﬂy describe and discuss in Appendix C.\\n4. Experiments\\nAn additional beneﬁt of the learning bounds presented in\\nSection 2 is that they are data-dependent. They are based\\non the Rademacher complexity of the base hypothesis sets\\nHk, which in some cases can be well estimated from the\\ntraining sample. The algorithm DeepBoost directly inher-\\nits this advantage. For example, if the hypothesis set Hk\\nis based on a positive deﬁnite kernel with sample matrix\\nKk, it is known that its empirical Rademacher complexity\\ncan be upper bounded by\\np\\nTr[Kk]\\nm\\nand lower bounded by\\n1\\np\\n2\\np\\nTr[Kk]\\nm\\n. In other cases, when Hk is a family of func-\\ntions taking binary values, we can use an upper bound on\\nthe Rademacher complexity in terms of the growth func-\\ntion of Hk, ⇧Hk(m): Rm(Hk) \\uf8ff\\nq\\n2 log ⇧Hk (m)\\nm\\n. Thus,\\nfor the family Hstumps\\n1\\nof boosting stumps in dimension d,\\n⇧Hstumps\\n1\\n(m) \\uf8ff2md, since there are 2m distinct threshold\\nfunctions for each dimension with m points. Thus, the fol-\\nlowing inequality holds:\\nRm(Hstumps\\n1\\n) \\uf8ff\\nr\\n2 log(2md)\\nm\\n.\\n(7)\\nSimilarly, we consider the family of decision trees Hstumps\\n2\\nof depth 2 with the same question at the internal nodes of\\ndepth 1. We have ⇧Hstumps\\n2\\n(m) \\uf8ff(2m)2 d(d−1)\\n2\\nsince there\\nare d(d −1)/2 distinct trees of this type and since each\\ninduces at most (2m)2 labelings. Thus, we can write\\nRm(Hstumps\\n2\\n) \\uf8ff\\nr\\n2 log(2m2d(d −1))\\nm\\n.\\n(8)\\nMore generally, we also consider the family of all binary\\ndecision trees Htrees\\nk\\nof depth k. For this family it is known\\nthat VC-dim(Htrees\\nk\\n) \\uf8ff(2k + 1) log2(d + 1) (Mansour,\\n1997). More generally, the VC-dimension of Tn, the fam-\\nily of decision trees with n nodes in dimension d can be\\nbounded by (2n + 1) log2(d + 2) (see for example (Mohri\\net al., 2012)). Since Rm(H) \\uf8ff\\nq\\n2 VC-dim(H) log(m+1)\\nm\\n,\\nfor any hypothesis class H we have\\nRm(Tn) \\uf8ff\\nr\\n(4n + 2) log2(d + 2) log(m + 1)\\nm\\n.\\n(9)\\nThe experiments with DeepBoost described below use ei-\\nther Hstumps = Hstumps\\n1\\n[ Hstumps\\n2\\nor Htrees\\nK\\n= SK\\nk=1 Htrees\\nk\\n,\\nfor some K > 0, as the base hypothesis sets. For any hy-\\npothesis in these sets, DeepBoost will use the upper bounds\\ngiven above as a proxy for the Rademacher complexity\\nof the set to which it belongs. We leave it to the future\\nto experiment with ﬁner data-dependent estimates or up-\\nper bounds on the Rademacher complexity, which could\\nfurther improve the performance of our algorithm.\\nRe-\\ncall that each iteration of DeepBoost searches for the base\\nhypothesis that is optimal with respect to a certain crite-\\nrion (see lines 5-10 of Figure 2).\\nWhile an exhaustive\\nsearch is feasible for Hstumps\\n1\\n, it would be far too expen-\\nsive to visit all trees in Htrees\\nK\\nwhen K is large. There-\\nfore, when using Htrees\\nK\\nand also Hstumps\\n2\\nas the base hy-\\npotheses we use the following heuristic search procedure\\nin each iteration t: First, the optimal tree h⇤\\n1 2 Htrees\\n1\\nis\\nfound via exhaustive search. Next, for all 1 < k \\uf8ffK,\\na locally optimal tree h⇤\\nk 2 Htrees\\nk\\nis found by consider-\\ning only trees that can be obtained by adding a single layer\\nof leaves to h⇤\\nk−1. Finally, we select the best hypotheses\\nin the set {h⇤\\n1, . . . , h⇤\\nK, h1, . . . , ht−1}, where h1, . . . , ht−1\\nare the hypotheses selected in previous iterations.\\n\\nDeep Boosting\\nTable 1. Results for boosted decision stumps and the exponential loss function.\\nAdaBoost\\nAdaBoost\\nAdaBoost\\nAdaBoost\\nbreastcancer\\nHstumps\\n1\\nHstumps\\n2\\nAdaBoost-L1\\nDeepBoost\\nocr17\\nHstumps\\n1\\nHstumps\\n2\\nAdaBoost-L1\\nDeepBoost\\nError\\n0.0429\\n0.0437\\n0.0408\\n0.0373\\nError\\n0.0085\\n0.008\\n0.0075\\n0.0070\\n(std dev)\\n(0.0248)\\n(0.0214)\\n(0.0223)\\n(0.0225)\\n(std dev)\\n0.0072\\n0.0054\\n0.0068\\n(0.0048)\\nAvg tree size\\n1\\n2\\n1.436\\n1.215\\nAvg tree size\\n1\\n2\\n1.086\\n1.369\\nAvg no. of trees\\n100\\n100\\n43.6\\n21.6\\nAvg no. of trees\\n100\\n100\\n37.8\\n36.9\\nAdaBoost\\nAdaBoost\\nAdaBoost\\nAdaBoost\\nionosphere\\nHstumps\\n1\\nHstumps\\n2\\nAdaBoost-L1\\nDeepBoost\\nocr49\\nHstumps\\n1\\nHstumps\\n2\\nAdaBoost-L1\\nDeepBoost\\nError\\n0.1014\\n0.075\\n0.0708\\n0.0638\\nError\\n0.0555\\n0.032\\n0.03\\n0.0275\\n(std dev)\\n(0.0414)\\n(0.0413)\\n(0.0331)\\n(0.0394)\\n(std dev)\\n0.0167\\n0.0114\\n0.0122\\n(0.0095)\\nAvg tree size\\n1\\n2\\n1.392\\n1.168\\nAvg tree size\\n1\\n2\\n1.99\\n1.96\\nAvg no. of trees\\n100\\n100\\n39.35\\n17.45\\nAvg no. of trees\\n100\\n100\\n99.3\\n96\\nAdaBoost\\nAdaBoost\\nAdaBoost\\nAdaBoost\\ngerman\\nHstumps\\n1\\nHstumps\\n2\\nAdaBoost-L1\\nDeepBoost\\nocr17-mnist\\nHstumps\\n1\\nHstumps\\n2\\nAdaBoost-L1\\nDeepBoost\\nError\\n0.243\\n0.2505\\n0.2455\\n0.2395\\nError\\n0.0056\\n0.0048\\n0.0046\\n0.0040\\n(std dev)\\n(0.0445)\\n(0.0487)\\n(0.0438)\\n(0.0462)\\n(std dev)\\n0.0017\\n0.0014\\n0.0013\\n(0.0014)\\nAvg tree size\\n1\\n2\\n1.54\\n1.76\\nAvg tree size\\n1\\n2\\n2\\n1.99\\nAvg no. of trees\\n100\\n100\\n54.1\\n76.5\\nAvg no. of trees\\n100\\n100\\n100\\n100\\nAdaBoost\\nAdaBoost\\nAdaBoost\\nAdaBoost\\ndiabetes\\nHstumps\\n1\\nHstumps\\n2\\nAdaBoost-L1\\nDeepBoost\\nocr49-mnist\\nHstumps\\n1\\nHstumps\\n2\\nAdaBoost-L1\\nDeepBoost\\nError\\n0.253\\n0.260\\n0.254\\n0.253\\nError\\n0.0414\\n0.0209\\n0.0200\\n0.0177\\n(std dev)\\n(0.0330)\\n(0.0518)\\n(0.04868)\\n(0.0510)\\n(std dev)\\n0.00539\\n0.00521\\n0.00408\\n(0.00438)\\nAvg tree size\\n1\\n2\\n1.9975\\n1.9975\\nAvg tree size\\n1\\n2\\n1.9975\\n1.9975\\nAvg no. of trees\\n100\\n100\\n100\\n100\\nAvg no. of trees\\n100\\n100\\n100\\n100\\nBreiman (1999) and Reyzin & Schapire (2006) extensively\\ninvestigated the relationship between the complexity of de-\\ncision trees in an ensemble learned by AdaBoost and the\\ngeneralization error of the ensemble. We tested DeepBoost\\non the same UCI datasets used by these authors, http://\\narchive.ics.uci.edu/ml/datasets.html,\\nspeciﬁ-\\ncally breastcancer, ionosphere, german(numeric)\\nand diabetes. We also experimented with two optical\\ncharacter recognition datasets used by Reyzin & Schapire\\n(2006), ocr17 and ocr49, which contain the handwritten\\ndigits 1 and 7, and 4 and 9 (respectively). Finally, because\\nthese OCR datasets are fairly small, we also constructed\\nthe analogous datasets from all of MNIST, http://yann.\\nlecun.com/exdb/mnist/, which we call ocr17-mnist\\nand ocr49-mnist. More details on all the datasets are\\ngiven in Table 4, Appendix D.1.\\nAs we discussed in Section 3.2, by ﬁxing the parameters β\\nand λ to certain values, we recover some known algorithms\\nas special cases of DeepBoost. Our experiments compared\\nDeepBoost to AdaBoost (β = λ = 0 with exponential\\nloss), to Logistic Regression (β = λ = 0 with logistic\\nloss), which we abbreviate as LogReg, to L1-norm regular-\\nized AdaBoost (e.g., see (R¨atsch et al., 2001a)) abbreviated\\nas AdaBoost-L1, and also to the L1-norm regularized ad-\\nditive Logistic Regression algorithm studied by (Duchi &\\nSinger, 2009) (β > 0, λ = 0) abbreviated as LogReg-L1.\\nIn the ﬁrst set of experiments reported in Table 1, we com-\\npared AdaBoost, AdaBoost-L1, and DeepBoost with the\\nexponential loss (Φ(−u) = exp(−u)) and base hypothe-\\nses Hstumps. We tested standard AdaBoost with base hy-\\npotheses Hstumps\\n1\\nand Hstumps\\n2\\n. For AdaBoost-L1, we op-\\ntimized over β 2 {2−i : i = 6, . . . , 0} and for Deep-\\nBoost, we optimized over β in the same range and λ 2\\n{0.0001, 0.005, 0.01, 0.05, 0.1, 0.5}. The exact parameter\\noptimization procedure is described below.\\nIn the second set of experiments reported in Table 2, we\\nused base hypotheses Htrees\\nK\\ninstead of Hstumps, where the\\nmaximum tree depth K was an additional parameter to be\\noptimized. Speciﬁcally, for AdaBoost we optimized over\\nK 2 {1, . . . , 6}, for AdaBoost-L1 we optimized over those\\nsame values for K and β 2 {10−i : i = 3, . . . , 7}, and for\\nDeepBoost we optimized over those same values for K, β\\nand λ 2 {10−i : i = 3, . . . , 7}.\\nThe last set of experiments, reported in Table 3, are identi-\\ncal to the experiments reported in Table 2, except we used\\nthe logistic loss Φ(−u) = log2(1 + exp(−u)).\\nWe used the following parameter optimization procedure\\nin all experiments: Each dataset was randomly partitioned\\ninto 10 folds, and each algorithm was run 10 times, with a\\ndifferent assignment of folds to the training set, validation\\nset and test set for each run. Speciﬁcally, for each run i 2\\n{0, . . . , 9}, fold i was used for testing, fold i + 1 (mod 10)\\nwas used for validation, and the remaining folds were used\\nfor training. For each run, we selected the parameters that\\nhad the lowest error on the validation set and then measured\\nthe error of those parameters on the test set. The average\\nerror and the standard deviation of the error over all 10 runs\\nis reported in Tables 1, 2 and 3, as is the average number of\\ntrees and the average size of the trees in the ensembles.\\nIn all of our experiments, the number of iterations was set\\nto 100. We also experimented with running each algorithm\\n\\nDeep Boosting\\nTable 2. Results for boosted decision trees and the exponential loss function.\\nbreastcancer\\nAdaBoost\\nAdaBoost-L1\\nDeepBoost\\nocr17\\nAdaBoost\\nAdaBoost-L1\\nDeepBoost\\nError\\n0.0267\\n0.0264\\n0.0243\\nError\\n0.004\\n0.003\\n0.002\\n(std dev)\\n(0.00841)\\n(0.0098)\\n(0.00797)\\n(std dev)\\n(0.00316)\\n(0.00100)\\n(0.00100)\\nAvg tree size\\n29.1\\n28.9\\n20.9\\nAvg tree size\\n15.0\\n30.4\\n26.0\\nAvg no. of trees\\n67.1\\n51.7\\n55.9\\nAvg no. of trees\\n88.3\\n65.3\\n61.8\\nionosphere\\nAdaBoost\\nAdaBoost-L1\\nDeepBoost\\nocr49\\nAdaBoost\\nAdaBoost-L1\\nDeepBoost\\nError\\n0.0661\\n0.0657\\n0.0501\\nError\\n0.0180\\n0.0175\\n0.0175\\n(std dev)\\n(0.0315)\\n(0.0257)\\n(0.0316)\\n(std dev)\\n(0.00555)\\n(0.00357)\\n(0.00510)\\nAvg tree size\\n29.8\\n31.4\\n26.1\\nAvg tree size\\n30.9\\n62.1\\n30.2\\nAvg no. of trees\\n75.0\\n69.4\\n50.0\\nAvg no. of trees\\n92.4\\n89.0\\n83.0\\ngerman\\nAdaBoost\\nAdaBoost-L1\\nDeepBoost\\nocr17-mnist\\nAdaBoost\\nAdaBoost-L1\\nDeepBoost\\nError\\n0.239\\n0.239\\n0.234\\nError\\n0.00471\\n0.00471\\n0.00409\\n(std dev)\\n(0.0165)\\n(0.0201)\\n(0.0148)\\n(std dev)\\n(0.0022)\\n(0.0021)\\n(0.0021)\\nAvg tree size\\n3\\n7\\n16.0\\nAvg tree size\\n15\\n33.4\\n22.1\\nAvg no. of trees\\n91.3\\n87.5\\n14.1\\nAvg no. of trees\\n88.7\\n66.8\\n59.2\\ndiabetes\\nAdaBoost\\nAdaBoost-L1\\nDeepBoost\\nocr49-mnist\\nAdaBoost\\nAdaBoost-L1\\nDeepBoost\\nError\\n0.249\\n0.240\\n0.230\\nError\\n0.0198\\n0.0197\\n0.0182\\n(std dev)\\n(0.0272)\\n(0.0313)\\n(0.0399)\\n(std dev)\\n(0.00500)\\n(0.00512)\\n(0.00551)\\nAvg tree size\\n3\\n3\\n5.37\\nAvg tree size\\n29.9\\n66.3\\n30.1\\nAvg no. of trees\\n45.2\\n28.0\\n19.0\\nAvg no. of trees\\n82.4\\n81.1\\n80.9\\nTable 3. Results for boosted decision trees and the logistic loss function.\\nbreastcancer\\nLogReg\\nLogReg-L1\\nDeepBoost\\nocr17\\nLogReg\\nLogReg-L1\\nDeepBoost\\nError\\n0.0351\\n0.0264\\n0.0264\\nError\\n0.00300\\n0.00400\\n0.00250\\n(std dev)\\n(0.0101)\\n(0.0120)\\n(0.00876)\\n(std dev)\\n(0.00100)\\n(0.00141)\\n(0.000866)\\nAvg tree size\\n15\\n59.9\\n14.0\\nAvg tree size\\n15.0\\n7\\n22.1\\nAvg no. of trees\\n65.3\\n16.0\\n23.8\\nAvg no. of trees\\n75.3\\n53.8\\n25.8\\nionosphere\\nLogReg\\nLogReg-L1\\nDeepBoost\\nocr49\\nLogReg\\nLogReg-L1\\nDeepBoost\\nError\\n0.074\\n0.060\\n0.043\\nError\\n0.0205\\n0.0200\\n0.0170\\n(std dev)\\n(0.0236)\\n(0.0219)\\n(0.0188)\\n(std dev)\\n(0.00654)\\n(0.00245)\\n(0.00361)\\nAvg tree size\\n7\\n30.0\\n18.4\\nAvg tree size\\n31.0\\n31.0\\n63.2\\nAvg no. of trees\\n44.7\\n25.3\\n29.5\\nAvg no. of trees\\n63.5\\n54.0\\n37.0\\ngerman\\nLogReg\\nLogReg-L1\\nDeepBoost\\nocr17-mnist\\nLogReg\\nLogReg-L1\\nDeepBoost\\nError\\n0.233\\n0.232\\n0.225\\nError\\n0.00422\\n0.00417\\n0.00399\\n(std dev)\\n(0.0114)\\n(0.0123)\\n(0.0103)\\n(std dev)\\n(0.00191)\\n(0.00188)\\n(0.00211)\\nAvg tree size\\n7\\n7\\n14.4\\nAvg tree size\\n15\\n15\\n25.9\\nAvg no. of trees\\n72.8\\n66.8\\n67.8\\nAvg no. of trees\\n71.4\\n55.6\\n27.6\\ndiabetes\\nLogReg\\nLogReg-L1\\nDeepBoost\\nocr49-mnist\\nLogReg\\nLogReg-L1\\nDeepBoost\\nError\\n0.250\\n0.246\\n0.246\\nError\\n0.0211\\n0.0201\\n0.0201\\n(std dev)\\n(0.0374)\\n(0.0356)\\n(0.0356)\\n(std dev)\\n(0.00412)\\n(0.00433)\\n(0.00411)\\nAvg tree size\\n3\\n3\\n3\\nAvg tree size\\n28.7\\n33.5\\n72.8\\nAvg no. of trees\\n46.0\\n45.5\\n45.5\\nAvg no. of trees\\n79.3\\n61.7\\n41.9\\nfor up to 1,000 iterations, but observed that the test errors\\ndid not change signiﬁcantly, and more importantly the or-\\ndering of the algorithms by their test errors was unchanged\\nfrom 100 iterations to 1,000 iterations.\\nObserve that with the exponential loss, DeepBoost has a\\nsmaller test error than AdaBoost and AdaBoost-L1 on ev-\\nery dataset and for every set of base hypotheses, except for\\nthe ocr49-mnist dataset with decision trees where its per-\\nformance matches that of AdaBoost-L1. Similarly, with the\\nlogistic loss, DeepBoost performs always at least as well as\\nLogReg or LogReg-L1. For the small-sized UCI datasets it\\nis difﬁcult to obtain statistically signiﬁcant results, but, for\\nthe larger ocrXX-mnist datasets, our results with Deep-\\nBoost are statistically signiﬁcantly better at the 2% level\\nusing one-sided paired t-tests in all three sets of experi-\\nments (three tables), except for ocr49-mnist in Table 3,\\nwhere this holds only for the comparison with LogReg.\\nThis across-the-board improvement is the result of Deep-\\nBoost’s complexity-conscious ability to dynamically tune\\nthe sizes of the decision trees selected in each boosting\\nround, trading off between training error and hypothesis\\nclass complexity. The selected tree sizes should depend on\\nproperties of the training set, and this is borne out by our\\nexperiments: For some datasets, such as breastcancer,\\nDeepBoost selects trees that are smaller on average than\\nthe trees selected by AdaBoost-L1 or LogReg-L1, while,\\nfor other datasets, such as german, the average tree size\\nis larger. Note that AdaBoost and AdaBoost-L1 produce\\nensembles of trees that have a constant depth since neither\\nalgorithm penalizes tree size except for imposing a maxi-\\nmum tree depth K, while for DeepBoost the trees in one\\nensemble typically vary in size. Figure 3 plots the distri-\\n\\nDeep Boosting\\nIon: Histogram of tree sizes\\nTree sizes\\nFrequency\\n10\\n20\\n30\\n40\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\nFigure 3. Distribution of tree sizes when DeepBoost is run on the\\nionosphere dataset.\\nbution of tree sizes for one run of DeepBoost. It should\\nbe noted that the columns for AdaBoost in Table 1 simply\\nlist the number of stumps to be the same as the number of\\nboosting rounds; a careful examination of the ensembles\\nfor 100 rounds of boosting typically reveals a 5% duplica-\\ntion of stumps in the ensembles.\\nTheorem 1 is a margin-based generalization guarantee, and\\nis also the basis for the derivation of DeepBoost, so we\\nshould expect DeepBoost to induce large margins on the\\ntraining set. Figure 4 shows the margin distributions for\\nAdaBoost, AdaBoost-L1 and DeepBoost on the same sub-\\nset of the ionosphere dataset.\\n5. Conclusion\\nWe presented a theoretical analysis of learning with a\\nbase hypothesis set composed of increasingly complex sub-\\nfamilies, including very deep or complex ones, and de-\\nrived an algorithm, DeepBoost, which is precisely based\\non those guarantees. We also reported the results of exper-\\niments with this algorithm and compared its performance\\nwith that of AdaBoost and additive Logistic Regression,\\nand their L1-norm regularized counterparts in several tasks.\\nWe have derived similar theoretical guarantees in the multi-\\nclass setting and used them to derive a family of new multi-\\nclass deep boosting algorithms that we will present and dis-\\ncuss elsewhere. Our theoretical analysis and algorithmic\\ndesign could also be extended to ranking and to a broad\\nclass of loss functions. This should also lead to the gener-\\nalization of several existing algorithms and their use with a\\nricher hypothesis set structured as a union of families with\\ndifferent Rademacher complexity. In particular, the broad\\nfamily of maximum entropy models and conditional max-\\nimum entropy models and their many variants, which in-\\ncludes the already discussed logistic regression, could all\\nbe extended in a similar way. The resulting DeepMaxent\\nmodels (or their conditional versions) may admit an al-\\nternative theoretical justiﬁcation that we will discuss else-\\nwhere. Our algorithm can also be extended by consider-\\ning non-differentiable convex surrogate losses such as the\\nhinge loss. When used with kernel base classiﬁers, this\\nleads to an algorithm we have named DeepSVM. The the-\\nory we developed could perhaps be further generalized to\\nIon: AdaBoost−L1, fold = 6\\nNormalized Margin\\nFrequency\\n0.1\\n0.3\\n0.5\\n0.7\\n0\\n10\\n20\\n30\\n40\\n50\\nIon: AdaBoost, fold = 6\\nNormalized Margin\\nFrequency\\n0.1\\n0.3\\n0.5\\n0.7\\n0\\n10\\n20\\n30\\n40\\n50\\nIon: DeepBoost, fold = 6\\nNormalized Margin\\nFrequency\\n0.1\\n0.3\\n0.5\\n0.7\\n0\\n10\\n20\\n30\\n40\\n50\\n0.1\\n0.3\\n0.5\\n0.7\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nNormalized Margin\\nCumulative Dist.\\nCumulative Distribution of Margins\\nFigure 4. Distribution of normalized margins for AdaBoost (up-\\nper right), AdaBoost-L1 (upper left) and DeepBoost (lower left)\\non the same subset of ionosphere. The cumulative margin\\ndistributions (lower right) illustrate that DeepBoost (red) induces\\nlarger margins on the training set than either AdaBoost (black) or\\nAdaBoost-L1 (blue).\\nencompass the analysis of other learning techniques such\\nas multi-layer neural networks.\\nOur analysis and algorithm also shed some new light on\\nsome remaining questions left about the theory underly-\\ning AdaBoost.\\nThe primary theoretical justiﬁcation for\\nAdaBoost is a margin guarantee (Schapire et al., 1997;\\nKoltchinskii & Panchenko, 2002).\\nHowever, AdaBoost\\ndoes not precisely maximize the minimum margin, while\\nother algorithms such as arc-gv (Breiman, 1996) that are\\ndesigned to do so tend not to outperform AdaBoost (Reyzin\\n& Schapire, 2006). Two main reasons are suspected for this\\nobservation: (1) in order to achieve a better margin, algo-\\nrithms such as arc-gv may tend to select deeper decision\\ntrees or in general more complex hypotheses, which may\\nthen affect their generalization; (2) while those algorithms\\nachieve a better margin, they do not achieve a better mar-\\ngin distribution. Our theory may help better understand and\\nevaluate the effect of factor (1) since our learning bounds\\nexplicitly depend on the mixture weights and the contri-\\nbution of each hypothesis set Hk to the deﬁnition of the\\nensemble function. However, our guarantees also suggest a\\nbetter algorithm, DeepBoost.\\nAcknowledgments\\nWe thank Vitaly Kuznetsov for his comments on an ear-\\nlier draft of this paper. The work of M. Mohri was partly\\nfunded by the NSF award IIS-1117591.\\n\\nDeep Boosting\\nReferences\\nBartlett, Peter L. and Mendelson, Shahar. Rademacher and\\nGaussian complexities: Risk bounds and structural re-\\nsults. JMLR, 3, 2002.\\nBauer, Eric and Kohavi, Ron. An empirical comparison of\\nvoting classiﬁcation algorithms: Bagging, boosting, and\\nvariants. Machine Learning, 36(1-2):105–139, 1999.\\nBreiman, Leo. Bagging predictors. Machine Learning, 24\\n(2):123–140, 1996.\\nBreiman, Leo.\\nPrediction games and arcing algorithms.\\nNeural Computation, 11(7):1493–1517, 1999.\\nCaruana, Rich, Niculescu-Mizil, Alexandru, Crew, Geoff,\\nand Ksikes, Alex. Ensemble selection from libraries of\\nmodels. In ICML, 2004.\\nDietterich, Thomas G.\\nAn experimental comparison of\\nthree methods for constructing ensembles of decision\\ntrees: Bagging, boosting, and randomization. Machine\\nLearning, 40(2):139–157, 2000.\\nDuchi, John C. and Singer, Yoram. Boosting with structural\\nsparsity. In ICML, pp. 38, 2009.\\nFreund, Yoav and Schapire, Robert E. A decision-theoretic\\ngeneralization of on-line learning and an application to\\nboosting. Journal of Computer System Sciences, 55(1):\\n119–139, 1997.\\nFreund, Yoav, Mansour, Yishay, and Schapire, Robert E.\\nGeneralization bounds for averaged classiﬁers. The An-\\nnals of Statistics, 32:1698–1722, 2004.\\nFriedman, Jerome, Hastie, Trevor, and Tibshirani, Robert.\\nAdditive logistic regression: a statistical view of boost-\\ning. Annals of Statistics, 28:2000, 1998.\\nGrove, Adam J and Schuurmans, Dale. Boosting in the\\nlimit: Maximizing the margin of learned ensembles. In\\nAAAI/IAAI, pp. 692–699, 1998.\\nKivinen, Jyrki and Warmuth, Manfred K. Boosting as en-\\ntropy projection. In COLT, pp. 134–144, 1999.\\nKoltchinskii, Vladmir and Panchenko, Dmitry.\\nEmpiri-\\ncal margin distributions and bounding the generalization\\nerror of combined classiﬁers. Annals of Statistics, 30,\\n2002.\\nMacKay, David J. C. Bayesian methods for adaptive mod-\\nels. PhD thesis, California Institute of Technology, 1991.\\nMansour, Yishay. Pessimistic decision tree pruning based\\non tree size.\\nIn Proceedings of ICML, pp. 195–201,\\n1997.\\nMohri, Mehryar, Rostamizadeh, Afshin, and Talwalkar,\\nAmeet. Foundations of Machine Learning. The MIT\\nPress, 2012.\\nQuinlan, J. Ross.\\nBagging, boosting, and C4.5.\\nIn\\nAAAI/IAAI, Vol. 1, pp. 725–730, 1996.\\nR¨atsch, Gunnar and Warmuth, Manfred K. Maximizing the\\nmargin with boosting. In COLT, pp. 334–350, 2002.\\nR¨atsch, Gunnar and Warmuth, Manfred K. Efﬁcient margin\\nmaximizing with boosting. Journal of Machine Learning\\nResearch, 6:2131–2152, 2005.\\nR¨atsch, Gunnar, Mika, Sebastian, and Warmuth, Man-\\nfred K. On the convergence of leveraging. In NIPS, pp.\\n487–494, 2001a.\\nR¨atsch, Gunnar, Onoda, Takashi, and M¨uller, Klaus-\\nRobert. Soft margins for AdaBoost. Machine Learning,\\n42(3):287–320, 2001b.\\nReyzin, Lev and Schapire, Robert E. How boosting the\\nmargin can also boost classiﬁer complexity. In ICML,\\npp. 753–760, 2006.\\nSchapire, Robert E. Theoretical views of boosting and ap-\\nplications. In Proceedings of ALT 1999, volume 1720 of\\nLecture Notes in Computer Science, pp. 13–25. Springer,\\n1999.\\nSchapire, Robert E.\\nThe boosting approach to machine\\nlearning: An overview.\\nIn Nonlinear Estimation and\\nClassiﬁcation, pp. 149–172. Springer, 2003.\\nSchapire, Robert E., Freund, Yoav, Bartlett, Peter, and Lee,\\nWee Sun. Boosting the margin: A new explanation for\\nthe effectiveness of voting methods. In ICML, pp. 322–\\n330, 1997.\\nSmyth, Padhraic and Wolpert, David. Linearly combining\\ndensity estimators via stacking. Machine Learning, 36:\\n59–83, July 1999.\\nVapnik, Vladimir N. Statistical Learning Theory. Wiley-\\nInterscience, 1998.\\nWarmuth, Manfred K., Liao, Jun, and R¨atsch, Gunnar. To-\\ntally corrective boosting algorithms that maximize the\\nmargin. In ICML, pp. 1001–1008, 2006.\\n'}], Label: 1.0\n"
     ]
    }
   ],
   "source": [
    "# View the output\n",
    "for pair in pairs:\n",
    "    print(f\"Texts: {pair.texts}, Label: {pair.label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a459b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

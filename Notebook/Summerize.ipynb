{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d003bbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import fitz\n",
    "import re\n",
    "import textwrap\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# path to user functions\n",
    "sys.path.append(\"../Src\")\n",
    "import read_pdf\n",
    "\n",
    "# reload user functions\n",
    "from importlib import reload\n",
    "\n",
    "# PDF File Path\n",
    "PDF_FILE_PATH = \"/Users/sir/Downloads/Data/PDF/test/Matryoshka Representation Learning.pdf\" \n",
    "\n",
    "# LLM Local Paths\n",
    "LLM_PATH = \"/Users/sir/Downloads/HuggingFace/LLM/meta-Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# use mps if available, else cuda, else cpu\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7533d593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading full text from '/Users/sir/Downloads/Data/PDF/test/Matryoshka Representation Learning.pdf'...\n",
      "Extraction stop found immediately before: 'References'\n",
      "Successfully extracted and cleaned 39716 characters.\n",
      "39716\n",
      "Learned representations are a central component in modern ML systems, serving a multitude of downstream tasks. When training such representations, it is often the case that\n",
      "computational and statistical constraints for each downstream task are unknown. In this context, rigid fixedcapacity representations can be either over or underaccommodating to the\n",
      "task at hand. This leads us to ask: can we design a flexible representation that can adapt to multiple downstream tasks with varying computational resources? Our main contribution\n",
      "is Matryoshka Representation Learning (MRL) which encodes information at different granularities and allows a single embedding to adapt to the computational constraints of\n",
      "downstream tasks. MRL minimally modifies existing representation learning pipelines and imposes no additional cost during inference and deployment. MRL learns coarsetofine\n",
      "representations that are at least as accurate and rich as independently trained lowdimensional representations. The flexibility within the learned Matryoshka Representations offer:\n",
      "(a) up to 14× smaller embedding size for ImageNet-1K classification at the same level of accuracy; (b) up to 14× realworld speedups for largescale retrieval on ImageNet-1K and 4K;\n",
      "and (c) up to 2% accuracy improvements for longtail fewshot classification, all while being as robust as the original representations. Finally, we show that MRL extends seamlessly\n",
      "to webscale datasets (ImageNet, JFT) across various modalities – vision (ViT, ResNet), vision + language (ALIGN) and language (BERT). MRL code and pretrained models are opensourced\n",
      "at 1 Introduction Learned representations [57] are fundamental building blocks of realworld ML systems [66, 91]. Trained once and frozen, ddimensional representations encode rich\n",
      "information and can be used to perform multiple downstream tasks [4]. The deployment of deep representations has two steps: (1) an expensive yet constantcost forward pass to\n",
      "compute the representation [29] and (2) utilization of the representation for downstream applications [50, 89]. Compute costs for the latter part of the pipeline scale with the\n",
      "embedding dimensionality as well as the data size (N) and label space (L). At webscale [15, 85] this utilization cost overshadows the feature computation cost. The rigidity in\n",
      "these representations forces the use of highdimensional embedding vectors across multiple tasks despite the varying resource and accuracy constraints that require flexibility.\n",
      "Human perception of the natural world has a naturally coarsetofine granularity [28, 32]. However, perhaps due to the inductive bias of gradientbased training [84], deep learning\n",
      "models tend to diffuse “information” across the entire representation vector. The desired elasticity is usually enabled in the existing flat and fixed representations either\n",
      "through training multiple lowdimensional models [29], jointly optimizing subnetworks of varying capacity [9, 100] or posthoc compression [38, 60]. Each of these techniques struggle\n",
      "to meet the requirements for adaptive largescale deployment either ∗Equal contribution – AK led the project with extensive support from GB and AR for experimentation. 36th\n",
      "Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2205.13147v4 [cs.LG] 8 Feb 2024 due to training/maintenance overhead, numerous expensive forward passes\n",
      "through all of the data, storage and memory cost for multiple copies of encoded data, expensive onthefly feature selection or a significant drop in accuracy. By encoding\n",
      "coarsetofinegrained representations, which are as accurate as the independently trained counterparts, we learn with minimal overhead a representation that can be deployed\n",
      "adaptively at no additional cost during inference. We introduce Matryoshka Representation Learning (MRL) to induce flexibility in the learned representation. MRL learns\n",
      "representations of varying capacities within the same highdimensional vector through explicit optimization of O(log(d)) lowerdimensional vectors in a nested fashion, hence the name\n",
      "Matryoshka. MRL can be adapted to any existing representation pipeline and is easily extended to many standard tasks in computer vision and natural language processing. Figure 1\n",
      "illustrates the core idea of Matryoshka Representation Learning (MRL) and the adaptive deployment settings of the learned Matryoshka Representations. Adaptive Retrieval\n",
      "Shortlisting Reranking Adaptive Classification Training Inference Figure 1: Matryoshka Representation Learning is adaptable to any representation learning setup and begets a\n",
      "Matryoshka Representation z by optimizing the original loss L(.) at O(log(d)) chosen representation sizes. Matryoshka Representation can be utilized effectively for adaptive\n",
      "deployment across environments and downstream tasks. The first mdimensions, m ∈[d], of the Matryoshka Representation is an informationrich lowdimensional vector, at no additional\n",
      "training cost, that is as accurate as an independently trained mdimensional representation. The information within the Matryoshka Representation increases with the dimensionality\n",
      "creating a coarsetofine grained representation, all without significant training or additional deployment overhead. MRL equips the representation vector with the desired\n",
      "flexibility and multifidelity that can ensure a nearoptimal accuracyvscompute tradeoff. With these advantages, MRL enables adaptive deployment based on accuracy and compute\n",
      "constraints. The Matryoshka Representations improve efficiency for largescale classification and retrieval without any significant loss of accuracy. While there are potentially\n",
      "several applications of coarsetofine Matryoshka Representations, in this work we focus on two key building blocks of realworld ML systems: largescale classification and retrieval.\n",
      "For classification, we use adaptive cascades with the variablesize representations from a model trained with MRL, significantly reducing the average dimension of embeddings needed\n",
      "to achieve a particular accuracy. For example, on ImageNet-1K, MRL + adaptive classification results in up to a 14× smaller representation size at the same accuracy as baselines\n",
      "(Section 4.2.1). Similarly, we use MRL in an adaptive retrieval system. Given a query, we shortlist retrieval candidates using the first few dimensions of the query embedding, and\n",
      "then successively use more dimensions to rerank the retrieved set. A simple implementation of this approach leads to 128× theoretical (in terms of FLOPS) and 14× wallclock time\n",
      "speedups compared to a singleshot retrieval system that uses a standard embedding vector; note that MRL’s retrieval accuracy is comparable to that of singleshot retrieval (Section\n",
      "4.3.1). Finally, as MRL explicitly learns coarsetofine representation vectors, intuitively it should share more semantic information among its various dimensions (Figure 5). This\n",
      "is reflected in up to 2% accuracy gains in longtail continual learning settings while being as robust as the original embeddings. Furthermore, due to its coarsetofine grained\n",
      "nature, MRL can also be used as method to analyze hardness of classification among instances and information bottlenecks. We make the following key contributions: 1. We introduce\n",
      "Matryoshka Representation Learning (MRL) to obtain flexible representations (Matryoshka Representations) for adaptive deployment (Section 3). 2. Up to 14× faster yet accurate\n",
      "largescale classification and retrieval using MRL (Section 4). 3. Seamless adaptation of MRL across modalities (vision - ResNet & ViT, vision + language - ALIGN, language - BERT)\n",
      "and to webscale data (ImageNet-1K/4K, JFT-300M and ALIGN data). 4. Further analysis of MRL’s representations in the context of other downstream tasks (Section 5). 2 2 Related Work\n",
      "Representation Learning. Largescale datasets like ImageNet [16, 76] and JFT [85] enabled the learning of general purpose representations for computer vision [4, 98]. These\n",
      "representations are typically learned through supervised and un/selfsupervised learning paradigms. Supervised pretraining [29, 51, 82] casts representation learning as a\n",
      "multiclass/label classification problem, while un/selfsupervised learning learns representation via proxy tasks like instance classification [97] and reconstruction [31, 63].\n",
      "Recent advances [12, 30] in contrastive learning [27] enabled learning from webscale data [21] that powers largecapacity crossmodal models [18, 46, 71, 101]. Similarly, natural\n",
      "language applications are built [40] on large language models [8] that are pretrained [68, 75] in a un/selfsupervised fashion with masked language modelling [19] or autoregressive\n",
      "training [70]. Matryoshka Representation Learning (MRL) is complementary to all these setups and can be adapted with minimal overhead (Section 3). MRL equips representations with\n",
      "multifidelity at no additional cost which enables adaptive deployment based on the data and task (Section 4). Efficient Classification and Retrieval. Efficiency in classification\n",
      "and retrieval during inference can be studied with respect to the high yet constant deep featurization costs or the search cost which scales with the size of the label space and\n",
      "data. Efficient neural networks address the first issue through a variety of algorithms [25, 54] and design choices [39, 53, 87]. However, with a strong featurizer, most of the\n",
      "issues with scale are due to the linear dependence on number of labels (L), size of the data (N) and representation size (d), stressing RAM, disk and processor all at the same\n",
      "time. The sublinear complexity dependence on number of labels has been well studied in context of compute [3, 43, 69] and memory [20] using Approximate Nearest Neighbor Search\n",
      "(ANNS) [62] or leveraging the underlying hierarchy [17, 55]. In case of the representation size, often dimensionality reduction [77, 88], hashing techniques [14, 52, 78] and\n",
      "feature selection [64] help in alleviating selective aspects of the O(d) scaling at a cost of significant drops in accuracy. Lastly, most realworld search systems [11, 15] are\n",
      "often powered by largescale embedding based retrieval [10, 66] that scales in cost with the ever increasing webdata. While categorization [89, 99] clusters similar things together,\n",
      "it is imperative to be equipped with retrieval capabilities that can bring forward every instance [7]. Approximate Nearest Neighbor Search (ANNS) [42] makes it feasible with\n",
      "efficient indexing [14] and traversal [5, 6] to present the users with the most similar documents/images from the database for a requested query. Widely adopted HNSW [62] (O(d\n",
      "log(N))) is as accurate as exact retrieval (O(dN)) at the cost of a graphbased index overhead for RAM and disk [44]. MRL tackles the linear dependence on embedding size, d, by\n",
      "learning multifidelity Matryoshka Representations. Lowerdimensional Matryoshka Representations are as accurate as independently trained counterparts without the multiple expensive\n",
      "forward passes. Matryoshka Representations provide an intermediate abstraction between highdimensional vectors and their efficient ANNS indices through the adaptive embeddings\n",
      "nested within the original representation vector (Section 4). All other aforementioned efficiency techniques are complementary and can be readily applied to the learned Matryoshka\n",
      "Representations obtained from MRL. Several works in efficient neural network literature [9, 93, 100] aim at packing neural networks of varying capacity within the same larger\n",
      "network. However, the weights for each progressively smaller network can be different and often require distinct forward passes to isolate the final representations. This is\n",
      "detrimental for adaptive inference due to the need for reencoding the entire retrieval database with expensive subnet forward passes of varying capacities. Several works [23, 26,\n",
      "65, 59] investigate the notions of intrinsic dimensionality and redundancy of representations and objective spaces pointing to minimum description length [74]. Finally, ordered\n",
      "representations proposed by Rippel et al. [73] use nested dropout in the context of autoencoders to learn nested representations. MRL differentiates itself in formulation by\n",
      "optimizing only for O(log(d)) nesting dimensions instead of O(d). Despite this, MRL diffuses information to intermediate dimensions interpolating between the optimized Matryoshka\n",
      "Representation sizes accurately (Figure 5); making webscale feasible. 3 Matryoshka Representation Learning For d ∈N, consider a set M ⊂[d] of representation sizes. For a datapoint\n",
      "x in the input domain X, our goal is to learn a ddimensional representation vector z ∈Rd. For every m ∈M, 3 Matryoshka Representation Learning (MRL) enables each of the first m\n",
      "dimensions of the embedding vector, z1:m ∈Rm to be independently capable of being a transferable and general purpose representation of the datapoint x. We obtain z using a deep\n",
      "neural network F( · ; θF ): X →Rd parameterized by learnable weights θF , i.e., z := F(x; θF ). The multigranularity is captured through the set of the chosen dimensions M, that\n",
      "contains less than log(d) elements, i.e., |M| ≤⌊log(d)⌋. The usual set M consists of consistent halving until the representation size hits a low information bottleneck. We discuss\n",
      "the design choices in Section 4 for each of the representation learning settings. For the ease of exposition, we present the formulation for fully supervised representation\n",
      "learning via multiclass classification. Matryoshka Representation Learning modifies the typical setting to become a multiscale representation learning problem on the same task. For\n",
      "example, we train ResNet50 [29] on ImageNet-1K [76] which embeds a 224 × 224 pixel image into a d = 2048 representation vector and then passed through a linear classifier to make a\n",
      "prediction, ˆy among the L = 1000 labels. For MRL, we choose M = {8, 16, . . . , 1024, 2048} as the nesting dimensions. Suppose we are given a labelled dataset D = {(x1, y1), . . .\n",
      ", (xN, yN)} where xi ∈X is an input point and yi ∈[L] is the label of xi for all i ∈[N]. MRL optimizes the multiclass classification loss for each of the nested dimension m ∈M\n",
      "using standard empirical risk minimization using a separate linear classifier, parameterized by W(m) ∈RL×m. All the losses are aggregated after scaling with their relative\n",
      "importance (cm ≥0)m∈M respectively. That is, we solve min {W(m)}m∈M, θF 1 N X i∈[N] X m∈M cm · L \u0010 W(m) · F(xi; θF )1:m ; yi \u0011 , (1) where L: RL × [L] →R+ is the multiclass softmax\n",
      "crossentropy loss function. This is a standard optimization problem that can be solved using subgradient descent methods. We set all the importance scales, cm = 1 for all m ∈M; see\n",
      "Section 5 for ablations. Lastly, despite only optimizing for O(log(d)) nested dimensions, MRL results in accurate representations, that interpolate, for dimensions that fall\n",
      "between the chosen granularity of the representations (Section 4.2). We call this formulation as Matryoshka Representation Learning (MRL). A natural way to make this efficient is\n",
      "through weighttying across all the linear classifiers, i.e., by defining W(m) = W1:m for a set of common weights W ∈RL×d. This would reduce the memory cost due to the linear\n",
      "classifiers by almost half, which would be crucial in cases of extremely large output spaces [89, 99]. This variant is called Efficient Matryoshka Representation Learning (MRL–E).\n",
      "Refer to Alg 1 and Alg 2 in Appendix A for the building blocks of Matryoshka Representation Learning (MRL). Adaptation to Learning Frameworks. MRL can be adapted seamlessly to most\n",
      "representation learning frameworks at webscale with minimal modifications (Section 4.1). For example, MRL’s adaptation to masked language modelling reduces to MRL–E due to the\n",
      "weighttying between the input embedding matrix and the linear classifier. For contrastive learning, both in context of vision & vision + language, MRL is applied to both the\n",
      "embeddings that are being contrasted with each other. The presence of normalization on the representation needs to be handled independently for each of the nesting dimension for\n",
      "best results (see Appendix C for more details). 4 Applications In this section, we discuss Matryoshka Representation Learning (MRL) for a diverse set of applications along with an\n",
      "extensive evaluation of the learned multifidelity representations. Further, we showcase the downstream applications of the learned Matryoshka Representations for flexible\n",
      "largescale deployment through (a) Adaptive Classification (AC) and (b) Adaptive Retrieval (AR). 4.1 Representation Learning We adapt Matryoshka Representation Learning (MRL) to\n",
      "various representation learning setups (a) Supervised learning for vision: ResNet50 [29] on ImageNet-1K [76] and ViTB/16 [22] on JFT-300M [85], (b) Contrastive learning for vision\n",
      "+ language: ALIGN model with ViTB/16 vision encoder and BERT language encoder on ALIGN data [46] and (c) Masked language modelling: BERT [19] on English Wikipedia and BooksCorpus\n",
      "[102]. Please refer to Appendices B and C for details regarding the model architectures, datasets and training specifics. 4 8 16 32 64 128 256 512 1024 2048 Representation Size 40\n",
      "50 60 70 80 Top-1 Accuracy (%) MRL MRLE FF SVD Slim. Net Rand. LP Figure 2: ImageNet-1K linear classification accuracy of ResNet50 models. MRL is as accurate as the independently\n",
      "trained FF models for every representation size. 8 16 32 64 128 256 512 1024 2048 Representation Size 40 50 60 70 1-NN Accuracy (%) MRL MRLE FF SVD Slim. Net Rand. FS Figure 3:\n",
      "ImageNet-1K 1-NN accuracy of ResNet50 models measuring the representation quality for downstream task. MRL outperforms all the baselines across all representation sizes. We do not\n",
      "search for best hyperparameters for all MRL experiments but use the same hyperparameters as the independently trained baselines. ResNet50 outputs a 2048-dimensional representation\n",
      "while ViTB/16 and BERTBase output 768-dimensional embeddings for each data point. We use M = {8, 16, 32, 64, 128, 256, 512, 1024, 2048} and M = {12, 24, 48, 96, 192, 384, 768} as\n",
      "the explicitly optimized nested dimensions respectively. Lastly, we extensively compare the MRL and MRL–E models to independently trained lowdimensional (fixed feature)\n",
      "representations (FF), dimensionality reduction (SVD), subnet method (slimmable networks [100]) and randomly selected features of the highest capacity FF model. In section 4.2, we\n",
      "evaluate the quality and capacity of the learned representations through linear classification/probe (LP) and 1-nearest neighbour (1-NN) accuracy. Experiments show that MRL models\n",
      "remove the dependence on |M| resourceintensive independently trained models for the coarsetofine representations while being as accurate. Lastly, we show that despite optimizing\n",
      "only for |M| dimensions, MRL models diffuse the information, in an interpolative fashion, across all the d dimensions providing the finest granularity required for adaptive\n",
      "deployment. 4.2 Classification Figure 2 compares the linear classification accuracy of ResNet50 models trained and evaluated on ImageNet-1K. ResNet50–MRL model is at least as\n",
      "accurate as each FF model at every representation size in M while MRL–E is within 1% starting from 16-dim. Similarly, Figure 3 showcases the comparison of learned representation\n",
      "quality through 1-NN accuracy on ImageNet-1K (trainset with 1.3M samples as the database and validation set with 50K samples as the queries). Matryoshka Representations are up to\n",
      "2% more accurate than their fixedfeature counterparts for the lowerdimensions while being as accurate elsewhere. 1-NN accuracy is an excellent proxy, at no additional training\n",
      "cost, to gauge the utility of learned representations in the downstream tasks. We also evaluate the quality of the representations from training ViTB/16 on JFT-300M alongside the\n",
      "ViTB/16 vision encoder of the ALIGN model – two webscale setups. Due to the expensive nature of these experiments, we only train the highest capacity fixed feature model and choose\n",
      "random features for evaluation in lowerdimensions. Webscale is a compelling setting for MRL due to its relatively inexpensive training overhead while providing multifidelity\n",
      "representations for downstream tasks. Figure 4, evaluated with 1-NN on ImageNet-1K, shows that all the MRL models for JFT and ALIGN are highly accurate while providing an excellent\n",
      "costvsaccuracy tradeoff at lowerdimensions. These experiments show that MRL seamlessly scales to largescale models and webscale datasets while providing the otherwise prohibitively\n",
      "expensive multigranularity in the process. We also have similar observations when pretraining BERT; please see Appendix D.2 for more details. Our experiments also show that posthoc\n",
      "compression (SVD), linear probe on random features, and subnet style slimmable networks drastically lose accuracy compared to MRL as the representation size decreases. Finally,\n",
      "Figure 5 shows that, while MRL explicitly optimizes O(log(d)) nested representations – removing the O(d) dependence [73] –, the coarsetofine grained information is interpolated\n",
      "across all d dimensions providing highest flexibility for adaptive deployment. 5 12 24 48 96 192 384 768 Representation Size 20 40 60 80 1-NN Accuracy (%) JFT MRL ALIGN MRL JFT\n",
      "MRLE JFT Rand. ALIGN Rand. Figure 4: ImageNet-1K 1-NN accuracy for ViTB/16 models trained on JFT-300M & as part of ALIGN. MRL scales seamlessly to webscale with minimal training\n",
      "overhead. 8 16 32 64 128 256 512 1024 2048 Representation Size 50 60 70 1-NN Accuracy (%) ViTALIGN ViTJFT RN50-IN1K ViTALIGNInt ViTJFTInt RN50-IN1KInt Figure 5: Despite optimizing\n",
      "MRL only for O(log(d)) dimensions for ResNet50 and ViTB/16 models; the accuracy in the intermediate dimensions shows interpolating behaviour. 4.2.1 Adaptive Classification The\n",
      "flexibility and coarsetofine granularity within Matryoshka Representations allows model cascades [90] for Adaptive Classification (AC) [28]. Unlike standard model cascades [95],\n",
      "MRL does not require multiple expensive neural network forward passes. To perform AC with an MRL trained model, we learn thresholds on the maximum softmax probability [33] for each\n",
      "nested classifier on a holdout validation set. We then use these thresholds to decide when to transition to the higher dimensional representation (e.g 8 →16 →32) of the MRL model.\n",
      "Appendix D.1 discusses the implementation and learning of thresholds for cascades used for adaptive classification in detail. Figure 6 shows the comparison between cascaded MRL\n",
      "representations (MRL–AC) and independently trained fixed feature (FF) models on ImageNet-1K with ResNet50. We computed the expected representation size for MRL–AC based on the\n",
      "final dimensionality used in the cascade. We observed that MRL–AC was as accurate, 76.30%, as a 512-dimensional FF model but required an expected dimensionality of ∼37 while being\n",
      "only 0.8% lower than the 2048-dimensional FF baseline. Note that all MRL–AC models are significantly more accurate than the FF baselines at comparable representation sizes. MRL–AC\n",
      "uses up to ∼14× smaller representation size for the same accuracy which affords computational efficiency as the label space grows [89]. Lastly, our results with MRL–AC indicate\n",
      "that instances and classes vary in difficulty which we analyze in Section 5 and Appendix J. 4.3 Retrieval Nearest neighbour search with learned representations powers a plethora of\n",
      "retrieval and search applications [15, 91, 11, 66]. In this section, we discuss the image retrieval performance of the pretrained ResNet50 models (Section 4.1) on two largescale\n",
      "datasets ImageNet-1K [76] and ImageNet-4K. ImageNet-1K has a database size of ∼1.3M and a query set of 50K samples uniformly spanning 1000 classes. We also introduce ImageNet-4K\n",
      "which has a database size of ∼4.2M and query set of ∼200K samples uniformly spanning 4202 classes (see Appendix B for details). A single forward pass on ResNet50 costs 4 GFLOPs\n",
      "while exact retrieval costs 2.6 GFLOPs per query for ImageNet-1K. Although retrieval overhead is 40% of the total cost, retrieval cost grows linearly with the size of the database.\n",
      "ImageNet-4K presents a retrieval benchmark where the exact search cost becomes the computational bottleneck (8.6 GFLOPs per query). In both these settings, the memory and disk\n",
      "usage are also often bottlenecked by the large databases. However, in most realworld applications exact search, O(dN), is replaced with an approximate nearest neighbor search\n",
      "(ANNS) method like HNSW [62], O(d log(N)), with minimal accuracy drop at the cost of additional memory overhead. The goal of image retrieval is to find images that belong to the\n",
      "same class as the query using representations obtained from a pretrained model. In this section, we compare retrieval performance using mean Average Precision @ 10 (mAP@10) which\n",
      "comprehensively captures the setup of relevant image retrieval at scale. We measure the cost per query using exact search in MFLOPs. All embeddings are unit normalized and\n",
      "retrieved using the L2 distance metric. Lastly, we report 6 14x smaller representation size Figure 6: Adaptive classification on MRL ResNet50 using cascades results in 14× smaller\n",
      "representation size for the same level of accuracy on ImageNet-1K (∼37 vs 512 dims for 76.3%). 8 16 32 64 128 256 512 1024 2048 Representation Size 40 45 50 55 60 65 mAP@10 (%) MRL\n",
      "MRLE FF SVD Slim. Net Rand. FS Figure 7: mAP@10 for Image Retrieval on ImageNet-1K with ResNet50. MRL consistently produces better retrieval performance over the baselines across\n",
      "all the representation sizes. an extensive set of metrics spanning mAP@k and P@k for k = {10, 25, 50, 100} and realworld wallclock times for exact search and HNSW. See Appendices E\n",
      "and F for more details. Figure 7 compares the mAP@10 performance of ResNet50 representations on ImageNet-1K across dimensionalities for MRL, MRL–E, FF, slimmable networks along\n",
      "with posthoc compression of vectors using SVD and random feature selection. Matryoshka Representations are often the most accurate while being up to 3% better than the FF\n",
      "baselines. Similar to classification, posthoc compression and slimmable network baselines suffer from significant dropoff in retrieval mAP@10 with ≤256 dimensions. Appendix E\n",
      "discusses the mAP@10 of the same models on ImageNet-4K. MRL models are capable of performing accurate retrieval at various granularities without the additional expense of multiple\n",
      "model forward passes for the webscale databases. FF models also generate independent databases which become prohibitively expense to store and switch in between. Matryoshka\n",
      "Representations enable adaptive retrieval (AR) which alleviates the need to use fullcapacity representations, d = 2048, for all data and downstream tasks. Lastly, all the vector\n",
      "compression techniques [60, 45] used as part of the ANNS pipelines are complimentary to Matryoshka Representations and can further improve the efficiencyvsaccuracy tradeoff. 4.3.1\n",
      "Adaptive Retrieval We benchmark MRL in the adaptive retrieval setting (AR) [50]. For a given query image, we obtained a shortlist, K = 200, of images from the database using a\n",
      "lowerdimensional representation, e.g. Ds = 16 followed by reranking with a higher capacity representation, e.g. Dr = 2048. In realworld scenarios where top ranking performance is\n",
      "the key objective, measured with mAP@k where k covers a limited yet crucial realestate, AR provides significant compute and memory gains over singleshot retrieval with\n",
      "representations of fixed dimensionality. Finally, the most expensive part of AR, as with any retrieval pipeline, is the nearest neighbour search for shortlisting. For example, even\n",
      "naive reranking of 200 images with 2048 dimensions only costs 400 KFLOPs. While we report exact search cost per query for all AR experiments, the shortlisting component of the\n",
      "pipeline can be spedup using ANNS (HNSW). Appendix I has a detailed discussion on compute cost for exact search, memory overhead of HNSW indices and wallclock times for both\n",
      "implementations. We note that using HNSW with 32 neighbours for shortlisting does not decrease accuracy during retrieval. Figure 8 showcases the computevsaccuracy tradeoff for\n",
      "adaptive retrieval using Matryoshka Representations compared to singleshot using fixed features with ResNet50 on ImageNet-1K. We observed that all AR settings lied above the Pareto\n",
      "frontier of singleshot retrieval with varying representation sizes. In particular for ImageNet-1K, we show that the AR model with Ds = 16 & Dr = 2048 is as accurate as singleshot\n",
      "retrieval with d = 2048 while being ∼128× more efficient in theory and ∼14× faster in practice (compared using HNSW on the same hardware). We show similar trends with ImageNet-4K,\n",
      "but note that we require Ds = 64 given the increased difficulty of the dataset. This results in ∼32× and ∼6× theoretical and inpractice speedups respectively. Lastly, while K = 200\n",
      "works well for our adaptive retrieval experiments, we ablated over the shortlist size k in Appendix K.2 and found that the accuracy gains stopped after a 7 128x theoretical speedup\n",
      "14x realworld speedup 8 16 32 64 128 256 512 1024 2048 Ds Dr 6x realworld speedup 32x theoretical speedup (a) ImageNet-1K (b) ImageNet-4K Figure 8: The tradeoff between mAP@10 vs\n",
      "MFLOPs/Query for Adaptive Retrieval (AR) on ImageNet-1K (left) and ImageNet-4K (right). Every combination of Ds & Dr falls above the Pareto line (orange dots) of singleshot\n",
      "retrieval with a fixed representation size while having configurations that are as accurate while being up to 14× faster in realworld deployment. Funnel retrieval is almost as\n",
      "accurate as the baseline while alleviating some of the parameter choices of Adaptive Retrieval. point, further strengthening the usecase for Matryoshka Representation Learning and\n",
      "adaptive retrieval. Even with adaptive retrieval, it is hard to determine the choice of Ds & Dr. In order to alleviate this issue to an extent, we propose Funnel Retrieval, a\n",
      "consistent cascade for adaptive retrieval. Funnel thins out the initial shortlist by a repeated reranking and shortlisting with a series of increasing capacity representations.\n",
      "Funnel halves the shortlist size and doubles the representation size at every step of reranking. For example on ImageNet-1K, a funnel with the shortlist progression of 200 →100 →50\n",
      "→25 →10 with the cascade of 16 →32 →64 →128 →256 →2048 representation sizes within Matryoshka Representation is as accurate as the singleshot 2048-dim retrieval while being ∼128×\n",
      "more efficient theoretically (see Appendix F for more results). All these results showcase the potential of MRL and AR for largescale multistage search systems [15]. 5 Further\n",
      "Analysis and Ablations Robustness. We evaluate the robustness of the MRL models trained on ImageNet-1K on outofdomain datasets, ImageNetV2/R/A/Sketch [72, 34, 35, 94], and compare\n",
      "them to the FF baselines. Table 17 in Appendix H demonstrates that Matryoshka Representations for classification are at least as robust as the original representation while\n",
      "improving the performance on ImageNetA by 0.6% – a 20% relative improvement. We also study the robustness in the context of retrieval by using ImageNetV2 as the query set for\n",
      "ImageNet-1K database. Table 9 in Appendix E shows that MRL models have more robust retrieval compared to the FF baselines by having up to 3% higher mAP@10 performance. This\n",
      "observation also suggests the need for further investigation into robustness using nearest neighbour based classification and retrieval instead of the standard linear probing\n",
      "setup. We also find that the zeroshot robustness of ALIGNMRL (Table 18 in Appendix H) agrees with the observations made by Wortsman et al. [96]. Lastly, Table 6 in Appendix D.2\n",
      "shows that MRL also improves the cosine similarity span between positive and random imagetext pairs. Fewshot and Longtail Learning. We exhaustively evaluated fewshot learning on\n",
      "MRL models using nearest class mean [79]. Table 15 in Appendix G shows that that representations learned through MRL perform comparably to FF representations across varying shots\n",
      "and number of classes. Matryoshka Representations realize a unique pattern while evaluating on FLUID [92], a longtail sequential learning framework. We observed that MRL provides\n",
      "up to 2% accuracy higher on novel classes in the tail of the distribution, without sacrificing accuracy on other classes (Table 16 in Appendix G). Additionally we find the accuracy\n",
      "between lowdimensional and highdimensional representations is marginal for pretrain classes. We hypothesize that the higherdimensional representations are required to differentiate\n",
      "the classes when few training examples of each are known. This results provides further evidence that different tasks require varying capacity based on their difficulty.\n",
      "Disagreement across Dimensions. The information packing in Matryoshka Representations often results in gradual increase of accuracy with increase in capacity. However, we observed\n",
      "that 8 (a) (b) (c) Figure 9: GradCAM [80] progression of predictions in MRL model across 8, 16, 32 and 2048 dimensions. (a) 8-dimensional representation confuses due to presence of\n",
      "other relevant objects (with a larger field of view) in the scene and predicts “shower cap” ; (b) 8-dim model confuses within the same superclass of “boa” ; (c) 8 and 16-dim models\n",
      "incorrectly focus on the eyes of the doll (\"sunglasses\") and not the \"sweatshirt\" which is correctly in focus at higher dimensions; MRL fails gracefully in these scenarios and\n",
      "shows potential use cases of disagreement across dimensions. this trend was not ubiquitous and certain instances and classes were more accurate when evaluated with lowerdimensions\n",
      "(Figure 12 in Appendix J). With perfect routing of instances to appropriate dimension, MRL can gain up to 4.6% classification accuracy. At the same time, the lowdimensional models\n",
      "are less accurate either due to confusion within the same superclass [24] of the ImageNet hierarchy or presence of multiple objects of interest. Figure 9 showcases 2 such examples\n",
      "for 8- dimensional representation. These results along with Appendix J put forward the potential for MRL to be a systematic framework for analyzing the utility and efficiency of\n",
      "information bottlenecks. Superclass Accuracy. As the information bottleneck becomes smaller, the overall accuracy on finegrained classes decreases rapidly (Figure 3). However, the\n",
      "dropoff is not as significant when evaluated at a superclass level (Table 24 in Appendix J). Figure 10 presents that this phenomenon 8 16 32 64 128 256 512 1024 2048 Representation\n",
      "Size 84 86 88 90 Top-1 Accuracy (%) MRL FF Figure 10: 31-way ImageNet-1K superclass classification across representation size for MRL & FF models showing the capture of underlying\n",
      "hierarchy through tight information bottlenecks. 8 16 32 64 128 256 512 1024 2048 Representation Size 65 70 75 80 85 90 95 Top-1 Accuracy (%) measuring device building garment tool\n",
      "nourishment protective covering vessel oscine Figure 11: Diverse persuperclass accuracy trends across representation sizes for ResNet50- MRL on ImageNet-1K. 9 occurs with both MRL\n",
      "and FF models; MRL is more accurate across dimensions. This shows that tight information bottlenecks while not highly accurate for finegrained classification, do capture required\n",
      "semantic information for coarser classification that could be leveraged for adaptive routing for retrieval and classification. Mutifidelity of Matryoshka Representation naturally\n",
      "captures the underlying hierarchy of the class labels with one single model. Lastly, Figure 11 showcases the accuracy trends per superclass with MRL. The utility of additional\n",
      "dimensions in distinguishing a class from others within the same superclass is evident for “garment” which has up to 11% improvement for 8 →16 dimensional representation\n",
      "transition. We also observed that superclasses such as “oscine (songbird)” had a clear visual distinction between the object and background and thus predictions using 8 dimensions\n",
      "also led to a good interclass separability within the superclass. 5.1 Ablations Table 26 in Appendix K presents that Matryoshka Representations can be enabled within offtheshelf\n",
      "pretrained models with inexpensive partial finetuning thus paving a way for ubiquitous adoption of MRL. At the same time, Table 27 in Appendix C indicates that with optimal\n",
      "weighting of the nested losses we could improve accuracy of lowerdimensions representations without accuracy loss. Tables 28 and 29 in Appendix C ablate over the choice of initial\n",
      "granularity and spacing of the granularites. Table 28 reaffirms the design choice to shun extremely low dimensions that have poor classification accuracy as initial granularity for\n",
      "MRL while Table 29 confirms the effectiveness of logarthmic granularity spacing inspired from the behaviour of accuracy saturation across dimensions over uniform. Lastly, Tables 30\n",
      "and 31 in Appendix K.2 show that the retrieval performance saturates after a certain shortlist dimension and length depending on the complexity of the dataset. 6 Discussion and\n",
      "Conclusions The results in Section 5.1 reveal interesting weaknesses of MRL that would be logical directions for future work. (1) Optimizing the weightings of the nested losses to\n",
      "obtain a Pareto optimal accuracyvsefficiency tradeoff – a potential solution could emerge from adaptive loss balancing aspects of anytime neural networks [41]. (2) Using different\n",
      "losses at various fidelities aimed at solving a specific aspect of adaptive deployment – e.g. high recall for 8-dimension and robustness for 2048-dimension. (3) Learning a search\n",
      "datastructure, like differentiable kd tree, on top of Matryoshka Representation to enable dataset and representation aware retrieval. (4) Finally, the joint optimization of\n",
      "multiobjective MRL combined with endtoend learnable search datastructure to have datadriven adaptive largescale retrieval for webscale search applications. In conclusion, we\n",
      "presented Matryoshka Representation Learning (MRL), a flexible representation learning approach that encodes information at multiple granularities in a single embedding vector.\n",
      "This enables the MRL to adapt to a downstream task’s statistical complexity as well as the available compute resources. We demonstrate that MRL can be used for largescale adaptive\n",
      "classification as well as adaptive retrieval. On standard benchmarks, MRL matches the accuracy of the fixedfeature baseline despite using 14× smaller representation size on\n",
      "average. Furthermore, the Matryoshka Representation based adaptive shortlisting and reranking system ensures comparable mAP@10 to the baseline while being 128× cheaper in FLOPs and\n",
      "14× faster in wallclock time. Finally, most of the efficiency techniques for model inference and vector search are complementary to MRL further assisting in deployment at the\n",
      "computeextreme environments. Acknowledgments We are grateful to Srinadh Bhojanapalli, Lovish Madaan, Raghav Somani, Ludwig Schmidt, and Venkata Sailesh Sanampudi for helpful\n",
      "discussions and feedback. Aditya Kusupati also thanks Tom Duerig and Rahul Sukthankar for their support. Part of the paper’s largescale experimentation is supported through a\n",
      "research GCP credit award from Google Cloud and Google Research. Gantavya Bhatt is supported in part by the CONIX Research Center, one of six centers in JUMP, a Semiconductor\n",
      "Research Corporation (SRC) program sponsored by DARPA. Sham Kakade acknowledges funding from the NSF award CCF-1703574 and ONR N00014-22-1-2377. Ali Farhadi acknowledges funding\n",
      "from the NSF awards IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543 and gifts from Allen Institute for Artificial Intelligence. 10\n"
     ]
    }
   ],
   "source": [
    "# Read PDF and extract text\n",
    "text = read_pdf.get_text_from_pdf(PDF_FILE_PATH)\n",
    "\n",
    "# print original length\n",
    "print(len(text))\n",
    "text = textwrap.fill(text, width=180)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4427f5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Generator: /Users/sir/Downloads/HuggingFace/LLM/meta-Llama-3.2-1B-Instruct\n",
      "Model context length: 131072\n"
     ]
    }
   ],
   "source": [
    "# --- MODEL 1: THE \"GENERATOR\" (Llama 3.1 for summarizing) ---\n",
    "print(f\"Loading Generator: {LLM_PATH}\")\n",
    "\n",
    "# This line will now work correctly\n",
    "generator_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_PATH, \n",
    "    device_map=DEVICE, # Automatically map to your M3 GPU\n",
    "    dtype=torch.bfloat16, # Use bfloat16 for M3\n",
    "    trust_remote_code=True\n",
    ")\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(LLM_PATH)\n",
    "\n",
    "\n",
    "# Check model context length\n",
    "print(\"Model context length:\", generator_model.config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfccd696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GENERATED SUMMARY ---\n",
      "Here is a concise summary of the text in detail:  **Introduction**  The text\n",
      "discusses the concept of Matryoshka Representation Learning (MRL), a method for\n",
      "learning flexible and adaptive representations that can be used for various\n",
      "downstream tasks, such as classification and retrieval. MRL is a technique that\n",
      "encodes information at multiple granularities in a single embedding vector,\n",
      "enabling adaptive deployment in various environments.  **Background**  The text\n",
      "reviews the existing representation learning techniques, including supervised\n",
      "and unsupervised learning paradigms, and their limitations. It highlights the\n",
      "need for a flexible and adaptive representation learning approach that can\n",
      "handle varying computational resources and accuracy constraints.\n",
      "**Methodology**  The text introduces Matryoshka Representation Learning (MRL), a\n",
      "method that encodes information at different granularities in a single embedding\n",
      "vector. The method is based on the idea of nested representations, where each\n",
      "nested dimension is a coarser representation that is learned independently of\n",
      "the previous one.  **Adaptive Classification**  The text demonstrates the\n",
      "effectiveness of MRL for adaptive classification on various datasets, including\n",
      "ImageNet-1K and ImageNet-4K. It shows that MRL can achieve state-of-the-art\n",
      "performance on these datasets, even with smaller representation sizes.\n",
      "**Adaptive Retrieval**  The text also explores the application of MRL for\n",
      "adaptive retrieval, where the goal is to find images that belong to the same\n",
      "class as a query image. It shows that MRL can achieve comparable performance to\n",
      "the baseline retrieval method, but with a significant reduction in computational\n",
      "cost.  **Robustness and Ablations**  The text discusses the robustness of MRL\n",
      "models on out-of-domain datasets and ablation studies on various ablation\n",
      "settings. It also explores the potential of MRL for off-the-shelf pre-trained\n",
      "models and the design of weighttying techniques to improve accuracy.\n",
      "**Superclass Accuracy**  The text highlights the importance of superclass\n",
      "accuracy in adaptive retrieval and demonstrates that MRL can achieve state-of-\n",
      "the-art performance on superclass classification tasks.  **Discussion and\n",
      "Conclusions**  The text concludes that MRL is a flexible and adaptive\n",
      "representation learning approach that can be used for various downstream tasks.\n",
      "It highlights the potential of MRL for largescale adaptive classification and\n",
      "retrieval, and suggests that it can be used in various off-the-shelf pre-trained\n",
      "models. The text also discusses the potential of MRL for off-the-shelf pre-\n",
      "trained models and the design of weighttying techniques to improve accuracy.\n",
      "**Limitations and Future Work**  The text notes that there are several\n",
      "limitations of MRL, including the need for optimization of weightings of nested\n",
      "losses, the potential for loss of accuracy in lower dimensions, and the need for\n",
      "further investigation into robustness and superclass accuracy. The text also\n",
      "suggests that future work should focus on adaptive loss balancing, search\n",
      "datastructures, and joint optimization of multiobjective MRL.\n"
     ]
    }
   ],
   "source": [
    "# prompt construction\n",
    "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Clearly summarize the following text in detail in a concise manner and \n",
    "avoid personal opinions or commentary including extraneous information:\n",
    "\n",
    "{text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "input = generator_tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = generator_model.generate(\n",
    "        **input,\n",
    "        max_new_tokens=10000,         # Number of tokens to generate\n",
    "        do_sample=True,             # Enable sampling for more natural output\n",
    "        temperature=0.06,            # Controls randomness\n",
    "        top_p=0.9,                  # Nucleus sampling\n",
    "        pad_token_id=generator_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# 1. Decode the raw text\n",
    "# We only want the generated part, not the input prompt\n",
    "output_token_ids = outputs[0][len(input['input_ids'][0]):]\n",
    "raw_output = generator_tokenizer.decode(output_token_ids, skip_special_tokens=True)\n",
    "\n",
    "# 2. Clean the output (it might have extra spaces)\n",
    "response_only = raw_output.strip()\n",
    "\n",
    "# 3. Wrap and print the final, correct summary\n",
    "formatted_text = textwrap.fill(response_only, width=80)\n",
    "print(\"\\n--- GENERATED SUMMARY ---\")\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64418327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GENERATED SUMMARY ---\n",
      "Here is a summary of the text in ten sentences:  Matryoshka Representation\n",
      "Learning (MRL) is a method for learning flexible representations that can adapt\n",
      "to multiple downstream tasks with varying computational resources. The method\n",
      "encodes information at different granularities and allows for a single embedding\n",
      "to adapt to the computational constraints of downstream tasks. MRL minimizes the\n",
      "computational cost of inference and deployment by learning coarsetofine\n",
      "representations that are as accurate as independently trained low-dimensional\n",
      "representations. The method is designed to be adaptable to various\n",
      "representation learning frameworks, including supervised and unsupervised\n",
      "learning. MRL can be used for largescale adaptive classification and retrieval,\n",
      "and has been evaluated on several benchmarks, including ImageNet-1K and\n",
      "ImageNet-4K. The method has been shown to be more accurate than fixed-feature\n",
      "representations, even at lower dimensions. MRL has also been shown to be robust\n",
      "to out-of-domain datasets and has improved the accuracy of few-shot learning.\n",
      "The method has been adapted to off-the-shelf pre-trained models and has shown\n",
      "promising results in terms of efficiency and accuracy. MRL has also been shown\n",
      "to be able to capture the underlying hierarchy of class labels and to provide a\n",
      "single model that can adapt to different classes. The method has several\n",
      "potential applications, including adaptive classification, retrieval, and few-\n",
      "shot learning, and has the potential to be used in a wide range of applications.\n",
      "Overall, MRL is a flexible and efficient method for learning representations\n",
      "that can adapt to multiple downstream tasks with varying computational\n",
      "resources.\n"
     ]
    }
   ],
   "source": [
    "# prompt construction\n",
    "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Summarize the following text in no more than ten sentences, using a neutral \n",
    "and objective tone. Make sure the summary is clear, concise, and avoids personal \n",
    "opinions or commentary\n",
    "\n",
    "{text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "input = generator_tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = generator_model.generate(\n",
    "        **input,\n",
    "        max_new_tokens=10000,         # Number of tokens to generate\n",
    "        do_sample=True,             # Enable sampling for more natural output\n",
    "        temperature=0.06,            # Controls randomness\n",
    "        top_p=0.9,                  # Nucleus sampling\n",
    "        pad_token_id=generator_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# 1. Decode the raw text\n",
    "# We only want the generated part, not the input prompt\n",
    "output_token_ids = outputs[0][len(input['input_ids'][0]):]\n",
    "raw_output = generator_tokenizer.decode(output_token_ids, skip_special_tokens=True)\n",
    "\n",
    "# 2. Clean the output (it might have extra spaces)\n",
    "response_only = raw_output.strip()\n",
    "\n",
    "# 3. Wrap and print the final, correct summary\n",
    "formatted_text = textwrap.fill(response_only, width=80)\n",
    "print(\"\\n--- GENERATED SUMMARY ---\")\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0abe297",
   "metadata": {},
   "source": [
    "### New Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1eac8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading full text from '/Users/sir/Downloads/Data/PDF/test/Unlearn Dataset Bias in Natural Language Inference.pdf'...\n",
      "Extraction stop found immediately before: 'References'\n",
      "Successfully extracted and cleaned 36017 characters.\n",
      "36017\n",
      "Statistical natural language inference (NLI) models are susceptible to learning dataset bias: superﬁcial cues that happen to associate with the label on a particular dataset, but\n",
      "are not useful in general, e.g., negation words indicate contradiction. As exposed by several recent challenge datasets, these models perform poorly when such association is\n",
      "absent, e.g., predicting that “I love dogs.” contradicts “I don’t love cats.”. Our goal is to design learning algorithms that guard against known dataset bias. We formalize the\n",
      "concept of dataset bias under the framework of distribution shift and present a simple debiasing algorithm based on residual ﬁtting, which we call DRiFt. We ﬁrst learn a biased\n",
      "model that only uses features that are known to relate to dataset bias. Then, we train a debiased model that ﬁts to the residual of the biased model, focusing on examples that\n",
      "cannot be predicted well by biased features only. We use DRiFt to train three highperforming NLI models on two benchmark datasets, SNLI and MNLI. Our debiased models achieve\n",
      "signiﬁ- cant gains over baseline models on two challenge test sets, while maintaining reasonable performance on the original test sets. 1 Introduction Machine learning models have\n",
      "surpassed humanperformance on multiple language understanding benchmarks. However, transferring the success to realworld applications has been much slower due to the brittleness of\n",
      "these systems. For example, McCoy et al. (2019) show that models blindly predict the entailment relation for two sentences with high word overlap even if they have very different\n",
      "meanings, e.g., “The man hit a dog” and “The dog hit a man”. Jia and Liang (2017) show that reading comprehension models are easily distracted by irrelevant sentences containing\n",
      "key phrases from the g(x) y b(x) a Semantics: P: The little girl is sad. H: The girl is not sad. Word choice: “not” Label: contradiction Bias cause: annotation strategy g(x) y b(x)\n",
      "Semantics: P: The little girl is sad. H: The girl is not happy. Word choice: “not” Label: entailment Unknown cause a′ Training Testing Figure 1: An example of dataset bias in NLI.\n",
      "On the training data, the biased feature (“not”) is affected by crowd workers’ strategy of negating the premise to create a contradicting pair. However, at test time the word\n",
      "choice is affected by unknown sources, thus “not” may not be associated with the label “contradiction”. A model relying on the negation word to predict “contradiction” would fail\n",
      "on the shown test example. question. Similar failures have also been observed on paraphrase identiﬁcation and story cloze test. A common problem behind these failures is\n",
      "distribution shift. Our training data is often not a representative sample of realworld data due to their different datagenerating processes, thus models are susceptible to\n",
      "learning simple cues (e.g., lexical overlap) that work well on the majority of training examples but fail on more challenging test examples. Consider generating a contradicting\n",
      "pair of sentences for natural language inference (NLI) in Figure 1. Crowd workers tend to mechanically negate the premise sentence to save time, introducing an association between\n",
      "negation words (e.g., “not”) and the contradiction label. However, at test time, such association may not exist as data is now generated by end users. Thus, a model that heavily\n",
      "relies on the biased feature “not” would 133 fail. In this paper, we formalize dataset bias (Torralba and Efros, 2011) under the label shift assumption: the conditional\n",
      "distribution of the label given biased features changes at test time. Our goal is to design learning algorithms that are robust to dataset bias with a focus on NLI, i.e. predicting\n",
      "whether the premise sentence entails the hypothesis sentence. Typical debiasing approaches aim to remove biased features (e.g., gender and image texture) in the learned\n",
      "representation. However, biased features in textual data often con- ﬂate useful semantic information and superﬁcial cues, thus completely removing them might signiﬁcantly hurt\n",
      "prediction performance. Even when we are conﬁdent that the bias is irrelevant to prediction (e.g., gender), Gonen and Goldberg (2019) show that existing bias removal methods are\n",
      "insuf- ﬁcient. Instead of debiasing the data representation, our method (along with the concurrent work of Clark et al. (2019)) accounts for label shift given biased features by\n",
      "focusing on “hard” examples that cannot be predicted well using only biased features. We train a model in two steps. First, we train a biased model using insufﬁcient features such\n",
      "as overlapping words between the premise and the hypothesis. Next, we train a debiased model by ﬁtting to the residuals of the biased model. This step “unlearns” the bias by taking\n",
      "additional negative gradient updates on examples with low loss under the biased model (Section 3.2).1 At test time, only the debiased model is used for prediction. We call this\n",
      "learning algorithm DRiFt (Debias by Residual Fitting). We use DRiFt to train three highperforming NLI models on two benchmark datasets, SNLI and MNLI. Compared to baseline models\n",
      "trained by maximum likelihood estimation, our debiased models improve performance on several challenge datasets with only slight degradation on the original test sets. 2 Problem\n",
      "Statement Dataset bias. Let x 2 X be the input and y 2 Y be the label we want to predict. Given training examples (x, y) drawn from a distribution P, we 1 Note that dataset bias is\n",
      "ﬂagged by good performance despite insufﬁcient input, e.g., a highaccuracy hypothesisonly classiﬁer. deﬁne dataset bias as (partial) representation of x that exhibits label shift\n",
      "on the test distribution Q. Formally, assume that x can be represented by two components b(x) and g(x) conditionally independent given y. We have p(x, y) = p(b(x), g(x), y) (1) =\n",
      "p(g(x) | y)p(y | b(x))p(b(x)). (2) Let g(x) be the true effect of y such that their relationship does not change normally, i.e. p(g(x) | y) = q(g(x) | y). Let b(x) be biased\n",
      "features that happen to be predictive of y on P. For example, in Figure 1, g(x) represents semantics of the premis and hypothesis sentences, whereas b(x) represents speciﬁc word\n",
      "choices affected by varying sources. In the training data, the word “not” has a strong association with “contradiction” due to crowd workers’ writing strategies. Consequently, a\n",
      "model learned on the training data distribution P would degrade when such association no longer exists. Formally, both training and testing examples may exhibit biased features:\n",
      "p(b(x)) = q(b(x)), but dependence between these features and the label can change: p(y | b(x)) 6= q(y | b(x)). In a typical supervised learning setting with dataset bias, we do not\n",
      "observe examples from Q thus b(x) is unknown. Without additional information, achieving good performance on Q is impossible. Fortunately, oftentimes we do have domainspeciﬁc\n",
      "knowledge on what b(x) might be, e.g., the word overlapping heuristic in NLI. Therefore, our goal is to correct the model trained on P to perform well on Q given known dataset\n",
      "bias. Bias in NLI data. Dataset bias in SNLI and MNLI are largely due to the crowdsourcing process. Both are created by asking crowd workers to write three sentences (hypotheses)\n",
      "that are entailed by, neutral with, or contradict a given sentence drawn from a corpus (the premise). Gururangan et al. (2018); Poliak et al. (2018) show that certain words in the\n",
      "hypothesis have high pointwise mutual information with class labels regardless of the premise, which could be artifacts of speciﬁc annotation strategies. For example, one can\n",
      "create a neutral sentence by adding a cause (“because”) to the premise and create a contradicting sentence by negating (“no”, “never”) the premise. As a result, the majority of\n",
      "training examples can be 134 solved without much reasoning about sentence meanings. Subsequently, McCoy et al. (2019) report that models rely on high word overlap to predict\n",
      "entailment; Glockner et al. (2018); Naik et al. (2018) demonstrate that models struggle at even lexicallevel inference involving antonyms, hypernyms, etc. A natural question to ask\n",
      "then is whether there exist better data collection procedures that guard against these biases. We argue that this is not easy because in practice, we almost always have different\n",
      "datagenerating processes during training (generated from selected corpora and annotators) and test (generated by end users). Then, can we remove biased features from training\n",
      "examples? This is also infeasible because sometimes they contain the necessary information for prediction, e.g., removing words may destroy the sentence meaning. It is not the\n",
      "features that are biased but their relation with the label. Next, we describe our approach to mitigating this biased relation. 3 Approach 3.1 Overview The key idea of our approach\n",
      "is to ﬁrst detect biased examples given prior knowledge on potential dataset bias, then focus on learning from unbiased, hard examples. We describe the two steps in details below.\n",
      "Detect biased examples. How do we know if an example exhibits biased features? Although we cannot directly measure label shift without accessing the test data, we know that NLI\n",
      "models are unlikely to work well given insufﬁcient features. When it does work well given only partial semantics of the input, the good performance is likely due to dataset bias.\n",
      "For example, Gururangan et al. (2018) exposes annotation artifacts by showing that hypothesisonly models have unexpected high accuracy. Similarly, we train a biased classiﬁer using\n",
      "insufﬁcient features I(x), e.g., the hypothesis sentence. We assume that examples predicted well by the biased classiﬁer exhibit dataset bias, i.e. p(y | I(x)) is high but q(y |\n",
      "I(x)) is low. Importantly, while I(x) approximates b(x) given our prior knowledge, it does not necessarily capture all dataset bias, which depends on the unknown test distribution.\n",
      "In addition, I(x) may include useful information. For example, although bagofwords (BOW) features are insufﬁcient to represent precise sentence meaning, it encodes a distribution\n",
      "of possible meanings. Thus good performance of a BOW classiﬁer is not fully due to ﬁtting dataset bias. In practice, as we will see in the experiments (Section 4.5), good choices\n",
      "of I(x) capture biased features precisely, resulting in signiﬁcant performance drop of the biased classi- ﬁer on Q. Learn residuals of the biased classiﬁer. Our intuition is that\n",
      "the debiased classiﬁer should capture information beyond those contained in the biased classiﬁer. If the biased classiﬁer already has a small loss on an example, then there is not\n",
      "much to learn beyond the biased features; otherwise, the debiased classiﬁer should correct predictions of the biased classiﬁer. We implement the idea through a residual ﬁtting\n",
      "procedure (DRiFt). Let fs: X ! R and fd: X ! R be the biased and the debiased classiﬁers, and let L be the loss function. First, we learn fs with insufﬁcient features I(x) as the\n",
      "input: ✓⇤= arg min ✓ EP [L(fs(I(x); ✓), y)] . (3) Let f⇤(x) be the optimal predictor that minimizes the empirical risk on P. We deﬁne f⇤(x) def = fs(I(x); ✓⇤) + fd(x; φ⇤). (4) Thus\n",
      "fd ﬁts the residual of fs with respect to the target f⇤. To estimate parameters φ of fd, we ﬁx parameters of fs and minimize the loss: min φ EP [L(fs(I(x); ✓⇤) + fd(x; φ), y)] .\n",
      "(5) At test time, we only use the debiased classiﬁer fd. Consider the typical empirical risk minimization approach that estimates φ by minimizing EP [L(fd(x; φ), y)]. It is\n",
      "susceptible to relying on biased features when they predict well on the majority examples. In contrast, DRiFt ﬁrst learns fs which is intended to ﬁt potential bias in the data. It\n",
      "then learns fd that compensates fs without ﬁtting to the bias already captured by it. Next, we analyze the behavior of DRiFt using the crossentropy loss function, which is\n",
      "typically used for classiﬁcation problems. 3.2 Analysis with the CrossEntropy Loss In this section, we show that DRiFt adjusts the gradient on each example depending on how well it\n",
      "is predicted by the pretrained biased classiﬁer. 135 Given the crossentropy loss, our goal is to maximize the expected conditional loglikelihood of the data, EP [log p(y | x)]. A\n",
      "classiﬁer outputs a vector of scores for each of the K classes, f(x) = (f1(x), . . . , fK(x)) 2 RK, which are then mapped to a probability distribution p(y | x) by the softmax\n",
      "function. Given classiﬁers fs and fd, we have three choices of parametrization of the conditional probability p(y | x): ps(y | I(x)) / exp (fy s (I(x); ✓)) (6) pd(y | x) / exp ! fy\n",
      "d (x; φ) \" (7) pa(y | x) / exp ! fy s (I(x); ✓) + fy d (x; φ) \" / ps(y | I(x))pd(y | x). (8) To learn the classiﬁer fd, standard maximum likelihood estimation (MLE) uses pd(y | x),\n",
      "whereas DRiFt uses pa(y | x) given pretrained fs with ﬁxed parameters. Let us ﬁrst compare the two learning objectives. Denote ps(y | I(x); ✓⇤) by p⇤ s(y | I(x)). DRiFt maximizes\n",
      "JD(φ) = X (x,y)⇠D log pa(y | x; ✓⇤, φ) (9) = C + X (x,y)⇠D [log pd(y | x; φ)− log K X k=1 p⇤ s(k | I(x))pd(k | x; φ)] , (10) where D denotes the training set and C = P (x,y)⇠D log\n",
      "p⇤ s(k | I(x)) is a constant. Compare (10) with the MLE objective: JMLE(φ) = X (x,y)⇠D log pd(y | x; φ) . (11) We see that JD(φ) has an additional regularizer for each example x:\n",
      "R(x) def = −log K X k=1 p⇤ s(k | I(x))pd(k | x) . (12) Geometrically, it encourages output from the debiased classiﬁer, pd, to have minimal projection on ps predicted by the biased\n",
      "classiﬁer. Next, let’s look at the effect of this regularizer through its gradient. Let Z(x) be the normalizer P k p⇤ s(k | I(x))pd(k | x). Then, we have rφR(x) = − P k p⇤ s(k |\n",
      "I(x))rφpd(k | x) P k p⇤s(k | I(x))pd(k | x) = − X k pa(k | x)rφ log pd(k | x), which is derived by writing rφpd as pdrφ log pd. Taking a negative step in the direction of rφ log\n",
      "pd(k | x) corresponds to downweighting the probability pd(k | x). Intuitively, the model tries to reweight the output distribution by the gradient weights pa(k | x). Note that pa(k\n",
      "| x) / p⇤ s(k | I(x))pd(k | x) . (13) For an example (x, y), large values of p⇤ s(y | I(x)) indicate that I(x) is likely to contain biased features. If pd(y | x) is also large, the\n",
      "model is probably picking up the bias since pd has access to complete information in x including the biased features, in which case a relatively large negative step is taken to\n",
      "correct it. In the extreme case where the biased classiﬁer makes perfect prediction, we have p⇤ s(y | I(x)) ! 1 thus rφR(x) ! −rφ log pd(y | x), canceling the MLE gradient rφ log\n",
      "pd(y | x). As a result, the gradient on this example is zero, and there is nothing to be learned. At the other end where I(x) does not provide any useful information, the biased\n",
      "classiﬁer outputs a uniform distribution p⇤ s(y | I(x)) = 1/K, thus pa(y | x) = pd(y | x) and the gradient on this example is reduced to the MLE gradient. 4 Experiments We ﬁrst\n",
      "evaluate our method using synthetic bias to show its effectiveness under different amount of dataset bias. We then test on two challenge datasets using different biased classiﬁers.\n",
      "We show that DRiFt consistently outperforms MLE on the challenge datasets given different NLI models, especially when the insufﬁcient features capture dataset bias exploited by the\n",
      "challenge data. 4.1 Training Data We evaluate DRiFt on two benchmarking NLI datasets: SNLI and MNLI. Each pair of premise and hypothesis sentences has a label from one of\n",
      "“entailment”, “contradiction”, or “neutral”. Sentences from SNLI are derived from image captions, whereas MNLI covers a broader range of styles and topics. Statistics of the two\n",
      "datasets are shown in Table 1. All MNLI results are on the matched development set.2 2 MNLI has two development sets, one from the same source as the training data (matched) and\n",
      "one from different sources (mismatched). We trained two sets of models using their corresponding development sets for model selection and obtained similar results. Thus we focus on\n",
      "the “matched” results. 136 Dataset Train Dev Test SNLI 549,367 9842 9842 MNLI 392,702 9815 - Table 1: Statistics of training datasets. The test sets of MNLI are hosted through\n",
      "Kaggle competitions. 4.2 Models and Training Details DRiFt is a general learning algorithm that works with any biased/debiased models. Below we describe the three key components of\n",
      "our approaches: the learning algorithm, the biased model with its insufﬁcient features, and the debiased model. Learning algorithms. We compare DRiFt with MLE, as well as a simpler\n",
      "variant of DRiFt: instead of the residual ﬁtting, we remove the examples predicted correctly by the biased classi- ﬁer and train on the rest. We call this baseline RM, which is\n",
      "also conceived by Gururangan et al. (2018). MLE only trains the debiased model. Both DRiFt and RM rely on an additional biased model that captures potential dataset bias. Biased\n",
      "models. We consider three insufﬁcient representations that exploit various NLI dataset biases reported in prior work. HYPO is a ﬁnetuned BERT classiﬁer that uses only the\n",
      "hypothesis sentence. CBOW is a continuous bagofwords classiﬁer. Similar to Mou et al. (2016), we represent both the premise and the hypothesis as the respective sums of their word\n",
      "embeddings. We then concatenate the premise and the hypothesis embeddings, their difference, and their elementwise product. The ﬁnal representation is passed through a onelayer\n",
      "fully connected network with ReLU activation. HAND is a classiﬁer using handcrafted features based on error analysis in Naik et al. (2018). Speciﬁcally, we include tokens in the\n",
      "hypothesis that are also in the premise, tokens unique to the hypothesis, Jaccard similarity between the two sentences, whether negation words (“not” and “n’t”) are included, and\n",
      "length difference computed by |Lp−Lh| Lp+Lh where Lp and Lh are numbers of tokens in the premise and the hypothesis. We represent the overlapping and the nonoverlapping tokens as\n",
      "the respective sums of their word embeddings. The embeddings are then concatenated with the dense features and passed through a onelayer fully connected network with ReLU\n",
      "activation. Debiased models. We choose three highperforming models of different capability. DA is the Decomposable Attention model introduced by Parikh et al. (2016), which relies\n",
      "on the interaction between words in the premise and the hypothesis. It does not use any word order information. We used the variant without intrasentence attention.3 ESIM is the\n",
      "Enhanced Sequential Inference Model. It ﬁrst encodes the premise and the hypothesis by a bidirectional LSTM, aligns the contextual word embeddings similar to Parikh et al. (2016),\n",
      "and uses another “inference” bidirectional LSTM to aggregate information. Thus it has access to the nonlocal context. BERT is the Bidirectional Encoder Representations from\n",
      "Transformers that recently improved performance on MNLI signiﬁ- cantly. It uses contextual embeddings pretrained from large corpora. Hyperparameters. For nonBERT models, word\n",
      "embeddings are initialized with the 840B.300d pretrained GloVe word vectors and ﬁnetuned during training. For DA and ESIM, hyperparameters of the model architecture are the same as\n",
      "those reported in the original papers. We ﬁnetune all BERT models from the pretrained BERTbaseuncased model.4 We train all models using the Adam (Kingma and Ba, 2014) optimizer\n",
      "with β1 = 0.9, β2 = 0.999, L2 weight decay of 0.01, learning rate warmup for the ﬁrst 10% of updates and linear decay afterwards. We use a dropout rate of 0.1 for all models except\n",
      "ESIM, which has a dropout rate of 0.5. BERT and nonBERT models are trained with a learning rate of 2e-5 and 1e-4, respectively. For MLE, we train BERT for 4 epochs and the rest for\n",
      "30 epochs. When training the debiased model in DRiFt, we ﬁnd that the models converge slowly thus we train BERT for 8 epochs and the rest for 80 epochs. 3 We removed the projection\n",
      "layers of the word embeddings as it speeds up training without hurting performance in our experiments. 4 bert/index.html 137 model 0.6 0.7 0.8 0.9 1.0 accuracy BERT DA ESIM 0.2 0.4\n",
      "0.6 0.8 cheating rate 0.2 0.4 0.6 0.8 cheating rate 0.2 0.4 0.6 0.8 cheating rate DRiFthypo MLE Rmcheat method Figure 2: Accuracy on SNLI test set augmented with cheating features,\n",
      "which leak the groundtruth labels on training data but not on test data. Models trained by MLE degrade signiﬁcantly when a majority of examples are cheatable, whereas debiased\n",
      "models trained by DRiFt maintain similar accuracies across different cheating rates. 4.3 InDistribution Performance We ﬁrst evaluate the models’ indistribution performance where\n",
      "they are trained and evaluated on splits from the same dataset. Results of the biased models are reported in Table 2. All exceeds the majorityclass baseline by a large margin,\n",
      "indicating that a majority of examples can be solved by superﬁcial cues. Results of the debiased models are reported in Table 3. Baseline results from our implementations are\n",
      "comparable to prior reported performance (row “MLE”). Debiased models trained by DRiFt show some degradation on indistribution data, especially for the less powerful DA and ESIM\n",
      "models. The accuracy drop is expected due to two reasons. First, DRiFt assumes distribution shift thus does not optimize performance on the training distribution P. Second, the\n",
      "effective training data size is reduced by negative gradients on potentially biased examples; this effect is exaggerated by RM, which shows signiﬁcant indistribution degradation.\n",
      "Similar tradeoff between indistribution accuracy and robustness on outofdistribution data has also been observed in adversarial training. Dataset majority HYPO CBOW HAND SNLI 34.2\n",
      "61.8 81.2 76.7 MNLI 35.4 52.5 66.1 65.4 Table 2: Accuracy of biased classiﬁers on SNLI test set and MNLI development set. All exceeds the majorityclass baseline by a large margin,\n",
      "signaling dataset bias. SNLI MNLI BERT DA ESIM BERT DA ESIM MLE 90.8 85.3 88.0 84.5 72.2 78.1 DRiFtHYPO 89.8 83.9 86.3 84.3 68.6 75.0 DRiFtCBOW 84.7 62.6 62.3 82.1 56.3 68.8\n",
      "DRiFtHAND 86.5 75.0 79.2 81.7 58.8 68.9 RMHYPO 71.2 67.0 70.3 65.5 57.5 63.0 RMCBOW 35.8 27.1 22.2 54.9 26.8 27.1 RMHAND 46.3 37.2 38.1 51.7 34.6 37.4 Table 3: Accuracy of models\n",
      "trained by MLE, DRiFt, and RM with different biased models. Training and test examples are from the same dataset. Intensity of the red highlights corresponds to absolute drop in\n",
      "accuracy with respect to the MLE baseline. RM signiﬁ- cantly hurts indistribution performance. DRiFt maintains reasonable performance. 4.4 Synthetic Bias In this section, we\n",
      "evaluate our model under controlled, synthetic dataset bias on SNLI. Recall our deﬁnition of dataset bias: the conditional distribution of the label y given biased features are\n",
      "different on training and test sets. Therefore, we inject bias into each example by adding a cheating feature that encodes its label. On training and development examples, the\n",
      "cheating feature encodes the ground truth label with probability pcheat (the cheating rate), and a random label otherwise. On test examples, the cheating feature always encodes a\n",
      "random label. Thus a model relying on the cheating feature would perform poorly on the test set. Speciﬁcally, we prepend the hypothesis with a string “{label} and” where label 2\n",
      "{entailment, contradiction, neutral}. To simulate 138 the fact that we often cannot pinpoint biased features until the model fails on some test examples, we choose HYPO as our\n",
      "biased classiﬁer. That is to say, we have a rough idea that the bias might be in the hypothesis but do not know what it is exactly. We train all three base models (DA, ESIM, and\n",
      "BERT) using MLE and DRiFt, respectively. Our results are shown in Figure 2. All MLE models are reasonably robust to a mild amount of bias. However, when a majority (pcheat > 0.6)\n",
      "of training examples contains the bias, their accuracy decreases signiﬁcantly: about 20% drop at pcheat = 0.9 compared to the baseline accuracy when no cheating features are\n",
      "injected. BERT is slightly more robust than DA and ESIM, possibly due to the regularization effect of pretrained embeddings. In contrast, our debiased models (DRiFtHYPO) maintain\n",
      "similar accuracies with increasing cheating rates and have a maximum accuracy drop of about 5%. Two questions remain, though: (1) Why does the accuracy of debiased models still\n",
      "drop a bit at high cheating rates? (2) Why is the baseline accuracy of DRiFt lower than MLE? We answer these questions by analyzing the upper bound performance of our method below.\n",
      "Bestcase scenario. In the ideal case, we know precisely what the bias is. Consider a biased classiﬁer that only uses the cheating feature as its input. It predicts biased examples\n",
      "perfectly, i.e. ps(y | b(x)) = 1 and ps(k | b(x)) = 0 8k 6= y, and predicts the rest unbiased examples uniformly at random. Based on our discussion at the end of Section 3.2, the\n",
      "biased examples have zero gradients and unbiased examples have the same gradients as in MLE. In this case, our method is equivalent to removing biased examples and training a\n",
      "classiﬁer on the rest, i.e. RMcheat. In Figure 2, we see that it completely dominates MLE. The accuracy of RMcheat still drops when pcheat is large, because there are fewer “good”\n",
      "examples to learn from, not due to ﬁtting the bias. Similarly, DRiFtHYPO has lower overall accuracy compared to RMcheat, because HYPO captures additional (unbiased) features that\n",
      "cannot be fully learned by the debiased model. Worstcase scenario. In the extreme case when pcheat = 1, all models’ predictions on the test set are random guesses. For MLE, the\n",
      "biased features are no longer differentiable from the generalizable ones, thus there is no reason not to use them. For DRiFt, since the biased model achieves perfect prediction on\n",
      "all training examples, the debiased model receives zero gradient. Therefore, when strong bias presents on all examples, we need more information to correct the bias, e.g.,\n",
      "collecting additional data or augmenting examples. method lexical subseq const E ¬E E ¬E E ¬E HYPO 52.6 44.4 54.5 44.3 45.6 16.7 CBOW 63.2 16.0 66.2 33.7 63.2 38.5 HAND 66.7 0.0\n",
      "66.7 0.0 66.7 0.0 model: BERT MLE 67.2 7.8 66.7 0.4 68.1 11.9 DRiFtHYPO 84.7 79.8 69.0 23.7 72.7 40.8 DRiFtCBOW 80.8 75.2 68.5 29.5 71.5 40.3 DRiFtHAND 77.4 70.9 71.2 41.2 75.8\n",
      "61.0 RMHYPO 67.2 46.0 65.2 36.6 75.5 72.2 RMCBOW 5.4 66.4 8.5 64.2 34.8 65.3 RMHAND 10.0 66.0 4.7 66.3 9.1 67.3 model: DA MLE 66.6 0.5 66.6 0.3 66.5 0.4 DRiFtHYPO 66.3 1.7 66.9 5.5\n",
      "66.3 8.4 DRiFtCBOW 65.3 7.2 66.1 9.6 65.1 9.1 DRiFtHAND 60.5 27.1 61.4 44.9 55.9 48.3 RMHYPO 65.1 9.6 66.2 15.0 66.2 18.8 RMCBOW 0.4 66.6 1.3 66.7 0.8 66.5 RMHAND 10.3 65.8 8.9\n",
      "65.7 13.9 64.7 model: ESIM MLE 65.8 3.2 67.2 4.6 65.5 2.8 DRiFtHYPO 64.3 10.5 68.3 16.3 68.1 29.3 DRiFtCBOW 63.2 14.4 66.8 20.1 64.9 22.7 DRiFtHAND 61.2 19.6 63.7 39.4 64.8 48.3\n",
      "RMHYPO 63.3 12.8 64.1 24.8 71.3 46.0 RMCBOW 4.5 65.7 6.0 65.2 16.9 63.8 RMHAND 25.8 60.8 18.3 67.3 13.1 65.9 Table 4: F1 scores of the entailment (E) and nonentailment (¬E) classes\n",
      "on HANS. All models are trained on MNLI and results are shown on three subsets targeting at different biases: lexical overlap (lexical), subsequence overlap (subseq), and\n",
      "constituent overlap (const). Intensity of the Blue and red highlights corresponds to absolute increase and decrease of scores with respect to MLE. DRiFt signiﬁcantly improves\n",
      "results on challenging ¬E examples without hurting performance on E, whereas RM improves scores on ¬E at the cost of performance on E. 4.5 Word Overlap Bias We evaluate our method\n",
      "on word overlap bias in NLI. McCoy et al. (2019) show that models 139 trained on MNLI largely rely on word overlap between the premise and the hypothesis to make entailment\n",
      "predictions. They created a challenge dataset (HANS) where premises may not entail high wordoverlapping hypotheses. Speciﬁcally, a model biased by word overlap would fail on three\n",
      "types of nonentailment examples: (1) Lexical overlap, e.g., “The doctor visited the lawyer.” ; “The lawyer visited the doctor.”. (2) Subsequence, e.g., “The senator near the lawyer\n",
      "danced.” ; “The lawyer danced.”. (3) Constituent, e.g., “The lawyers resigned, or the artist slept.” ; “The artist slept.”. We evaluate both biased and debiased models on the three\n",
      "subsets of HANS and show F1 scores for each class in Table 4. As expected, models trained by MLE almost always predict entailment (E), and thus performs poorly for the\n",
      "nonentailment class (¬E). DRiFt improves performance on ¬E in all cases with little degradation on E. In contrast, RM improves performance on ¬E at the cost of signiﬁcant\n",
      "degradation on E. Among all biased models, HAND produces the best debiasing results because it is designed to ﬁt the word overlap bias, and indeed has zero recall on ¬E when tested\n",
      "on HANS. On the contrary, the improvement from HYPO is lower because it does not capture any word overlap bias. Correspondingly, its performance drop on HANS is minimal compared to\n",
      "its indistribution performance. Among all debiased models, BERT has the best overall performance. We hypothesize that pretraining on large data improves model robustness in\n",
      "addition to the debiasing effect from DRiFt. 4.6 Stress Tests In addition to the word overlap bias exploited by HANS, there are other known biases such as negation words and\n",
      "sentence lengths. Naik et al. (2018) conduct a detailed error anlaysis on MNLI and create six stress test sets (STRESS) targeting at each type of error. We focus on the word\n",
      "overlap and negation stress test sets, which expose dataset bias as opposed to model weakness according to Liu et al. (2019). A model biased by word overlap rate and negation words\n",
      "are expected to have low accuracy on the entailment class on challenge data. The complete results are shown in Appendix A. In Table 5, we show the F1 scores of each class for all\n",
      "models on STRESS.5 Compared to results 5 Since results of RM are similar to those in Table 4, we method Negation Overlap E C N E C N HYPO 41.2 52.4 50.5 44.2 52.8 51.7 CBOW 20.1\n",
      "48.2 53.9 49.7 52.9 55.6 HAND 37.5 45.0 57.3 56.7 50.1 57.8 model: BERT MLE 2.4 81.1 56.5 19.2 83.3 59.4 DRiFtHYPO 7.3 80.7 55.6 27.5 81.1 59.1 DRiFtCBOW 17.9 81.7 55.5 18.3 80.0\n",
      "56.6 DRiFtHAND 4.3 80.6 55.5 15.0 81.9 57.4 model: DA MLE 17.4 47.3 55.3 46.7 60.5 57.8 DRiFtHYPO 11.8 47.0 51.8 41.6 59.4 55.6 DRiFtCBOW 28.4 21.4 39.5 35.2 41.7 43.8 DRiFtHAND\n",
      "24.7 42.0 46.4 42.2 56.0 49.9 model: ESIM MLE 12.0 72.7 54.6 27.6 76.4 57.5 DRiFtHYPO 22.8 67.7 54.0 37.5 73.2 56.7 DRiFtCBOW 32.7 62.3 46.9 30.4 65.6 49.8 DRiFtHAND 15.8 64.6 51.8\n",
      "39.2 70.7 53.9 Table 5: F1 scores of each class on STRESS. Intensity of the Blue and red highlights corresponds to absolute increase and decrease of scores with respect to MLE.\n",
      "DRiFt improves results on E (that exhibits label shift) with some degradation on other classes for DA and ESIM. on HANS, STRESS sees lower overall improvement from debiasing. One\n",
      "reason is that STRESS decreases word overlap rate and injects negation words by appending distractor phrases, i.e. “true is true” and “false is not true”. While this introduces\n",
      "label shift on biased features, it also introduces covariate shift on the input. For example, although HAND contains features designed to use word overlap rate (Jaccard similarity)\n",
      "and negation words, its does not have big performance drop on the challenge data compared to its indistribution performance, showing that that distractor phrases may affect the\n",
      "model in other ways. While all debiased models show improvement on E, both DA and ESIM suffer from degradation on the other two classes, especially when trained by DRiFtCBOW. We\n",
      "posit two reasons. First, while CBOW is insufﬁcient to represent complete sentence meaning, it does encode a distribution of possible meanings. Thus models debiased by DRiFtCBOW\n",
      "might discard useful information. Second, model capacity limits what is learned beyond a BOW representation. DA shows the most put them in Appendix A. 140 degradation since it only\n",
      "uses local word interaction, thus is essentially a BOW model. In contrast, BERT has little degradation on indistribution examples regardless of the biased classiﬁer. 5 Related Work\n",
      "and Discussion Adversarial data collection. Aside from NLI, dataset bias has been exposed on benchmarks for other NLP tasks as well, e.g., paraphrase identiﬁcation, story close\n",
      "test, reading comprehension (Kaushik and Lipton, 2018), coreference resolution, and visual question answering. Most bias is resulted from artifacts in the data selection procedure\n",
      "and shortcuts taken by crowd workers. To systematically minimize bias during data collection, adversarial ﬁltering methods have been proposed to discard examples predicted well by\n",
      "a simple classiﬁer. This is similar to the RM baseline, except that we apply “ﬁltering” at training time. In general, our debiasing methods are complementary to adversarial data\n",
      "collection methods. Debiased representation. Our work is closely related to the line of work on removing bias in data representations. Bolukbasi et al. (2016); Zhao et al. (2018b)\n",
      "learn genderneutral word embeddings by forcing certain dimensions to be free of gender information. Similarly, Wang et al. (2019a) construct a biased classiﬁer and project its\n",
      "representation out of the model’s representation. For NLI, Belinkov et al. (2019) use adversarial learning to remove hypothesisrelated bias in the sentence representations.\n",
      "However, for some NLP applications it may not be easy to separate biased features from useful semantic representations, thus we correct the conditional distribution of the class\n",
      "label given these biased features instead of removing them from the input. Concurrently, Clark et al. (2019) take the same approach and further show its effectiveness on additional\n",
      "tasks including reading comparehension and visual question answering. Distribution shift. Covariate shift and label shift are two wellstudied settings under distribution shift,\n",
      "which makes different assumptions on how p(x, y) changes. However, most works in these settings assume access to unlabeled data from the target distribution. Our objective is more\n",
      "related to distributionally robust optimization, which does not assume access to target data and optimizes the worstcase performance under unknown, bounded distribution shift. In\n",
      "contrast, we leverage prior knowledge on potential dataset bias. Data augmentation. An effective way to tackle the challenge datasets is to train or ﬁnetune on similar examples,\n",
      "which explicitly correct the training data distribution. However, constructing challenge examples often rely on handcrafted rules that target a speciﬁc type of bias, e.g., swapping\n",
      "male and female entities, synonym/antonym substitution, and syntactic rules, and may require human veriﬁcation. Data augmentation provides a way to encode our prior knowledge on\n",
      "the task, e.g., swapping genders does not affect coreference resolution result, and syntactic transformations may affect sentence meanings. Therefore, a related direction is to\n",
      "develop generic augmentation techniques with linguistic priors. 6 Conclusion Across all different dataset biases, the fundamental problem is that the majority training examples are\n",
      "not representative of the realworld data distribution (including the challenge data), thus minimizing the average training loss no longer accurately describes our objective. In\n",
      "this paper, we tackle the problem by adapting the learning objective to focus on examples that cannot be easily solved by biased features. We show that our debiasing method\n",
      "improves model performance on challenge data given known dataset bias. However, current improvements largely rely on taskspeciﬁc prior knowledge, thus an important next step is to\n",
      "develop more general methods that tackle different types of biases. Acknowledgments Yanchao Ni worked on an earlier version of this project while he was at New York University. We\n",
      "thank the GluonNLP community for their support on reproducing prior results. 141\n"
     ]
    }
   ],
   "source": [
    "# PDF File Path\n",
    "PDF_FILE_PATH = \"/Users/sir/Downloads/Data/PDF/test/Unlearn Dataset Bias in Natural Language Inference.pdf\" \n",
    "\n",
    "# Read PDF and extract text\n",
    "text = read_pdf.get_text_from_pdf(PDF_FILE_PATH)\n",
    "\n",
    "# print original length\n",
    "print(len(text))\n",
    "text = textwrap.fill(text, width=180)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e36fb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GENERATED SUMMARY ---\n",
      "Here is a summary of the text in ten sentences:  The paper discusses the problem\n",
      "of dataset bias in natural language inference (NLI) models, which can lead to\n",
      "poor performance on challenging datasets. Dataset bias occurs when the model is\n",
      "trained on a dataset that is not representative of the real-world data\n",
      "distribution. The authors propose a debiasing algorithm called DRiFt, which\n",
      "learns a debiased model by first learning a biased model on a training dataset\n",
      "and then removing the biased features from the debiased model. The debiased\n",
      "model is then used to predict the test data. The authors analyze the behavior of\n",
      "DRiFt using the cross-entropy loss function and show that it adjusts the\n",
      "gradient on each example depending on how well it is predicted by the biased\n",
      "model. The authors also evaluate DRiFt on two benchmark datasets, SNLI and MNLI,\n",
      "and show that it outperforms a baseline model on the challenge datasets. They\n",
      "also evaluate the method on a synthetic dataset bias and find that it improves\n",
      "performance on challenging examples. The authors also evaluate the method on a\n",
      "stress test set and find that it improves performance on non-entailment examples\n",
      "but degrades performance on entailment examples. The authors propose a related\n",
      "method called Negation Overlap E, which improves performance on entailment\n",
      "examples but degrades performance on non-entailment examples. The authors also\n",
      "discuss the challenges of dealing with word overlap bias and negation bias, and\n",
      "propose a method to correct for these biases. The authors conclude that their\n",
      "debiasing method is a promising approach to addressing dataset bias in NLI\n",
      "models.\n"
     ]
    }
   ],
   "source": [
    "# prompt construction\n",
    "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Summarize the following text in no more than ten sentences, using a neutral \n",
    "and objective tone. Make sure the summary is clear, concise, and avoids personal \n",
    "opinions or commentary\n",
    "\n",
    "{text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "input = generator_tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = generator_model.generate(\n",
    "        **input,\n",
    "        max_new_tokens=700,         # Number of tokens to generate\n",
    "        do_sample=True,             # Enable sampling for more natural output\n",
    "        temperature=0.06,            # Controls randomness\n",
    "        top_p=0.9,                  # Nucleus sampling\n",
    "        pad_token_id=generator_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# 1. Decode the raw text\n",
    "# We only want the generated part, not the input prompt\n",
    "output_token_ids = outputs[0][len(input['input_ids'][0]):]\n",
    "raw_output = generator_tokenizer.decode(output_token_ids, skip_special_tokens=True)\n",
    "\n",
    "# 2. Clean the output (it might have extra spaces)\n",
    "response_only = raw_output.strip()\n",
    "\n",
    "# 3. Wrap and print the final, correct summary\n",
    "formatted_text = textwrap.fill(response_only, width=80)\n",
    "print(\"\\n--- GENERATED SUMMARY ---\")\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e64382b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9596])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2e1183b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full text: user\n",
      "\n",
      "Summarize the following text in no more than ten sentences, using a neutral \n",
      "and objective tone. Make sure the summary is clear, concise, and avoids personal \n",
      "opinions or commentary\n",
      "\n",
      "Statistical natural language inference (NLI) models are susceptible to learning dataset bias: superﬁcial cues that happen to associate with the label on a particular dataset, but\n",
      "are not useful in general, e.g., negation words indicate contradiction. As exposed by several recent challenge datasets, these models perform poorly when such association is\n",
      "absent, e.g., predicting that “I love dogs.” contradicts “I don’t love cats.”. Our goal is to design learning algorithms that guard against known dataset bias. We formalize the\n",
      "concept of dataset bias under the framework of distribution shift and present a simple debiasing algorithm based on residual ﬁtting, which we call DRiFt. We ﬁrst learn a biased\n",
      "model that only uses features that are known to relate to dataset bias. Then, we train a debiased model that ﬁts to the residual of the biased model, focusing on examples that\n",
      "cannot be predicted well by biased features only. We use DRiFt to train three highperforming NLI models on two benchmark datasets, SNLI and MNLI. Our debiased models achieve\n",
      "signiﬁ- cant gains over baseline models on two challenge test sets, while maintaining reasonable performance on the original test sets. 1 Introduction Machine learning models have\n",
      "surpassed humanperformance on multiple language understanding benchmarks. However, transferring the success to realworld applications has been much slower due to the brittleness of\n",
      "these systems. For example, McCoy et al. (2019) show that models blindly predict the entailment relation for two sentences with high word overlap even if they have very different\n",
      "meanings, e.g., “The man hit a dog” and “The dog hit a man”. Jia and Liang (2017) show that reading comprehension models are easily distracted by irrelevant sentences containing\n",
      "key phrases from the g(x) y b(x) a Semantics: P: The little girl is sad. H: The girl is not sad. Word choice: “not” Label: contradiction Bias cause: annotation strategy g(x) y b(x)\n",
      "Semantics: P: The little girl is sad. H: The girl is not happy. Word choice: “not” Label: entailment Unknown cause a′ Training Testing Figure 1: An example of dataset bias in NLI.\n",
      "On the training data, the biased feature (“not”) is affected by crowd workers’ strategy of negating the premise to create a contradicting pair. However, at test time the word\n",
      "choice is affected by unknown sources, thus “not” may not be associated with the label “contradiction”. A model relying on the negation word to predict “contradiction” would fail\n",
      "on the shown test example. question. Similar failures have also been observed on paraphrase identiﬁcation and story cloze test. A common problem behind these failures is\n",
      "distribution shift. Our training data is often not a representative sample of realworld data due to their different datagenerating processes, thus models are susceptible to\n",
      "learning simple cues (e.g., lexical overlap) that work well on the majority of training examples but fail on more challenging test examples. Consider generating a contradicting\n",
      "pair of sentences for natural language inference (NLI) in Figure 1. Crowd workers tend to mechanically negate the premise sentence to save time, introducing an association between\n",
      "negation words (e.g., “not”) and the contradiction label. However, at test time, such association may not exist as data is now generated by end users. Thus, a model that heavily\n",
      "relies on the biased feature “not” would 133 fail. In this paper, we formalize dataset bias (Torralba and Efros, 2011) under the label shift assumption: the conditional\n",
      "distribution of the label given biased features changes at test time. Our goal is to design learning algorithms that are robust to dataset bias with a focus on NLI, i.e. predicting\n",
      "whether the premise sentence entails the hypothesis sentence. Typical debiasing approaches aim to remove biased features (e.g., gender and image texture) in the learned\n",
      "representation. However, biased features in textual data often con- ﬂate useful semantic information and superﬁcial cues, thus completely removing them might signiﬁcantly hurt\n",
      "prediction performance. Even when we are conﬁdent that the bias is irrelevant to prediction (e.g., gender), Gonen and Goldberg (2019) show that existing bias removal methods are\n",
      "insuf- ﬁcient. Instead of debiasing the data representation, our method (along with the concurrent work of Clark et al. (2019)) accounts for label shift given biased features by\n",
      "focusing on “hard” examples that cannot be predicted well using only biased features. We train a model in two steps. First, we train a biased model using insufﬁcient features such\n",
      "as overlapping words between the premise and the hypothesis. Next, we train a debiased model by ﬁtting to the residuals of the biased model. This step “unlearns” the bias by taking\n",
      "additional negative gradient updates on examples with low loss under the biased model (Section 3.2).1 At test time, only the debiased model is used for prediction. We call this\n",
      "learning algorithm DRiFt (Debias by Residual Fitting). We use DRiFt to train three highperforming NLI models on two benchmark datasets, SNLI and MNLI. Compared to baseline models\n",
      "trained by maximum likelihood estimation, our debiased models improve performance on several challenge datasets with only slight degradation on the original test sets. 2 Problem\n",
      "Statement Dataset bias. Let x 2 X be the input and y 2 Y be the label we want to predict. Given training examples (x, y) drawn from a distribution P, we 1 Note that dataset bias is\n",
      "ﬂagged by good performance despite insufﬁcient input, e.g., a highaccuracy hypothesisonly classiﬁer. deﬁne dataset bias as (partial) representation of x that exhibits label shift\n",
      "on the test distribution Q. Formally, assume that x can be represented by two components b(x) and g(x) conditionally independent given y. We have p(x, y) = p(b(x), g(x), y) (1) =\n",
      "p(g(x) | y)p(y | b(x))p(b(x)). (2) Let g(x) be the true effect of y such that their relationship does not change normally, i.e. p(g(x) | y) = q(g(x) | y). Let b(x) be biased\n",
      "features that happen to be predictive of y on P. For example, in Figure 1, g(x) represents semantics of the premis and hypothesis sentences, whereas b(x) represents speciﬁc word\n",
      "choices affected by varying sources. In the training data, the word “not” has a strong association with “contradiction” due to crowd workers’ writing strategies. Consequently, a\n",
      "model learned on the training data distribution P would degrade when such association no longer exists. Formally, both training and testing examples may exhibit biased features:\n",
      "p(b(x)) = q(b(x)), but dependence between these features and the label can change: p(y | b(x)) 6= q(y | b(x)). In a typical supervised learning setting with dataset bias, we do not\n",
      "observe examples from Q thus b(x) is unknown. Without additional information, achieving good performance on Q is impossible. Fortunately, oftentimes we do have domainspeciﬁc\n",
      "knowledge on what b(x) might be, e.g., the word overlapping heuristic in NLI. Therefore, our goal is to correct the model trained on P to perform well on Q given known dataset\n",
      "bias. Bias in NLI data. Dataset bias in SNLI and MNLI are largely due to the crowdsourcing process. Both are created by asking crowd workers to write three sentences (hypotheses)\n",
      "that are entailed by, neutral with, or contradict a given sentence drawn from a corpus (the premise). Gururangan et al. (2018); Poliak et al. (2018) show that certain words in the\n",
      "hypothesis have high pointwise mutual information with class labels regardless of the premise, which could be artifacts of speciﬁc annotation strategies. For example, one can\n",
      "create a neutral sentence by adding a cause (“because”) to the premise and create a contradicting sentence by negating (“no”, “never”) the premise. As a result, the majority of\n",
      "training examples can be 134 solved without much reasoning about sentence meanings. Subsequently, McCoy et al. (2019) report that models rely on high word overlap to predict\n",
      "entailment; Glockner et al. (2018); Naik et al. (2018) demonstrate that models struggle at even lexicallevel inference involving antonyms, hypernyms, etc. A natural question to ask\n",
      "then is whether there exist better data collection procedures that guard against these biases. We argue that this is not easy because in practice, we almost always have different\n",
      "datagenerating processes during training (generated from selected corpora and annotators) and test (generated by end users). Then, can we remove biased features from training\n",
      "examples? This is also infeasible because sometimes they contain the necessary information for prediction, e.g., removing words may destroy the sentence meaning. It is not the\n",
      "features that are biased but their relation with the label. Next, we describe our approach to mitigating this biased relation. 3 Approach 3.1 Overview The key idea of our approach\n",
      "is to ﬁrst detect biased examples given prior knowledge on potential dataset bias, then focus on learning from unbiased, hard examples. We describe the two steps in details below.\n",
      "Detect biased examples. How do we know if an example exhibits biased features? Although we cannot directly measure label shift without accessing the test data, we know that NLI\n",
      "models are unlikely to work well given insufﬁcient features. When it does work well given only partial semantics of the input, the good performance is likely due to dataset bias.\n",
      "For example, Gururangan et al. (2018) exposes annotation artifacts by showing that hypothesisonly models have unexpected high accuracy. Similarly, we train a biased classiﬁer using\n",
      "insufﬁcient features I(x), e.g., the hypothesis sentence. We assume that examples predicted well by the biased classiﬁer exhibit dataset bias, i.e. p(y | I(x)) is high but q(y |\n",
      "I(x)) is low. Importantly, while I(x) approximates b(x) given our prior knowledge, it does not necessarily capture all dataset bias, which depends on the unknown test distribution.\n",
      "In addition, I(x) may include useful information. For example, although bagofwords (BOW) features are insufﬁcient to represent precise sentence meaning, it encodes a distribution\n",
      "of possible meanings. Thus good performance of a BOW classiﬁer is not fully due to ﬁtting dataset bias. In practice, as we will see in the experiments (Section 4.5), good choices\n",
      "of I(x) capture biased features precisely, resulting in signiﬁcant performance drop of the biased classi- ﬁer on Q. Learn residuals of the biased classiﬁer. Our intuition is that\n",
      "the debiased classiﬁer should capture information beyond those contained in the biased classiﬁer. If the biased classiﬁer already has a small loss on an example, then there is not\n",
      "much to learn beyond the biased features; otherwise, the debiased classiﬁer should correct predictions of the biased classiﬁer. We implement the idea through a residual ﬁtting\n",
      "procedure (DRiFt). Let fs: X! R and fd: X! R be the biased and the debiased classiﬁers, and let L be the loss function. First, we learn fs with insufﬁcient features I(x) as the\n",
      "input: ✓⇤= arg min ✓ EP [L(fs(I(x); ✓), y)]. (3) Let f⇤(x) be the optimal predictor that minimizes the empirical risk on P. We deﬁne f⇤(x) def = fs(I(x); ✓⇤) + fd(x; φ⇤). (4) Thus\n",
      "fd ﬁts the residual of fs with respect to the target f⇤. To estimate parameters φ of fd, we ﬁx parameters of fs and minimize the loss: min φ EP [L(fs(I(x); ✓⇤) + fd(x; φ), y)].\n",
      "(5) At test time, we only use the debiased classiﬁer fd. Consider the typical empirical risk minimization approach that estimates φ by minimizing EP [L(fd(x; φ), y)]. It is\n",
      "susceptible to relying on biased features when they predict well on the majority examples. In contrast, DRiFt ﬁrst learns fs which is intended to ﬁt potential bias in the data. It\n",
      "then learns fd that compensates fs without ﬁtting to the bias already captured by it. Next, we analyze the behavior of DRiFt using the crossentropy loss function, which is\n",
      "typically used for classiﬁcation problems. 3.2 Analysis with the CrossEntropy Loss In this section, we show that DRiFt adjusts the gradient on each example depending on how well it\n",
      "is predicted by the pretrained biased classiﬁer. 135 Given the crossentropy loss, our goal is to maximize the expected conditional loglikelihood of the data, EP [log p(y | x)]. A\n",
      "classiﬁer outputs a vector of scores for each of the K classes, f(x) = (f1(x),..., fK(x)) 2 RK, which are then mapped to a probability distribution p(y | x) by the softmax\n",
      "function. Given classiﬁers fs and fd, we have three choices of parametrization of the conditional probability p(y | x): ps(y | I(x)) / exp (fy s (I(x); ✓)) (6) pd(y | x) / exp! fy\n",
      "d (x; φ) \" (7) pa(y | x) / exp! fy s (I(x); ✓) + fy d (x; φ) \" / ps(y | I(x))pd(y | x). (8) To learn the classiﬁer fd, standard maximum likelihood estimation (MLE) uses pd(y | x),\n",
      "whereas DRiFt uses pa(y | x) given pretrained fs with ﬁxed parameters. Let us ﬁrst compare the two learning objectives. Denote ps(y | I(x); ✓⇤) by p⇤ s(y | I(x)). DRiFt maximizes\n",
      "JD(φ) = X (x,y)⇠D log pa(y | x; ✓⇤, φ) (9) = C + X (x,y)⇠D [log pd(y | x; φ)− log K X k=1 p⇤ s(k | I(x))pd(k | x; φ)], (10) where D denotes the training set and C = P (x,y)⇠D log\n",
      "p⇤ s(k | I(x)) is a constant. Compare (10) with the MLE objective: JMLE(φ) = X (x,y)⇠D log pd(y | x; φ). (11) We see that JD(φ) has an additional regularizer for each example x:\n",
      "R(x) def = −log K X k=1 p⇤ s(k | I(x))pd(k | x). (12) Geometrically, it encourages output from the debiased classiﬁer, pd, to have minimal projection on ps predicted by the biased\n",
      "classiﬁer. Next, let’s look at the effect of this regularizer through its gradient. Let Z(x) be the normalizer P k p⇤ s(k | I(x))pd(k | x). Then, we have rφR(x) = − P k p⇤ s(k |\n",
      "I(x))rφpd(k | x) P k p⇤s(k | I(x))pd(k | x) = − X k pa(k | x)rφ log pd(k | x), which is derived by writing rφpd as pdrφ log pd. Taking a negative step in the direction of rφ log\n",
      "pd(k | x) corresponds to downweighting the probability pd(k | x). Intuitively, the model tries to reweight the output distribution by the gradient weights pa(k | x). Note that pa(k\n",
      "| x) / p⇤ s(k | I(x))pd(k | x). (13) For an example (x, y), large values of p⇤ s(y | I(x)) indicate that I(x) is likely to contain biased features. If pd(y | x) is also large, the\n",
      "model is probably picking up the bias since pd has access to complete information in x including the biased features, in which case a relatively large negative step is taken to\n",
      "correct it. In the extreme case where the biased classiﬁer makes perfect prediction, we have p⇤ s(y | I(x))! 1 thus rφR(x)! −rφ log pd(y | x), canceling the MLE gradient rφ log\n",
      "pd(y | x). As a result, the gradient on this example is zero, and there is nothing to be learned. At the other end where I(x) does not provide any useful information, the biased\n",
      "classiﬁer outputs a uniform distribution p⇤ s(y | I(x)) = 1/K, thus pa(y | x) = pd(y | x) and the gradient on this example is reduced to the MLE gradient. 4 Experiments We ﬁrst\n",
      "evaluate our method using synthetic bias to show its effectiveness under different amount of dataset bias. We then test on two challenge datasets using different biased classiﬁers.\n",
      "We show that DRiFt consistently outperforms MLE on the challenge datasets given different NLI models, especially when the insufﬁcient features capture dataset bias exploited by the\n",
      "challenge data. 4.1 Training Data We evaluate DRiFt on two benchmarking NLI datasets: SNLI and MNLI. Each pair of premise and hypothesis sentences has a label from one of\n",
      "“entailment”, “contradiction”, or “neutral”. Sentences from SNLI are derived from image captions, whereas MNLI covers a broader range of styles and topics. Statistics of the two\n",
      "datasets are shown in Table 1. All MNLI results are on the matched development set.2 2 MNLI has two development sets, one from the same source as the training data (matched) and\n",
      "one from different sources (mismatched). We trained two sets of models using their corresponding development sets for model selection and obtained similar results. Thus we focus on\n",
      "the “matched” results. 136 Dataset Train Dev Test SNLI 549,367 9842 9842 MNLI 392,702 9815 - Table 1: Statistics of training datasets. The test sets of MNLI are hosted through\n",
      "Kaggle competitions. 4.2 Models and Training Details DRiFt is a general learning algorithm that works with any biased/debiased models. Below we describe the three key components of\n",
      "our approaches: the learning algorithm, the biased model with its insufﬁcient features, and the debiased model. Learning algorithms. We compare DRiFt with MLE, as well as a simpler\n",
      "variant of DRiFt: instead of the residual ﬁtting, we remove the examples predicted correctly by the biased classi- ﬁer and train on the rest. We call this baseline RM, which is\n",
      "also conceived by Gururangan et al. (2018). MLE only trains the debiased model. Both DRiFt and RM rely on an additional biased model that captures potential dataset bias. Biased\n",
      "models. We consider three insufﬁcient representations that exploit various NLI dataset biases reported in prior work. HYPO is a ﬁnetuned BERT classiﬁer that uses only the\n",
      "hypothesis sentence. CBOW is a continuous bagofwords classiﬁer. Similar to Mou et al. (2016), we represent both the premise and the hypothesis as the respective sums of their word\n",
      "embeddings. We then concatenate the premise and the hypothesis embeddings, their difference, and their elementwise product. The ﬁnal representation is passed through a onelayer\n",
      "fully connected network with ReLU activation. HAND is a classiﬁer using handcrafted features based on error analysis in Naik et al. (2018). Speciﬁcally, we include tokens in the\n",
      "hypothesis that are also in the premise, tokens unique to the hypothesis, Jaccard similarity between the two sentences, whether negation words (“not” and “n’t”) are included, and\n",
      "length difference computed by |Lp−Lh| Lp+Lh where Lp and Lh are numbers of tokens in the premise and the hypothesis. We represent the overlapping and the nonoverlapping tokens as\n",
      "the respective sums of their word embeddings. The embeddings are then concatenated with the dense features and passed through a onelayer fully connected network with ReLU\n",
      "activation. Debiased models. We choose three highperforming models of different capability. DA is the Decomposable Attention model introduced by Parikh et al. (2016), which relies\n",
      "on the interaction between words in the premise and the hypothesis. It does not use any word order information. We used the variant without intrasentence attention.3 ESIM is the\n",
      "Enhanced Sequential Inference Model. It ﬁrst encodes the premise and the hypothesis by a bidirectional LSTM, aligns the contextual word embeddings similar to Parikh et al. (2016),\n",
      "and uses another “inference” bidirectional LSTM to aggregate information. Thus it has access to the nonlocal context. BERT is the Bidirectional Encoder Representations from\n",
      "Transformers that recently improved performance on MNLI signiﬁ- cantly. It uses contextual embeddings pretrained from large corpora. Hyperparameters. For nonBERT models, word\n",
      "embeddings are initialized with the 840B.300d pretrained GloVe word vectors and ﬁnetuned during training. For DA and ESIM, hyperparameters of the model architecture are the same as\n",
      "those reported in the original papers. We ﬁnetune all BERT models from the pretrained BERTbaseuncased model.4 We train all models using the Adam (Kingma and Ba, 2014) optimizer\n",
      "with β1 = 0.9, β2 = 0.999, L2 weight decay of 0.01, learning rate warmup for the ﬁrst 10% of updates and linear decay afterwards. We use a dropout rate of 0.1 for all models except\n",
      "ESIM, which has a dropout rate of 0.5. BERT and nonBERT models are trained with a learning rate of 2e-5 and 1e-4, respectively. For MLE, we train BERT for 4 epochs and the rest for\n",
      "30 epochs. When training the debiased model in DRiFt, we ﬁnd that the models converge slowly thus we train BERT for 8 epochs and the rest for 80 epochs. 3 We removed the projection\n",
      "layers of the word embeddings as it speeds up training without hurting performance in our experiments. 4 bert/index.html 137 model 0.6 0.7 0.8 0.9 1.0 accuracy BERT DA ESIM 0.2 0.4\n",
      "0.6 0.8 cheating rate 0.2 0.4 0.6 0.8 cheating rate 0.2 0.4 0.6 0.8 cheating rate DRiFthypo MLE Rmcheat method Figure 2: Accuracy on SNLI test set augmented with cheating features,\n",
      "which leak the groundtruth labels on training data but not on test data. Models trained by MLE degrade signiﬁcantly when a majority of examples are cheatable, whereas debiased\n",
      "models trained by DRiFt maintain similar accuracies across different cheating rates. 4.3 InDistribution Performance We ﬁrst evaluate the models’ indistribution performance where\n",
      "they are trained and evaluated on splits from the same dataset. Results of the biased models are reported in Table 2. All exceeds the majorityclass baseline by a large margin,\n",
      "indicating that a majority of examples can be solved by superﬁcial cues. Results of the debiased models are reported in Table 3. Baseline results from our implementations are\n",
      "comparable to prior reported performance (row “MLE”). Debiased models trained by DRiFt show some degradation on indistribution data, especially for the less powerful DA and ESIM\n",
      "models. The accuracy drop is expected due to two reasons. First, DRiFt assumes distribution shift thus does not optimize performance on the training distribution P. Second, the\n",
      "effective training data size is reduced by negative gradients on potentially biased examples; this effect is exaggerated by RM, which shows signiﬁcant indistribution degradation.\n",
      "Similar tradeoff between indistribution accuracy and robustness on outofdistribution data has also been observed in adversarial training. Dataset majority HYPO CBOW HAND SNLI 34.2\n",
      "61.8 81.2 76.7 MNLI 35.4 52.5 66.1 65.4 Table 2: Accuracy of biased classiﬁers on SNLI test set and MNLI development set. All exceeds the majorityclass baseline by a large margin,\n",
      "signaling dataset bias. SNLI MNLI BERT DA ESIM BERT DA ESIM MLE 90.8 85.3 88.0 84.5 72.2 78.1 DRiFtHYPO 89.8 83.9 86.3 84.3 68.6 75.0 DRiFtCBOW 84.7 62.6 62.3 82.1 56.3 68.8\n",
      "DRiFtHAND 86.5 75.0 79.2 81.7 58.8 68.9 RMHYPO 71.2 67.0 70.3 65.5 57.5 63.0 RMCBOW 35.8 27.1 22.2 54.9 26.8 27.1 RMHAND 46.3 37.2 38.1 51.7 34.6 37.4 Table 3: Accuracy of models\n",
      "trained by MLE, DRiFt, and RM with different biased models. Training and test examples are from the same dataset. Intensity of the red highlights corresponds to absolute drop in\n",
      "accuracy with respect to the MLE baseline. RM signiﬁ- cantly hurts indistribution performance. DRiFt maintains reasonable performance. 4.4 Synthetic Bias In this section, we\n",
      "evaluate our model under controlled, synthetic dataset bias on SNLI. Recall our deﬁnition of dataset bias: the conditional distribution of the label y given biased features are\n",
      "different on training and test sets. Therefore, we inject bias into each example by adding a cheating feature that encodes its label. On training and development examples, the\n",
      "cheating feature encodes the ground truth label with probability pcheat (the cheating rate), and a random label otherwise. On test examples, the cheating feature always encodes a\n",
      "random label. Thus a model relying on the cheating feature would perform poorly on the test set. Speciﬁcally, we prepend the hypothesis with a string “{label} and” where label 2\n",
      "{entailment, contradiction, neutral}. To simulate 138 the fact that we often cannot pinpoint biased features until the model fails on some test examples, we choose HYPO as our\n",
      "biased classiﬁer. That is to say, we have a rough idea that the bias might be in the hypothesis but do not know what it is exactly. We train all three base models (DA, ESIM, and\n",
      "BERT) using MLE and DRiFt, respectively. Our results are shown in Figure 2. All MLE models are reasonably robust to a mild amount of bias. However, when a majority (pcheat > 0.6)\n",
      "of training examples contains the bias, their accuracy decreases signiﬁcantly: about 20% drop at pcheat = 0.9 compared to the baseline accuracy when no cheating features are\n",
      "injected. BERT is slightly more robust than DA and ESIM, possibly due to the regularization effect of pretrained embeddings. In contrast, our debiased models (DRiFtHYPO) maintain\n",
      "similar accuracies with increasing cheating rates and have a maximum accuracy drop of about 5%. Two questions remain, though: (1) Why does the accuracy of debiased models still\n",
      "drop a bit at high cheating rates? (2) Why is the baseline accuracy of DRiFt lower than MLE? We answer these questions by analyzing the upper bound performance of our method below.\n",
      "Bestcase scenario. In the ideal case, we know precisely what the bias is. Consider a biased classiﬁer that only uses the cheating feature as its input. It predicts biased examples\n",
      "perfectly, i.e. ps(y | b(x)) = 1 and ps(k | b(x)) = 0 8k 6= y, and predicts the rest unbiased examples uniformly at random. Based on our discussion at the end of Section 3.2, the\n",
      "biased examples have zero gradients and unbiased examples have the same gradients as in MLE. In this case, our method is equivalent to removing biased examples and training a\n",
      "classiﬁer on the rest, i.e. RMcheat. In Figure 2, we see that it completely dominates MLE. The accuracy of RMcheat still drops when pcheat is large, because there are fewer “good”\n",
      "examples to learn from, not due to ﬁtting the bias. Similarly, DRiFtHYPO has lower overall accuracy compared to RMcheat, because HYPO captures additional (unbiased) features that\n",
      "cannot be fully learned by the debiased model. Worstcase scenario. In the extreme case when pcheat = 1, all models’ predictions on the test set are random guesses. For MLE, the\n",
      "biased features are no longer differentiable from the generalizable ones, thus there is no reason not to use them. For DRiFt, since the biased model achieves perfect prediction on\n",
      "all training examples, the debiased model receives zero gradient. Therefore, when strong bias presents on all examples, we need more information to correct the bias, e.g.,\n",
      "collecting additional data or augmenting examples. method lexical subseq const E ¬E E ¬E E ¬E HYPO 52.6 44.4 54.5 44.3 45.6 16.7 CBOW 63.2 16.0 66.2 33.7 63.2 38.5 HAND 66.7 0.0\n",
      "66.7 0.0 66.7 0.0 model: BERT MLE 67.2 7.8 66.7 0.4 68.1 11.9 DRiFtHYPO 84.7 79.8 69.0 23.7 72.7 40.8 DRiFtCBOW 80.8 75.2 68.5 29.5 71.5 40.3 DRiFtHAND 77.4 70.9 71.2 41.2 75.8\n",
      "61.0 RMHYPO 67.2 46.0 65.2 36.6 75.5 72.2 RMCBOW 5.4 66.4 8.5 64.2 34.8 65.3 RMHAND 10.0 66.0 4.7 66.3 9.1 67.3 model: DA MLE 66.6 0.5 66.6 0.3 66.5 0.4 DRiFtHYPO 66.3 1.7 66.9 5.5\n",
      "66.3 8.4 DRiFtCBOW 65.3 7.2 66.1 9.6 65.1 9.1 DRiFtHAND 60.5 27.1 61.4 44.9 55.9 48.3 RMHYPO 65.1 9.6 66.2 15.0 66.2 18.8 RMCBOW 0.4 66.6 1.3 66.7 0.8 66.5 RMHAND 10.3 65.8 8.9\n",
      "65.7 13.9 64.7 model: ESIM MLE 65.8 3.2 67.2 4.6 65.5 2.8 DRiFtHYPO 64.3 10.5 68.3 16.3 68.1 29.3 DRiFtCBOW 63.2 14.4 66.8 20.1 64.9 22.7 DRiFtHAND 61.2 19.6 63.7 39.4 64.8 48.3\n",
      "RMHYPO 63.3 12.8 64.1 24.8 71.3 46.0 RMCBOW 4.5 65.7 6.0 65.2 16.9 63.8 RMHAND 25.8 60.8 18.3 67.3 13.1 65.9 Table 4: F1 scores of the entailment (E) and nonentailment (¬E) classes\n",
      "on HANS. All models are trained on MNLI and results are shown on three subsets targeting at different biases: lexical overlap (lexical), subsequence overlap (subseq), and\n",
      "constituent overlap (const). Intensity of the Blue and red highlights corresponds to absolute increase and decrease of scores with respect to MLE. DRiFt signiﬁcantly improves\n",
      "results on challenging ¬E examples without hurting performance on E, whereas RM improves scores on ¬E at the cost of performance on E. 4.5 Word Overlap Bias We evaluate our method\n",
      "on word overlap bias in NLI. McCoy et al. (2019) show that models 139 trained on MNLI largely rely on word overlap between the premise and the hypothesis to make entailment\n",
      "predictions. They created a challenge dataset (HANS) where premises may not entail high wordoverlapping hypotheses. Speciﬁcally, a model biased by word overlap would fail on three\n",
      "types of nonentailment examples: (1) Lexical overlap, e.g., “The doctor visited the lawyer.” ; “The lawyer visited the doctor.”. (2) Subsequence, e.g., “The senator near the lawyer\n",
      "danced.” ; “The lawyer danced.”. (3) Constituent, e.g., “The lawyers resigned, or the artist slept.” ; “The artist slept.”. We evaluate both biased and debiased models on the three\n",
      "subsets of HANS and show F1 scores for each class in Table 4. As expected, models trained by MLE almost always predict entailment (E), and thus performs poorly for the\n",
      "nonentailment class (¬E). DRiFt improves performance on ¬E in all cases with little degradation on E. In contrast, RM improves performance on ¬E at the cost of signiﬁcant\n",
      "degradation on E. Among all biased models, HAND produces the best debiasing results because it is designed to ﬁt the word overlap bias, and indeed has zero recall on ¬E when tested\n",
      "on HANS. On the contrary, the improvement from HYPO is lower because it does not capture any word overlap bias. Correspondingly, its performance drop on HANS is minimal compared to\n",
      "its indistribution performance. Among all debiased models, BERT has the best overall performance. We hypothesize that pretraining on large data improves model robustness in\n",
      "addition to the debiasing effect from DRiFt. 4.6 Stress Tests In addition to the word overlap bias exploited by HANS, there are other known biases such as negation words and\n",
      "sentence lengths. Naik et al. (2018) conduct a detailed error anlaysis on MNLI and create six stress test sets (STRESS) targeting at each type of error. We focus on the word\n",
      "overlap and negation stress test sets, which expose dataset bias as opposed to model weakness according to Liu et al. (2019). A model biased by word overlap rate and negation words\n",
      "are expected to have low accuracy on the entailment class on challenge data. The complete results are shown in Appendix A. In Table 5, we show the F1 scores of each class for all\n",
      "models on STRESS.5 Compared to results 5 Since results of RM are similar to those in Table 4, we method Negation Overlap E C N E C N HYPO 41.2 52.4 50.5 44.2 52.8 51.7 CBOW 20.1\n",
      "48.2 53.9 49.7 52.9 55.6 HAND 37.5 45.0 57.3 56.7 50.1 57.8 model: BERT MLE 2.4 81.1 56.5 19.2 83.3 59.4 DRiFtHYPO 7.3 80.7 55.6 27.5 81.1 59.1 DRiFtCBOW 17.9 81.7 55.5 18.3 80.0\n",
      "56.6 DRiFtHAND 4.3 80.6 55.5 15.0 81.9 57.4 model: DA MLE 17.4 47.3 55.3 46.7 60.5 57.8 DRiFtHYPO 11.8 47.0 51.8 41.6 59.4 55.6 DRiFtCBOW 28.4 21.4 39.5 35.2 41.7 43.8 DRiFtHAND\n",
      "24.7 42.0 46.4 42.2 56.0 49.9 model: ESIM MLE 12.0 72.7 54.6 27.6 76.4 57.5 DRiFtHYPO 22.8 67.7 54.0 37.5 73.2 56.7 DRiFtCBOW 32.7 62.3 46.9 30.4 65.6 49.8 DRiFtHAND 15.8 64.6 51.8\n",
      "39.2 70.7 53.9 Table 5: F1 scores of each class on STRESS. Intensity of the Blue and red highlights corresponds to absolute increase and decrease of scores with respect to MLE.\n",
      "DRiFt improves results on E (that exhibits label shift) with some degradation on other classes for DA and ESIM. on HANS, STRESS sees lower overall improvement from debiasing. One\n",
      "reason is that STRESS decreases word overlap rate and injects negation words by appending distractor phrases, i.e. “true is true” and “false is not true”. While this introduces\n",
      "label shift on biased features, it also introduces covariate shift on the input. For example, although HAND contains features designed to use word overlap rate (Jaccard similarity)\n",
      "and negation words, its does not have big performance drop on the challenge data compared to its indistribution performance, showing that that distractor phrases may affect the\n",
      "model in other ways. While all debiased models show improvement on E, both DA and ESIM suffer from degradation on the other two classes, especially when trained by DRiFtCBOW. We\n",
      "posit two reasons. First, while CBOW is insufﬁcient to represent complete sentence meaning, it does encode a distribution of possible meanings. Thus models debiased by DRiFtCBOW\n",
      "might discard useful information. Second, model capacity limits what is learned beyond a BOW representation. DA shows the most put them in Appendix A. 140 degradation since it only\n",
      "uses local word interaction, thus is essentially a BOW model. In contrast, BERT has little degradation on indistribution examples regardless of the biased classiﬁer. 5 Related Work\n",
      "and Discussion Adversarial data collection. Aside from NLI, dataset bias has been exposed on benchmarks for other NLP tasks as well, e.g., paraphrase identiﬁcation, story close\n",
      "test, reading comprehension (Kaushik and Lipton, 2018), coreference resolution, and visual question answering. Most bias is resulted from artifacts in the data selection procedure\n",
      "and shortcuts taken by crowd workers. To systematically minimize bias during data collection, adversarial ﬁltering methods have been proposed to discard examples predicted well by\n",
      "a simple classiﬁer. This is similar to the RM baseline, except that we apply “ﬁltering” at training time. In general, our debiasing methods are complementary to adversarial data\n",
      "collection methods. Debiased representation. Our work is closely related to the line of work on removing bias in data representations. Bolukbasi et al. (2016); Zhao et al. (2018b)\n",
      "learn genderneutral word embeddings by forcing certain dimensions to be free of gender information. Similarly, Wang et al. (2019a) construct a biased classiﬁer and project its\n",
      "representation out of the model’s representation. For NLI, Belinkov et al. (2019) use adversarial learning to remove hypothesisrelated bias in the sentence representations.\n",
      "However, for some NLP applications it may not be easy to separate biased features from useful semantic representations, thus we correct the conditional distribution of the class\n",
      "label given these biased features instead of removing them from the input. Concurrently, Clark et al. (2019) take the same approach and further show its effectiveness on additional\n",
      "tasks including reading comparehension and visual question answering. Distribution shift. Covariate shift and label shift are two wellstudied settings under distribution shift,\n",
      "which makes different assumptions on how p(x, y) changes. However, most works in these settings assume access to unlabeled data from the target distribution. Our objective is more\n",
      "related to distributionally robust optimization, which does not assume access to target data and optimizes the worstcase performance under unknown, bounded distribution shift. In\n",
      "contrast, we leverage prior knowledge on potential dataset bias. Data augmentation. An effective way to tackle the challenge datasets is to train or ﬁnetune on similar examples,\n",
      "which explicitly correct the training data distribution. However, constructing challenge examples often rely on handcrafted rules that target a speciﬁc type of bias, e.g., swapping\n",
      "male and female entities, synonym/antonym substitution, and syntactic rules, and may require human veriﬁcation. Data augmentation provides a way to encode our prior knowledge on\n",
      "the task, e.g., swapping genders does not affect coreference resolution result, and syntactic transformations may affect sentence meanings. Therefore, a related direction is to\n",
      "develop generic augmentation techniques with linguistic priors. 6 Conclusion Across all different dataset biases, the fundamental problem is that the majority training examples are\n",
      "not representative of the realworld data distribution (including the challenge data), thus minimizing the average training loss no longer accurately describes our objective. In\n",
      "this paper, we tackle the problem by adapting the learning objective to focus on examples that cannot be easily solved by biased features. We show that our debiasing method\n",
      "improves model performance on challenge data given known dataset bias. However, current improvements largely rely on taskspeciﬁc prior knowledge, thus an important next step is to\n",
      "develop more general methods that tackle different types of biases. Acknowledgments Yanchao Ni worked on an earlier version of this project while he was at New York University. We\n",
      "thank the GluonNLP community for their support on reproducing prior results. 141assistant\n",
      "Here is a summary of the text in ten sentences:\n",
      "\n",
      "The paper discusses the problem of dataset bias in natural language inference (NLI) models, which can lead to poor performance on challenging datasets. Dataset bias occurs when the model is trained on a dataset that is not representative of the real-world data distribution. The authors propose a debiasing algorithm called DRiFt, which learns a debiased model by first learning a biased model on a training dataset and then removing the biased features from the debiased model. The debiased model is then used to predict the test data. The authors analyze the behavior of DRiFt using the cross-entropy loss function and show that it adjusts the gradient on each example depending on how well it is predicted by the biased model. The authors also evaluate DRiFt on two benchmark datasets, SNLI and MNLI, and show that it outperforms a baseline model on the challenge datasets. They also evaluate the method on a synthetic dataset bias and find that it improves performance on challenging examples. The authors also evaluate the method on a stress test set and find that it improves performance on non-entailment examples but degrades performance on entailment examples. The authors propose a related method called Negation Overlap E, which improves performance on entailment examples but degrades performance on non-entailment examples. The authors also discuss the challenges of dealing with word overlap bias and negation bias, and propose a method to correct for these biases. The authors conclude that their debiasing method is a promising approach to addressing dataset bias in NLI models.\n"
     ]
    }
   ],
   "source": [
    "full_text = generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Full text:\", full_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f372239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Size([327]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output_token_ids), output_token_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d38ff5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is a summary of the text in ten sentences:\\n\\nThe paper discusses the problem of dataset bias in natural language inference (NLI) models, which can lead to poor performance on challenging datasets. Dataset bias occurs when the model is trained on a dataset that is not representative of the real-world data distribution. The authors propose a debiasing algorithm called DRiFt, which learns a debiased model by first learning a biased model on a training dataset and then removing the biased features from the debiased model. The debiased model is then used to predict the test data. The authors analyze the behavior of DRiFt using the cross-entropy loss function and show that it adjusts the gradient on each example depending on how well it is predicted by the biased model. The authors also evaluate DRiFt on two benchmark datasets, SNLI and MNLI, and show that it outperforms a baseline model on the challenge datasets. They also evaluate the method on a synthetic dataset bias and find that it improves performance on challenging examples. The authors also evaluate the method on a stress test set and find that it improves performance on non-entailment examples but degrades performance on entailment examples. The authors propose a related method called Negation Overlap E, which improves performance on entailment examples but degrades performance on non-entailment examples. The authors also discuss the challenges of dealing with word overlap bias and negation bias, and propose a method to correct for these biases. The authors conclude that their debiasing method is a promising approach to addressing dataset bias in NLI models.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9417945b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of the text in ten sentences:\n",
      "\n",
      "The paper discusses the problem of dataset bias in natural language inference (NLI) models, which can lead to poor performance on challenging datasets. Dataset bias occurs when the model is trained on a dataset that is not representative of the real-world data distribution. The authors propose a debiasing algorithm called DRiFt, which learns a debiased model by first learning a biased model on a training dataset and then removing the biased features from the debiased model. The debiased model is then used to predict the test data. The authors analyze the behavior of DRiFt using the cross-entropy loss function and show that it adjusts the gradient on each example depending on how well it is predicted by the biased model. The authors also evaluate DRiFt on two benchmark datasets, SNLI and MNLI, and show that it outperforms a baseline model on the challenge datasets. They also evaluate the method on a synthetic dataset bias and find that it improves performance on challenging examples. The authors also evaluate the method on a stress test set and find that it improves performance on non-entailment examples but degrades performance on entailment examples. The authors propose a related method called Negation Overlap E, which improves performance on entailment examples but degrades performance on non-entailment examples. The authors also discuss the challenges of dealing with word overlap bias and negation bias, and propose a method to correct for these biases. The authors conclude that their debiasing method is a promising approach to addressing dataset bias in NLI models.\n"
     ]
    }
   ],
   "source": [
    "print(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd308c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

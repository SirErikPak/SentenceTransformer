{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "981a72ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AdamW' from 'transformers' (/opt/homebrew/Caskroom/miniconda/base/envs/LLM/lib/python3.13/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel, AdamW\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Load model and tokenizer\u001b[39;00m\n\u001b[32m      6\u001b[39m MODEL_PATH =\u001b[33m\"\u001b[39m\u001b[33m/Users/sir/Downloads/HuggingFace/sentence_transformer/intfloat_e5-large-v2\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'AdamW' from 'transformers' (/opt/homebrew/Caskroom/miniconda/base/envs/LLM/lib/python3.13/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "\n",
    "# Load model and tokenizer\n",
    "MODEL_PATH =\"/Users/sir/Downloads/HuggingFace/sentence_transformer/intfloat_e5-large-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModel.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Example batch of paired inputs: (query, positive passage)\n",
    "texts = [\n",
    "    (\"how much protein should a female eat\", \"The CDC average protein requirement for women is 46 grams per day.\"),\n",
    "    (\"define summit\", \"Definition of summit is the highest point of a mountain.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda76ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize batch\n",
    "queries, positives = zip(*texts)\n",
    "query_inputs = tokenizer(list(queries), return_tensors='pt', padding=True, truncation=True)\n",
    "positive_inputs = tokenizer(list(positives), return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2eec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass to get embeddings\n",
    "query_embeds = model(**query_inputs).last_hidden_state.mean(dim=1)\n",
    "positive_embeds = model(**positive_inputs).last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Normalize embeddings\n",
    "query_embeds = F.normalize(query_embeds, p=2, dim=1)\n",
    "positive_embeds = F.normalize(positive_embeds, p=2, dim=1)\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "similarity_matrix = torch.matmul(query_embeds, positive_embeds.t())\n",
    "\n",
    "# Create labels (diagonal matches)\n",
    "labels = torch.arange(len(query_embeds)).to(similarity_matrix.device)\n",
    "\n",
    "# Temperature parameter to scale similarities\n",
    "temperature = 0.05\n",
    "logits = similarity_matrix / temperature\n",
    "\n",
    "# Contrastive loss with cross-entropy\n",
    "loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "# Backprop and optimization\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Contrastive loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

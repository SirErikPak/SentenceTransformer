{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47134223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loading model...\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Define model path\n",
    "MODEL_PATH =\"/Users/sir/Downloads/HuggingFace/sentence_transformer/intfloat_e5-large-v2\"\n",
    "\n",
    "# use mps if available, else cuda, else cpu\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(\"Loading model...\")\n",
    "model = AutoModel.from_pretrained(MODEL_PATH).to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ffd1ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
      "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faccacc9",
   "metadata": {},
   "source": [
    "#### Training\n",
    "Training using Matryoshka Representation Learning (MRL) is quite elementary: rather than applying some loss function on only the full-size embeddings, we also apply that same loss function on truncated portions of the embeddings. For example, if a model has an embedding dimension of 768 by default, it can now be trained on 768, 512, 256, 128, 64 and 32. Each of these losses will be added together, optionally with some weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2340d4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.losses import CoSENTLoss, MatryoshkaLoss\n",
    "\n",
    "\n",
    "model = SentenceTransformer(MODEL_PATH).to(DEVICE)\n",
    "\n",
    "base_loss = CoSENTLoss(model=model)\n",
    "loss = MatryoshkaLoss(model=model, loss=base_loss, matryoshka_dims=[1024, 768, 512, 256, 128, 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6303ad4",
   "metadata": {},
   "source": [
    "Additionally, this can be combined with the `AdaptiveLayerLoss` such that the resulting model can be reduced both in the size of the output dimensions, but also in the number of layers for faster inference. See also the [Adaptive Layers](https://sbert.net/examples/sentence_transformer/training/adaptive_layer/README.html) for more information on reducing the number of model layers. In Sentence Transformers, the combination of these two losses is called `Matryoshka2dLoss`, and a shorthand is provided for simpler training.\n",
    "\n",
    "Reference: [Matryoshka2dLoss](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#matryoshka2dloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d01bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae9061c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7c691cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 1. Matryoshka Text Encoder\n",
    "# ----------------------------\n",
    "class MatryoshkaTextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=MODEL_PATH, dims=[128, 256, 384, 512]):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.projection = nn.Linear(self.encoder.config.hidden_size, max(dims))\n",
    "        self.dims = dims\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
    "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "        cls_emb = out.last_hidden_state[:, 0]             # [CLS] token\n",
    "        z = self.projection(cls_emb)                      # [batch, max_dim]\n",
    "        z = F.normalize(z, dim=-1)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f08a737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matryoshka_text_loss(z1, z2, dims=[128, 256, 384, 512], temperature=0.05):\n",
    "    total_loss = 0\n",
    "    for m in dims:\n",
    "        z1_m = F.normalize(z1[:, :m], dim=-1)\n",
    "        z2_m = F.normalize(z2[:, :m], dim=-1)\n",
    "        logits = (z1_m @ z2_m.T) / temperature\n",
    "        labels = torch.arange(z1.size(0), device=z1.device)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        total_loss += loss\n",
    "    return total_loss / len(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98c17006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.058626338839530945\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = MatryoshkaTextEncoder().to(DEVICE)\n",
    "\n",
    "texts = [\n",
    "    \"The cat sits on the mat.\",\n",
    "    \"A small feline rests on a rug.\"\n",
    "]\n",
    "\n",
    "enc = tokenizer(texts, padding=True, truncation=True, return_tensors='pt').to(DEVICE)\n",
    "\n",
    "z1 = model(**enc)\n",
    "z2 = model(**enc)\n",
    "\n",
    "loss = matryoshka_text_loss(z1, z2)\n",
    "loss.backward()\n",
    "print(\"Training loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c29ceb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity (128D): 0.8485\n",
      "Cosine similarity (256D): 0.8547\n",
      "Cosine similarity (384D): 0.8628\n",
      "Cosine similarity (512D): 0.8701\n"
     ]
    }
   ],
   "source": [
    "dims = [128, 256, 384, 512]\n",
    "\n",
    "for m in dims:\n",
    "    sim = F.cosine_similarity(z1[0, :m].unsqueeze(0), z2[1, :m].unsqueeze(0)).item()\n",
    "    print(f\"Cosine similarity ({m}D): {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554229b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
